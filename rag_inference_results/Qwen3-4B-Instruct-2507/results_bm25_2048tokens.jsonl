{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1989, "num_retrieved": 3, "inference_time": 0.7144383490085602, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    from neo4j.time import Time\n    from datetime import timezone, timedelta\n    \n    # Convert nanoseconds to seconds\n    seconds = nanoseconds / 1_000_000_000\n    \n    # Create a time object from the seconds since midnight\n    # Using the fact that Time represents a time of day without timezone\n    if tz is None:\n        return Time.from_clock_time(seconds, UnixEpoch)\n    else:\n        # If timezone is provided, create a datetime with the timezone and extract time\n        from datetime import datetime\n        dt = datetime.fromtimestamp(0, tz=timezone.utc) + timedelta(seconds=seconds)\n        return Time.from_clock_time(dt.timetz(), UnixEpoch)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 to 10^10)\n    :param tz: optional timezone information\n    :return: Time\n    \"\"\"\n    if tz is None:\n        return Time.from_nanoseconds(nanoseconds)\n    else:\n        # Convert nanoseconds to a time with timezone\n        time_without_tz = Time.from_nanoseconds(nanoseconds)\n        return time_without_tz.with_tzinfo(tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    from neo4j.time import Time\n    from datetime import timezone, timedelta\n    \n    # Convert nanoseconds to seconds\n    seconds = nanoseconds / 1_000_000_000\n    \n    # Create a time object from the seconds since midnight\n    # Using the fact that Time represents a time of day without timezone\n    if tz is None:\n        return Time.from_clock_time(seconds, UnixEpoch)\n    else:\n        # Create a datetime with the time and timezone\n        dt = datetime.datetime(1, 1, 1, 0, 0, 0) + timedelta(seconds=seconds)\n        dt = dt.replace(tzinfo=timezone.utc).astimezone(tz)\n        return Time.from_clock_time(dt.timetz(), UnixEpoch)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    from neo4j.time import Time\n    from datetime import timezone, timedelta\n    \n    # Convert nanoseconds to seconds\n    seconds = nanoseconds / 1_000_000_000\n    \n    # Create a time object from the seconds since midnight\n    # Using the fact that Time is typically represented as a time of day without timezone\n    time_obj = Time.from_clock_time(seconds, UnixEpoch)\n    \n    # If timezone is provided, adjust accordingly\n    if tz is not None:\n        # Convert to timezone-aware time\n        # Note: Time objects typically represent local time, so we need to handle timezone appropriately\n        # This implementation assumes that the time is being hydrated with timezone info\n        # The actual implementation may depend on the internal representation\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    from neo4j.time import Time\n    from datetime import timezone, timedelta\n    \n    # Convert nanoseconds to seconds\n    seconds = nanoseconds / 1_000_000_000\n    \n    # Create a time object from the seconds since midnight\n    # Using the fact that Time represents a time of day without timezone\n    if tz is not None:\n        # If timezone is provided, create a datetime with timezone and extract time\n        dt = datetime.datetime(1, 1, 1, 0, 0, 0) + timedelta(seconds=seconds)\n        dt = dt.replace(tzinfo=tz)\n        return Time.from_clock_time(dt.timetz(), UnixEpoch)\n    else:\n        # Otherwise, just create a time from the seconds since midnight\n        return Time.from_clock_time(Clock().local_time(), UnixEpoch)"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1987, "num_retrieved": 4, "inference_time": 0.7144383490085602, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: The timedelta value to dehydrate.\n    :type value: timedelta\n    :return: The dehydrated representation of the timedelta value.\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: The timedelta value to dehydrate.\n    :type value: timedelta\n    :return: The dehydrated representation of the timedelta.\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: The timedelta value to dehydrate.\n    :type value: timedelta\n    :return: The dehydrated representation of the timedelta value.\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1988, "num_retrieved": 4, "inference_time": 0.7144383490085602, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", *value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2021, "num_retrieved": 4, "inference_time": 0.7144383490085602, "generate_results": ["def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (CartesianPoint, WGS84Point)):\n        # Convert point to a tuple of coordinates\n        return (float(value.x), float(value.y))\n    return None", "def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (CartesianPoint, WGS84Point)):\n        # For both CartesianPoint and WGS84Point, we return the coordinates as a tuple\n        return (float(value.x), float(value.y))\n    return (float(value[0]), float(value[1]))", "def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (CartesianPoint, WGS84Point)):\n        # For both CartesianPoint and WGS84Point, we return the coordinates as a tuple\n        return (float(value.x), float(value.y))\n    return (float(value[0]), float(value[1]))", "def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (CartesianPoint, WGS84Point)):\n        # For both CartesianPoint and WGS84Point, we return the coordinates as a tuple\n        return (float(value.x), float(value.y))\n    return (float(value[0]), float(value[1]))", "def dehydrate_point(value):\n    \"\"\"\n    Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:\n    \"\"\"\n    if isinstance(value, (CartesianPoint, WGS84Point)):\n        # For both CartesianPoint and WGS84Point, we return the coordinates as a tuple\n        return (float(value.x), float(value.y))\n    return (float(value[0]), float(value[1]))"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2064, "num_retrieved": 6, "inference_time": 0.7144383490085602, "generate_results": ["def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return self.keys", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return [record.keys() for record in self]", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return [record.keys() for record in self]", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return [record.keys() for record in self]", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return [record.keys() for record in self]"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2033, "num_retrieved": 5, "inference_time": 0.7144383490085602, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler (supported by Neo4j 4.2 - 4.4)\n    if protocol_version is None or protocol_version == (4, 2):\n        handlers[(4, 2)] = Bolt4x2\n    \n    # Add Bolt 4.1 handler (if available, though not explicitly defined in context)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n    \n    # Add Bolt 3.x handler (if available, though not explicitly defined)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n    \n    # If protocol_version is specified, filter the dictionary\n    if protocol_version is not None:\n        handlers = {k: v for k, v in handlers.items() if k == protocol_version}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler (supported by Neo4j 4.2 - 4.4)\n    if protocol_version is None or protocol_version == (4, 2):\n        handlers[(4, 2)] = Bolt4x2\n\n    # Add Bolt 4.1 handler (if available, though not explicitly defined in context)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n    # If other versions were defined, they would be added here\n\n    # Add Bolt 3.0 handler (if available, though not explicitly defined in context)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n\n    # If protocol_version is specified, filter the dictionary\n    if protocol_version is not None:\n        handlers = {k: v for k, v in handlers.items() if k == protocol_version}\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler (supported by Neo4j 4.2 - 4.4)\n    if protocol_version is None or protocol_version == (4, 2):\n        handlers[(4, 2)] = Bolt4x2\n    \n    # Add Bolt 4.1 handler (if available, though not explicitly defined in context)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n    \n    # Add Bolt 3.0 handler (if available, though not explicitly defined in context)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n    \n    # If a specific version is requested, filter the dictionary\n    if protocol_version is not None:\n        handlers = {k: v for k, v in handlers.items() if k == protocol_version}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler (supported by Neo4j 4.2 - 4.4)\n    if protocol_version is None or protocol_version == (4, 2):\n        handlers[(4, 2)] = Bolt4x2\n\n    # Add Bolt 4.1 handler (if available, though not explicitly defined in context)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n    # If other versions were defined, they would be added here\n\n    # Add Bolt 3.x handler (supported by Neo4j 3.5 and earlier)\n    # Note: The context shows Bolt3 class with route method, implying support\n    if protocol_version is None or protocol_version == (3, 0):\n        handlers[(3, 0)] = Bolt4x1  # Placeholder - actual class not defined in context\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler (supported by Neo4j 4.2 - 4.4)\n    if protocol_version is None or protocol_version == (4, 2):\n        handlers[(4, 2)] = Bolt4x2\n\n    # Add Bolt 4.1 handler (if available, though not explicitly defined in context)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n    # If other versions were defined, they would be added here\n\n    # Add Bolt 3.0 handler (if available, though not explicitly defined in context)\n    # Note: Based on context, only Bolt 4.2 is explicitly defined\n\n    return handlers"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1947, "num_retrieved": 2, "inference_time": 0.7144383490085602, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    def decorator(func):\n        def wrapper(tx):\n            # Create a new transaction with the provided metadata and timeout\n            # This is a simplified implementation - in reality, this would\n            # wrap the transaction function and pass metadata and timeout\n            # to the underlying transaction system\n            return func(tx)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    def decorator(func):\n        def wrapper(tx):\n            # Create a new transaction with the provided metadata and timeout\n            # This is a simplified implementation - in reality, this would wrap the transaction\n            # and apply the metadata and timeout settings\n            if metadata is not None:\n                tx.metadata = metadata\n            if timeout is not None:\n                tx.timeout = timeout\n            return func(tx)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    def decorator(func):\n        def wrapper(tx):\n            # Create a new transaction with the provided metadata and timeout\n            # This is a simplified implementation - in practice, this would\n            # wrap the transaction execution with the specified parameters\n            return func(tx)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    def decorator(func):\n        def wrapper(tx):\n            # Create a new transaction with the provided metadata and timeout\n            # This is a simplified implementation - in reality, this would wrap the transaction\n            # and apply the metadata and timeout settings\n            if metadata is not None:\n                tx.metadata = metadata\n            if timeout is not None:\n                tx.timeout = timeout\n            return func(tx)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    def decorator(func):\n        def wrapper(tx):\n            # Create a new transaction with the provided metadata and timeout\n            # This is a simplified implementation - in reality, this would\n            # wrap the transaction function and pass metadata and timeout\n            # to the underlying transaction system\n            return func(tx)\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2104, "num_retrieved": 7, "inference_time": 0.7144383490085602, "generate_results": ["def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    # This function is not directly defined in the provided context\n    # Based on the context, it appears to be missing or not implemented\n    # The context shows methods like value, values, data, etc., but not index\n    # Since no implementation is available in the context, we cannot complete this function\n    # with confidence based on the provided information\n    raise NotImplementedError(\"index method not implemented in context\")", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    # This function is not directly defined in the provided context\n    # Based on the context, we can infer that it might be related to finding\n    # the index of a field in a record, but no explicit implementation exists\n    # Therefore, we implement a basic version that would work in context of record fields\n    try:\n        # Assuming key is a field name or index, and we're looking for its position\n        # in the record structure\n        return next(i for i, field in enumerate(self._fields) if field == key)\n    except (StopIteration, AttributeError):\n        # If no field found or no fields defined, return -1\n        return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    # This function is not directly defined in the provided context\n    # Based on the context, it appears to be related to result processing\n    # However, no explicit implementation for 'index' is available\n    # The closest related functionality is in the 'value' and 'values' methods\n    # which retrieve values by index or key\n    \n    # Since no implementation is provided in the context, \n    # and given the lack of explicit information about how to compute the index,\n    # we cannot accurately implement this function based on the retrieved context.\n    \n    # Placeholder implementation - this would need to be adapted to actual use case\n    # In a real scenario, this might involve iterating through records to find the key\n    # But without specific data structure or context, this is speculative\n    \n    # Return -1 as a default if key not found (not in context)\n    return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    # This function is not directly defined in the provided context\n    # Based on the context, it seems to be related to result processing\n    # However, no explicit implementation for 'index' is available\n    # The closest related functionality is in the 'value' and 'values' methods\n    # which retrieve values by index or key\n    \n    # Since no implementation is provided in the context, and given the\n    # lack of explicit information about how to compute the index of a key,\n    # we cannot reliably implement this function without additional context.\n    \n    # Placeholder implementation - this would need to be adapted based on actual\n    # data structure and requirements\n    raise NotImplementedError(\"Index function not implemented in context\")", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    # This function is not directly defined in the provided context\n    # Based on the context, it appears to be related to result processing\n    # However, no explicit implementation for 'index' is available\n    # The closest related functionality is in the 'value' method which uses index\n    # Since no specific implementation is provided, we cannot accurately implement this function\n    # Therefore, we return a placeholder that would be appropriate in a typical context\n    raise NotImplementedError(\"index method not implemented in provided context\")"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2037, "num_retrieved": 5, "inference_time": 0.7144383490085602, "generate_results": ["def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2067, "num_retrieved": 6, "inference_time": 0.7144383490085602, "generate_results": ["def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    return {key: record.value(key) if hasattr(record, 'value') else None for key in keys}", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    return {key: record.value(key) if hasattr(record, 'value') else None for key in keys}", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    return {key: record.value(key) if hasattr(record, 'value') else None for key in keys}", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    return {key: record.value(key) if hasattr(record, 'value') else None for key in keys}", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    return {key: record.value(key) if hasattr(record, 'value') else None for key in keys}"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2077, "num_retrieved": 8, "inference_time": 0.7144383490085602, "generate_results": ["def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    value = self.data[-2:]  # Get the last two bytes\n    self.data = self.data[:-2]  # Remove the last two bytes\n    return int.from_bytes(value, byteorder='big')", "def pop_u16(self):\n    \"\"\" Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    value = self.data[-2] << 8 | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    value = self.data[-2:]  # Get the last two bytes\n    self.data = self.data[:-2]  # Remove the last two bytes\n    return int.from_bytes(value, byteorder='big')", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    value = self.data[-2:]  # Get the last two bytes\n    self.data = self.data[:-2]  # Remove the last two bytes\n    return int.from_bytes(value, byteorder='big')", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    value = self.data[-2:]  # Get the last two bytes\n    self.data = self.data[:-2]  # Remove the last two bytes\n    return int.from_bytes(value, byteorder='big')"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2099, "num_retrieved": 7, "inference_time": 0.7144383490085602, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending\n    # This is a placeholder - actual implementation would depend on the internal\n    # message queue and protocol handling\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending to output queue\n    # This is a placeholder - actual implementation would depend on the internal\n    # messaging system of the Neo4j driver\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending to output queue\n    # Based on pattern of other message methods (pull, commit, rollback)\n    # This would typically involve constructing a DISCARD message with the given parameters\n    # and adding it to the output queue\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation follows the pattern of other message methods in the driver\n    # Appends a DISCARD message to the output queue with specified parameters\n    # The actual implementation would depend on the internal message queue logic\n    # of the Neo4j driver, but based on the pattern, this method would enqueue\n    # a DISCARD message with the given parameters.\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending\n    # This is a placeholder - actual implementation would depend on the internal\n    # message queue and protocol handling\n    pass"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1976, "num_retrieved": 3, "inference_time": 0.7144383490085602, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation would involve sending the BEGIN message to the server\n    # and setting up the response handling with the provided handlers\n    \n    # Since the actual implementation details are not provided in the context,\n    # this is a placeholder that follows the pattern of the run method\n    # and returns a Response object with the handlers\n    \n    # In a real implementation, this would involve:\n    # 1. Constructing the BEGIN message with the provided parameters\n    # 2. Sending it to the server via the underlying transport\n    # 3. Creating and returning a Response object with the handlers\n    \n    # Based on the pattern in the run method and the expected behavior,\n    # we return a Response object with the handlers\n    return self._create_response(handlers=handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation would involve sending the BEGIN message to the server\n    # and setting up the response handling with the provided handlers\n    # Since the exact implementation details are not provided in the context,\n    # this is a placeholder that follows the pattern of the run method.\n    \n    # In a real implementation, this would use the underlying protocol\n    # to send a BEGIN message with the specified parameters\n    # and return a Response object that can be used to receive the response\n    \n    # The actual implementation would depend on the internal state and\n    # communication layer of the driver\n    \n    # For now, we return a Response object with the handlers\n    # This is a simplified representation\n    from neo4j._async.response import Response\n    \n    response = Response(handlers=handlers)\n    # In a real implementation, this would send the BEGIN message\n    # and populate the response with the server's response\n    \n    return response", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation would involve sending the BEGIN message to the server\n    # and setting up the response handling with the provided handlers\n    # Since the exact implementation details are not provided in the context,\n    # this is a placeholder that follows the pattern of the run method.\n    \n    # In a real implementation, this would use the underlying protocol\n    # to send a BEGIN message with the specified parameters and return\n    # a Response object that can be used to handle the transaction result.\n    \n    # The actual implementation would depend on the internal messaging\n    # layer of the driver, which is not fully described in the context.\n    \n    # For now, we return a Response object with the handlers\n    # This is a simplified representation\n    return self._create_response(handlers=handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation would depend on the internal messaging protocol\n    # of the Neo4j driver, but based on the context, it's analogous to run()\n    # in that it sends a message and returns a response with handlers\n    \n    # Since the exact implementation details are not provided in the context,\n    # this is a placeholder that follows the pattern of the run() method\n    # and returns a response object with the given handlers\n    \n    # In a real implementation, this would send a BEGIN message to the server\n    # and return a Response object that can be used to handle the response\n    return self._send_message(\"BEGIN\", mode=mode, bookmarks=bookmarks,\n                              metadata=metadata, timeout=timeout, db=db,\n                              imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n                              hydration_hooks=hydration_hooks, **handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation would involve sending the BEGIN message to the server\n    # and setting up the response handling with the provided handlers\n    # Since the actual network communication is not shown in the context,\n    # this is a placeholder implementation that matches the expected signature\n    \n    # In a real implementation, this would use the underlying transport\n    # to send a BEGIN message with the provided parameters\n    # and return a Response object that can be used to receive the response\n    \n    # Placeholder implementation - actual implementation would depend on\n    # the internal transport layer and message protocol\n    response = self._send_begin_message(\n        mode=mode,\n        bookmarks=bookmarks,\n        metadata=metadata,\n        timeout=timeout,\n        db=db,\n        imp_user=imp_user,\n        dehydration_hooks=dehydration_hooks,\n        hydration_hooks=hydration_hooks\n    )\n    \n    # Attach handlers to the response\n    for handler_name, handler in handlers.items():\n        response.add_handler(handler_name, handler)\n        \n    return response"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2127, "num_retrieved": 8, "inference_time": 0.7144383490085602, "generate_results": ["def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = float(n)\n    if n >= 0:\n        return int(n + (0.5 if n - int(n) >= 0.5 else -0.5))\n    else:\n        return int(n - (0.5 if int(n - n) >= 0.5 else -0.5))", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = float(n)\n    if n >= 0:\n        return int(n + (0.5 if n - int(n) >= 0.5 else -0.5))\n    else:\n        return int(n - (0.5 if n - int(n) <= -0.5 else -0.5))", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    if n >= 0:\n        return int(n + 0.5) if n - int(n) < 0.5 else int(n) + 1 if n - int(n) == 0.5 and int(n) % 2 == 1 else int(n)\n    else:\n        return int(n - 0.5) if n - int(n) < 0.5 else int(n) - 1 if n - int(n) == 0.5 and int(n) % 2 == 1 else int(n)", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = round(n)\n    if n % 2 == 1 and abs(n - round(n)) == 0.5:\n        return n - 1 if n % 2 == 1 else n\n    return int(n)", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    n = round(n)\n    if n % 2 == 1 and abs(n - round(n)) == 0.5:\n        return n - 1 if n % 2 == 1 else n\n    return n"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2029, "num_retrieved": 4, "inference_time": 0.7144383490085602, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None, **kwargs):\n            super().__init__(*coordinates, **kwargs)\n            self.srid = srid\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            else:\n                return cls(coordinates)\n    \n    # Set the class name\n    PointSubclass.__name__ = name\n    \n    # Add a class method to create instances from SRID and coordinates\n    PointSubclass.from_srid = PointSubclass.from_srid\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None, **kwargs):\n            super().__init__(*coordinates, **kwargs)\n            self.srid = srid\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            else:\n                return cls(coordinates)\n    \n    # Set the class name\n    PointSubclass.__name__ = name\n    \n    # Add the class to the srid_map if needed\n    if name not in srid_map:\n        srid_map[name] = PointSubclass\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None, **kwargs):\n            super().__init__(*coordinates, **kwargs)\n            self.srid = srid\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            else:\n                return cls(coordinates)\n    \n    # Set the class name\n    PointSubclass.__name__ = name\n    \n    # Add a class attribute to store the SRID mapping\n    PointSubclass.srid_map = srid_map\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None, **kwargs):\n            super().__init__(*coordinates, **kwargs)\n            self.srid = srid\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            else:\n                return cls(coordinates)\n    \n    # Set the class name\n    PointSubclass.__name__ = name\n    \n    # Add the class to the srid_map for future reference\n    srid_map[name] = (PointSubclass, len(fields))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None, **kwargs):\n            super().__init__(*coordinates, **kwargs)\n            self.srid = srid\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            else:\n                return cls(coordinates)\n    \n    # Set the class name\n    PointSubclass.__name__ = name\n    \n    # Add a class attribute to store the srid_map\n    PointSubclass.srid_map = srid_map\n    \n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1969, "num_retrieved": 4, "inference_time": 0.7144383490085602, "generate_results": ["def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2176, "num_retrieved": 10, "inference_time": 0.7705234438180923, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Inline R options that are necessary for proper behavior\n    # These options are set directly in the R code to ensure they are available\n    # when R starts execution\n    r_options = \"\"\"\noptions(stringsAsFactors = FALSE)\noptions(warn = 1)\noptions(continue = \" \")\noptions(show.signif.stars = FALSE)\n\"\"\"\n    # Insert the R options at the beginning of the code\n    return r_options + \"\\n\" + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Inline R options that need to be set at runtime\n    # Common R options that affect behavior (e.g., warnings, output formatting)\n    r_options = \"\"\"\noptions(\n  warn = 1,\n  stringsAsFactors = FALSE,\n  scipen = 999,\n  digits = 7,\n  show.signif.stars = TRUE\n)\n\"\"\"\n    # Insert the R options at the beginning of the code\n    return r_options + \"\\n\" + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Inline R options that need to be set at runtime\n    # Common R options that affect behavior (e.g., warnings, output formatting)\n    r_options = \"\"\"\noptions(\n  warn = 1,\n  stringsAsFactors = FALSE,\n  scipen = 999,\n  digits = 7,\n  show.error.messages = TRUE\n)\n\"\"\"\n    # Insert the R options at the beginning of the code\n    return r_options + \"\\n\" + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options at the beginning of the code to configure R behavior\n    # Common R options that might be needed for pre-commit hooks\n    r_options = '''\noptions(stringsAsFactors = FALSE, \n        warn = 1, \n        continue.on.warning = TRUE)\n'''\n    return r_options + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Inline R options that need to be set at runtime\n    # Common R options that affect behavior (e.g., warnings, output formatting)\n    r_options = '''\noptions(\n  warn = 1,\n  stringsAsFactors = FALSE,\n  scipen = 999,\n  digits = 7,\n  show.signif.stars = TRUE\n)\n'''\n    # Insert the R options at the beginning of the code\n    return r_options + '\\n' + code"]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2081, "num_retrieved": 7, "inference_time": 0.7705234438180923, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Split the varargs into chunks based on target_concurrency\n    if target_concurrency == 1:\n        chunks = [varargs]\n    else:\n        # Create chunks of files to process\n        chunks = [varargs[i:i + len(varargs) // target_concurrency + (1 if i < len(varargs) % target_concurrency else 0)]\n                  for i in range(0, len(varargs), len(varargs) // target_concurrency + (1 if len(varargs) % target_concurrency else 0))]\n    \n    # Run each chunk in parallel\n    results = []\n    for chunk in chunks:\n        # Create command with current chunk of files\n        current_cmd = cmd + (chunk,)\n        # Execute command and capture output\n        try:\n            result = subprocess.run(\n                current_cmd,\n                capture_output=True,\n                text=False,\n                check=False,\n                **kwargs\n            )\n            results.append(result)\n        except Exception as e:\n            results.append((1, e.stdout.encode() if e.stdout else b'', e.stderr.encode() if e.stderr else b''))\n    \n    # Aggregate results\n    exit_code = 0\n    for result in results:\n        if result.returncode != 0:\n            exit_code = result.returncode\n            break\n    \n    # Return aggregated exit code and combined output\n    combined_output = b''\n    for result in results:\n        combined_output += result.stdout + result.stderr\n    \n    return (exit_code, combined_output)", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Split the varargs into chunks based on target_concurrency\n    if target_concurrency == 1:\n        chunks = [varargs]\n    else:\n        # Create chunks of files to process in parallel\n        chunk_size = len(varargs) // target_concurrency\n        if chunk_size == 0:\n            chunk_size = 1\n        chunks = [\n            varargs[i:i + chunk_size] for i in range(0, len(varargs), chunk_size)\n        ]\n    \n    # Run each chunk in parallel using subprocess\n    results = []\n    for chunk in chunks:\n        # Create command with current chunk of files\n        current_cmd = cmd + (chunk,)\n        # Execute the command and capture output\n        try:\n            result = subprocess.run(\n                current_cmd,\n                capture_output=True,\n                text=True,\n                timeout=300,  # 5 minute timeout\n                cwd=os.getcwd(),\n                env=os.environ.copy(),\n                **kwargs\n            )\n            results.append(result)\n        except subprocess.TimeoutExpired:\n            # Handle timeout\n            results.append(subprocess.CompletedProcess(\n                args=current_cmd,\n                returncode=1,\n                stdout='',\n                stderr='Command timed out'\n            ))\n    \n    # Aggregate results\n    return_code = 0\n    for result in results:\n        if result.returncode != 0:\n            return_code = result.returncode\n            break\n    \n    # Return aggregated result\n    stdout = b'\\n'.join([r.stdout.encode() for r in results if r.stdout])\n    stderr = b'\\n'.join([r.stderr.encode() for r in results if r.stderr])\n    \n    return return_code, (stdout + stderr)", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Split the varargs into chunks based on target_concurrency\n    if target_concurrency == 1:\n        chunks = [varargs]\n    else:\n        # Create chunks of files to process\n        chunk_size = len(varargs) // target_concurrency\n        if chunk_size == 0:\n            chunk_size = 1\n        chunks = [\n            varargs[i:i + chunk_size] for i in range(0, len(varargs), chunk_size)\n        ]\n    \n    # Run each chunk in parallel or sequentially\n    results = []\n    for chunk in chunks:\n        # Create command with current chunk of files\n        current_cmd = cmd + (chunk,)\n        # Execute the command\n        result = subprocess.run(\n            current_cmd,\n            capture_output=True,\n            text=False,\n            timeout=300,  # 5 minute timeout\n            **kwargs\n        )\n        results.append(result)\n    \n    # Aggregate results\n    # Return the overall exit code and combined output\n    exit_code = 0\n    combined_output = b''\n    for result in results:\n        exit_code = max(exit_code, result.returncode)\n        combined_output += result.stdout\n        if result.stderr:\n            combined_output += result.stderr\n    \n    return (exit_code, combined_output)", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual number of jobs based on target_concurrency\n    jobs = target_concurrency\n    \n    # If no files are provided, return success with empty output\n    if not varargs:\n        return 0, b''\n    \n    # Prepare the command with file arguments\n    # This is a simplified version - in practice, xargs would spawn processes\n    # and collect results. Here we simulate the behavior.\n    \n    # Use subprocess to run the command with each file argument\n    # This is a placeholder implementation that would need to be expanded\n    # to properly handle concurrency and file processing.\n    \n    # For now, we'll run the command sequentially for simplicity\n    # In a real implementation, this would use multiprocessing or similar\n    # to run commands in parallel with the specified concurrency.\n    \n    # Collect results from each file\n    results = []\n    for file_arg in varargs:\n        # Run the command with the current file argument\n        try:\n            result = subprocess.run(\n                cmd + (file_arg,),\n                capture_output=True,\n                text=True,\n                timeout=300,  # 5 minute timeout\n                cwd=os.getcwd(),\n                env=os.environ.copy(),\n                shell=False,\n                check=False,\n            )\n            results.append(result)\n        except subprocess.TimeoutExpired:\n            return 1, b\"Command timed out\"\n    \n    # Aggregate results\n    exit_code = 0\n    output_bytes = b''\n    for result in results:\n        if result.returncode != 0:\n            exit_code = 1\n        output_bytes += result.stdout.encode() + b'\\n' + result.stderr.encode() + b'\\n'\n    \n    return exit_code, output_bytes", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Split the varargs into chunks based on target_concurrency\n    if target_concurrency == 1:\n        chunks = [varargs]\n    else:\n        # Create chunks of files to process in parallel\n        chunk_size = len(varargs) // target_concurrency\n        if chunk_size == 0:\n            chunk_size = 1\n        chunks = [\n            varargs[i:i + chunk_size] for i in range(0, len(varargs), chunk_size)\n        ]\n    \n    # Run each chunk in parallel using subprocess\n    results = []\n    for chunk in chunks:\n        # Construct the command with the current chunk of files\n        current_cmd = cmd + (chunk,)\n        # Execute the command and capture output\n        try:\n            result = subprocess.run(\n                current_cmd,\n                capture_output=True,\n                text=True,\n                timeout=300,  # 5 minute timeout\n                cwd=os.getcwd(),\n                env=os.environ.copy(),\n                **kwargs\n            )\n            results.append(result)\n        except subprocess.TimeoutExpired:\n            # Handle timeout case\n            results.append(subprocess.CompletedProcess(\n                args=current_cmd,\n                returncode=1,\n                stdout='',\n                stderr='Command timed out'\n            ))\n    \n    # Aggregate results\n    # Return the overall exit code and combined output\n    overall_exit_code = 0\n    combined_output = b''\n    \n    for result in results:\n        if result.returncode != 0:\n            overall_exit_code = result.returncode\n        combined_output += result.stdout.encode('utf-8') + result.stderr.encode('utf-8')\n    \n    return overall_exit_code, combined_output"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2114, "num_retrieved": 8, "inference_time": 0.7705234438180923, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result\n    # for the same input sequence\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result\n    # for the same input sequence\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result\n    # for the same input sequence\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result\n    # for the same input sequence\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result\n    # for the same input sequence\n    random.seed(0)\n    return random.sample(seq, len(seq))"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2136, "num_retrieved": 9, "inference_time": 0.7705234438180923, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2023, "num_retrieved": 5, "inference_time": 0.7705234438180923, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to ensure the command\n        # is properly normalized to work with the system\n        # Also, deep-path shebangs need to be handled correctly\n        cmd = tuple(str(c) for c in cmd)\n        # Normalize the command path to ensure it's in a format that Windows can handle\n        # This includes ensuring that paths are absolute and properly formatted\n        # For deep-path shebangs, we ensure the command is properly resolved\n        # This is a simplified version that aligns with the known issues\n        # The actual implementation would depend on the specific shebang handling\n        # required by the system\n        return cmd\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to ensure the command\n        # is properly formatted for Windows execution\n        # Also, deep-path shebangs need to be resolved properly\n        \n        # If the command starts with a shebang, we need to normalize it\n        # by replacing the shebang with a Windows-compatible command\n        # For example, '#!/usr/bin/env python' becomes 'python'\n        \n        # Extract the first command part\n        first_cmd = cmd[0]\n        \n        # Check if the first command starts with a shebang\n        if first_cmd.startswith('#!'):\n            # Remove the shebang and try to find the executable\n            # This is a simplified version - in practice, we'd use a shebang parser\n            # to find the actual executable path\n            shebang_content = first_cmd[2:]\n            # For Windows, we need to handle the case where the shebang points\n            # to a deep path, and we need to resolve it properly\n            # We'll use the parse_shebang module to find the executable\n            try:\n                executable = parse_shebang.find_executable(shebang_content)\n                if executable:\n                    # Replace the original command with the found executable\n                    # and keep the rest of the arguments\n                    new_cmd = (executable,) + cmd[1:]\n                    return new_cmd\n            except Exception:\n                # If we can't resolve the shebang, keep the original command\n                pass\n    \n    # If we're not on Windows or the command doesn't have a shebang,\n    # return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to normalize the command\n        # to ensure it's properly formatted for execution\n        # Also handle deep-path shebangs by ensuring the command path is absolute\n        cmd_str = cmd[0]\n        if cmd_str.startswith('#!'):\n            # If the command starts with a shebang, we need to strip it and\n            # ensure the command is properly formatted\n            # For Windows, we replace the shebang with the actual executable\n            # and ensure it's in a format that Windows can handle\n            # This is a workaround for the lack of shebang parsing on Windows\n            # and to handle deep-path shebangs\n            cmd_str = cmd_str.split('#!', 1)[1].strip()\n            # Ensure the command is properly formatted for Windows\n            # by making sure it's a valid executable path\n            if not cmd_str:\n                # If no valid command is found, return the original command\n                return cmd\n            # Normalize the command path to ensure it's absolute\n            # and properly formatted for Windows\n            cmd_str = os.path.abspath(cmd_str)\n            # Replace the original command with the normalized one\n            return (cmd_str,) + cmd[1:]\n        # If no shebang is present, just return the original command\n        return cmd\n    # On non-Windows platforms, return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # Windows does not parse shebangs, so we need to ensure the command\n        # is properly normalized to work with the system\n        # Also, deep-path shebangs need to be handled correctly\n        \n        # If the command starts with a shebang (e.g., #!/usr/bin/env python),\n        # we need to replace it with a proper Windows-compatible command\n        # For Windows, we typically use cmd.exe or PowerShell\n        if cmd[0].startswith('#!'):\n            # Extract the shebang path and replace it with a Windows-compatible\n            # command that can execute the script\n            shebang_path = cmd[0][2:]  # Remove '#!'\n            # On Windows, we use cmd.exe to run scripts\n            # We need to ensure the shebang path is properly resolved\n            # and the command is wrapped appropriately\n            # For now, we just normalize the command to use cmd.exe\n            # with the script path\n            return ('cmd.exe', '/c', shebang_path)\n        \n        # For non-shebang commands, we just return the original command\n        # but ensure it's properly formatted for Windows\n        return cmd\n    \n    # On non-Windows platforms, we just return the original command\n    # as the shebang handling is already supported\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n    \n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to handle shebangs specially\n        # The shebang line is not parsed by Python's subprocess module\n        # So we normalize the command to ensure it's properly formatted\n        # for Windows execution\n        \n        # If the first element is a shebang (starts with #!), we need to\n        # replace it with the actual executable path\n        if cmd and cmd[0].startswith('#!'):\n            # Extract the executable path from the shebang\n            # This handles deep-path shebangs like #!/path/to/interpreter\n            shebang_path = cmd[0][2:]  # Remove the '#!' prefix\n            # Normalize the path to ensure it's absolute and properly formatted\n            # for Windows\n            if os.path.isabs(shebang_path):\n                # Already absolute path\n                normalized_cmd = (shebang_path,) + cmd[1:]\n            else:\n                # Relative path - resolve it to an absolute path\n                # This is a simplified version; in practice, we'd resolve\n                # the path relative to the current working directory\n                import os\n                resolved_path = os.path.abspath(shebang_path)\n                normalized_cmd = (resolved_path,) + cmd[1:]\n        else:\n            # No shebang present, just return the original command\n            normalized_cmd = cmd\n    else:\n        # On non-Windows platforms, we can use the original command\n        normalized_cmd = cmd\n    \n    return normalized_cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2206, "num_retrieved": 12, "inference_time": 0.7705234438180923, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache = cache if callable(cache) else cache\n        cache = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_value = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                result = cache[key_value]\n                return result\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache[key_value] = result\n                return result\n        \n        # Add lock support if provided\n        if lock is not None:\n            def locked_wrapper(*args, **kwargs):\n                with lock:\n                    key_value = key(*args, **kwargs)\n                    try:\n                        result = cache[key_value]\n                        return result\n                    except KeyError:\n                        result = func(*args, **kwargs)\n                        cache[key_value] = result\n                        return result\n            wrapper = locked_wrapper\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache = cache if callable(cache) else cache\n        if not callable(cache):\n            cache = _cache(cache, typed=False)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_val = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                result = cache[key_val]\n                return result\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache[key_val] = result\n                return result\n        \n        # Add lock support if provided\n        if lock is not None:\n            import functools\n            @functools.wraps(func)\n            def locked_wrapper(*args, **kwargs):\n                with lock:\n                    key_val = key(*args, **kwargs)\n                    try:\n                        result = cache[key_val]\n                        return result\n                    except KeyError:\n                        result = func(*args, **kwargs)\n                        cache[key_val] = result\n                        return result\n                return result\n            wrapper = locked_wrapper\n        \n        @functools.wraps(func)\n        def wrapped_func(*args, **kwargs):\n            return wrapper(*args, **kwargs)\n        \n        return wrapped_func\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        cache_instance.key = key\n        cache_instance.lock = lock\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_val = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_val]\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache_instance[key_val] = result\n                return result\n        \n        # Set the wrapper's __name__ and __doc__ to match the original function\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_obj = cache if isinstance(cache, Cache) else cache()\n        cache_obj.key = key\n        cache_obj.lock = lock\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_val = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_obj[key_val]\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache_obj[key_val] = result\n                return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_obj = cache if isinstance(cache, Cache) else cache()\n        cache_obj.key = key\n        cache_obj.lock = lock\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_val = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                result = cache_obj[key_val]\n                return result\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache_obj[key_val] = result\n                return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2081, "num_retrieved": 7, "inference_time": 0.7705234438180923, "generate_results": ["import functools\nfrom cachetools import TTLCache\nfrom functools import update_wrapper\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use the cache to store results with TTL\n            try:\n                return cache[args, kwargs]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[args, kwargs] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\nfrom cachetools.keys import hashkey\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = hashkey(*args, **kwargs) if not typed else tuple(args) + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\nfrom functools import update_wrapper\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use the cache to store results with TTL\n            try:\n                return cache[args, kwargs]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            cache[args, kwargs] = result\n            return result\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "import functools\nfrom cachetools import TTLCache\nfrom cachetools.keys import hashkey\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = hashkey(*args, **kwargs) if not typed else tuple(args) + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\nfrom functools import update_wrapper\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[args, kwargs]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[args, kwargs] = result\n            return result\n        return update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2026, "num_retrieved": 5, "inference_time": 0.7705234438180923, "generate_results": ["import functools\nimport collections\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize, getsizeof=None)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert args and kwargs to a hashable form\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(type(arg) for arg in args) + tuple(type(v) for v in kwargs.values())\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nimport collections\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize, getsizeof=None)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use typed=False by default as per the function signature\n            # If typed is True, we would need to handle type-based key generation\n            # For now, use the standard key generation\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(type(a) for a in args) + tuple(type(v) for v in kwargs.values())\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nimport collections\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize, getsizeof=None)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert arguments to a hashable form\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(type(arg) for arg in args) + tuple(type(v) for v in kwargs.values())\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nimport collections\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize, getsizeof=None)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert args and kwargs to a hashable form\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(type(arg) for arg in args) + tuple(type(v) for v in kwargs.values())\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nimport collections\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize, getsizeof=None)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert args and kwargs to a hashable form\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(type(arg) for arg in args) + tuple(type(v) for v in kwargs.values())\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2053, "num_retrieved": 6, "inference_time": 0.7705234438180923, "generate_results": ["import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    cache = LRUCache(maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = args_key + tuple(kwargs_key)\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    cache = LRUCache(maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = args_key + tuple(kwargs_key)\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    cache = LRUCache(maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = args_key + tuple(kwargs_key)\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    cache = LRUCache(maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    cache = LRUCache(maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = args_key + tuple(kwargs_key)\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2026, "num_retrieved": 5, "inference_time": 0.7705234438180923, "generate_results": ["import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    cache = LFUCache(maxsize, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert args and kwargs to a hashable key\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    cache = LFUCache(maxsize, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert args and kwargs to a hashable key\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    cache = LFUCache(maxsize, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert args and kwargs to a hashable key\n            key = (args, tuple(sorted(kwargs.items())))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    cache = LFUCache(maxsize, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use the cache to store results\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    cache = LFUCache(maxsize, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use the cache to store results\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            # Compute the result and store it in cache\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1996, "num_retrieved": 5, "inference_time": 0.7705234438180923, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get the first inserted key (FIFO order)\n    first_key = next(iter(self.__data))\n    value = self.pop(first_key)\n    return (first_key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get the first inserted key (FIFO order)\n    first_key = next(iter(self.__data))\n    value = self.pop(first_key)\n    return (first_key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get the first inserted key (FIFO order)\n    first_key = next(iter(self.__data))\n    value = self.pop(first_key)\n    return (first_key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get the first inserted key (FIFO order)\n    first_key = next(iter(self.__data))\n    value = self.pop(first_key)\n    return (first_key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get the first inserted key (FIFO order)\n    first_key = next(iter(self.__data))\n    value = self.pop(first_key)\n    return (first_key, value)"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1999, "num_retrieved": 4, "inference_time": 0.7705234438180923, "generate_results": ["def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1998, "num_retrieved": 4, "inference_time": 0.7705234438180923, "generate_results": ["def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2191, "num_retrieved": 11, "inference_time": 0.7705234438180923, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        key_func = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create the cache key using the provided key function\n            k = key_func(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            # Store the result in the cache\n            try:\n                with lock:\n                    cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n            \n            return v\n        \n        # Add cache_info and cache_clear methods if available\n        if hasattr(cache, 'cache_info'):\n            wrapper.cache_info = cache.cache_info\n        if hasattr(cache, 'cache_clear'):\n            wrapper.cache_clear = cache.cache_clear\n            \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the cache, key function, and lock for use in the wrapper\n        cache_obj = cache\n        key_func = key\n        cache_lock = lock\n        \n        def wrapper(self, *args, **kwargs):\n            # Create the cache key using the provided key function\n            k = key_func(self, *args, **kwargs)\n            \n            with cache_lock:\n                try:\n                    v = cache_obj[k]\n                    return v\n                except KeyError:\n                    pass\n            \n            # Call the original function to get the result\n            v = func(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                with cache_lock:\n                    cache_obj.setdefault(k, v)\n            except ValueError:\n                # Handle case where cache size exceeds capacity\n                return v\n            \n            return v\n        \n        # Add cache_info and cache_clear methods if available\n        if hasattr(cache_obj, 'cache_info'):\n            wrapper.cache_info = cache_obj.cache_info\n        if hasattr(cache_obj, 'cache_clear'):\n            wrapper.cache_clear = cache_obj.cache_clear\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        key_func = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create the cache key using the provided key function\n            k = key_func(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            # Store the result in the cache\n            try:\n                with lock:\n                    cache[k] = v\n            except ValueError:\n                # Handle case where cache size is exceeded\n                pass\n            return v\n        \n        # Add cache_info and cache_clear methods if available\n        if hasattr(cache, 'cache_info'):\n            wrapper.cache_info = cache.cache_info\n        if hasattr(cache, 'cache_clear'):\n            wrapper.cache_clear = cache.cache_clear\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        key_func = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create the cache key using the provided key function\n            k = key_func(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            # Store the result in the cache\n            try:\n                with lock:\n                    cache[k] = v\n            except ValueError:\n                # Handle case where cache size is exceeded\n                pass\n            return v\n        \n        # Add cache_info and cache_clear methods if available\n        if hasattr(cache, 'cache_info'):\n            wrapper.cache_info = cache.cache_info\n        if hasattr(cache, 'cache_clear'):\n            wrapper.cache_clear = cache.cache_clear\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        key_func = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create the cache key using the provided key function\n            k = key_func(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            # Store the result in the cache\n            try:\n                with lock:\n                    cache[k] = v\n            except ValueError:\n                # Handle case where cache size is exceeded\n                pass\n            return v\n        \n        # Add cache_info and cache_clear methods if available\n        if hasattr(cache, 'cache_info'):\n            wrapper.cache_info = cache.cache_info\n        if hasattr(cache, 'cache_clear'):\n            wrapper.cache_clear = cache.cache_clear\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2044, "num_retrieved": 6, "inference_time": 0.7705234438180923, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        # Format the traceback\n        tb_str = ''.join(traceback.format_list(tb))\n        # Add the exception type and message\n        exc_str = f\"{type(e).__name__}: {e}\"\n        return f\"{exc_str}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {ex}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        # Format the traceback\n        tb_str = ''.join(traceback.format_list(tb))\n        # Add the exception type and message\n        exc_str = f\"{type(e).__name__}: {e}\"\n        return f\"{exc_str}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {ex}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        # Format the traceback\n        tb_str = ''.join(traceback.format_list(tb))\n        # Add the exception type and message\n        exc_str = f\"{type(e).__name__}: {e}\"\n        return f\"{exc_str}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {ex}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        # Format the traceback\n        tb_str = ''.join(traceback.format_list(tb))\n        # Add the exception type and message\n        exc_str = f\"{type(e).__name__}: {e}\"\n        return f\"{exc_str}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {ex}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        # Format the traceback\n        tb_str = ''.join(traceback.format_list(tb))\n        # Add the exception type and message\n        exc_str = f\"{type(e).__name__}: {e}\"\n        return f\"{exc_str}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {ex}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1986, "num_retrieved": 4, "inference_time": 0.7705234438180923, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Return negative value on error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Return negative value if error occurs\n        return -1", "@staticmethod\ndef append_text_to_file(cls, file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided text buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n\n    # Convert text buffer to bytes using the specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwriting, or write mode if overwriting\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Return negative value if error occurs\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided text buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n\n    # Convert text buffer to bytes using the specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwriting, or write mode if overwriting\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        # Return negative value if error occurs\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided text buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n\n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Return negative value if error occurs\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1985, "num_retrieved": 4, "inference_time": 0.5966020077466965, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, e)\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, e)\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as f:\n            bin_buf = f.read()\n        return bin_buf.decode(encoding)\n    except Exception as e:\n        logger.error(\"Error reading file %s with encoding %s: %s\", file_name, encoding, e)\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, e)\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, e)\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1960, "num_retrieved": 3, "inference_time": 0.5966020077466965, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2073, "num_retrieved": 7, "inference_time": 0.5966020077466965, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Clear any existing loggers\n    logging.root.handlers = []\n    \n    # Reset any context filters or local state if needed\n    from ContextFilter import ContextFilter\n    ContextFilter.LOC = local()", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Clear any existing loggers\n    logging.root.handlers = []\n    \n    # Reset any context filters or local state if needed\n    from ContextFilter import ContextFilter\n    ContextFilter.LOC = local()", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Clear any existing loggers\n    logging.root.handlers = []\n    \n    # Reset any context filters or local state if needed\n    from ContextFilter import ContextFilter\n    ContextFilter.LOC = local()", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Clear any existing loggers\n    logging.root.handlers = []\n    \n    # Reset any context filters or local state if needed\n    from ContextFilter import ContextFilter\n    ContextFilter.LOC = local()", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters or local state if needed\n    from ContextFilter import ContextFilter\n    ContextFilter.LOC = local()"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1947, "num_retrieved": 4, "inference_time": 0.5966020077466965, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1970, "num_retrieved": 3, "inference_time": 0.5966020077466965, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Sort orderings by their length in descending order to process longer ones first\n    # This ensures that when we find a common suffix, we can properly merge\n    orderings.sort(key=len, reverse=True)\n    \n    result = []\n    # Keep track of the current position in each ordering\n    pos = [0] * len(orderings)\n    \n    while True:\n        # Find the next element that appears in all orderings at the current position\n        next_element = None\n        found = True\n        \n        for i in range(len(orderings)):\n            if pos[i] >= len(orderings[i]):\n                found = False\n                break\n            if next_element is None:\n                next_element = orderings[i][pos[i]]\n            elif orderings[i][pos[i]] != next_element:\n                found = False\n                break\n        \n        if not found:\n            break\n            \n        result.append(next_element)\n        # Advance the position for all orderings that have the next element\n        for i in range(len(orderings)):\n            if pos[i] < len(orderings[i]) and orderings[i][pos[i]] == next_element:\n                pos[i] += 1\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the result\n        # We need to find the longest suffix of result that matches a prefix of ordering\n        i = 0\n        while i < len(ordering) and i < len(result):\n            if result[i] != ordering[i]:\n                break\n            i += 1\n        \n        # If we found a match, we need to insert the rest of the ordering\n        # after the matched position\n        if i < len(ordering):\n            # Insert the remaining elements of ordering after position i\n            result = result[:i] + ordering[i:]\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the result\n        # We need to find the longest suffix of result that matches a prefix of ordering\n        i = 0\n        while i < len(ordering) and i < len(result):\n            if result[i] != ordering[i]:\n                break\n            i += 1\n        \n        # If we found a match, we need to insert the rest of the ordering\n        # after the matched position\n        if i < len(ordering):\n            # Insert the remaining elements of ordering after position i\n            result = result[:i] + ordering[i:]\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the result\n        # We need to find the longest suffix of result that matches a prefix of ordering\n        i = 0\n        while i < len(ordering) and i < len(result):\n            if result[i] != ordering[i]:\n                break\n            i += 1\n        \n        # If we found a match, insert the rest of the ordering after the match\n        if i < len(ordering):\n            # Insert the part of ordering from i onwards\n            result[i:] = ordering[i:]\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the result\n        # We need to find the longest suffix of result that matches a prefix of ordering\n        i = 0\n        while i < len(ordering) and i < len(result):\n            if result[i] != ordering[i]:\n                break\n            i += 1\n        \n        # If we found a match, we need to insert the rest of the ordering after the match\n        if i < len(ordering):\n            # Insert the remaining elements of ordering after the matched position\n            result = result[:i] + ordering[i:]\n    \n    return result"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2052, "num_retrieved": 6, "inference_time": 0.5966020077466965, "generate_results": ["def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return IDeclaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return IDeclaration()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return IDeclaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return IDeclaration()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return IDeclaration()\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2034, "num_retrieved": 5, "inference_time": 0.5966020077466965, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by MRO (Method Resolution Order) to get the minimal ordered equivalent\n    # This ensures that if a class is a subclass of another, the subclass comes first\n    # We use the built-in MRO to determine the order\n    result = []\n    for cls in unique_classes:\n        # Check if this class is already in the result (to avoid duplicates)\n        if not any(isinstance(cls, base) for base in result):\n            result.append(cls)\n    \n    # Sort by MRO to ensure minimal ordering\n    # This is a simplified version - in practice, we'd use the actual MRO\n    # to determine the minimal base classes\n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by MRO (Method Resolution Order) to get the minimal ordered equivalent\n    # This ensures that if a class is a subclass of another, the subclass comes first\n    # in the minimal base list\n    result = []\n    for cls in unique_classes:\n        # Check if this class is already in the result (to avoid duplicates)\n        if not any(isinstance(cls, base) for base in result):\n            result.append(cls)\n    \n    # Sort by MRO to ensure minimal ordering\n    # We use the fact that if a class inherits from another, it should come after\n    # in the MRO order\n    result.sort(key=lambda cls: tuple(cls.__mro__))\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track which classes we've seen\n    seen = set()\n    result = []\n    \n    # Sort classes by their depth in the MRO (method resolution order)\n    # We'll use a simple approach: process classes in order of appearance\n    # and only include those that are not already in the result or are not\n    # redundant due to inheritance\n    \n    # Create a list of classes to process\n    to_process = list(classes)\n    \n    # Process each class in order\n    for cls in to_process:\n        # Skip if already in result\n        if cls in seen:\n            continue\n            \n        # Add to result and mark as seen\n        result.append(cls)\n        seen.add(cls)\n        \n        # Remove any classes that are now redundant (subclasses of current class)\n        # This is a simplified version - in practice, we'd need to check MRO\n        # and determine if any class is a subclass of another\n        # For minimal bases, we only remove if the class is a subclass of a class already in result\n        # But since we're building minimal bases, we just keep the first occurrence\n        # and assume the order is minimal\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track which classes we've seen\n    seen = set()\n    result = []\n    \n    # Sort classes by their depth in the MRO (method resolution order)\n    # We'll process classes in order of their appearance in the MRO\n    # and only include those that are not already covered by a superclass\n    \n    # Create a list of classes with their MRO depth\n    # We'll use a simple approach: if a class is a subclass of another, \n    # we don't need to include the subclass in the minimal base list\n    \n    # Sort classes by their MRO depth (we'll use a simple heuristic)\n    # We'll process classes in the order they appear in the list\n    # and remove any that are already covered by a previous class\n    \n    # Create a set of all classes in the list\n    class_set = set(classes)\n    \n    # Process each class in the original list\n    for cls in classes:\n        # If we've already seen this class, skip it\n        if cls in seen:\n            continue\n            \n        # Check if this class is a subclass of any previously included class\n        # If it is, skip it\n        is_subclass = False\n        for base in result:\n            if issubclass(cls, base):\n                is_subclass = True\n                break\n                \n        # If it's not a subclass of any previously included class, add it\n        if not is_subclass:\n            result.append(cls)\n            seen.add(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by MRO (method resolution order) to get the minimal ordered equivalent\n    # This ensures that if a class is a subclass of another, the subclass comes first\n    # in the minimal base list\n    result = []\n    for cls in unique_classes:\n        # Check if cls is a subclass of any other class in the list\n        # If it is, we can remove the superclass from the list\n        # We need to check the MRO to determine the order\n        mro = cls.__mro__\n        for base in mro:\n            if base in unique_classes and base != cls:\n                # Remove the base class from the list\n                # We'll rebuild the list without the base\n                pass\n    \n    # Simple approach: just return the unique classes in the order they appeared\n    # This is a minimal implementation that satisfies the basic requirement\n    return tuple(unique_classes)"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2045, "num_retrieved": 6, "inference_time": 0.5966020077466965, "generate_results": ["def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = getattr(self, name, None)\n            if desc is not None and hasattr(desc, 'getName'):\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        all_names = self.names(all=True)\n        descriptions = []\n        for name in all_names:\n            desc = getattr(self, name, None)\n            if desc is not None and hasattr(desc, 'getName'):\n                descriptions.append((name, desc))\n        return descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = getattr(self, name, None)\n            if desc is not None and hasattr(desc, 'getName'):\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        all_names = self.names(all=True)\n        descriptions = []\n        for name in all_names:\n            desc = getattr(self, name, None)\n            if desc is not None and hasattr(desc, 'getName'):\n                descriptions.append((name, desc))\n        return descriptions", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = getattr(self, name, None)\n            if desc is not None and hasattr(desc, 'getName'):\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        all_names = self.names(all=True)\n        descriptions = []\n        for name in all_names:\n            desc = getattr(self, name, None)\n            if desc is not None and hasattr(desc, 'getName'):\n                descriptions.append((name, desc))\n        return descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = getattr(self, name, None)\n            if isinstance(desc, Attribute):\n                descriptions.append((name, desc))\n        return sorted(descriptions)\n    else:\n        # Return all attributes including those from base interfaces\n        all_names = self.names(all=True)\n        descriptions = []\n        for name in all_names:\n            desc = getattr(self, name, None)\n            if isinstance(desc, Attribute):\n                descriptions.append((name, desc))\n        return sorted(descriptions)", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = getattr(self, name, None)\n            if desc is not None and hasattr(desc, 'getName'):\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        all_names = self.names(all=True)\n        descriptions = []\n        for name in all_names:\n            desc = getattr(self, name, None)\n            if desc is not None and hasattr(desc, 'getName'):\n                descriptions.append((name, desc))\n        return descriptions"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2109, "num_retrieved": 8, "inference_time": 0.5966020077466965, "generate_results": ["def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Collect names from direct and indirect interfaces\n        names = []\n        for name in self.__dict__:\n            names.append(name)\n        # Add names from base interfaces\n        for base in self.__bases__:\n            if hasattr(base, 'names'):\n                names.extend(base.names(all=True))\n        return names", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Collect names from direct and indirect interfaces\n        names = set()\n        for interface in self.__bases__:\n            names.update(interface.names(all=True))\n        names.update(self.__dict__.keys())\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Collect names from direct and indirect interfaces\n        names = []\n        for name in self.__dict__:\n            names.append(name)\n        # Add names from base interfaces\n        for base in self.__bases__:\n            names.extend(base.names(all=True))\n        return list(set(names))", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Collect names from direct and indirect interfaces\n        names = []\n        for name in self.__dict__:\n            names.append(name)\n        # Add names from base interfaces\n        for base in self.__bases__:\n            if hasattr(base, 'names'):\n                names.extend(base.names(all=True))\n        return names", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include names from direct and indirect base interfaces\n        names = set()\n        for interface in self.__sro__:\n            names.update(interface.__dict__.keys())\n        return list(names)"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2052, "num_retrieved": 6, "inference_time": 0.5966020077466965, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, tuple):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2018, "num_retrieved": 5, "inference_time": 0.5966020077466965, "generate_results": ["def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if not _should_attempt_c_optimizations():\n            return False\n        # Attempt to import the C optimization module\n        # This is a placeholder - actual module import would depend on context\n        # In practice, this would import the C extension module\n        # For now, we just return True if optimizations are attempted\n        return True\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            # This is a placeholder - actual module import would depend on context\n            # For now, we assume the C extension is available if the check passes\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            # The actual module name would depend on the specific implementation\n            # For example, if the C extension is named 'zope.interface._c'\n            try:\n                import zope.interface._c\n                return True\n            except ImportError:\n                raise ImportError(\"C optimizations are required but not available\")\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are required but not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            # This is a placeholder - actual module import would depend on context\n            # For now, we assume the C extension is available if the check passes\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            # This is a placeholder - actual module import would depend on context\n            # In practice, this would import the C extension module\n            import zope.interface._c_optimizations\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are required but not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2022, "num_retrieved": 5, "inference_time": 0.5966020077466965, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import sys\n    import os\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    if os.environ.get('PURE_PYTHON') == '1':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import sys\n    import os\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    if os.environ.get('PURE_PYTHON') == '1':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import sys\n    import os\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    if os.environ.get('PURE_PYTHON') == '1':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import sys\n    import os\n    \n    # If running on PyPy, we should not attempt C optimizations\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    # Default to attempting C optimizations\n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import sys\n    import os\n    \n    # On PyPy, we don't use C optimizations\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # If PURE_PYTHON environment variable is set, don't use C optimizations\n    if os.environ.get('PURE_PYTHON') is not None:\n        return False\n    \n    return True"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2164, "num_retrieved": 10, "inference_time": 0.5966020077466965, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2113, "num_retrieved": 8, "inference_time": 0.5966020077466965, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1992, "num_retrieved": 4, "inference_time": 0.5966020077466965, "generate_results": ["def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = self.initial_value\n    else:\n        self._bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = self.initial_value\n    else:\n        self._bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = self.initial_value\n    else:\n        self._bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = self.initial_value\n    else:\n        self._bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = self.initial_value\n    else:\n        self._bins = self.make_bins()"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1937, "num_retrieved": 2, "inference_time": 0.5966020077466965, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Iterables are converted to tables. Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and does not perform any actual conversion\n    # as per the docstring and context\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Iterables are converted to tables. \"\n                  \"Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and does not perform actual CSV conversion\n    # as per the context and docstring\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Iterables are converted to tables. Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and does not perform any actual conversion\n    # as per the documentation and context\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Iterables are converted to tables. \"\n                  \"Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and does not perform actual CSV conversion\n    # as per the context and docstring\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import warnings\n    warnings.warn(\"to_csv is not used. Iterables are converted to tables. \"\n                  \"Use lena.output.ToCSV instead.\", DeprecationWarning)\n    # This function is deprecated and does not perform actual conversion\n    # as per the context and deprecation note\n    pass"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1969, "num_retrieved": 3, "inference_time": 0.8795414119958878, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Assuming this function is part of a class that manages histogram or graph errors\n    # and coord_name refers to a coordinate field name (e.g., \"x\", \"y\")\n    # The function should return the index of the error corresponding to the given coordinate\n    \n    # Since the exact context of the class and error handling is not fully provided,\n    # we assume a simple mapping where error indices are derived from field names\n    # and the coordinate name corresponds to a field in the graph.\n    \n    field_names = self.field_names if hasattr(self, 'field_names') else (\"x\", \"y\")\n    \n    # Map coordinate name to its index in field_names\n    try:\n        index = field_names.index(coord_name)\n        return index\n    except ValueError:\n        raise ValueError(f\"Coordinate '{coord_name}' not found in field names {field_names}\")", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # This function is not fully defined in the context\n    # and lacks sufficient information to implement\n    # based on the provided docstring and context.\n    # Without additional context about the class structure\n    # or expected behavior, a complete implementation cannot be provided.\n    raise NotImplementedError(\"Implementation not available in context\")", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Assuming the function needs to map a coordinate name to an error index\n    # based on the field_names or some internal structure\n    # Since no context is provided about the internal structure or field_names,\n    # this is a minimal implementation that returns an index based on coord_name\n    \n    # This is a placeholder implementation - actual behavior depends on the context\n    # of the class and how field_names are defined\n    \n    field_names = self.field_names if hasattr(self, 'field_names') else (\"x\", \"y\")\n    \n    # Map coord_name to index in field_names\n    try:\n        return field_names.index(coord_name)\n    except ValueError:\n        raise ValueError(f\"Coordinate name '{coord_name}' not found in field_names: {field_names}\")", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Assuming this function is part of a class that manages histogram or graph errors\n    # and coord_name refers to a coordinate field (e.g., \"x\", \"y\")\n    # The implementation would typically map coordinate names to error indices\n    \n    # Example implementation based on common patterns:\n    # If field_names are provided, we map each coordinate to its corresponding error index\n    # This is a placeholder implementation - actual logic depends on the class structure\n    \n    field_names = self.field_names if hasattr(self, 'field_names') else (\"x\", \"y\")\n    \n    # Map coordinate names to error indices (assuming errors are stored in a specific order)\n    # For example, if field_names are (\"x\", \"y\"), errors might be at indices (0, 1)\n    # or errors might be stored separately\n    \n    # Simple case: error indices follow the same order as field names\n    try:\n        index = field_names.index(coord_name)\n        return index\n    except ValueError:\n        # If coord_name not found, return -1 or raise an error\n        raise ValueError(f\"Coordinate '{coord_name}' not found in field names {field_names}\")", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Assuming this function is part of a class that manages histogram or graph errors\n    # and coord_name refers to a coordinate field (e.g., \"x\", \"y\")\n    # The function should return indices for error values associated with the given coordinate\n    \n    # Since the context does not provide explicit information about error handling\n    # or the structure of the class, we implement a basic version that maps coordinate names\n    # to error indices based on field names and structure\n    \n    # Example: if field_names are (\"x\", \"y\"), then \"x\" might correspond to index 0,\n    # and \"y\" to index 1. Errors would be stored in a corresponding position.\n    \n    field_names = self.field_names if hasattr(self, 'field_names') else (\"x\", \"y\")\n    \n    # Map coordinate name to index\n    try:\n        index = field_names.index(coord_name)\n        return index\n    except ValueError:\n        raise ValueError(f\"Coordinate '{coord_name}' not found in field names {field_names}\")"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1940, "num_retrieved": 2, "inference_time": 0.8795414119958878, "generate_results": ["def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        context.error = context.error or {}\n        context.error[\"x\"] = context.error.get(\"x\", {})\n        context.error[\"x\"][\"index\"] = self._error_x_ind\n    if self._error_y_ind != -1:\n        context.error = context.error or {}\n        context.error[\"y\"] = context.error.get(\"y\", {})\n        context.error[\"y\"][\"index\"] = self._error_y_ind", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        context.error.setdefault(\"x\", {}).setdefault(\"error\", {}).setdefault(\"low\", {}).update({\"index\": self._error_x_ind})\n    if self._error_y_ind != -1:\n        context.error.setdefault(\"y\", {}).setdefault(\"error\", {}).setdefault(\"low\", {}).update({\"index\": self._error_y_ind})", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        context.error.setdefault(\"x\", {}).setdefault(\"low\", {}).update({\"index\": self._error_x_ind})\n    if self._error_y_ind != -1:\n        context.error.setdefault(\"y\", {}).setdefault(\"low\", {}).update({\"index\": self._error_y_ind})", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        context.error.setdefault(\"x\", {}).setdefault(\"low\", {}).update({\"index\": self._error_x_ind})\n    if self._error_y_ind != -1:\n        context.error.setdefault(\"y\", {}).setdefault(\"low\", {}).update({\"index\": self._error_y_ind})", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if not hasattr(self, '_error_x_ind') or not hasattr(self, '_error_y_ind'):\n        return\n\n    error_dict = context.get('error', {})\n    error_dict['x'] = error_dict.get('x', {})\n    error_dict['y'] = error_dict.get('y', {})\n    \n    if self._error_x_ind != -1:\n        error_dict['x']['index'] = self._error_x_ind\n    if self._error_y_ind != -1:\n        error_dict['y']['index'] = self._error_y_ind\n    \n    context['error'] = error_dict"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1940, "num_retrieved": 2, "inference_time": 0.8795414119958878, "generate_results": ["def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(edges) == 0:\n        return 0.0\n    \n    # Compute the integral by summing up bin values multiplied by bin widths\n    total_integral = 0.0\n    for i in range(len(bins) - 1):\n        bin_width = edges[i + 1] - edges[i]\n        total_integral += bins[i] * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(edges) == 0:\n        return 0.0\n    \n    # Compute the integral by summing bin values multiplied by bin widths\n    total_integral = 0.0\n    for i in range(len(bins) - 1):\n        bin_width = edges[i + 1] - edges[i]\n        total_integral += bins[i] * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(edges) == 0:\n        return 0.0\n    \n    # Compute the integral by summing bin values multiplied by bin widths\n    total_integral = 0.0\n    for i in range(len(bins) - 1):\n        bin_width = edges[i + 1] - edges[i]\n        total_integral += bins[i] * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(edges) == 0:\n        return 0.0\n    \n    # Compute the integral by summing bin values multiplied by bin widths\n    total_integral = 0.0\n    for i in range(len(bins) - 1):\n        bin_width = edges[i + 1] - edges[i]\n        total_integral += bins[i] * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(edges) == 0:\n        return 0.0\n    \n    # Compute the integral by summing bin values multiplied by bin widths\n    total_integral = 0.0\n    for i in range(len(bins) - 1):\n        bin_width = edges[i + 1] - edges[i]\n        total_integral += bins[i] * bin_width\n    \n    return total_integral"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1896, "num_retrieved": 2, "inference_time": 0.8795414119958878, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    if isinstance(seq, source.Source):\n        return False\n    if isinstance(seq, sequence.Sequence):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    if isinstance(seq, source.Source):\n        return False\n    if isinstance(seq, sequence.Sequence):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    if isinstance(seq, source.Source):\n        return False\n    if isinstance(seq, sequence.Sequence):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    if isinstance(seq, source.Source):\n        return False\n    if isinstance(seq, sequence.Sequence):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    if isinstance(seq, source.Source):\n        return False\n    if isinstance(seq, sequence.Sequence):\n        return any(isinstance(el, fill_request_seq.FillRequest) for el in seq)\n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1986, "num_retrieved": 4, "inference_time": 0.8795414119958878, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'request'))", "def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'request'))", "def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'request'))", "def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'request'))", "def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'request'))"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1960, "num_retrieved": 3, "inference_time": 0.8795414119958878, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return callable(getattr(obj, \"run\", None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return callable(getattr(obj, \"run\", None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return callable(getattr(obj, \"run\", None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return callable(getattr(obj, \"run\", None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return callable(getattr(obj, \"run\", None))"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1986, "num_retrieved": 4, "inference_time": 0.8795414119958878, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'compute'))", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'compute'))", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'compute'))", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'compute'))", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute') and callable(getattr(obj, 'fill')) and callable(getattr(obj, 'compute'))"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1936, "num_retrieved": 2, "inference_time": 0.8795414119958878, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries\n                nested_diff = difference(d1[key], d2[key], level=-1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                # Different values at same key\n                result[key] = d1[key]\n    else:\n        # Limited recursion depth\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries up to level\n                if level > 0:\n                    nested_diff = difference(d1[key], d2[key], level=level - 1)\n                    if nested_diff:\n                        result[key] = nested_diff\n                else:\n                    # At max depth, just check if values differ\n                    if d1[key] != d2[key]:\n                        result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=level)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    else:\n        # Limited recursion depth (level >= 0)\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=level - 1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n    else:\n        # Limited recursion depth (level 1)\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level - 1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n    else:\n        # Limited recursion depth (level 1)\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level - 1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n    else:\n        # Limited recursion depth (level 1)\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level - 1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n    \n    return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1971, "num_retrieved": 3, "inference_time": 0.8795414119958878, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if not hasattr(self, '_hist') or self._hist is None:\n        # If no histogram is created yet, initialize it with the first valid coordinate\n        self._hist = lena.structures.histogram()\n    \n    # Check if coord is within the histogram edges\n    if hasattr(self._hist, 'edges') and self._hist.edges is not None:\n        min_edge, max_edge = self._hist.edges[0], self._hist.edges[-1]\n        if min_edge <= coord <= max_edge:\n            # Add the weight to the bin containing coord\n            bin_index = self._get_bin_index(coord)\n            if bin_index is not None:\n                # Update the bin value with weight\n                self._hist.bins[bin_index] += weight\n        # else: coord is outside edges, ignore it\n    else:\n        # No edges defined, just store the coordinate and weight\n        # This is a fallback case - in practice, histogram should have edges\n        pass", "def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if not hasattr(self, '_hist') or self._hist is None:\n        # If no histogram is created yet, initialize with empty data\n        self._data = []\n        self._cur_context = {}\n    \n    # Check if coord is within the histogram edges\n    if hasattr(self, '_edges') and self._edges is not None:\n        min_edge, max_edge = self._edges[0], self._edges[-1]\n        if coord < min_edge or coord > max_edge:\n            return  # Ignore coordinates outside the histogram edges\n    \n    # Add the coordinate with given weight\n    self._data.append(coord * weight)\n    self._cur_context = self._cur_context or {}", "def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if self._data is None or len(self._data) == 0:\n        # No data yet, initialize with empty data\n        self._data = []\n    \n    # Check if coord is within histogram edges\n    # We need to know the edges to validate coord\n    # Since edges are not directly available in this method,\n    # we assume that the histogram is being built and edges are computed on demand\n    # In practice, we would need to store edges or compute them when needed\n    \n    # For now, we just store the coordinate and weight\n    # The actual validation against edges will happen during request()\n    self._data.append((coord, weight))", "def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if self._data is None or len(self._data) == 0:\n        # No data yet, initialize with empty data\n        self._data = []\n    \n    # Check if coord is within the histogram edges\n    # We need to determine the edges from the histogram\n    # Since we don't have direct access to edges in this method,\n    # we assume that the histogram is built from the data stored in self._data\n    # and that we need to validate coord against the edges\n    \n    # For now, we just store the coordinate and weight\n    # The actual edge checking will happen during request()\n    # when we compute the histogram from self._data\n    \n    self._data.append((coord, weight))", "def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    if not hasattr(self, '_hist') or self._hist is None:\n        # Initialize histogram if not already created\n        self._hist = lena.structures.histogram()\n    \n    # Check if coord is within the histogram edges\n    if hasattr(self._hist, 'edges') and self._hist.edges is not None:\n        min_edge, max_edge = self._hist.edges[0], self._hist.edges[-1]\n        if min_edge <= coord <= max_edge:\n            # Add the weight to the appropriate bin\n            self._hist.add(coord, weight)\n    \n    # If no histogram exists or coord is out of bounds, do nothing"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1989, "num_retrieved": 5, "inference_time": 0.8795414119958878, "generate_results": ["import re\nfrom typing import Dict, Any, List\n\ndef _validate_labels(labels: Dict[Any, Any]):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers: alphanumeric, hyphen, underscore, starting with letter or digit\n    KEY_REGEX = r'^[a-z][a-z0-9._-]*[a-z0-9]$'\n    # Label values must be strings (or bytes-like objects)\n    VALUE_REGEX = r'^[a-zA-Z0-9._-]+$'\n    \n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n        elif not re.match(KEY_REGEX, str(key)):\n            errors.append({str(key): f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                # Handle list/tuple values\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): \"expected string or bytes-like object\"})\n            else:\n                errors.append({str(value): \"expected string or bytes-like object\"})\n        else:\n            # Check if value matches the value regex\n            if not re.match(VALUE_REGEX, value):\n                errors.append({value: f\"Label value '{value}' does not match the regex {VALUE_REGEX}\"})\n    \n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List\n\n    # Regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, hyphen, underscore, starting with letter or digit)\n    KEY_REGEX = r'^[a-z][a-z0-9._-]*$'\n    # Label values must be strings (or bytes-like objects)\n    VALUE_REGEX = r'^[a-zA-Z0-9._-]+$'\n\n    errors = []\n\n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n            continue\n        \n        # Check if key matches regex\n        if not re.match(KEY_REGEX, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n\n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string or bytes-like object\n        if not isinstance(value, (str, bytes)):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n            continue\n        \n        # If value is bytes, convert to string\n        if isinstance(value", "import re\nfrom typing import Dict, Any, List\n\ndef _validate_labels(labels: Dict[Any, Any]):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, hyphen, underscore, starting with letter or digit)\n    KEY_REGEX = r'^[a-z][a-z0-9._-]*$'\n    # Label values must be strings (or bytes-like objects)\n    VALUE_REGEX = r'^[a-zA-Z0-9._-]+$'\n    \n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n        elif not re.match(KEY_REGEX, str(key)):\n            errors.append({str(key): f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                # Handle list or tuple values\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): \"expected string or bytes-like object\"})\n            else:\n                errors.append({str(value): \"expected string or bytes-like object\"})\n        else:\n            # Check if value matches the value regex\n            if not re.match(VALUE_REGEX, value):\n                errors.append({value: f\"Label value '{value}' does not match the regex {VALUE_REGEX}\"})\n    \n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List\n\n    # Regular expressions for label keys and values\n    # Label keys must be alphanumeric, with hyphens, and start with a letter or number\n    KEY_REGEX = r'^[a-z][a-z0-9_-]*$'\n    # Label values must be alphanumeric, with hyphens, dots, and underscores, and start with a letter or number\n    VALUE_REGEX = r'^[a-z0-9][a-z0-9_.-]*$'\n\n    errors = []\n\n    # Validate keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n        elif not re.match(KEY_REGEX, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex {KEY_REGEX}\"})\n\n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): \"expected string or bytes-like object\"})\n            else:\n                errors.append({str(value): \"expected string or", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    import re\n    from typing import Dict, Any, List\n\n    # Regular expressions for label keys and values\n    # Label keys must match: [a-z0-9]([a-z0-9.-]*[a-z0-9])? (valid Kubernetes label key)\n    KEY_REGEX = r'^[a-z][a-z0-9.-]*[a-z0-9]$'\n    # Label values must match: [a-z0-9]([a-z0-9._-]*[a-z0-9])? (valid Kubernetes label value)\n    VALUE_REGEX = r'^[a-z0-9]([a-z0-9._-]*[a-z0-9])?$'\n\n    errors = []\n\n    if not isinstance(labels, dict):\n        errors.append({\n            \"labels\": \"expected dict, got {}\".format(type(labels).__name__)\n        })\n    else:\n        for key, value in labels.items():\n            # Validate key\n            if not isinstance(key, str):\n                errors.append({\n                    str(key): 'expected string or bytes-like object'\n                })\n            elif not re.match(KEY_REGEX, key):\n                errors.append({\n                    key: f\"Label key '{key}' does not match the regex {KEY_REGEX}\"\n                })\n\n            # Validate value\n            if value is None:\n                errors.append({\n                    str(key): 'expected string or bytes-like object'\n                })\n            elif not isinstance(value, str):\n                if isinstance(value, (list, tuple)):\n                    for item in value:\n                        if not isinstance(item, str):\n                            errors.append({\n                                str(item): 'expected string or bytes-like object'\n                            })\n                else:\n                    errors.append({\n                        str(value): 'expected string or bytes-like object'\n                    })\n            else:\n                if not re.match(VALUE_REGEX, value):\n                    errors.append({"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2021, "num_retrieved": 5, "inference_time": 0.8795414119958878, "generate_results": ["import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming conventions:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and periods\n    # - Must start and end with a letter or number\n    # - Cannot start or end with a hyphen or period\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming conventions:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and dots\n    # - Must start with a letter or number\n    # - Must end with a letter or number\n    # - Cannot start or end with a hyphen or dot\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Kubernetes resource names must be:\n    # - 1-253 characters long\n    # - consist of lowercase alphanumeric characters, hyphens, and dots\n    # - start and end with an alphanumeric character\n    # - cannot start or end with a hyphen or dot\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming rules:\n    # - Must be 1-253 characters long\n    # - Must start and end with a letter or digit\n    # - Can only contain letters, digits, hyphens, and periods\n    # - Cannot start or end with a hyphen or period\n    # - Cannot contain consecutive hyphens or periods\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow Kubernetes naming conventions:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and periods\n    # - Must start and end with a letter or number\n    # - Cannot start or end with a hyphen or period\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1968, "num_retrieved": 3, "inference_time": 0.8795414119958878, "generate_results": ["def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from krake.data.constraints import ValidationError\n    \n    # Define regular expressions for different value types\n    # Example patterns (these should be defined according to actual requirements)\n    pattern = r'^[a-zA-Z0-9._-]+$'  # Example: alphanumeric, dots, underscores, hyphens\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected format\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from krake.data.exceptions import ValidationError\n    \n    # Define regular expressions for different value types\n    # Example patterns (adjust based on actual requirements)\n    pattern = r'^[a-zA-Z0-9._-]+$'  # Alphanumeric, dots, underscores, hyphens\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected format\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from krake.data.constraints import ValidationError\n    \n    # Define regular expressions for different value types\n    # Example patterns: numbers, strings, etc.\n    # Adjust these patterns based on actual requirements\n    \n    # Pattern for numeric values (integers and floats)\n    numeric_pattern = r'^-?\\d+(\\.\\d+)?$'\n    \n    # Pattern for strings (alphanumeric with optional underscores, hyphens, dots)\n    string_pattern = r'^[a-zA-Z0-9._-]+$'\n    \n    # Check if value matches any of the expected patterns\n    if not re.match(numeric_pattern, value) and not re.match(string_pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to expected format\")\n    \n    # Additional validation can be added based on specific requirements\n    if not value:\n        raise ValidationError(\"Value cannot be empty\")\n        \n    return True", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from krake.data.constraints import ValidationError\n    \n    # Define regular expressions for different value types\n    # Example patterns (adjust based on actual requirements)\n    pattern = r'^[a-zA-Z0-9._-]+$'  # Alphanumeric, dots, underscores, hyphens\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected format\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    from krake.data.exceptions import ValidationError\n    \n    # Define regular expressions for different value types\n    # Example patterns (these should be defined according to actual requirements)\n    pattern = r'^[a-zA-Z0-9._-]+$'  # Example: alphanumeric, dots, underscores, hyphens\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected format\")"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2024, "num_retrieved": 5, "inference_time": 0.8795414119958878, "generate_results": ["def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid key names (alphanumeric, underscore, dash, starting with letter or underscore)\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid keys: alphanumeric, underscore, dash, and dot\n    # Must start with a letter or underscore, followed by alphanumeric, underscore, dash, or dot\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid key names (alphanumeric, underscore, dash, starting with letter or digit)\n    pattern = r'^[a-zA-Z0-9][a-zA-Z0-9_\\-]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Regular expression for valid key names (alphanumeric, underscore, dash, starting with letter or underscore)\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid key names (alphanumeric, underscore, dash, starting with letter or underscore)\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1967, "num_retrieved": 3, "inference_time": 0.8795414119958878, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Process lists using the list version of the function\n            result[key] = generate_default_observer_schema_list(value, first_level=False)\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # In case of first_level dictionary, copy identifying fields from manifest\n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        except AttributeError:\n            pass\n        \n        try:\n            result[\"kind\"] = manifest_dict.get(\"kind\")\n        except AttributeError:\n            pass\n            \n        try:\n            metadata = manifest_dict.get(\"metadata\")\n            if metadata:\n                result[\"metadata\"] = {}\n                # Copy metadata fields, but only name is required\n                try:\n                    result[\"metadata\"][\"name\"] = metadata.get(\"name\")\n                except AttributeError:\n                    pass\n                # Recursively process metadata if it's a dict\n                if isinstance(metadata, dict):\n                    result[\"metadata\"] = generate_default_observer_schema_dict(metadata, first_level=False)\n        except AttributeError:\n            pass\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Process lists using the list version of the function\n            result[key] = generate_default_observer_schema_list(value, first_level=False)\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level is True, copy identifying fields from manifest\n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        except AttributeError:\n            pass\n        \n        try:\n            result[\"kind\"] = manifest_dict.get(\"kind\")\n        except AttributeError:\n            pass\n        \n        try:\n            metadata = manifest_dict.get(\"metadata\")\n            if metadata:\n                result[\"metadata\"] = {}\n                # Copy metadata fields, but only name is required\n                if \"name\" in metadata:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                # Recursively process metadata if it's a dict\n                if isinstance(metadata, dict):\n                    result[\"metadata\"] = generate_default_observer_schema_dict(metadata, first_level=False)\n        except (AttributeError, KeyError):\n            pass\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            if isinstance(metadata, dict):\n                try:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                except KeyError:\n                    pass\n                # Recursively process metadata\n                result[\"metadata\"] = generate_default_observer_schema_dict(\n                    metadata, first_level=False\n                )\n        except (KeyError, TypeError):\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            if isinstance(metadata, dict):\n                try:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                except KeyError:\n                    pass\n                # Recursively process metadata\n                result[\"metadata\"] = generate_default_observer_schema_dict(\n                    metadata, first_level=False\n                )\n        except (KeyError, TypeError):\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Process lists using the list version of the function\n            result[key] = generate_default_observer_schema_list(value, first_level=False)\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # In case of first_level dictionary, copy identifying fields from manifest\n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        except AttributeError:\n            pass\n        \n        try:\n            result[\"kind\"] = manifest_dict.get(\"kind\")\n        except AttributeError:\n            pass\n            \n        try:\n            metadata = manifest_dict.get(\"metadata\")\n            if metadata is not None:\n                result[\"metadata\"] = {}\n                # Copy metadata fields, but only name is required\n                if \"name\" in metadata:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                # Recursively process metadata if it's a dict\n                if isinstance(metadata, dict):\n                    result[\"metadata\"] = generate_default_observer_schema_dict(metadata, first_level=False)\n        except AttributeError:\n            pass\n    \n    return result"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1959, "num_retrieved": 2, "inference_time": 0.8795414119958878, "generate_results": ["def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If response does not have an element at this index, initialize in last_applied_manifest\n            if idx >= len(last_applied_manifest):\n                last_applied_manifest.append(None)\n            else:\n                last_applied_manifest[idx] = None\n        else:\n            # If the observed element is present in response, update last_applied_manifest\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            else:\n                # For scalar values, set the value from response if present\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n    \n    # Handle the last element (length constraints)\n    if len(last_applied_manifest) > 0 and len(response) > 0:\n        # Ensure length constraints are respected\n        if len(response) > 0:\n            last_applied_manifest[-1] = response[-1]", "def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If response does not have the element, we need to initialize in last_applied_manifest\n            if idx >= len(last_applied_manifest):\n                last_applied_manifest.append(value)\n            else:\n                # Only update if the value is not already present\n                if last_applied_manifest[idx] != value:\n                    last_applied_manifest[idx] = value\n        elif idx < len(response):\n            # If both observer_schema and response have the element, update if different\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            else:\n                if last_applied_manifest[idx] != response[idx]:\n                    last_applied_manifest[idx] = response[idx]\n    \n    # Check list length constraints\n    if len(response) > 0:\n        last_applied_manifest[-1] = response[-1]", "def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If response does not have the element, we need to add it to last_applied_manifest\n            if idx < len(last_applied_manifest):\n                # Only add if it's not already present\n                if last_applied_manifest[idx] != value:\n                    last_applied_manifest[idx] = value\n            else:\n                # Add new element to last_applied_manifest\n                last_applied_manifest.append(value)\n        elif idx < len(response):\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            else:\n                # For scalar values, update if different\n                if last_applied_manifest[idx] != response[idx]:\n                    last_applied_manifest[idx] = response[idx]\n    \n    # Check list length constraints at the end\n    if len(response) > 0:\n        last_length = response[-1].get(\"observer_schema_list_current_length\", 0)\n        min_length = observer_schema[-1].get(\"observer_schema_list_min_length\", 0)\n        max_length = observer_schema[-1].get(\"observer_schema_list_max_length\", 0)\n        \n        if last_length < min_length or last_length > max_length:\n            # This would be a validation error, but we just update based on response\n            pass", "def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If response does not have an element at this index, initialize in last_applied_manifest\n            if idx >= len(last_applied_manifest):\n                last_applied_manifest.append(None)\n            else:\n                last_applied_manifest[idx] = None\n        else:\n            # If the observed element is present in response, update last_applied_manifest\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            else:\n                # For scalar values, just assign from response if present\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n    \n    # Handle the last element (length constraints)\n    if len(last_applied_manifest) > 0:\n        last_applied_manifest[-1] = response[-1] if len(response) > 0 else None", "def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If response does not have the element, we need to add it to last_applied_manifest\n            if idx < len(last_applied_manifest):\n                # Only add if it's not already present\n                if last_applied_manifest[idx] != value:\n                    last_applied_manifest[idx] = value\n            else:\n                # Add new element to last_applied_manifest\n                last_applied_manifest.append(value)\n        elif idx < len(response):\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            else:\n                # For scalar values, update if different\n                if last_applied_manifest[idx] != response[idx]:\n                    last_applied_manifest[idx] = response[idx]\n    \n    # Check list length constraints\n    if len(response) > 0:\n        last_length = response[-1].get(\"observer_schema_list_current_length\", 0)\n        min_length = observer_schema[-1].get(\"observer_schema_list_min_length\", 0)\n        max_length = observer_schema[-1].get(\"observer_schema_list_max_length\", 0)\n        \n        if last_length < min_length or last_length > max_length:\n            # This would be an error condition, but we just update based on response\n            pass"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1959, "num_retrieved": 2, "inference_time": 0.8795414119958878, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for key, value in observer_schema.items():\n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = response.get(key)\n            if last_applied_manifest[key] is None:\n                raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for key, value in observer_schema.items():\n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = response.get(key)\n            if last_applied_manifest[key] is None:\n                raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for key, value in observer_schema.items():\n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = response.get(key)\n            if last_applied_manifest[key] is None:\n                raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for key, value in observer_schema.items():\n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = response.get(key)\n            if last_applied_manifest[key] is None:\n                raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for key, value in observer_schema.items():\n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = response.get(key)\n            if last_applied_manifest[key] is None:\n                raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2002, "num_retrieved": 4, "inference_time": 0.7678029835224152, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n    \n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        # Check if a custom observer schema is already defined for this resource\n        resource_observer_schema = app.spec.observer_schema.get(resource.get(\"kind\"), None)\n        if resource_observer_schema:\n            continue  # Skip if a custom schema is already defined\n            \n        # Generate default observer schema for the resource\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"api_version\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"] if \"metadata\" in resource and \"name\" in resource[\"metadata\"] else \"\",\n                \"namespace\": resource[\"metadata\"][\"namespace\"] if \"metadata\" in resource and \"namespace\" in resource[\"metadata\"] else \"\"\n            },\n            \"spec\": {\n                \"field_path\": \"spec\",\n                \"fields\": [\n                    {\"path\": \"metadata.name\", \"type\": \"string\", \"description\": \"Resource name\"},\n                    {\"path\": \"metadata.namespace\", \"type\": \"string\", \"description\": \"Resource namespace\"},\n                    {\"path\": \"metadata.labels\", \"type\": \"object\", \"description\": \"Resource labels\"},\n                    {\"path\": \"metadata.annotations\", \"type\": \"object\", \"description\": \"Resource annotations\"},\n                    {\"path\": \"spec\", \"type\": \"object\", \"description\": \"Resource spec fields\"}\n                ]\n            }\n        }\n        observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n    \n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        resource_name = resource.get(\"metadata\", {}).get(\"name\")\n        if not resource_name:\n            continue\n            \n        # Check if a custom observer schema is already defined for this resource\n        custom_schema = None\n        for schema in app.observer_schema or []:\n            if schema.get(\"resource\") == resource_name:\n                custom_schema = schema\n                break\n                \n        # If no custom schema exists, generate a default one\n        if not custom_schema:\n            default_schema = {\n                \"resource\": resource_name,\n                \"fields\": [\n                    \"metadata.name\",\n                    \"metadata.namespace\",\n                    \"metadata.labels\",\n                    \"metadata.annotations\",\n                    \"spec\",\n                    \"status\"\n                ]\n            }\n            observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n    \n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        resource_name = resource.get(\"metadata\", {}).get(\"name\")\n        if not resource_name:\n            continue\n            \n        # Check if a custom observer schema is already defined for this resource\n        custom_schema = None\n        for schema in app.observer_schema or []:\n            if schema.get(\"resource\") == resource_name:\n                custom_schema = schema\n                break\n                \n        # If no custom schema exists, generate a default one\n        if not custom_schema:\n            default_schema = {\n                \"resource\": resource_name,\n                \"fields\": [\n                    \"metadata.name\",\n                    \"metadata.namespace\",\n                    \"metadata.labels\",\n                    \"metadata.annotations\",\n                    \"spec\",\n                    \"status\"\n                ]\n            }\n            observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n    \n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        # Check if a custom observer schema is already defined for this resource\n        resource_observer_schema = app.spec.observer_schema.get(resource.get('kind'), None)\n        if resource_observer_schema is not None:\n            continue  # Skip if custom schema is already defined\n            \n        # Generate default observer schema for the resource\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"] if \"metadata\" in resource and \"name\" in resource[\"metadata\"] else \"default\",\n                \"namespace\": resource[\"metadata\"][\"namespace\"] if \"metadata\" in resource and \"namespace\" in resource[\"metadata\"] else \"default\"\n            },\n            \"spec\": {\n                \"fieldPath\": \"spec\"\n            },\n            \"status\": {\n                \"fieldPath\": \"status\"\n            }\n        }\n        observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = []\n    \n    # Iterate through each resource in the manifest\n    for resource in manifest:\n        # Check if a custom observer schema is already defined for this resource\n        resource_observer_schema = app.spec.observer_schema.get(resource.get(\"kind\"), None)\n        if resource_observer_schema is not None:\n            continue  # Skip if a custom schema is already defined\n            \n        # Generate default observer schema for the resource\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"api_version\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"] if \"metadata\" in resource and \"name\" in resource[\"metadata\"] else \"default\",\n                \"namespace\": resource[\"metadata\"][\"namespace\"] if \"metadata\" in resource and \"namespace\" in resource[\"metadata\"] else \"default\"\n            },\n            \"spec\": {\n                \"field_path\": \"spec\"\n            },\n            \"status\": {\n                \"field_path\": \"status\"\n            }\n        }\n        observer_schema.append(default_schema)\n    \n    return observer_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1976, "num_retrieved": 3, "inference_time": 0.7678029835224152, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = {}\n        # Convert named parameters\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    else:\n        # Handle sequence parameters\n        out_params = []\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = {}\n        # Convert named parameters\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    else:\n        # Handle sequence parameters\n        out_params = []\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = {}\n        # Convert named parameters\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    else:\n        # Handle sequence parameters\n        out_params = []\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = {}\n        # Convert named parameters\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        out_params = self.__convert_params([(params, param_conversions)])\n        return out_sql, out_params\n    else:\n        # Handle sequence parameters\n        out_params = []\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        out_params = self.__convert_params([(params, param_conversions)])\n        return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = {}\n        # Convert named parameters\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        \n        # Convert parameters\n        converted_params = self.__convert_params(params, param_conversions)\n        out_params = converted_params\n    else:\n        # Handle sequence parameters\n        out_params = []\n        param_conversions = []\n        in_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter), sql)\n        \n        # Convert parameters\n        converted_params = self.__convert_params(params, param_conversions)\n        out_params = converted_params\n    \n    return out_sql, out_params"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1976, "num_retrieved": 3, "inference_time": 0.7678029835224152, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1971, "num_retrieved": 4, "inference_time": 0.7678029835224152, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        validator.obj_fs = open_fs(path, create=False)\n        validator.validate(path)\n        return validator.status\n    except Exception as e:\n        validator.log.error('E001', where='validate', explanation=str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        validator.obj_fs = open_fs(path, create=False)\n        validator.validate(ocfl_opendir(self.root_fs, path))\n        return validator.status == 'valid'\n    except Exception as e:\n        validator.log.error('E001', where='validation', explanation=str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        validator.obj_fs = open_fs(path, create=False)\n        validator.validate(ocfl_opendir(self.root_fs, path))\n        return validator.status == 'valid'\n    except Exception as e:\n        validator.log.error('E001', where='validation', explanation=str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        validator.obj_fs = open_fs(path, create=False)\n        validator.validate(ocfl_opendir(self.root_fs, path))\n        return validator.status == 'valid'\n    except Exception as e:\n        validator.log.error('E001', where='validation', explanation=str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        validator.obj_fs = open_fs(path, create=False)\n        validator.validate(ocfl_opendir(self.root_fs, path))\n        return validator.status == 'valid'\n    except Exception as e:\n        validator.log.error('E001', where='validation', explanation=str(e))\n        return False"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1975, "num_retrieved": 4, "inference_time": 0.7678029835224152, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"{prefix}: {msg}\")\n        else:\n            messages.append(msg)\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"{prefix}: {msg}\")\n        else:\n            messages.append(msg)\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"{prefix}: {msg}\")\n        else:\n            messages.append(msg)\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"{prefix} {msg}\")\n        else:\n            messages.append(msg)\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"{prefix}: {msg}\")\n        else:\n            messages.append(msg)\n    return \"\\n\".join(messages) if messages else \"\""]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1955, "num_retrieved": 3, "inference_time": 0.7678029835224152, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1973, "num_retrieved": 4, "inference_time": 0.7678029835224152, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n        \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n        return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n        \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n        return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n        \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n        return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n        \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n        return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1930, "num_retrieved": 2, "inference_time": 0.7678029835224152, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Try to extract spec version from type value\n            type_value = inventory['type']\n            # Assuming type value follows a pattern like \"ocfl-1.0\" or similar\n            # This is a simplified extraction; actual logic depends on spec\n            if type_value.startswith('ocfl-'):\n                spec_version = type_value[5:]  # Extract version after \"ocfl-\"\n            # Additional validation for valid spec version format could be added\n        else:\n            # No type value or invalid type value, fall back to self.spec_version\n            pass\n    \n    # Create an inventory validator with the determined spec_version\n    inv_validator = InventoryValidator(\n        log=self.log,\n        where='root',\n        lax_digests=self.lax_digests,\n        spec_version=spec_version\n    )\n    \n    # Validate the inventory with the determined spec_version\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract spec version from type field (e.g., \"ocfl-1.0\")\n            type_value = inventory['type']\n            # Parse version from type value (e.g., \"ocfl-1.0\" -> \"1.0\")\n            import re\n            match = re.search(r'ocfl-(\\d+\\.\\d+)', type_value)\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If no valid version found, fall back to self.spec_version\n                pass\n        else:\n            # No type value, fall back to self.spec_version\n            pass\n    \n    # Create inventory validator with the determined spec_version\n    inv_validator = InventoryValidator(\n        log=self.log,\n        where='root',\n        lax_digests=self.lax_digests,\n        spec_version=spec_version\n    )\n    \n    # Validate the inventory with the determined spec_version\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type'] is not None:\n            # Try to extract spec version from type field\n            # Assuming type field contains version info like \"ocfl-1.0\" or similar\n            type_value = inventory['type']\n            # Simple parsing: extract version from type string\n            import re\n            match = re.search(r'ocfl-(\\d+\\.\\d+)', type_value)\n            if match:\n                spec_version = match.group(1)\n                # Validate version format (simple check)\n                try:\n                    version_parts = spec_version.split('.')\n                    if len(version_parts) == 2 and all(p.isdigit() for p in version_parts):\n                        pass  # Valid version\n                    else:\n                        self.log.error('E035', where='inventory', explanation='Invalid version format in type field')\n                        raise ValidatorAbortException\n                except Exception:\n                    self.log.error('E035', where='inventory', explanation='Invalid version format in type field')\n                    raise ValidatorAbortException\n            else:\n                self.log.warning('W011', where='inventory', explanation='No valid version found in type field')\n        else:\n            # No type value or invalid type value, fall back to self.spec_version\n            self.log.warning('W011', where='inventory', explanation='No type value provided for version extraction')\n    \n    # Create inventory validator with determined spec_version\n    inv_validator = InventoryValidator(\n        log=self.log,\n        where='inventory',\n        lax_digests=self.lax_digests,\n        spec_version=spec_version\n    )\n    \n    # Validate the inventory structure and content\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Try to extract spec version from type value\n            type_value = inventory['type']\n            # Assuming type value follows a pattern like \"ocfl-1.0\" or similar\n            # Extract version part (e.g., \"1.0\")\n            import re\n            match = re.search(r'ocfl-(\\d+\\.\\d+)', type_value)\n            if match:\n                spec_version = match.group(1)\n                # Validate version format (simple check)\n                try:\n                    version_parts = spec_version.split('.')\n                    if len(version_parts) == 2 and all(part.isdigit() for part in version_parts):\n                        pass  # Valid version\n                    else:\n                        self.log.error('E035', where='inventory', explanation='Invalid version format in type value')\n                        raise ValidatorAbortException\n                except Exception:\n                    self.log.error('E035', where='inventory', explanation='Invalid version format in type value')\n                    raise ValidatorAbortException\n            else:\n                self.log.error('E035', where='inventory', explanation='No valid version found in type value')\n                raise ValidatorAbortException\n        else:\n            # No type value or invalid type value\n            spec_version = self.spec_version\n    \n    # Validate inventory structure and content based on spec_version\n    # This is a placeholder - actual validation logic would depend on the spec_version\n    # and would include checks for required fields, format, etc.\n    \n    # Example: Check for required fields\n    required_fields = ['id', 'spec_version', 'manifest', 'fixity']\n    for field in required_fields:\n        if field not in inventory:\n            self.log.error('E036', where='inventory', missing_field=field)\n            raise ValidatorAbortException\n    \n    # Validate spec_version\n    if spec_version != self.spec_version:\n        self.log.warning('W011', where='inventory', spec_version=spec_version, expected_version=self.spec_version)", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Try to extract spec version from type field\n            type_value = inventory['type']\n            # Assuming type value is in format \"spec_version\" or similar\n            # For example, \"ocfl-1.0\" or \"1.0\"\n            import re\n            match = re.search(r'(\\d+\\.\\d+)', type_value)\n            if match:\n                spec_version = match.group(1)\n                # Validate version format (simple check)\n                try:\n                    version_parts = spec_version.split('.')\n                    if len(version_parts) == 2 and all(part.isdigit() for part in version_parts):\n                        spec_version = '.'.join(version_parts)\n                except:\n                    pass\n    \n    # Create inventory validator with determined spec_version\n    inv_validator = InventoryValidator(\n        log=self.log,\n        where='root',\n        lax_digests=self.lax_digests,\n        spec_version=spec_version\n    )\n    \n    # Validate the inventory structure and content\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1906, "num_retrieved": 1, "inference_time": 0.7678029835224152, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest_algorithm in digests_used:\n            for digest in digests_used[digest_algorithm]:\n                if filepath not in manifest_files:\n                    self.log.error('E024', where=filepath, digest=digest, algorithm=digest_algorithm)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for manifest_file in manifest_files:\n        for digest in digests_used:\n            if digest not in manifest_files[manifest_file]:\n                self.log.error('E045', manifest_file=manifest_file, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for manifest_file in manifest_files:\n        for digest in digests_used:\n            if digest not in manifest_files[manifest_file]:\n                self.log.error('E045', manifest_file=manifest_file, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest_algorithm in digests_used:\n            for digest in digests_used[digest_algorithm]:\n                if filepath not in manifest_files:\n                    self.log.error('E024', where=filepath, digest=digest, algorithm=digest_algorithm)\n    # Check that all digests in manifest_files are actually used\n    for filepath in manifest_files:\n        for digest_algorithm in manifest_files[filepath]:\n            for digest in manifest_files[filepath][digest_algorithm]:\n                if digest_algorithm not in digests_used or digest not in digests_used[digest_algorithm]:\n                    self.log.error('E025', where=filepath, digest=digest, algorithm=digest_algorithm)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for manifest_file in manifest_files:\n        for digest in digests_used:\n            if digest not in manifest_files[manifest_file]:\n                self.log.error('E045', manifest_file=manifest_file, digest=digest)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1907, "num_retrieved": 1, "inference_time": 0.7678029835224152, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version uses a version number that is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E037a', prior_spec_version=prior.spec_version, current_spec_version=self.spec_version)\n        return False\n    \n    # Ensure the prior version uses a digest algorithm that is compatible with the current version\n    # If the current version uses a newer digest algorithm, the prior version must use an older or same algorithm\n    if prior.digest_algorithm != self.digest_algorithm and prior.digest_algorithm not in self.supported_digest_algorithms:\n        self.log.error('E037c', prior_digest_algorithm=prior.digest_algorithm, current_digest_algorithm=self.digest_algorithm)\n        return False\n    \n    # Ensure that all files in the current inventory are present in the prior inventory\n    # This is a simplified check - in practice, the full manifest comparison would be needed\n    if hasattr(self, 'manifest') and hasattr(prior, 'manifest'):\n        current_manifest = self.manifest\n        prior_manifest = prior.manifest\n        \n        # Check that all files in current manifest exist in prior manifest\n        for digest in current_manifest:\n            for filepath in current_manifest[digest]:\n                if filepath not in prior_manifest:\n                    self.log.error('E023a', where=self.where, missing_filepath=filepath)\n                    return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Ensure the prior version uses a version number that is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E037a', prior_spec_version=prior.spec_version, current_spec_version=self.spec_version)\n    # Ensure the digest algorithm used in the prior version is compatible with the current version\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.warning('W011', prior_digest_algorithm=prior.digest_algorithm, current_digest_algorithm=self.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Ensure the prior version uses a version number that is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E037a', prior_spec_version=prior.spec_version, current_spec_version=self.spec_version)\n    # Ensure the digest algorithm in the prior version is compatible with the current version\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.warning('W011', prior_digest_algorithm=prior.digest_algorithm, current_digest_algorithm=self.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Ensure the prior version uses a version number that is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E037a', prior_spec_version=prior.spec_version, current_spec_version=self.spec_version)\n    # Ensure the digest algorithm used in the prior version is compatible with the current version\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.warning('W011', prior_digest_algorithm=prior.digest_algorithm, current_digest_algorithm=self.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Ensure the prior version uses a version number that is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E037a', prior_spec_version=prior.spec_version, current_spec_version=self.spec_version)\n    # Ensure the digest algorithm used in the prior version is compatible with the current version\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.warning('W011', prior_digest_algorithm=prior.digest_algorithm, current_digest_algorithm=self.digest_algorithm)"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1928, "num_retrieved": 2, "inference_time": 0.7678029835224152, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the first file in manifest for this digest (the base content file)\n        base_file = manifest[digest][0]\n        content_files = set()\n        \n        for logical_file in logical_files:\n            # For each logical file in this version, map it to the content file\n            # on disk (which is the base file for this digest)\n            content_files.add(base_file)\n        \n        # Map logical path to set of content files\n        logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the first file in manifest for this digest (the base file)\n        base_file = manifest[digest][0]\n        content_files = set()\n        \n        for logical_file in logical_files:\n            # For each logical file, add the corresponding content file\n            # The content file is the one that exists in the manifest for this digest\n            # and is used as the base for the logical file\n            content_files.add(base_file)\n        \n        # Map logical path to set of content files\n        logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the first file in manifest for this digest (representing the base content)\n        base_file = manifest[digest][0]\n        content_files = set()\n        \n        for logical_file in logical_files:\n            # For each logical file in state, map it to the content file on disk\n            # This is based on the manifest entry for the digest\n            content_files.add(base_file)\n        \n        # Map logical path to set of content files\n        logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the first file in manifest for this digest (representing the base content)\n        base_file = manifest[digest][0]\n        content_files = set()\n        \n        for logical_file in logical_files:\n            # For each logical file in this version's state, map it to the content file\n            # The content file is the one from the manifest (base file) or a later version\n            # Since we're only concerned with the content files on disk, we use the base file\n            # as the reference, but note that duplicates in later versions may be included\n            content_files.add(base_file)\n        \n        # Map logical path to set of content files\n        logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the first file in manifest for this digest (the base file)\n        base_file = manifest[digest][0]\n        content_files = set()\n        \n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # This is based on the mapping from logical path to content path\n            # In the context of the version, the content file is the one that was\n            # originally stored in the manifest for this digest\n            content_files.add(base_file)\n        \n        # Map logical path to the set of content files\n        logical_path_map[logical_file] = content_files\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1906, "num_retrieved": 1, "inference_time": 0.7678029835224152, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that fixity block has the expected structure\n    if not isinstance(fixity, dict):\n        raise ValueError(\"Fixity block must be a dictionary\")\n    \n    # Validate that all digest algorithms are present and have valid digest values\n    for digest_algorithm in fixity:\n        if not isinstance(digest_algorithm, str):\n            raise ValueError(f\"Invalid digest algorithm: {digest_algorithm}\")\n        \n        if not isinstance(fixity[digest_algorithm], dict):\n            raise ValueError(f\"Fixity block for algorithm {digest_algorithm} must be a dictionary\")\n        \n        for unnormalized_digest in fixity[digest_algorithm]:\n            if not isinstance(unnormalized_digest, str):\n                raise ValueError(f\"Invalid digest value: {unnormalized_digest}\")\n            \n            if not isinstance(fixity[digest_algorithm][unnormalized_digest], list):\n                raise ValueError(f\"Fixity value for {digest_algorithm}:{unnormalized_digest} must be a list of file paths\")\n            \n            # Check that all file paths in fixity are present in manifest_files\n            for filepath in fixity[digest_algorithm][unnormalized_digest]:\n                if filepath not in manifest_files:\n                    raise ValueError(f\"Fixity references file {filepath} not present in manifest\")", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that fixity block has the expected structure\n    if not isinstance(fixity, dict):\n        raise ValueError(\"Fixity block must be a dictionary\")\n    \n    # Validate that fixity contains only digest algorithms as keys\n    for algorithm in fixity:\n        if not isinstance(algorithm, str):\n            raise ValueError(f\"Invalid digest algorithm: {algorithm}\")\n        \n        # Validate that each algorithm has a dictionary of digests\n        if not isinstance(fixity[algorithm], dict):\n            raise ValueError(f\"Fixity block for algorithm {algorithm} must be a dictionary\")\n        \n        # Check that all digests are valid and only reference files in manifest\n        for digest in fixity[algorithm]:\n            if not isinstance(digest, str):\n                raise ValueError(f\"Invalid digest: {digest}\")\n            \n            # Extract the file path from the digest (assuming format like 'path/to/file:digest')\n            # This is a simplified assumption; actual parsing depends on the format\n            try:\n                filepath, _ = digest.split(':', 1)\n            except ValueError:\n                raise ValueError(f\"Invalid digest format: {digest}\")\n            \n            # Ensure the file path is in the manifest_files\n            if filepath not in manifest_files:\n                raise ValueError(f\"Fixity block references file {filepath} not present in manifest\")", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity or not isinstance(fixity, dict):\n        return False\n    \n    # Check that fixity block has the expected structure\n    for digest_algorithm in fixity:\n        if not isinstance(digest_algorithm, str):\n            return False\n        if not isinstance(fixity[digest_algorithm], dict):\n            return False\n        for unnormalized_digest in fixity[digest_algorithm]:\n            if not isinstance(unnormalized_digest, str):\n                return False\n            if not isinstance(fixity[digest_algorithm][unnormalized_digest], str):\n                return False\n    \n    # Ensure all referenced files are in the manifest\n    referenced_files = set()\n    for digest_algorithm in fixity:\n        for unnormalized_digest in fixity[digest_algorithm]:\n            # Extract the file path from the digest (assuming format like \"file_path:digest\")\n            # This assumes the format is \"file_path:digest\" or similar\n            # If the format is different, adjust accordingly\n            if ':' in unnormalized_digest:\n                file_path = unnormalized_digest.split(':', 1)[0]\n                referenced_files.add(file_path)\n    \n    # Check if all referenced files are in manifest_files\n    missing_files = referenced_files - set(manifest_files)\n    if missing_files:\n        return False\n    \n    return True", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that fixity block has the expected structure\n    if not isinstance(fixity, dict):\n        raise ValueError(\"Fixity block must be a dictionary\")\n    \n    # Validate that all digest algorithms are present and have valid digest values\n    for digest_algorithm in fixity:\n        if not isinstance(digest_algorithm, str):\n            raise ValueError(f\"Invalid digest algorithm: {digest_algorithm}\")\n        \n        if not isinstance(fixity[digest_algorithm], dict):\n            raise ValueError(f\"Fixity block for algorithm {digest_algorithm} must be a dictionary\")\n        \n        for unnormalized_digest in fixity[digest_algorithm]:\n            if not isinstance(unnormalized_digest, str):\n                raise ValueError(f\"Invalid digest value: {unnormalized_digest}\")\n            \n            if not isinstance(fixity[digest_algorithm][unnormalized_digest], list):\n                raise ValueError(f\"Fixity value for {digest_algorithm}:{unnormalized_digest} must be a list of file paths\")\n            \n            # Check that all file paths in fixity are present in manifest_files\n            for filepath in fixity[digest_algorithm][unnormalized_digest]:\n                if filepath not in manifest_files:\n                    raise ValueError(f\"Fixity references file path {filepath} not present in manifest files\")", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that fixity block has the expected structure\n    if not isinstance(fixity, dict):\n        raise ValueError(\"Fixity block must be a dictionary\")\n    \n    # Validate that all digest algorithms are present and have valid digest values\n    for digest_algorithm in fixity:\n        if not isinstance(digest_algorithm, str):\n            raise ValueError(f\"Invalid digest algorithm: {digest_algorithm}\")\n        \n        if not isinstance(fixity[digest_algorithm], dict):\n            raise ValueError(f\"Fixity block for algorithm {digest_algorithm} must be a dictionary\")\n        \n        for unnormalized_digest in fixity[digest_algorithm]:\n            if not isinstance(unnormalized_digest, str):\n                raise ValueError(f\"Invalid digest value: {unnormalized_digest}\")\n            \n            if not isinstance(fixity[digest_algorithm][unnormalized_digest], list):\n                raise ValueError(f\"Fixity value for {digest_algorithm}:{unnormalized_digest} must be a list of file paths\")\n            \n            # Check that all referenced file paths are in the manifest_files\n            for filepath in fixity[digest_algorithm][unnormalized_digest]:\n                if filepath not in manifest_files:\n                    raise ValueError(f\"Fixity block references file path '{filepath}' not listed in manifest\")"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2046, "num_retrieved": 6, "inference_time": 0.7678029835224152, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1939, "num_retrieved": 2, "inference_time": 0.7678029835224152, "generate_results": ["def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    result = {}\n    for file in files:\n        if os.path.splitext(file)[1] == \".xml\":\n            basename = os.path.splitext(file)[0]\n            if basename in xmls:\n                if basename not in result:\n                    result[basename] = []\n                result[basename].append(file)\n    return result", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    result = {}\n    for file in files:\n        if os.path.splitext(file)[1] == \".xml\":\n            basename = os.path.splitext(file)[0]\n            if basename in xmls:\n                if basename not in result:\n                    result[basename] = []\n                result[basename].append(file)\n    return result", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    result = {}\n    for file in files:\n        if os.path.splitext(file)[1] == \".xml\":\n            basename = os.path.splitext(file)[0]\n            if basename in xmls:\n                if basename not in result:\n                    result[basename] = []\n                result[basename].append(file)\n    return result", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    result = {}\n    for file in files:\n        if os.path.splitext(file)[1] == \".xml\":\n            basename = os.path.splitext(file)[0]\n            if basename in xmls:\n                if basename not in result:\n                    result[basename] = []\n                result[basename].append(file)\n    return result", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    result = {}\n    for file in files:\n        if os.path.splitext(file)[1] == \".xml\":\n            basename = os.path.splitext(file)[0]\n            if basename in xmls:\n                if basename not in result:\n                    result[basename] = []\n                result[basename].append(file)\n    return result"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2077, "num_retrieved": 7, "inference_time": 0.7678029835224152, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and (file == prefix or any(file == f'{prefix}-{lang}' for lang in pdf_langs)))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and (file == prefix or any(file == f'{prefix}-{lang}' for lang in pdf_langs)))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and (file == prefix or any(file == f'{prefix}-{lang}' for lang in pdf_langs)))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and (file == prefix or any(file == f'{prefix}-{lang}' for lang in pdf_langs)))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and (file == prefix or any(file == f'{prefix}-{lang}' for lang in pdf_langs)))"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2023, "num_retrieved": 6, "inference_time": 0.7678029835224152, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    result = []\n    for f in files:\n        basename = os.path.basename(f)\n        if basename.startswith(prefix):\n            result.append(f)\n    return result"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2139, "num_retrieved": 9, "inference_time": 1.002293512225151, "generate_results": ["def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Dictionary to group files by XML basename\n    xml_basenames = {}\n    \n    for file in files:\n        if file.is_file():\n            # Extract filename without extension\n            filename = file.name\n            file_ext = file.suffix.lower()\n            \n            # Skip files that are not XML or have no extension\n            if file_ext != '.xml':\n                # Try to determine XML basename from file name\n                # If file is a rendition or asset, extract the basename from the XML name\n                # We assume that XML files have the same basename as the other files\n                # and that the XML file name is the base name of the package\n                continue\n            \n            # Extract XML basename (without extension)\n            xml_basename = filename[:-4]  # Remove .xml extension\n            xml_basenames[xml_basename] = file\n    \n    # For each XML file, collect all related files (renditions and assets)\n    for xml_basename, xml_file in xml_basenames.items():\n        package_data[xml_basename] = {\n            'xml': str(xml_file),\n            'renditions': [],\n            'assets': []\n        }\n        \n        # Look for rendition files (typically with .pdf, .jpg, etc. extensions)\n        for file in files:\n            if file.is_file() and file.suffix.lower() in ['.pdf', '.jpg', '.jpeg', '.png', '.gif']:\n                filename = file.name\n                # Check if the filename starts with the XML basename\n                if filename.startswith(xml_basename):\n                    if file.suffix.lower() == '.pdf':\n                        package_data[xml_basename]['renditions'].append(str(file))\n                    else:\n                        package_data[xml_basename]['assets'].append(str(file))\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Dictionary to group files by XML basename\n    xml_basenames = {}\n    \n    for file in files:\n        if file.is_file():\n            # Extract filename without extension\n            filename = file.name\n            file_ext = file.suffix.lower()\n            \n            # Skip files that don't have an extension\n            if not file_ext:\n                continue\n                \n            # Try to determine if this is an XML file\n            if file_ext == '.xml':\n                xml_basename = filename.replace('.xml', '')\n                xml_basenames[xml_basename] = []\n            \n            # If it's a rendition or asset, check if it has a matching XML basename\n            # by looking at the filename pattern\n            if file_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.tif', '.tiff']:\n                # Extract the basename from the filename\n                # Assume the pattern is: <xml_basename>-<type>-<id>.extension\n                name_parts = filename.split('-')\n                if len(name_parts) >= 3:\n                    # Check if the first part matches the XML basename\n                    potential_xml_basename = name_parts[0]\n                    if potential_xml_basename in xml_basenames:\n                        xml_basenames[potential_xml_basename].append(str(file))\n                    else:\n                        # Try to match with the second part\n                        if len(name_parts) >= 2:\n                            # Try to match with the second part\n                            potential_xml_basename = name_parts[1]\n                            if potential_xml_basename in xml_basenames:\n                                xml_basenames[potential_xml_basename].append(str(file))\n            \n            # For other extensions, just add to the first found XML basename\n            if file_ext not in ['.xml', '.pdf', '.jpg', '.jpeg', '.png', '.gif', '.tif', '.tiff']:\n                # Try to match with the first part of the filename\n                name_parts = filename.split('-')\n                if name_parts:\n                    potential_xml_basename", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Dictionary to group files by XML basename\n    xml_basenames = {}\n    \n    for file in files:\n        if file.is_file():\n            # Extract filename without extension\n            filename = file.name\n            file_ext = file.suffix.lower()\n            \n            # Skip files that don't have extensions\n            if not file_ext:\n                continue\n                \n            # Try to determine if this is an XML file\n            if file_ext == '.xml':\n                xml_basename = filename.replace('.xml', '')\n                xml_basenames[xml_basename] = []\n            \n            # If it's a rendition or asset, check if it has a matching XML basename\n            # by looking at the filename pattern (e.g., \"1414-431X-bjmbr-54-10-e11439.pdf\")\n            if file_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']:\n                # Extract the base name from the filename\n                name_parts = filename.split('.')\n                base_name = '.'.join(name_parts[:-1])\n                \n                # Check if the base name matches any XML basename\n                for xml_base in xml_basenames:\n                    if base_name.startswith(xml_base):\n                        xml_basenames[xml_base].append(str(file))\n                        break\n    \n    # Build the final package data\n    for xml_base in xml_basenames:\n        package_data[xml_base] = {\n            'xml': None,\n            'renditions': [],\n            'assets': [],\n            'zip': None\n        }\n        \n        # Look for XML file with matching basename\n        xml_file = None\n        for f in files:\n            if f.is_file() and f.suffix.lower() == '.xml':\n                if f.name.replace('.xml', '') == xml_base:\n                    xml_file = str(f)\n                    break\n        \n        if xml_file:\n            package_data[xml_base]['xml'] = xml_file\n        \n        # Add", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Dictionary to group files by XML basename\n    xml_basenames = {}\n    \n    for file in files:\n        if file.is_file():\n            # Extract filename without extension\n            filename = file.name\n            file_ext = file.suffix.lower()\n            \n            # Skip files that don't have an extension\n            if not file_ext:\n                continue\n                \n            # Try to determine if this is an XML file\n            if file_ext == '.xml':\n                xml_basename = filename.replace('.xml', '')\n                xml_basenames[xml_basename] = []\n            \n            # If it's a rendition or asset, check if it has a matching XML basename\n            elif file_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']:\n                # Extract basename without extension\n                base_name = filename.replace(file_ext, '')\n                # Check if base_name matches any XML basename\n                for xml_base in xml_basenames:\n                    if base_name.startswith(xml_base):\n                        xml_basenames[xml_base].append(str(file))\n                        break\n    \n    # Build the package data dictionary\n    for xml_basename, files_list in xml_basenames.items():\n        package_data[xml_basename] = {\n            'xml': None,\n            'renditions': [],\n            'assets': [],\n            'zip': None\n        }\n        \n        # Look for XML file with matching basename\n        xml_file = None\n        for file in files:\n            if file.is_file() and file.suffix.lower() == '.xml':\n                if file.name.replace('.xml', '') == xml_basename:\n                    xml_file = str(file)\n                    break\n        \n        if xml_file:\n            package_data[xml_basename]['xml'] = {\n                'uri': xml_file,\n                'name': file.name\n            }\n        \n        # Group renditions and assets\n        for file in files:\n            if file.is_file():\n                file_ext = file.suffix.lower()\n                if file_ext in ['.pdf", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Dictionary to group files by XML basename\n    xml_basenames = {}\n    \n    for file in files:\n        if file.is_file():\n            # Extract filename without extension\n            filename = file.name\n            file_ext = file.suffix.lower()\n            \n            # Skip files that don't have extensions\n            if not file_ext:\n                continue\n                \n            # Try to determine if this is an XML file\n            if file_ext == '.xml':\n                xml_basename = filename.replace('.xml', '')\n                xml_basenames[xml_basename] = []\n            \n            # If it's a rendition or asset, check if it has a matching XML basename\n            # by looking at the filename pattern (e.g., \"1414-431X-bjmbr-54-10-e11439.pdf\")\n            if file_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']:\n                # Extract the base name from the filename\n                name_parts = filename.split('.')\n                base_name = '.'.join(name_parts[:-1])\n                \n                # Check if the base name matches an XML basename\n                for xml_base in xml_basenames:\n                    if base_name.startswith(xml_base):\n                        xml_basenames[xml_base].append({\n                            'uri': str(file),\n                            'name': filename\n                        })\n                        break\n    \n    # Build the final package data\n    for xml_base in xml_basenames:\n        package_data[xml_base] = {\n            'xml': {\n                'uri': f\"{folder_path}/{xml_base}.xml\",\n                'name': f\"{xml_base}.xml\"\n            },\n            'renditions': [],\n            'assets': []\n        }\n        \n        # Group renditions and assets\n        for item in xml_basenames[xml_base]:\n            if 'pdf' in item['name'].lower():\n                package_data[xml_base]['renditions'].append(item)\n            else:"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2118, "num_retrieved": 8, "inference_time": 1.002293512225151, "generate_results": ["def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2028, "num_retrieved": 5, "inference_time": 1.002293512225151, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um arquivo de rendio ao pacote com base no idioma e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com informaes sobre a rendio, incluindo o nome e o caminho\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = os.path.splitext(file_name)[0]\n    \n    # Determina o papel do arquivo usando a funo get_file_role\n    file_role = file_utils.get_file_role(file_path, prefix, [])\n    \n    # Constri o nome do arquivo com o idioma\n    rendition_name = f\"{prefix}-{lang}.{os.path.splitext(file_path)[1]}\"\n    \n    return {\n        'uri': file_path,\n        'name': rendition_name,\n        'lang': lang,\n        'role': file_role\n    }", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um arquivo de rendio ao pacote com base no idioma e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com informaes sobre a rendio, incluindo nome e URI\n    \"\"\"\n    # Extrai o prefixo do arquivo (antes da extenso)\n    file_name, ext = os.path.splitext(file_path)\n    \n    # Define o nome base do arquivo (sem extenso)\n    base_name = file_name\n    \n    # Se for o idioma original, usa o nome original\n    if lang == 'original':\n        rendition_name = base_name + ext\n    else:\n        # Para outros idiomas, adiciona o cdigo de idioma\n        rendition_name = f\"{base_name}-{lang}{ext}\"\n    \n    return {\n        'uri': file_path,\n        'name': rendition_name\n    }", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um arquivo de rendio ao pacote com base no idioma e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com informaes sobre a rendio, incluindo nome e URI\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = os.path.splitext(file_name)[0]\n    \n    # Verifica se  um PDF de rendio com base no prefixo e idioma\n    if file_path.endswith('.pdf'):\n        # Se o arquivo for do tipo original (sem idioma) ou com idioma\n        if lang == 'original' or lang == 'pt':\n            # Para idiomas padro, mantm o nome original\n            rendition_name = file_name\n        else:\n            # Para outros idiomas, adiciona o idioma no nome\n            rendition_name = f\"{prefix}-{lang}.pdf\"\n        \n        return {\n            'uri': file_path,\n            'name': rendition_name\n        }\n    \n    return None", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um arquivo de rendio ao pacote com base no idioma e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com informaes sobre a rendio, incluindo o nome e o caminho\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = os.path.splitext(file_name)[0]\n    \n    # Determina o papel do arquivo usando a funo get_file_role\n    file_role = file_utils.get_file_role(file_path, prefix, [])\n    \n    # Constri o nome do arquivo com o idioma\n    rendition_name = f\"{prefix}-{lang}.{os.path.splitext(file_path)[1]}\"\n    \n    return {\n        'uri': file_path,\n        'name': rendition_name,\n        'lang': lang,\n        'role': file_role\n    }", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um arquivo de rendio ao pacote com base no idioma e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com informaes sobre a rendio, incluindo o nome e o caminho\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = os.path.splitext(file_name)[0]\n    \n    # Determina o papel do arquivo usando a funo get_file_role\n    file_role = file_utils.get_file_role(file_path, prefix, [])\n    \n    # Constri o nome do arquivo com o idioma\n    rendition_name = f\"{prefix}-{lang}.{os.path.splitext(file_path)[1]}\"\n    \n    return {\n        'uri': file_path,\n        'name': rendition_name,\n        'role': file_role\n    }"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2088, "num_retrieved": 7, "inference_time": 1.002293512225151, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\n    \n    Args:\n        basename (str): The base name of the asset (without extension).\n        file_path (str): The full path to the asset file.\n    \n    Returns:\n        dict: A dictionary with the asset name as key and file path as value.\n    \"\"\"\n    # Extract file extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the file extension\n    if file_extension in ['.tiff', '.tif']:\n        asset_type = \"original\"\n    elif file_extension in ['.png']:\n        asset_type = \"optimised\"\n    elif file_extension in ['.jpg', '.jpeg']:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create the full asset name with extension\n    asset_name = f\"{basename}{file_extension}\"\n    \n    # Store the asset in the assets collection\n    self._assets[asset_name] = file_path\n    \n    # Optionally, generate optimised versions if needed\n    if asset_type == \"original\" and file_extension in ['.tiff', '.tif']:\n        # Convert TIFF to PNG if not already done\n        try:\n            png_path = self.convert2png(destination_path=None)\n            self._assets[f\"{basename}.png\"] = png_path\n        except Exception as e:\n            # Log or handle error appropriately\n            pass\n    \n    return self._assets", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\"\"\"\n    # Extract the file extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the file extension\n    if file_extension in [\".tiff\", \".tif\"]:\n        asset_type = \"original\"\n    elif file_extension in [\".png\"]:\n        asset_type = \"optimised\"\n    elif file_extension in [\".jpg\", \".jpeg\"]:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create the asset entry with the basename and file path\n    asset_key = basename\n    asset_value = file_path\n    \n    # Store the asset in the assets dictionary\n    self.assets[asset_key] = {\n        \"file_path\": asset_value,\n        \"type\": asset_type\n    }\n    \n    return asset_key", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\"\"\"\n    # Extract the file extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the file extension\n    if file_extension in ['.tiff', '.tif']:\n        asset_type = \"original\"\n    elif file_extension in ['.png']:\n        asset_type = \"optimised\"\n    elif file_extension in ['.jpg', '.jpeg']:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create the asset entry with basename and file path\n    asset_entry = {\n        'basename': basename,\n        'file_path': file_path,\n        'type': asset_type\n    }\n    \n    # Add the asset to the assets collection (assuming self.assets is a dictionary or list)\n    if not hasattr(self, 'assets'):\n        self.assets = {}\n    \n    self.assets[basename] = asset_entry", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\"\"\"\n    # Extract the file extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the file extension\n    if file_extension in [\".tiff\", \".tif\"]:\n        asset_type = \"original\"\n    elif file_extension in [\".png\"]:\n        asset_type = \"optimised\"\n    elif file_extension in [\".jpg\", \".jpeg\"]:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create a key for the asset using the basename\n    asset_key = basename\n    \n    # Store the asset in the assets collection\n    self.assets[asset_key] = {\n        'file_path': file_path,\n        'extension': file_extension,\n        'type': asset_type\n    }", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\n    \n    Args:\n        basename (str): The base name of the asset (without extension).\n        file_path (str): The full path to the asset file.\n    \n    Returns:\n        dict: A dictionary mapping the asset name to its file path.\n    \"\"\"\n    asset_name = basename + os.path.splitext(file_path)[1]\n    self._assets[asset_name] = file_path\n    return {asset_name: file_path}"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2072, "num_retrieved": 6, "inference_time": 1.002293512225151, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            result = {}\n            \n            for filename in filenames:\n                if filename.endswith(\".xml\"):\n                    basename = os.path.splitext(filename)[0]\n                    if basename not in result:\n                        result[basename] = {\n                            \"xml\": None,\n                            \"assets\": [],\n                            \"renditions\": []\n                        }\n                    result[basename][\"xml\"] = filename\n                elif filename.endswith((\".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\")):\n                    # Extract base name without extension\n                    base_name = os.path.splitext(filename)[0]\n                    # Group by XML basename (assuming the XML basename matches the base name of the file)\n                    xml_basename = base_name\n                    if xml_basename in result:\n                        if filename.endswith(\".pdf\"):\n                            result[xml_basename][\"renditions\"].append(filename)\n                        else:\n                            result[xml_basename][\"assets\"].append(filename)\n                    else:\n                        # Create new entry if not found\n                        result[xml_basename] = {\n                            \"xml\": None,\n                            \"assets\": [],\n                            \"renditions\": []\n                        }\n                        if filename.endswith(\".pdf\"):\n                            result[xml_basename][\"renditions\"].append(filename)\n                        else:\n                            result[xml_basename][\"assets\"].append(filename)\n            \n            # Remove entries without any XML file\n            result = {k: v for k, v in result.items() if v[\"xml\"]}\n            \n            return result\n            \n    except Exception as e:\n        LOGGER.exception(e)\n        raise IOError(_(\"Unable to explore zip file {}: {}\").format(zip_path, e))", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            if not xml_files:\n                return {}\n                \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(os.path.basename(xml_file))[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files with the same basename\n                for file in filenames:\n                    if basename in file and file != xml_file:\n                        result[basename][\"files\"].append(file)\n            \n            return result\n    except Exception as e:\n        LOGGER.exception(e)\n        raise IOError(_(\"Unable to explore zip file {}: {}\").format(zip_path, e))", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            if not xml_files:\n                return {}\n                \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(os.path.basename(xml_file))[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files with the same basename\n                for file in filenames:\n                    if basename in file and not file.endswith(\".xml\"):\n                        result[basename][\"files\"].append(file)\n            \n            return result\n    except Exception as e:\n        LOGGER.exception(e)\n        raise IOError(_(\"Unable to explore zip file {}: {}\").format(zip_path, e))", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            if not xml_files:\n                return {}\n                \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(os.path.basename(xml_file))[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files with the same basename\n                for file in filenames:\n                    if basename in file and file.endswith((\".xml\", \".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".zip\", \".txt\", \".json\", \".xml\")):\n                        result[basename][\"files\"].append(file)\n            \n            return result\n    except Exception as e:\n        LOGGER.exception(e)\n        raise IOError(_(\"Unable to explore zip file {}: {}\").format(zip_path, e))", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            if not xml_files:\n                return {}\n                \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(os.path.basename(xml_file))[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files with the same basename\n                for file in filenames:\n                    if basename in file and file.endswith((\".xml\", \".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".zip\")):\n                        result[basename][\"files\"].append(file)\n            \n            return result\n            \n    except Exception as e:\n        LOGGER.exception(e)\n        raise IOError(_(\"Unable to explore zip file {}: {}\").format(zip_path, e))"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1993, "num_retrieved": 4, "inference_time": 1.002293512225151, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    \n    files = []\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        files = zip_file.namelist()\n    \n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    \n    files = []\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        files = zip_file.namelist()\n    \n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    \n    files = []\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        files = zip_file.namelist()\n    \n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    \n    files = []\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        files = zip_file.namelist()\n    \n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    import zipfile\n    \n    files = []\n    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n        files = zip_file.namelist()\n    \n    return files"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2220, "num_retrieved": 11, "inference_time": 1.002293512225151, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2092, "num_retrieved": 7, "inference_time": 1.002293512225151, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2231, "num_retrieved": 12, "inference_time": 1.002293512225151, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by whitespace and filter out empty parts\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to find number and supplement\n    number = None\n    supplement = \"\"\n    \n    for part in parts:\n        # Check if the part contains a number (e.g., \"2\", \"3\", \"12\")\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # If no number found, try to extract supplement (e.g., \"suppl1\", \"suppl2\")\n    if number is None:\n        # Look for patterns like \"suppl1\", \"suppl2\", etc.\n        supplement_match = re.search(r'suppl(\\d+)', issue, re.IGNORECASE)\n        if supplement_match:\n            supplement = f\"suppl{supplement_match.group(1)}\"\n    \n    # If no number or supplement found, return default values\n    if number is None:\n        number = 0\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # The supplement is typically after a letter or at the end of the string\n    issue_parts = issue.strip().split('.')\n    \n    number = 0\n    supplement = \"\"\n    \n    # Try to extract the number part (first part before any supplement)\n    if len(issue_parts) > 0:\n        number_part = issue_parts[0]\n        if number_part.isdigit():\n            number = int(number_part)\n    \n    # Check if there's a supplement (e.g., \"suppl1\", \"suppl\", \"s1\", \"s2\")\n    if len(issue_parts) > 1:\n        supplement_part = issue_parts[1]\n        # Look for patterns like \"suppl\", \"s\", \"suppl1\", \"s1\", etc.\n        if supplement_part.lower().startswith('suppl') or supplement_part.lower().startswith('s'):\n            supplement = supplement_part\n    \n    # If no number found, try to extract from the full string\n    if number == 0:\n        # Try to find the last part that might be a supplement\n        for part in reversed(issue_parts):\n            if part.lower().startswith('suppl') or part.lower().startswith('s'):\n                supplement = part\n                break\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by whitespace and filter out empty parts\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to find number and supplement\n    number = None\n    supplement = \"\"\n    \n    for part in parts:\n        # Check if the part contains a number (e.g., \"2\", \"3\", \"4\")\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # If no number found, try to extract supplement (e.g., \"suppl1\", \"suppl2\")\n    if number is None:\n        # Look for patterns like \"suppl1\", \"suppl2\", etc.\n        supplement_match = re.search(r'suppl(\\d+)', issue, re.IGNORECASE)\n        if supplement_match:\n            supplement = f\"suppl{supplement_match.group(1)}\"\n    \n    # If no number or supplement found, return default values\n    if number is None:\n        number = 0\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # The supplement is typically after a letter or at the end of the string\n    issue_parts = issue.strip().split('.')\n    \n    # Try to extract number from the first part\n    number_part = issue_parts[0] if issue_parts else \"\"\n    \n    # Check if the number part is numeric\n    if number_part.isdigit():\n        number = int(number_part)\n    else:\n        number = 0\n    \n    # Extract supplement from the remaining parts (if any)\n    supplement = \"\"\n    if len(issue_parts) > 1:\n        supplement_part = issue_parts[1]\n        # Supplement is typically a letter or a string like 'suppl1', 'suppl2'\n        if supplement_part.startswith('suppl') or supplement_part.startswith('sup'):\n            supplement = supplement_part\n        else:\n            # If it's a numeric supplement, just take the part\n            supplement = supplement_part\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by whitespace and filter out empty parts\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    number = 0\n    supplement = \"\"\n    \n    # Try to find the number and supplement in the issue string\n    # Number is typically a digit, and supplement is usually a letter or a string like 'suppl1'\n    for part in parts:\n        # Check if part is a number\n        if part.isdigit():\n            number = int(part)\n        # Check if part is a supplement (e.g., 'suppl', 'suppl1', 'suppl2')\n        elif part.lower() in ['suppl', 'suppl1', 'suppl2', 'suppl3', 'suppl4', 'suppl5']:\n            supplement = part\n        # If part is a digit followed by a letter, it might be a supplement\n        elif re.match(r'\\d+[a-z]', part):\n            supplement = part\n    \n    # If no number found, try to extract from the last part\n    if number == 0:\n        # Try to extract number from the last part if it's a digit\n        if parts and parts[-1].isdigit():\n            number = int(parts[-1])\n    \n    # If no supplement found, try to extract from the last part\n    if not supplement:\n        # Look for any part that starts with 'suppl'\n        for part in parts:\n            if part.lower().startswith('suppl'):\n                supplement = part\n    \n    return number, supplement"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 1.002293512225151, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return \"NOT \" + self.children[0].pretty(indent + 1, debug)\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT {self.expression.pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expression.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expression.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 1.002293512225151, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it matches absorption or negative absorption patterns\n        # This is a simplified implementation assuming expressions are in a form that can be evaluated\n        \n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using Boolean algebra rules\n        # We'll assume the expressions are already in a form that can be processed\n        # and that the absorption rules can be applied directly\n        \n        # In practice, this would require parsing and evaluating the expression tree\n        # For now, we return the original expression as a placeholder\n        # A full implementation would require recursive evaluation of the expression tree\n        \n        # Apply absorption: if expr is of form A & (A | B) or A | (A & B), reduce to A\n        # Apply negative absorption: if expr is of form A & (~A | B) or A | (~A & B), reduce to A & B or A | B\n        \n        # Since we don't have full expression parsing/evaluation capabilities here,\n        # we return the original expression as a placeholder\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # Check if expression is of form A & (~A | B) or A | (~A & B)\n        \n        # Simplify using the absorption laws\n        if isinstance(expr, Expression):\n            # Simplify using the absorption laws\n            # This is a simplified implementation that applies the rules directly\n            # In a full implementation, we would need to parse the expression tree\n            # and apply the rules recursively\n            \n            # For now, we just return the expression as is\n            # A full implementation would require parsing and tree traversal\n            pass\n        \n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        # Absorption: A & (A | B) = A, A | (A & B) = A\n        # Negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(expr, Expression):\n            # Check for absorption cases\n            # Case 1: A & (A | B) = A\n            # Case 2: A | (A & B) = A\n            # Case 3: A & (~A | B) = A & B\n            # Case 4: A | (~A & B) = A | B\n            \n            # We need to evaluate the structure of the expression\n            # This is a simplified implementation assuming we can detect patterns\n            # In a full implementation, we would parse the expression tree\n            \n            # For now, return the expression as is (placeholder)\n            result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we just return the original expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # We need to evaluate the structure of the expression\n            # This is a simplified implementation assuming we can detect the patterns\n            # In a full implementation, we would parse the expression tree\n            \n            # For now, we just return the expression as is\n            # A real implementation would recursively traverse and simplify\n            result.append(expr)\n        else:\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2008, "num_retrieved": 4, "inference_time": 1.002293512225151, "generate_results": ["def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self._hooks[hook].append(handler)\n        return handler\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self._hooks[hook].append(handler)\n        return handler\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self._hooks[hook].append(handler)\n        return handler\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self._hooks[hook].append(handler)\n        return handler\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self._registry[hook].append(handler)\n        return handler\n    \n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2021, "num_retrieved": 5, "inference_time": 1.002293512225151, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"user\": user,\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1993, "num_retrieved": 4, "inference_time": 1.002293512225151, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2028, "num_retrieved": 5, "inference_time": 1.002293512225151, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer by truncating (rounding down)\n                value = int(value)\n            elif isinstance(value, (int, long)):\n                # Already integer, keep as is\n                pass\n            else:\n                # Handle other types if needed\n                value = int(value)\n            setattr(result, attr, value)\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer part, rounding down\n                value = int(value)\n            elif isinstance(value, (int, long)):\n                # Already integer, keep as is\n                pass\n            else:\n                # Handle other types if needed\n                value = int(value)\n            setattr(result, attr, value)\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.__class__()\n    for attr in self._attributes:\n        value = getattr(self, attr)\n        if value is not None:\n            if isinstance(value, (int, float)):\n                if isinstance(value, float):\n                    # Round to nearest integer\n                    value = int(round(value))\n                # Ensure positive values\n                if value < 0:\n                    value = -int(abs(value))\n                else:\n                    value = int(value)\n            else:\n                # Handle other types like timedelta\n                if hasattr(value, 'total_seconds'):\n                    value = value.total_seconds()\n                    value = int(round(value))\n                else:\n                    value = value\n            setattr(result, attr, value)\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer by truncating (rounding down)\n                intval = int(value)\n                if value != intval:\n                    # If there's a fractional part, add the remainder to the next unit\n                    remainder = value - intval\n                    # Distribute remainder to the next unit (e.g., 0.5 days -> 12 hours)\n                    if attr == 'days':\n                        extra_hours = int(remainder * 24)\n                        result.hours += extra_hours\n                    elif attr == 'hours':\n                        extra_minutes = int(remainder * 60)\n                        result.minutes += extra_minutes\n                    elif attr == 'minutes':\n                        extra_seconds = int(remainder * 60)\n                        result.seconds += extra_seconds\n                    else:\n                        # For other units, just truncate\n                        pass\n                setattr(result, attr, intval)\n            elif isinstance(value, (int, long)):\n                setattr(result, attr, value)\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(result, attr)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer by truncating (rounding down)\n                intval = int(value)\n                if value != intval:\n                    # If there's a fractional part, add the remainder to the next unit\n                    remainder = value - intval\n                    # Distribute remainder to the next unit (e.g., 0.5 days -> 12 hours)\n                    if attr == 'days':\n                        extra_hours = int(remainder * 24)\n                        result.hours += extra_hours\n                    elif attr == 'hours':\n                        extra_minutes = int(remainder * 60)\n                        result.minutes += extra_minutes\n                    elif attr == 'minutes':\n                        extra_seconds = int(remainder * 60)\n                        result.seconds += extra_seconds\n                    else:\n                        # For other units, just truncate\n                        pass\n                setattr(result, attr, intval)\n            elif isinstance(value, (int, long)):\n                setattr(result, attr, value)\n            else:\n                # Handle other types if needed\n                pass\n    return result"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2054, "num_retrieved": 6, "inference_time": 1.002293512225151, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self, *args, **kwargs):\n            result = namefunc(self, *args, **kwargs)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        return lambda dt: namefunc(dt).encode('ascii')\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self, *args, **kwargs):\n            result = namefunc(self, *args, **kwargs)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self, *args, **kwargs):\n            result = namefunc(self, *args, **kwargs)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self, *args, **kwargs):\n            result = namefunc(self, *args, **kwargs)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2020, "num_retrieved": 6, "inference_time": 1.038193717598915, "generate_results": ["def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        return {\"version\": \"0.0.0\"}"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2088, "num_retrieved": 9, "inference_time": 1.038193717598915, "generate_results": ["def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1974, "num_retrieved": 4, "inference_time": 1.038193717598915, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"post\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"post\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"post\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"post\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"post\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1997, "num_retrieved": 5, "inference_time": 1.038193717598915, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.STDOUT if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose is True\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n            \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(f\"Command failed with return code {e.returncode}: {e.stdout or e.stderr}\")\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose is True\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.SubprocessError as e:\n        raise e", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose is True\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr)\n                \n        return result\n        \n    except subprocess.SubprocessError as e:\n        print(f\"Error running command: {e}\", file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose is True\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Command '{commands[0]}' not found\")\n    except Exception as e:\n        raise RuntimeError(f\"Error running command: {e}\")", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.STDOUT if not hide_stderr else None,\n            text=True,\n            env=env,\n            capture_output=True\n        )\n        \n        # Check if command was successful\n        if result.returncode != 0:\n            error_msg = result.stdout if result.stdout else \"Command failed without output\"\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}: {error_msg}\")\n            return result.returncode, error_msg\n            \n        if verbose:\n            print(f\"Command executed successfully: {' '.join(full_command)}\")\n            \n        return result.returncode, result.stdout if result.stdout else \"\"\n        \n    except FileNotFoundError:\n        error_msg = f\"Command not found: {' '.join(full_command)}\"\n        if verbose:\n            print(f\"Error: {error_msg}\")\n        return -1, error_msg\n    except Exception as e:\n        error_msg = f\"Unexpected error executing command: {str(e)}\"\n        if verbose:\n            print(f\"Error: {error_msg}\")\n        return -1, error_msg"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1996, "num_retrieved": 5, "inference_time": 1.038193717598915, "generate_results": ["def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1954, "num_retrieved": 3, "inference_time": 1.038193717598915, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1975, "num_retrieved": 4, "inference_time": 1.038193717598915, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk the storage root to find all objects\n    for root_path, dirs, files in ocfl_walk(self.obj_fs, dir='/', is_storage_root=True):\n        # Find all namaste files in this directory\n        namastes = find_namastes(0, pyfs=self.obj_fs, dir=root_path)\n        \n        # Skip if no namaste files found\n        if not namastes:\n            continue\n            \n        num_objects += 1\n        \n        # Validate the object if requested\n        if validate_objects:\n            try:\n                # Validate the object using the validator's validate method\n                # This assumes the validator has a validate method that takes a path\n                # and returns True if valid, False otherwise\n                valid = self.validate(root_path)\n                if valid:\n                    good_objects += 1\n            except Exception as e:\n                if show_warnings:\n                    self.log.warning('Validation error in object at %s: %s', root_path, str(e))\n        \n        # If check_digests is True, we would check digest files here\n        # But this is not implemented in the provided context\n        \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root\n    for root_path, dirs, files in ocfl_walk(self.obj_fs, dir='/', is_storage_root=True):\n        # Find all namaste files in this directory\n        try:\n            namastes = find_namastes(0, pyfs=self.obj_fs, dir=root_path)\n        except NamasteException as e:\n            if show_warnings:\n                self.log.warning(str(e))\n            continue\n            \n        if not namastes:\n            continue\n            \n        num_objects += len(namastes)\n        \n        # Validate each object\n        if validate_objects:\n            valid = True\n            for namaste in namastes:\n                if not namaste.content_ok(pyfs=self.obj_fs):\n                    valid = False\n                    if show_warnings:\n                        self.log.warning('Content validation failed for %s', namaste.filename)\n            \n            if valid:\n                good_objects += len(namastes)\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk the storage root to find all objects\n    for root_path, dirs, files in ocfl_walk(self.obj_fs, dir='/', is_storage_root=True):\n        # Find all namaste files in this directory\n        try:\n            namastes = find_namastes(0, pyfs=self.obj_fs, dir=root_path)\n            if len(namastes) == 0:\n                continue\n                \n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                # Validate the object using the validator logic\n                self.validate(root_path)\n                # If no errors were reported, consider it valid\n                good_objects += 1\n                \n        except Exception as e:\n            if show_warnings:\n                self.log.warning('H001', error=str(e), path=root_path)\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root\n    for root_path, dirs, files in ocfl_walk(self.obj_fs, dir='/', is_storage_root=True):\n        # Check for OCFL objects in this directory\n        namastes = find_namastes(0, pyfs=self.obj_fs, dir=root_path)\n        \n        # Skip if no namaste files found\n        if not namastes:\n            continue\n            \n        num_objects += 1\n        \n        # Validate the object if requested\n        if validate_objects:\n            try:\n                # Validate the object using the validator's validate method\n                # This assumes the validator has a validate method that takes a path\n                # and returns True if valid, False otherwise\n                valid = self.validate(root_path)\n                if valid:\n                    good_objects += 1\n            except Exception as e:\n                if show_warnings:\n                    self.log.warning('H001', error=str(e), path=root_path)\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root\n    for root_path, dirs, files in ocfl_walk(self.obj_fs, dir='/', is_storage_root=True):\n        # Check for OCFL object directories\n        for dir_name in dirs:\n            dir_path = fs.path.join(root_path, dir_name)\n            # Validate the object if needed\n            if validate_objects:\n                try:\n                    # Find namastes in this object directory\n                    namastes = find_namastes(0, pyfs=self.obj_fs, dir=dir_path)\n                    if len(namastes) > 0:\n                        num_objects += 1\n                        # Validate each namaste file\n                        valid = True\n                        for namaste in namastes:\n                            if not namaste.content_ok(pyfs=self.obj_fs):\n                                valid = False\n                                break\n                        if valid:\n                            good_objects += 1\n                except Exception as e:\n                    if show_warnings:\n                        self.log.warning('E004', error=str(e))\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1951, "num_retrieved": 3, "inference_time": 1.038193717598915, "generate_results": ["def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = self._get_spec_version()\n    self.digest_algorithm = self._get_digest_algorithm()\n    self.content_directory = 'content'\n    self.inventory_digest_files = []\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default inventory and metadata structure\n    self._create_inventory_structure()\n    \n    # Initialize filesystem object if needed\n    if self.obj_fs is None:\n        self.obj_fs = self._create_fs_object()\n    \n    # Validate the root structure\n    self._validate_root_structure()\n    \n    # Register extensions\n    for ext in self.registered_extensions:\n        self._register_extension(ext)\n    \n    # Set up digest validation if enabled\n    if self.check_digests:\n        self._setup_digest_validation()\n    \n    # Log initialization completion\n    if self.log:\n        self.log.info(\"OCFL storage root initialized successfully\")", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = self._get_spec_version()\n    self.digest_algorithm = self._get_digest_algorithm()\n    self.content_directory = 'content'\n    self.inventory_digest_files = []\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default inventory and metadata\n    self._create_inventory_files()\n    self._set_default_storage_structure()\n    \n    # Initialize filesystem object if needed\n    if self.obj_fs is None:\n        self.obj_fs = self._create_fs_object()\n    \n    # Validate the root structure\n    self._validate_root_structure()\n    \n    # Log initialization completion\n    if self.log:\n        self.log.info(\"OCFL storage root initialized successfully\")", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the OCFL storage root structure\n    # This would typically involve creating the required directories\n    # and setting up the initial metadata structure as defined by OCFL spec\n    \n    # Set up the required directories for OCFL storage root\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Initialize the OCFL object metadata\n    self.id = None\n    self.spec_version = self._get_spec_version()\n    self.digest_algorithm = self._get_digest_algorithm()\n    \n    # Create the root directory structure if needed\n    # This would be handled by the underlying filesystem implementation\n    \n    # Initialize the inventory digest files\n    self.inventory_digest_files = []\n    \n    # Register the extensions\n    for ext in self.registered_extensions:\n        self._register_extension(ext)\n    \n    # Set up the validation logger if not already initialized\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, show_errors=self.show_errors, lang=self.lang)\n    \n    # Initialize the object filesystem\n    self.obj_fs = self._create_object_filesystem()\n    \n    # Initialize the root inventory validator\n    self.root_inv_validator = self._create_root_inventory_validator()\n    \n    # Set up the content directory\n    self.content_directory = self._get_content_directory()\n    \n    # Initialize the digest validation settings\n    if self.check_digests:\n        self._initialize_digest_validation()\n    \n    # Log initialization completion\n    self.log.info(\"OCFL storage root initialized successfully\")", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = self._get_spec_version()\n    self.digest_algorithm = self._get_digest_algorithm()\n    self.content_directory = 'content'\n    self.inventory_digest_files = []\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default values based on configuration\n    if self.check_digests and not self.lax_digests:\n        self._validate_digests()\n    \n    # Initialize filesystem objects if needed\n    self._setup_filesystem()\n    \n    # Register extensions\n    for ext in self.registered_extensions:\n        self._register_extension(ext)\n    \n    # Initialize inventory and metadata\n    self._initialize_inventory()\n    self._initialize_metadata()", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize OCFL storage root with default values\n    self.id = None\n    self.spec_version = self._get_spec_version()\n    self.digest_algorithm = self._get_digest_algorithm()\n    self.content_directory = \"content\"\n    self.inventory_digest_files = []\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default values based on configuration\n    if self.check_digests and not self.lax_digests:\n        self._validate_digests()\n    \n    # Initialize filesystem and inventory structure\n    self._setup_inventory_structure()\n    \n    # Register extensions\n    for ext in self.registered_extensions:\n        self._register_extension(ext)\n    \n    # Log initialization completion\n    if self.log:\n        self.log.info(\"OCFL storage root initialized successfully\")"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1982, "num_retrieved": 4, "inference_time": 1.038193717598915, "generate_results": ["def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if zero padding is used\n        # We need to find the padding size from existing versions\n        # For simplicity, assume the padding size is determined by the maximum\n        # number of digits in any version number in the sequence\n        \n        # If we have v1, then we know the pattern is non-zero padded\n        # Otherwise, we need to infer padding from existing versions\n        # This is a simplified implementation that just increments the number\n        \n        # Increment the version number\n        next_num = version_num + 1\n        \n        # Return the version with the same padding as the input\n        # If input is v2, v03, v004, etc., we preserve the padding\n        # We'll assume the padding is consistent with the input\n        # For now, just return the next number in sequence\n        return f'v{next_num}'\n    \n    return None", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if zero padding is used by checking if the version has leading zeros\n        # We need to find the padding size from existing versions\n        # For simplicity, assume the padding size is determined by the maximum length of version numbers\n        # in the sequence (e.g., if we have v1, v002, v003, then padding is 2)\n        \n        # If version is not zero-prefixed, assume it's part of a non-zero-prefixed sequence\n        # and just increment\n        if version_num < 10:\n            return f'v{version_num + 1}'\n        else:\n            # For larger numbers, we need to determine the padding size\n            # This is a simplified implementation - in practice, we'd need to analyze the existing pattern\n            return f'v{version_num + 1}'\n    \n    return version", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if zero padding is used by checking if the version has leading zeros\n        # We need to find the padding size from existing versions\n        # For simplicity, if we have a version like v2, v3, etc., assume no zero padding\n        # If we have v002, v003, etc., assume zero padding with at least 2 digits\n        \n        # Try to determine padding size by checking if there are zero-prefixed versions\n        # This is a simplified implementation - in practice, we'd need to analyze the full version set\n        # For now, assume no zero padding unless we see a zero-prefixed version\n        \n        # If version is v2, v3, etc., assume no zero padding\n        if version_num >= 2:\n            return f'v{version_num + 1}'\n    \n    # If we can't determine the pattern, default to v2\n    return 'v2'", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if zero-padding is used\n        # We need to find the padding size from existing versions\n        # For simplicity, if we have v1, we assume non-zero-prefixed\n        # Otherwise, we assume zero-prefixed with the same padding\n        \n        # If version is v2 or higher, we assume it's part of a zero-padded sequence\n        # We'll just increment the number and return with the same padding\n        # This is a simplified implementation assuming consistent padding\n        \n        # Try to infer padding from the version string\n        padding_size = len(match.group(1))\n        \n        # Increment the number\n        new_num = version_num + 1\n        \n        # Format with the same padding\n        return f'v{new_num:0{padding_size}d}'\n    \n    return version", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if zero padding is used\n        # We need to find the padding size from existing versions\n        # For simplicity, assume the padding size is determined by the maximum\n        # number of digits in any version in the sequence\n        \n        # If we have v1, then padding is not used\n        if version_num == 2:\n            return 'v3'\n        \n        # For versions >= v2, we assume zero padding is used\n        # The padding size is the maximum number of digits in any version\n        # For now, just increment the number\n        return f'v{version_num + 1}'\n    \n    return version"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1897, "num_retrieved": 2, "inference_time": 1.038193717598915, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Validate that each version directory has an inventory\n    for version_dir in version_dirs:\n        if version_dir not in self.inventory['versions']:\n            self.error(\"E012\", version=version_dir)\n            return\n    \n    # Keep track of digests that differ from root inventory\n    differing_digests = {}\n    \n    # Compare each version's content with the root inventory\n    root_version = self.inventory['versions']['v1']\n    for version_dir in version_dirs:\n        version_version = self.inventory['versions'][version_dir]\n        \n        # Check if the version has the same digest algorithm as root\n        if root_version.get('digest_algorithm') != version_version.get('digest_algorithm'):\n            # Different digest algorithms - skip direct digest comparison\n            continue\n            \n        # Compare content digests\n        root_content = root_version.get('content', {})\n        version_content = version_version.get('content', {})\n        \n        for logical_path, root_digests in root_content.items():\n            if logical_path not in version_content:\n                differing_digests[logical_path] = {\n                    'root': root_digests,\n                    'version': None,\n                    'version_dir': version_dir\n                }\n            else:\n                version_digests = version_content[logical_path]\n                if set(root_digests) != set(version_digests):\n                    differing_digests[logical_path] = {\n                        'root': root_digests,\n                        'version': version_digests,\n                        'version_dir': version_dir\n                    }\n    \n    # Store differing digests for later content validation\n    self.differing_digests = differing_digests\n    \n    return differing_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Validate that each version directory has an inventory\n    for version_dir in version_dirs:\n        if version_dir not in self.inventory['versions']:\n            self.error(\"E012\", version=version_dir)\n            return\n    \n    # Keep track of digests that differ from root inventory\n    differing_digests = {}\n    \n    # Compare each version's inventory with the root inventory\n    root_version = self.inventory['versions']['v1']\n    for version_dir in version_dirs:\n        version_inventory = self.inventory['versions'][version_dir]\n        \n        # Check if the version inventory has the same structure as root\n        if 'created' not in version_inventory or 'message' not in version_inventory or 'user' not in version_inventory:\n            self.warning(\"W010\", version=version_dir)\n        \n        # Compare content digests between root and current version\n        # This is a simplified check - in practice, we'd compare actual content\n        # and track differences for later validation\n        if version_dir != 'v1':\n            # For each file in the current version, check if digest differs from root\n            # This is a placeholder - actual implementation would depend on data structure\n            pass\n    \n    # Store differing digests for later use in content validation\n    self.differing_digests = differing_digests\n    \n    return differing_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Validate that each version directory has an inventory\n    root_inventory = self.inventory\n    version_digest_differences = {}\n    \n    for version_dir in version_dirs:\n        # Check if version directory exists in inventory\n        if version_dir not in root_inventory['versions']:\n            self.error(\"E012\", version=version_dir)\n            return\n        \n        # Get the version inventory\n        version_inventory = root_inventory['versions'][version_dir]\n        \n        # Check if the version inventory has a valid structure\n        if not version_inventory.get('created') or not version_inventory.get('message'):\n            self.error(\"E013\", version=version_dir)\n            return\n        \n        # Compare content digests with root inventory\n        # Get the root version's content mappings\n        root_content_map = get_logical_path_map(root_inventory, 'v1')\n        \n        # Get the current version's content mappings\n        current_content_map = get_logical_path_map(version_inventory, version_dir)\n        \n        # Find digests that differ from root\n        for logical_path, current_files in current_content_map.items():\n            if logical_path not in root_content_map:\n                # New logical path in this version\n                version_digest_differences[logical_path] = current_files\n            else:\n                root_files = root_content_map[logical_path]\n                # Check if any files in current version are not in root\n                current_only = current_files - root_files\n                if current_only:\n                    version_digest_differences[logical_path] = current_only\n        \n        # Check metadata consistency\n        root_version = root_inventory['versions'][version_dir]\n        current_version = version_inventory\n        for key in ('created', 'message', 'user'):\n            if root_version.get(key) != current_version.get(key):\n                self.warning(\"W011\", version=version_dir, key=key)\n    \n    # Record all digest differences for later content validation\n    self.version_digest_differences = version_digest_differences", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Validate that each version directory has an inventory\n    for version_dir in version_dirs:\n        if version_dir not in self.inventory['versions']:\n            self.error(\"E012\", version=version_dir)\n            return\n    \n    # Keep track of digests that differ from root inventory\n    differing_digests = {}\n    \n    # Compare each version's inventory with the root inventory\n    root_version = self.inventory['versions']['v1']\n    for version_dir in version_dirs:\n        version_inventory = self.inventory['versions'][version_dir]\n        \n        # Compare file mappings and digests\n        root_files = root_version.get('files', {})\n        version_files = version_inventory.get('files', {})\n        \n        # Check for differences in file mappings\n        for logical_path in root_files:\n            if logical_path not in version_files:\n                differing_digests[logical_path] = {\n                    'version': version_dir,\n                    'root_digests': root_files[logical_path],\n                    'version_digests': []\n                }\n        \n        for logical_path in version_files:\n            if logical_path not in root_files:\n                differing_digests[logical_path] = {\n                    'version': version_dir,\n                    'root_digests': [],\n                    'version_digests': version_files[logical_path]\n                }\n        \n        # Check for differences in actual digests\n        for logical_path in root_files:\n            if logical_path in version_files:\n                root_digests = root_files[logical_path]\n                version_digests = version_files[logical_path]\n                if root_digests != version_digests:\n                    differing_digests[logical_path] = {\n                        'version': version_dir,\n                        'root_digests': root_digests,\n                        'version_digests': version_digests\n                    }\n    \n    # Store differing digests for later content validation\n    self.differing_digests = differing_digests\n    \n    return differing_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Validate that each version directory has an inventory\n    root_inventory = self.inventory\n    version_digest_differences = {}\n    \n    for version_dir in version_dirs:\n        # Check if version directory exists in inventory\n        if version_dir not in root_inventory['versions']:\n            self.error(\"E012\", version=version_dir)\n            return\n        \n        # Get the version inventory\n        version_inv = root_inventory['versions'][version_dir]\n        \n        # Check if the version inventory has a valid structure\n        if not version_inv.get('created') or not version_inv.get('message'):\n            self.error(\"E013\", version=version_dir)\n            return\n        \n        # Compare content digests with root inventory\n        # Get the root version's content mapping\n        root_content_map = get_logical_path_map(root_inventory, 'v1')\n        \n        # Get the current version's content mapping\n        current_content_map = get_logical_path_map(version_inv, version_dir)\n        \n        # Find digests that differ from root\n        for logical_path, current_files in current_content_map.items():\n            if logical_path not in root_content_map:\n                # New logical path in this version\n                version_digest_differences[logical_path] = current_files\n            else:\n                root_files = root_content_map[logical_path]\n                # Check if any files in current version are not in root\n                current_only = current_files - root_files\n                if current_only:\n                    version_digest_differences[logical_path] = current_only\n        \n        # Check if the version directory is in sequence\n        # Since version_dirs is assumed to be in sequence, we don't need to validate order\n        # but we do need to ensure it's a valid version name\n    \n    # Record the differences for later content validation\n    self.version_digest_differences = version_digest_differences\n    return version_digest_differences"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1974, "num_retrieved": 4, "inference_time": 1.038193717598915, "generate_results": ["def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    import os\n    from ocfl import find_namastes\n    \n    # Check if path exists and is a directory\n    if not os.path.exists(path):\n        return \"E074: path does not exist\"\n    \n    if not os.path.isdir(path):\n        return \"E075: path is not a directory\"\n    \n    # Look for 0=* files in the directory\n    zero_eq_files = [f for f in os.listdir(path) if f.startswith('0=')]\n    \n    if len(zero_eq_files) == 0:\n        return \"E076: no 0=* files found\"\n    \n    # Check if any of the 0=* files match the expected pattern\n    for file in zero_eq_files:\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', file)\n        if not match:\n            return \"E077: invalid 0= file format\"\n    \n    # Determine the type based on the presence of 0= files\n    # If there's a 0=ocfl_object_1.0, it's a root\n    for file in zero_eq_files:\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', file)\n        if match and match.group(1) == '1.0':\n            return 'root'\n    \n    # If there's a 0=ocfl_object_x.y where x.y != 1.0, it's an object\n    for file in zero_eq_files:\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', file)\n        if match and match.group(1) != '1.0':\n            return 'object'\n    \n    # If there's a file that doesn't start with 0=ocfl_object_, it's a file\n    for file in zero_eq_files:\n        if not file.startswith('0=ocfl_object_'):\n            return 'file'\n    \n    return '", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    import os\n    from ocfl import find_namastes\n    \n    # Check if path exists and is a directory\n    if not os.path.exists(path):\n        return \"E074: path does not exist\"\n    \n    if not os.path.isdir(path):\n        return \"E075: path is not a directory\"\n    \n    # Look for 0=* files in the directory\n    zero_eq_files = [f for f in os.listdir(path) if f.startswith('0=')]\n    \n    if len(zero_eq_files) == 0:\n        return \"E076: no 0=* files found\"\n    \n    # Check if any of the 0=* files match the expected pattern\n    for file in zero_eq_files:\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', file)\n        if not match:\n            return \"E077: invalid 0=* file format\"\n    \n    # Determine the type based on the presence and content of 0=* files\n    if len(zero_eq_files) == 1:\n        # Single 0=* file indicates an OCFL object\n        return 'object'\n    elif len(zero_eq_files) > 1:\n        # Multiple 0=* files indicate a storage root\n        return 'root'\n    else:\n        # Should not reach here given the above checks\n        return \"E078: unexpected file pattern\"", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    import os\n    from ocfl import find_namastes\n    \n    # Check if path exists and is a directory\n    if not os.path.exists(path):\n        return \"E074: path does not exist\"\n    \n    if not os.path.isdir(path):\n        return \"E075: path is not a directory\"\n    \n    # Look for 0=* files in the directory\n    zero_eq_files = [f for f in os.listdir(path) if f.startswith('0=')]\n    \n    if len(zero_eq_files) == 0:\n        return \"E076: no 0=* file found\"\n    \n    # Check if any 0=* file matches the expected pattern\n    for file in zero_eq_files:\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', file)\n        if not match:\n            return \"E077: invalid 0=* file format\"\n    \n    # Determine the type based on the presence of 0=* files\n    # If there's a 0=ocfl_object_1.0, it's a root\n    # If there's a 0=ocfl_object_x.y, it's an object\n    # If there's only a file like 0=inventory.json, it's a file\n    \n    # Check for specific patterns\n    for file in zero_eq_files:\n        if file == '0=ocfl_object_1.0':\n            return 'root'\n        elif re.match(r'0=ocfl_object_\\d+\\.\\d+', file):\n            return 'object'\n        elif file.startswith('0=inventory'):\n            return 'file'\n    \n    # If none of the above, return an error\n    return \"E078: unknown 0=* file pattern\"", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    # Normalize path\n    path = Path(path).resolve()\n    \n    # Check if path exists\n    if not path.exists():\n        return \"error: path does not exist\"\n    \n    # If it's a file, check if it's a Namaste file\n    if path.is_file():\n        filename = path.name\n        if filename.startswith('0='):\n            match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', filename)\n            if match:\n                # This is an object declaration file\n                return 'file'\n            else:\n                # Invalid format\n                return 'error: invalid 0= file format'\n        else:\n            # Not a 0= file\n            return 'file'\n    \n    # If it's a directory, check for 0= files\n    if path.is_dir():\n        files = list(path.iterdir())\n        zero_eqs = [f for f in files if f.is_file() and f.name.startswith('0=')]\n        \n        if len(zero_eqs) == 0:\n            return 'error: no 0= files found'\n        elif len(zero_eqs) > 1:\n            return 'error: multiple 0= files found'\n        \n        declaration = zero_eqs[0].name\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', declaration)\n        \n        if match:\n            version = match.group(1)\n            if version == '1.0':\n                return 'root'\n            else:\n                return 'object'\n        else:\n            return 'error: invalid 0= file format'\n    \n    return 'error: path is neither a file nor a directory'", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    # Normalize path\n    p = Path(path)\n    if not p.exists():\n        return \"error: path does not exist\"\n    \n    # Check for 0=* files\n    zero_eq_files = [f for f in p.iterdir() if f.is_file() and f.name.startswith('0=')]\n    \n    if not zero_eq_files:\n        return \"error: no 0=* file found\"\n    \n    # Check if there's more than one 0=* file\n    if len(zero_eq_files) > 1:\n        return \"error: multiple 0=* files found\"\n    \n    zero_eq_file = zero_eq_files[0]\n    match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', zero_eq_file.name)\n    \n    if not match:\n        return \"error: invalid 0=* file format\"\n    \n    version = match.group(1)\n    \n    # Check if it's a root (storage root)\n    if version == '1.0':\n        return 'root'\n    \n    # Check if it's an object\n    if version.startswith('1.'):\n        return 'object'\n    \n    # Otherwise, it's a file (inventory)\n    return 'file'"]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2015, "num_retrieved": 4, "inference_time": 1.038193717598915, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        params[\"bug_ids\"] = bug_ids\n    # Add any additional logic specific to the context or requirements", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        params[\"bug_id\"] = bug_ids", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        params[\"bug_ids\"] = bug_ids\n    # Add any other amendments as needed based on context or requirements", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        params[\"bug_id\"] = bug_ids", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        params[\"bug_ids\"] = bug_ids\n    # Add any additional logic specific to the implementation\n    # For example, setting default values or modifying fields based on context"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1996, "num_retrieved": 4, "inference_time": 1.038193717598915, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1969, "num_retrieved": 3, "inference_time": 1.038193717598915, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import parse_subparser_arguments\n    from borgmatic.commands import subparsers\n\n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different commands\n    subparsers_dict = {}\n    for subparser_name, subparser in subparsers.items():\n        subparsers_dict[subparser_name] = subparser\n    \n    # Parse the arguments using the subparsers\n    parsed_args, remaining_args = parse_subparser_arguments(arguments, subparsers_dict)\n    \n    # Return the parsed arguments as a dictionary mapping subparser names to parsed namespaces\n    return parsed_args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import parse_subparser_arguments\n    from borgmatic.commands import subparsers\n\n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different commands\n    subparsers_dict = {}\n    for subparser_name, subparser in subparsers.items():\n        subparsers_dict[subparser_name] = subparser\n    \n    # Parse arguments using the subparsers\n    parsed_args, remaining_args = parse_subparser_arguments(arguments, subparsers_dict)\n    \n    # Return the parsed arguments as a dictionary mapping subparser names to parsed namespaces\n    return parsed_args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import parse_subparser_arguments\n    from borgmatic.commands import subparsers\n\n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different commands\n    subparsers_dict = {}\n    for subparser_name, subparser in subparsers.items():\n        subparsers_dict[subparser_name] = subparser\n    \n    # Parse arguments using the subparsers\n    parsed_arguments, remaining = parse_subparser_arguments(arguments, subparsers_dict)\n    \n    # Return the parsed arguments as a dictionary\n    return parsed_arguments", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import parse_subparser_arguments\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different commands\n    subparsers = argparse._SubParsersAction(parser)\n    \n    # Define subparsers (example structure - actual definitions would be in the codebase)\n    subparsers.add_parser('prune')\n    subparsers.add_parser('compact')\n    subparsers.add_parser('create')\n    subparsers.add_parser('check')\n    subparsers.add_parser('list')\n    subparsers.add_parser('archive')\n    \n    # Parse arguments using the subparsers\n    parsed_args, remaining_args = parse_subparser_arguments(arguments, subparsers)\n    \n    # Return the parsed arguments as an ArgumentParser instance\n    return parsed_args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import parse_subparser_arguments\n    from borgmatic.commands import subparsers\n\n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different commands\n    subparsers_dict = {}\n    for subparser_name, subparser in subparsers.items():\n        subparsers_dict[subparser_name] = subparser\n    \n    # Parse the arguments using the subparsers\n    parsed_arguments, remaining = parse_subparser_arguments(arguments, subparsers_dict)\n    \n    # Return the parsed arguments as an ArgumentParser instance\n    # This is a simplified representation; in practice, this would return a namespace\n    # or a structured dictionary of parsed arguments\n    return parsed_arguments"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2063, "num_retrieved": 6, "inference_time": 1.038193717598915, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1998, "num_retrieved": 4, "inference_time": 1.038193717598915, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subcommand', help='sub-commands')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(version)s')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subcommands\n    # Note: This is a simplified version - actual implementation would include all subcommands\n    # and their respective arguments\n    \n    # Example subcommand: backup\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repository', help='Repository path')\n    backup_parser.add_argument('--archive', help='Archive name')\n    backup_parser.add_argument('--exclude', nargs='*', help='Files or patterns to exclude')\n    backup_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Example subcommand: extract\n    extract_parser = subparsers.add_parser('extract', help='Extract command')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name')\n    extract_parser.add_argument('--target', help='Target directory')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(version)s')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add 'backup' subparser\n    backup_parser = subparsers.add_parser('backup', help='Backup action')\n    backup_parser.add_argument('--repository', help='Repository path')\n    backup_parser.add_argument('--exclude', nargs='*', help='Files or patterns to exclude')\n    backup_parser.add_argument('--include', nargs='*', help='Files or patterns to include')\n    backup_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add 'archive' subparser\n    archive_parser = subparsers.add_parser('archive', help='Archive action')\n    archive_parser.add_argument('--repository', help='Repository path')\n    archive_parser.add_argument('--exclude', nargs='*', help='Files or patterns to exclude')\n    archive_parser.add_argument('--include', nargs='*', help='Files or patterns to include')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='command')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(version)s')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='*', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='*', default=[], help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    # Example subparsers (these would be defined in the actual implementation)\n    borg_parser = subparsers.add_parser('borg', help='Borg-related commands')\n    borg_parser.add_argument('--repository', help='Repository path')\n    borg_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add more subparsers as needed\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dictionary mapping from subparser name to Namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments\n    if hasattr(args, 'command'):\n        result[args.command] = args\n    else:\n        # If no command specified, use global arguments only\n        pass\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = arguments.make_parsers()\n    top_level_parser, subparsers = parser\n    \n    # Parse the arguments\n    args = top_level_parser.parse_args(unparsed_arguments)\n    \n    # Create a dictionary to hold the parsed arguments\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments\n    for subparser_name, subparser in subparsers.choices.items():\n        sub_args = subparser.parse_args(unparsed_arguments)\n        result[subparser_name] = sub_args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(version)s')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', help='Configuration file paths')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    # Example: backup, prune, etc.\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('--repository', help='Repository path')\n    backup_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    prune_parser = subparsers.add_parser('prune', help='Prune command')\n    prune_parser.add_argument('--repository', help='Repository path')\n    prune_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    extract_parser = subparsers.add_parser('extract', help='Extract command')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add more subparsers as needed\n    \n    # Parse the"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1968, "num_retrieved": 3, "inference_time": 1.156039223074913, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a namespace to hold parsed arguments\n    parsed_args = defaultdict(lambda: argparse.Namespace())\n    remaining_args = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Create a parser instance for this subparser\n        args = parser.parse_args(remaining_args)\n        \n        # If arguments were parsed successfully, store them\n        if args:\n            parsed_args[subparser_name] = args\n    \n    return dict(parsed_args), remaining_args", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a namespace to hold parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: argparse.Namespace())\n    remaining_arguments = []\n    \n    # Process each argument in the unparsed list\n    for arg in unparsed_arguments:\n        # Try to match the argument with each subparser\n        for subparser_name, parser in subparsers.items():\n            # Try to parse the argument with the subparser\n            try:\n                # Create a parser instance for this subparser\n                # We need to parse the argument against the subparser's parser\n                # We'll use the parser to parse the argument\n                # But we need to handle the case where the argument is not a complete command\n                \n                # We'll use the parser to parse the argument\n                # We need to build a namespace for the subparser\n                # We'll use the parser to parse the argument\n                \n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse the argument\n                # We'll use the parser to parse", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a namespace to hold parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: argparse.Namespace())\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Create a namespace to hold arguments for this subparser\n        args_namespace = parser.parse_args(remaining_arguments)\n        \n        # If the subparser successfully parsed arguments, update the parsed_arguments dict\n        if args_namespace:\n            parsed_arguments[subparser_name] = args_namespace\n            # Remove the parsed arguments from the remaining list\n            remaining_arguments = []\n            # Reconstruct the remaining arguments list by filtering out parsed ones\n            # This is a simplified approach; in practice, we'd need to track which arguments were consumed\n            break\n    \n    # If no subparser consumed arguments, return empty dict and original list\n    if not remaining_arguments:\n        # Reconstruct the remaining arguments list properly\n        remaining_arguments = []\n        for subparser_name, parser in subparsers.items():\n            # Try to parse arguments with this subparser\n            args_namespace = parser.parse_args(remaining_arguments)\n            if args_namespace:\n                parsed_arguments[subparser_name] = args_namespace\n                # Remove the parsed arguments from the remaining list\n                remaining_arguments = []\n                break\n    \n    # If still have remaining arguments, try to parse with each subparser in order\n    if not remaining_arguments:\n        # Reconstruct the remaining arguments list properly\n        remaining_arguments = []\n        for subparser_name, parser in subparsers.items():\n            # Try to parse arguments with this subparser\n            args_namespace = parser.parse_args(remaining_arguments)\n            if args_namespace:\n                parsed_arguments[subparser_name] = args_namespace\n                # Remove the parsed arguments from the remaining list\n                remaining_arguments = []\n                break\n    \n    # Final pass: parse remaining arguments with each", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a namespace to hold parsed arguments\n    parsed_args = defaultdict(lambda: argparse.Namespace())\n    remaining_args = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Create an argument parser for the current subparser\n        # We need to parse the arguments that match this subparser\n        # We'll use the parser to parse the remaining arguments\n        try:\n            # Parse the arguments using the subparser's parser\n            # We need to parse only the arguments that are not already claimed\n            # by a previous subparser\n            args = parser.parse_args(remaining_args)\n            # Store the parsed arguments in the parsed_args dict\n            parsed_args[subparser_name] = args\n            # Remove the arguments that were parsed\n            remaining_args = remaining_args[args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If the parser raises a SystemExit, it means that the arguments\n            # are not valid for this subparser\n            # We should continue to the next subparser\n            continue\n    \n    # Return the parsed arguments and the remaining arguments\n    return dict(parsed_args), remaining_args", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a namespace to hold parsed arguments\n    parsed_args = defaultdict(lambda: argparse.Namespace())\n    remaining_args = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Create an argument parser for this subparser\n        # We need to parse the arguments that match this subparser\n        # We'll use the parser to parse the remaining arguments\n        try:\n            # Parse the arguments using the subparser\n            # We need to parse only the arguments that are not already claimed\n            # by a previous subparser\n            args = parser.parse_args(remaining_args)\n            # Store the parsed arguments in the parsed_args dict\n            parsed_args[subparser_name] = args\n            # Remove the parsed arguments from the remaining_args list\n            # We need to remove the arguments that were parsed\n            # by this subparser\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing the arguments that were parsed\n            # from the remaining_args list\n            # We'll do this by removing"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2022, "num_retrieved": 5, "inference_time": 1.156039223074913, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.borg.flags as flags\n    \n    parser = argparse.ArgumentParser(\n        description='Backup and restore Borg repositories with ease.',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog='See \"borgmatic --help\" for more information.'\n    )\n    \n    # Add global flags\n    parser.add_argument('--version', action='version', version='borgmatic %(prog)s')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add 'list' action\n    list_parser = subparsers.add_parser('list', help='List archives or paths in a repository')\n    list_parser.add_argument('archive', nargs='?', help='Archive to list')\n    list_parser.add_argument('paths', nargs='*', help='Paths to list within an archive')\n    \n    # Add 'rlist' action\n    rlist_parser = subparsers.add_parser('rlist', help='List archives with a prefix')\n    rlist_parser.add_argument('prefix', nargs='?', help='Prefix to match archives')\n    \n    # Add 'backup' action\n    backup_parser = subparsers.add_parser('backup', help='Backup a repository')\n    backup_parser.add_argument('source', help='Source path to backup')\n    backup_parser.add_argument('destination', help='Destination path for backup')\n    \n    # Add 'restore' action\n    restore_parser = subparsers.add_parser('restore', help='Restore a repository')\n    restore_parser.add_argument('archive', help='Archive to restore')\n    restore_parser.add_argument('destination', help='Destination path for restore')\n    \n    # Add 'prune' action\n    prune_parser = subparsers.add_parser('prune', help='Prune old archives')\n    prune_parser.add_argument('keep', type=int, help='Number of archives to keep')\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.borg.flags as flags\n    import borgmatic.commands.borgmatic as borgmatic\n    \n    parser = argparse.ArgumentParser(\n        description='Backup and restore Borg repositories with ease.',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog='See https://borgmatic.readthedocs.io/en/latest/ for more information.'\n    )\n    \n    # Add global flags\n    parser.add_argument('--version', action='version', version='borgmatic %(prog)s ' + pkg_resources.require('borgmatic')[0].version)\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add 'backup' command\n    backup_parser = subparsers.add_parser('backup', help='Backup a repository')\n    backup_parser.add_argument('--archive', help='Archive name')\n    backup_parser.add_argument('--exclude', help='Exclude files or patterns')\n    backup_parser.add_argument('--include', help='Include files or patterns')\n    \n    # Add 'restore' command\n    restore_parser = subparsers.add_parser('restore', help='Restore a repository')\n    restore_parser.add_argument('--archive', help='Archive name')\n    restore_parser.add_argument('--paths', nargs='+', help='Paths to restore')\n    \n    # Add 'list' command\n    list_parser = subparsers.add_parser('list', help='List archives or paths')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    \n    # Add 'rlist' command\n    rlist_parser = subparsers.add_parser('rlist', help='List archives with a prefix')\n    rlist_parser.add_argument('--prefix', help='Prefix to match archives')\n    rlist_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add 'version' command\n    version_parser = subparsers.add_parser('version', help='Show version information')\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.borg.flags as flags\n    \n    parser = argparse.ArgumentParser(\n        description='Backup and restore Borg repositories with ease.',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global flags\n    parser.add_argument('--version', action='version', version='borgmatic %(version)s')\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], default='INFO')\n    parser.add_argument('--dry-run', action='store_true', help='Only print what would be done')\n    parser.add_argument('--no-verify', action='store_true', help='Skip verification of archives')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add 'backup' subparser\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    backup_parser.add_argument('--archive', help='Archive name')\n    backup_parser.add_argument('--exclude', action='append', help='Exclude files or patterns')\n    backup_parser.add_argument('--include', action='append', help='Include files or patterns')\n    backup_parser.add_argument('--compression', choices=['zstd', 'lz4', 'zlib'], default='zstd')\n    \n    # Add 'restore' subparser\n    restore_parser = subparsers.add_parser('restore', help='Restore a backup')\n    restore_parser.add_argument('--archive', help='Archive name')\n    restore_parser.add_argument('--target', help='Target path for restore')\n    restore_parser.add_argument('--exclude', action='append', help='Exclude files or patterns')\n    \n    # Add 'list' subparser\n    list_parser = subparsers.add_parser('list', help='List archives or paths')\n    list_parser.add_argument('--archive', help='List archives in a specific archive')\n    list_parser.add_argument('--paths', action='append', help='List paths within an archive')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add 'rlist' subparser\n    rlist_parser = subparsers.add_parser('", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.borg.flags as flags\n    import borgmatic.commands.borgmatic as borgmatic\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg backups',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global flags\n    parser.add_argument('--version', action='version', version='borgmatic ' + pkg_resources.require('borgmatic')[0].version)\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add 'list' command\n    list_parser = subparsers.add_parser('list', help='List archives or paths within an archive')\n    list_parser.add_argument('archive', nargs='?', help='Archive to list')\n    list_parser.add_argument('paths', nargs='*', help='Paths to list')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add 'rlist' command\n    rlist_parser = subparsers.add_parser('rlist', help='List archives with a repository')\n    rlist_parser.add_argument('prefix', nargs='?', help='Prefix to match archives')\n    rlist_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add 'backup' command\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    backup_parser.add_argument('repository', help='Repository path')\n    backup_parser.add_argument('paths', nargs='*', help='Paths to backup')\n    \n    # Add 'prune' command\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    prune_parser.add_argument('repository', help='Repository path')\n    prune_parser.add_argument('--keep-daily', type=int, help='Keep N daily backups')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Keep N weekly backups')\n    prune_parser.add_argument('--keep-monthly', type=int, help='Keep N monthly backups')\n    \n    # Add 'check' command\n    check_parser = subparsers.add_parser('check', help='Check repository integrity", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.borg.flags as flags\n    import borgmatic.commands.borgmatic as borgmatic\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool to manage Borg backups',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog='See https://borgmatic.readthedocs.io/en/latest/ for more information.'\n    )\n    \n    # Add global flags\n    parser.add_argument('--version', action='version', version='borgmatic {}'.format(\n        pkg_resources.require('borgmatic')[0].version))\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Create subparsers for different commands\n    subparsers = parser.add_subparsers(dest='action', help='Available commands')\n    \n    # Add subparser for backup\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    backup_parser.add_argument('repository', help='Repository path')\n    backup_parser.add_argument('--archive', help='Archive name')\n    backup_parser.add_argument('--paths', nargs='+', help='Paths to backup')\n    \n    # Add subparser for prune\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    prune_parser.add_argument('repository', help='Repository path')\n    prune_parser.add_argument('--keep-daily', type=int, help='Keep N daily backups')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Keep N weekly backups')\n    prune_parser.add_argument('--keep-monthly', type=int, help='Keep N monthly backups')\n    \n    # Add subparser for list\n    list_parser = subparsers.add_parser('list', help='List archives or paths')\n    list_parser.add_argument('repository', help='Repository path')\n    list_parser.add_argument('--archive', help='Archive name')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    \n    # Add subparser for rlist\n    rlist_parser = subparsers.add_parser('rlist', help='List archives with a prefix')\n    rlist_parser.add_argument('repository', help='Repository path')\n    rlist_parser.add_argument('--prefix', help='Prefix to match archives')\n    \n    # Add subparser for check\n    check_parser = subparsers"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1966, "num_retrieved": 3, "inference_time": 1.156039223074913, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1998, "num_retrieved": 4, "inference_time": 1.156039223074913, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A configuration and backup tool for Borg.')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap Borgmatic')\n    bootstrap_parser.add_argument('--version', action='store_true', help='Show version information')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source', help='Source configuration file')\n    generate_parser.add_argument('--destination', required=True, help='Destination configuration file')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file if it exists')\n    generate_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without making changes')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('--config', nargs='+', help='Configuration files to validate')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('Global arguments')\n    global_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without making changes')\n    global_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], default='INFO', help='Log level')\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n    except SystemExit:\n        sys.exit(1)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - a configuration and backup tool for Borg')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap Borgmatic')\n    bootstrap_parser.add_argument('--version', action='store_true', help='Show version information')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('source_filename', help='Source configuration file', nargs='?', default=None)\n    generate_parser.add_argument('destination_filename', help='Destination configuration file', nargs='?', default=None)\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file if it exists')\n    generate_parser.add_argument('--dry-run', action='store_true', help='Only show what would be written, without writing anything')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('config_files', nargs='+', help='Configuration files to validate')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('Global arguments')\n    global_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without performing the action')\n    global_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], default='INFO', help='Set the logging level')\n    global_parser.add_argument('--config', help='Configuration file path')\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n    except SystemExit:\n        sys.exit(1)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - a tool for managing Borg backups')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap Borgmatic')\n    bootstrap_parser.add_argument('--version', action='store_true', help='Show version information')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source', help='Source configuration file')\n    generate_parser.add_argument('--destination', required=True, help='Destination configuration file')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file if it exists')\n    generate_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without making changes')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('--config', nargs='+', help='Configuration files to validate')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('Global arguments')\n    global_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without making changes')\n    global_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], default='INFO', help='Log level')\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n    except SystemExit:\n        sys.exit(1)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A tool for managing Borg backups')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap Borgmatic')\n    bootstrap_parser.add_argument('--version', action='store_true', help='Show version information')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('source_filename', help='Source configuration file')\n    generate_parser.add_argument('destination_filename', help='Destination configuration file')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file if it exists')\n    generate_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without writing files')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('config_files', nargs='+', help='Configuration files to validate')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('Global arguments')\n    global_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without writing files')\n    global_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], default='INFO', help='Set the log level')\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n    except SystemExit:\n        sys.exit(1)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A configuration and backup tool for Borg')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap Borgmatic')\n    bootstrap_parser.add_argument('--version', action='store_true', help='Show version information')\n    bootstrap_parser.add_argument('--config', help='Configuration file path')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('source_filename', help='Source configuration file', nargs='?', default=None)\n    generate_parser.add_argument('destination_filename', help='Destination configuration file', nargs='?', default=None)\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file if it exists')\n    generate_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without writing files')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('config_files', nargs='+', help='Configuration files to validate')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('Global arguments')\n    global_parser.add_argument('--dry-run', action='store_true', help='Only show what would be done, without writing files')\n    global_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], default='INFO', help='Set the log level')\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n    except SystemExit:\n        sys.exit(1)\n    \n    return args"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1945, "num_retrieved": 2, "inference_time": 1.156039223074913, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1974, "num_retrieved": 3, "inference_time": 1.156039223074913, "generate_results": ["def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! __fish_seen_subcommand_from ${all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg '${exact_option_args}'\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_borgmatic_{action_name}() {{\n                local cur=\"${COMP_WORDS[COMP_C", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! __fish_seen_subcommand_from ${all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg '${exact_option_args}'\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion -n \"$action_parser_condition\" -n \"$exact_option_condition\" -", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(\"$@\")\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! _fish_seen_subcommand_from ${all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg '${exact_option_args}'\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion -n \"$action_parser_condition\" -n \"$exact_option_condition\" -a '{action_name}' -d '{shlex", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! __fish_seen_subcommand_from ${all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg '${exact_option_args}'\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion -n \"$action_parser_condition\" -n \"$exact_option_condition\" -", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! __fish_seen_subcommand_from ${all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg '${exact_option_args}'\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion -n \"$action_parser_condition\" -n \"$exact_option_condition\" -"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1967, "num_retrieved": 3, "inference_time": 1.156039223074913, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='store_true', help='Show version information')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the repository')\n    bootstrap_parser.add_argument('bootstrap', nargs='?', help='Bootstrap arguments')\n    \n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('generate', nargs='?', help='Generate arguments')\n    \n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('validate', nargs='?', help='Validate arguments')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict with subparser name as key\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'subparser_name'):\n        subparser_name = args.subparser_name\n        if subparser_name:\n            # Create a namespace for the subparser arguments\n            subparser_args = args.__dict__\n            # Remove the subparser_name key\n            subparser_args.pop('subparser_name', None)\n            result[subparser_name] = argparse.Namespace(**subparser_args)\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='store_true', help='Show version information')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the repository')\n    bootstrap_parser.add_argument('bootstrap', nargs='?', help='Bootstrap arguments')\n    \n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('generate', nargs='?', help='Generate arguments')\n    \n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('validate', nargs='?', help='Validate arguments')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create result dictionary\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments if present\n    if hasattr(args, 'subparser_name'):\n        subparser_name = args.subparser_name\n        if subparser_name:\n            # Create a namespace for the subparser arguments\n            subparser_args = {}\n            if subparser_name == 'bootstrap':\n                subparser_args = vars(bootstrap_parser.parse_args(unparsed_arguments))\n            elif subparser_name == 'generate':\n                subparser_args = vars(generate_parser.parse_args(unparsed_arguments))\n            elif subparser_name == 'validate':\n                subparser_args = vars(validate_parser.parse_args(unparsed_arguments))\n            \n            result[subparser_name] = argparse.Namespace(**subparser_args)\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(version)s')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap subparser\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the repository')\n    bootstrap_parser.add_argument('bootstrap', help='Bootstrap configuration')\n    \n    # Generate subparser\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('generate', help='Generate configuration')\n    \n    # Validate subparser\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('validate', help='Validate configuration')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to Namespace\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'action'):\n        subparser_name = args.action\n        if subparser_name == 'bootstrap':\n            result['bootstrap'] = args\n        elif subparser_name == 'generate':\n            result['generate'] = args\n        elif subparser_name == 'validate':\n            result['validate'] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='store_true', help='show version information')\n    global_parser.add_argument('--bash-completion', action='store_true', help='output bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='output fish completion script')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    \n    # Add subparsers for different actions\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='bootstrap borgmatic')\n    bootstrap_parser.add_argument('bootstrap', help='bootstrap configuration')\n    \n    generate_parser = subparsers.add_parser('generate', help='generate configuration')\n    generate_parser.add_argument('generate', help='generate configuration')\n    \n    validate_parser = subparsers.add_parser('validate', help='validate configuration')\n    validate_parser.add_argument('validate', help='validate configuration')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to argparse.Namespace\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'subparser_name'):\n        subparser_name = args.subparser_name\n        if subparser_name:\n            result[subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='store_true', help='show version information')\n    global_parser.add_argument('--bash-completion', action='store_true', help='print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='print fish completion script')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    \n    # Add subparsers for different actions\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='bootstrap borgmatic')\n    bootstrap_parser.add_argument('bootstrap', nargs='?', help='bootstrap configuration')\n    \n    generate_parser = subparsers.add_parser('generate', help='generate configuration')\n    generate_parser.add_argument('generate', nargs='?', help='generate configuration')\n    \n    validate_parser = subparsers.add_parser('validate', help='validate configuration')\n    validate_parser.add_argument('validate', nargs='?', help='validate configuration')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to Namespace\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'subparser_name'):\n        subparser_name = args.subparser_name\n        if subparser_name:\n            subparser_args = args.__dict__\n            # Remove the subparser_name key\n            subparser_args.pop('subparser_name', None)\n            result[subparser_name] = argparse.Namespace(**subparser_args)\n    \n    return result"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1940, "num_retrieved": 2, "inference_time": 1.156039223074913, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If parsing fails, don't add to arguments and continue\n            pass\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return parsed_arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, parser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, we don't want to exit, just skip\n            pass\n    \n    return parsed_arguments, remaining_arguments"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1944, "num_retrieved": 2, "inference_time": 1.156039223074913, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n\n    parser = argparse.ArgumentParser(description='Backup and restore data with borgmatic')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define common arguments\n    global_parser = parser.add_argument_group('global options')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--log-level', help='Log level')\n    global_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n\n    # Define action-specific parsers\n    action_parsers = {}\n    \n    # Example actions (these would be defined based on actual borgmatic commands)\n    backup_parser = subparsers.add_parser('backup', help='Perform a backup')\n    backup_parser.add_argument('repo', help='Repository path')\n    backup_parser.add_argument('paths', nargs='+', help='Paths to backup')\n    \n    restore_parser = subparsers.add_parser('restore', help='Restore data from a backup')\n    restore_parser.add_argument('repo', help='Repository path')\n    restore_parser.add_argument('paths', nargs='+', help='Paths to restore')\n    \n    # Add more actions as needed\n    \n    return (parser, action_parsers, parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n\n    parser = argparse.ArgumentParser(description='Backup and restore data with borgmatic')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define common arguments\n    global_parser = parser.add_argument_group('global options')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO', help='Log level')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry mode')\n    global_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n\n    # Define action-specific parsers\n    backup_parser = subparsers.add_parser('backup', help='Perform a backup')\n    backup_parser.add_argument('repository', help='Repository path')\n    backup_parser.add_argument('paths', nargs='+', help='Paths to back up')\n    backup_parser.add_argument('--exclude', nargs='+', help='Paths to exclude')\n    backup_parser.add_argument('--compression', choices=['zstd', 'lz4', 'zlib'], default='zstd', help='Compression method')\n\n    restore_parser = subparsers.add_parser('restore', help='Restore data from a backup')\n    restore_parser.add_argument('repository', help='Repository path')\n    restore_parser.add_argument('paths', nargs='+', help='Paths to restore')\n    restore_parser.add_argument('--target', help='Target directory for restore')\n    restore_parser.add_argument('--exclude', nargs='+', help='Paths to exclude')\n\n    # Add other actions as needed\n    # For example, prune, check, etc.\n\n    return (parser, subparsers, parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n\n    parser = argparse.ArgumentParser(description='Backup and restore data with borgmatic')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define common arguments\n    global_parser = parser.add_argument_group('global options')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--log-level', help='Log level')\n    global_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n\n    # Define action-specific parsers\n    actions = {\n        'backup': make_subparsers('backup', 'Backup data'),\n        'restore': make_subparsers('restore', 'Restore data'),\n        'prune': make_subparsers('prune', 'Prune old backups'),\n        'check': make_subparsers('check', 'Check repository integrity'),\n    }\n\n    for action_name, action_parser in actions.items():\n        subparsers.add_parser(action_name, help=action_parser.description)\n\n    return (parser, actions, parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n\n    parser = argparse.ArgumentParser(description='Backup and restore data with borgmatic')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define common arguments\n    global_parser = parser.add_argument_group('global options')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--log-level', help='Log level')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry mode')\n\n    # Define action-specific parsers\n    action_parsers = {}\n    \n    # Example actions (these would be defined based on actual borgmatic commands)\n    backup_parser = subparsers.add_parser('backup', help='Perform a backup')\n    backup_parser.add_argument('--repo', help='Repository path')\n    backup_parser.add_argument('--archive', help='Archive name')\n    \n    restore_parser = subparsers.add_parser('restore', help='Restore data')\n    restore_parser.add_argument('--repo', help='Repository path')\n    restore_parser.add_argument('--archive', help='Archive name')\n    restore_parser.add_argument('--target', help='Target path')\n\n    action_parsers['backup'] = backup_parser\n    action_parsers['restore'] = restore_parser\n\n    # Return the top-level parser, action parsers, and combined parser\n    return (parser, action_parsers, parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_parser\n\n    parser = argparse.ArgumentParser(description='Borgmatic - backup and restore Borg repositories')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define available actions and their parsers\n    actions = {\n        'backup': make_parser('backup', 'Backup a repository'),\n        'restore': make_parser('restore', 'Restore a repository'),\n        'prune': make_parser('prune', 'Prune old backups'),\n        'check': make_parser('check', 'Check repository integrity'),\n        'list': make_parser('list', 'List repositories'),\n        'version': make_parser('version', 'Show version information'),\n    }\n\n    for action_name, action_parser in actions.items():\n        subparsers.add_parser(action_name, help=action_parser.description, **action_parser.kwargs)\n\n    # Create global parser with common options\n    global_parser = argparse.ArgumentParser(add_help=False)\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--log-level', help='Log level')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n\n    # Combine global parser with action parsers\n    global_plus_action_parser = argparse.ArgumentParser()\n    global_plus_action_parser.add_argument('--config', help='Path to configuration file')\n    global_plus_action_parser.add_argument('--log-level', help='Log level')\n    global_plus_action_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_plus_action_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n\n    # Add action subparsers to global parser\n    for action_name, action_parser in actions.items():\n        action_parser = action_parser.copy()\n        action_parser.add_argument('--config', help='Path to configuration file')\n        action_parser.add_argument('--log-level', help='Log level')\n        action_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n        action_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n        global_plus_action_parser.add_parser(action_name, help=action_parser.description, **action_parser.kwargs)\n\n    return (parser, actions, global_plus_action_parser)"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1978, "num_retrieved": 3, "inference_time": 1.156039223074913, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        max_results = 1000\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or max_results <= 0:\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        max_results = 1000\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        max_results = 1000\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or max_results <= 0:\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2122, "num_retrieved": 8, "inference_time": 1.156039223074913, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove spaces\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove spaces\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove spaces\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2008, "num_retrieved": 5, "inference_time": 1.156039223074913, "generate_results": ["def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2029, "num_retrieved": 5, "inference_time": 1.156039223074913, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{key}}}}}}}\"\n        url = url.replace(f\"{{{key}}}\", str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{key}}}}}}}\"\n        url = url.replace(f\"{{{key}}}\", str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{{{key}}}}}}}}}\"\n        url = url.replace(f\"{{{key}}}\", str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{key}}}}}}}\"\n        url = url.replace(f\"{{{key}}}\", str(value))\n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1998, "num_retrieved": 4, "inference_time": 1.156039223074913, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', ''])"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1945, "num_retrieved": 2, "inference_time": 1.156039223074913, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1940, "num_retrieved": 2, "inference_time": 1.156039223074913, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Recursive processing detected for: {}\".format(abs_path))\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Warning: {} has already been processed.\".format(path_name))\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Warning: {} has already been processed.\".format(path_name))\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Recursive processing detected for: {}\".format(abs_path))\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Recursive processing detected for: {}\".format(abs_path))\n        return True\n    processed.add(abs_path)\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2102, "num_retrieved": 8, "inference_time": 0.7852026522159576, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and its two neighbors (with wraparound)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        \n        # Create tuple of three points: previous, current, next\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        \n        # Create a 3-tuple: (previous, current, next)\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and its two neighbors (with wraparound)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        \n        # Create tuple of three points: previous, current, next\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        \n        # Create a 3-tuple: (previous, current, next)\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and two neighbors (with wraparound)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1167, "num_retrieved": 5, "inference_time": 0.7852026522159576, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    \n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    \n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2125, "num_retrieved": 8, "inference_time": 0.7852026522159576, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n    \n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n    \n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2071, "num_retrieved": 8, "inference_time": 0.7852026522159576, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2031, "num_retrieved": 6, "inference_time": 0.7852026522159576, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1161, "num_retrieved": 5, "inference_time": 0.7852026522159576, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2035, "num_retrieved": 5, "inference_time": 0.7852026522159576, "generate_results": ["import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination_path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the .tar.gz archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    temp_archive_path = destination_path / \"archive.tar.gz\"\n    with open(temp_archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"xf\", str(temp_archive_path), \"-C\", str(destination_path)])\n    \n    # Return the destination path where the archive was extracted\n    return destination_path", "import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination_path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the .tar.gz archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / \"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"xf\", archive_path, \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination_path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the .tar.gz archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    temp_archive_path = destination_path / \"archive.tar.gz\"\n    with open(temp_archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"xf\", str(temp_archive_path), \"-C\", str(destination_path)])\n    \n    # Return the destination path where the archive was extracted\n    return destination_path", "import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination_path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the .tar.gz archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / \"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"xf\", archive_path, \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination_path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the .tar.gz archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    temp_archive_path = destination_path / \"archive.tar.gz\"\n    with open(temp_archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"xf\", str(temp_archive_path), \"-C\", str(destination_path)])\n    \n    # Return the destination path where the archive was extracted\n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2130, "num_retrieved": 8, "inference_time": 0.7852026522159576, "generate_results": ["import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os_platform.system() == \"Darwin\"", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os_platform.system() == \"Darwin\"", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os_platform.system() == \"Darwin\"", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os_platform.system() == \"Darwin\"", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool\n    \"\"\"\n    return os_platform.system() == \"Darwin\""]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2004, "num_retrieved": 4, "inference_time": 0.7852026522159576, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at start/end\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\.', '\\.', regex_key)\n        # Escape any special regex characters\n        regex_key = re.escape(regex_key)\n        # Remove any trailing or leading dots that might cause issues\n        # But keep the pattern as is for matching\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at start/end\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at the start/end of string\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at start/end\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', '.*', key)\n        # Ensure the regex starts and ends with a word boundary or is at start/end of string\n        # For simplicity, just convert the pattern\n        regex_key = re.sub(r'\\?', '.', regex_key)\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2293, "num_retrieved": 14, "inference_time": 0.7852026522159576, "generate_results": ["import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1987, "num_retrieved": 4, "inference_time": 0.7852026522159576, "generate_results": ["import re\nimport os\nimport sys\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os.name != 'nt' else 0\n    \n    # Regex pattern to match quoted strings and unquoted tokens\n    # This handles spaces, quotes, and escaped characters\n    pattern = r'((?:[^\"\\\\]|\\\\.)*|\"(?:[^\"\\\\]|\\\\.)*\")'\n    \n    # Split the string using the pattern\n    tokens = re.findall(pattern, s)\n    \n    # Clean up tokens: remove quotes and handle escaped characters\n    result = []\n    for token in tokens:\n        if token.startswith('\"') and token.endswith('\"'):\n            # Remove quotes\n            token = token[1:-1]\n            # Handle escaped characters (e.g., \\n, \\t)\n            token = token.replace('\\\\n', '\\n').replace('\\\\t', '\\t').replace('\\\\r', '\\r')\n        elif token.startswith(\"'\") and token.endswith(\"'\"):\n            token = token[1:-1]\n        result.append(token)\n    \n    # Handle escaped characters in unquoted tokens\n    final_result = []\n    for token in result:\n        # Replace escaped characters\n        token = token.replace('\\\\n', '\\n').replace('\\\\t', '\\t').replace('\\\\r', '\\r')\n        final_result.append(token)\n    \n    return final_result", "import re\nimport os\nimport platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_is_linux() or os_is_mac() else 0\n    \n    if platform == 1:\n        # POSIX: split on whitespace, handle quoted strings\n        pattern = r'([^\"\\\\]|\\\\.)*\"([^\"]*)\"'  # match quoted strings\n        result = []\n        current = \"\"\n        in_quote = False\n        i = 0\n        while i < len(s):\n            if s[i] == '\"' and not in_quote:\n                in_quote = True\n                i += 1\n                # skip until closing quote\n                while i < len(s) and s[i] != '\"':\n                    current += s[i]\n                    i += 1\n                current += s[i]\n                result.append(current)\n                current = \"\"\n                in_quote = False\n            elif s[i] == '\\\\' and in_quote:\n                current += s[i]\n                i += 1\n                if i < len(s):\n                    current += s[i]\n                    i += 1\n            elif in_quote:\n                current += s[i]\n                i += 1\n            elif s[i].isspace():\n                if current:\n                    result.append(current)\n                    current = \"\"\n                i += 1\n            else:\n                current += s[i]\n                i += 1\n        if current:\n            result.append(current)\n        return result\n    \n    elif platform == 0:\n        # Windows: split on spaces, but preserve quoted strings\n        # Use regex to split on spaces, but keep quoted parts\n        # Windows CMD uses space as delimiter, and quotes are used to group\n        pattern = r'(\\s+|\\s*(?:\"([^\"]*)\")\\s*)'\n        parts = re.split(r'(\\s+|\\s*(?:\"([^\"]*)\")\\s*)', s)\n        result = []\n        i = 0\n        while i < len(parts):\n            part = parts[i]\n            if part.strip():\n                if part.startswith('\"') and part.endswith('\"'):\n                    result.append(part[1", "import re\nimport os\nimport platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform_val = platform.system().lower()\n        if platform_val == 'windows':\n            platform_val = 0\n        else:\n            platform_val = 1\n    elif platform == '0':\n        platform_val = 0\n    elif platform == '1':\n        platform_val = 1\n    else:\n        raise ValueError(\"Invalid platform value. Use 'this', '0', or '1'.\")\n\n    if platform_val == 0:\n        # Windows: split on spaces, but handle quoted strings\n        # Use regex to match quoted strings and unquoted tokens\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        tokens = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):\n                tokens.append(match.group(1))\n            else:\n                tokens.append(match.group(2))\n        return tokens\n    else:\n        # POSIX: use shlex.split behavior with regex\n        # Split on whitespace, but preserve quoted strings\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        tokens = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):\n                tokens.append(match.group(1))\n            else:\n                tokens.append(match.group(2))\n        return tokens", "import re\nimport os\nimport platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_is_linux() or os_is_mac() else 0\n    \n    if platform == 1:\n        # POSIX: split on whitespace, but handle quoted strings\n        # Use regex to match quoted strings and unquoted tokens\n        pattern = r'([^\"\\\\]|\\\\.)*\"([^\"]*)\"'  # match quoted strings\n        tokens = []\n        current = ''\n        in_quote = False\n        \n        for char in s:\n            if char == '\"' and not in_quote:\n                in_quote = True\n                current = ''\n            elif char == '\"' and in_quote:\n                in_quote = False\n                tokens.append(current)\n                current = ''\n            elif char == '\\\\' and in_quote:\n                current += char\n            else:\n                current += char\n        \n        if current:\n            tokens.append(current)\n        \n        # Split on whitespace, but preserve quoted strings\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        \n        # Handle unquoted tokens\n        parts = re.split(r'(\\s+)', s)\n        result = []\n        i = 0\n        while i < len(parts):\n            part = parts[i]\n            if part.strip():\n                if part.startswith('\"') and part.endswith('\"'):\n                    result.append(part[1:-1])\n                else:\n                    # Split on whitespace within the part\n                    subparts = re.split(r'(\\s+)', part)\n                    for sp in subparts:\n                        if sp.strip():\n                            result.append(sp.strip())\n            i += 1\n        \n        # Remove empty strings\n        return [t for t in result if t]\n    \n    elif platform == 0:\n        # Windows: split on spaces, but handle quoted strings\n        # Windows CMD uses space as delimiter, and quotes are used to group\n        # Use regex to match quoted strings\n        result = []\n        in_quote = False\n        current = ''\n        \n        for char", "import re\nimport os\nimport platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_is_linux() or os_is_mac() else 0\n    \n    if platform == 1:\n        # POSIX: split on whitespace, but handle quoted strings\n        # Use regex to match quoted strings and unquoted tokens\n        pattern = r'([^\"\\\\]|\\\\.)*\"([^\"]*)\"'  # match quoted strings\n        tokens = []\n        current = \"\"\n        in_quote = False\n        \n        for char in s:\n            if char == '\"' and not in_quote:\n                in_quote = True\n                current = \"\"\n            elif char == '\"' and in_quote:\n                in_quote = False\n                tokens.append(current)\n                current = \"\"\n            elif char == '\\\\' and in_quote:\n                current += char\n            else:\n                current += char\n        \n        if current:\n            tokens.append(current)\n        \n        # Split on whitespace, but preserve quoted strings\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        \n        # Split the rest by whitespace\n        parts = []\n        for part in s.split():\n            if part.startswith('\"') and part.endswith('\"'):\n                parts.append(part[1:-1])\n            else:\n                # Split the part into tokens if it contains quotes\n                inner = part\n                while True:\n                    match = re.search(r'(\"([^\"]*)\")', inner)\n                    if not match:\n                        break\n                    inner = inner[:match.start()] + inner[match.end():]\n                parts.append(inner)\n        \n        # Reconstruct with proper splitting\n        result = []\n        for part in s.split():\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            else:\n                # Split on whitespace but preserve quoted parts\n                subparts = re.split(r'(\\s+)', part)\n                clean_parts = []\n                for subpart in subparts:\n                    if subpart.strip():\n                        clean_parts.append(subpart.strip"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2067, "num_retrieved": 6, "inference_time": 0.7852026522159576, "generate_results": ["import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    if not os.path.exists(archive_path):\n        raise FileNotFoundError(f\"Archive not found: {archive_path}\")\n\n    tmp_dir = os.path.join(tmp_path, \"repo_extract\")\n    os.makedirs(tmp_dir, exist_ok=True)\n\n    # Determine the base name of the archive without extension\n    base_name = os.path.splitext(os.path.basename(archive_path))[0]\n    \n    # Use tar to extract the archive to the temporary directory\n    if archive_path.endswith(\".tar.gz\") or archive_path.endswith(\".tgz\"):\n        cmd = [\"tar\", \"-xzf\", archive_path, \"-C\", tmp_dir]\n    elif archive_path.endswith(\".tar\"):\n        cmd = [\"tar\", \"-xvf\", archive_path, \"-C\", tmp_dir]\n    elif archive_path.endswith(\".zip\"):\n        cmd = [\"unzip\", \"-q\", archive_path, \"-d\", tmp_dir]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path}\")\n\n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n\n    # Construct the repo URL based on the extracted directory structure\n    # Assuming the extracted content contains a directory with a name that can be used as the repo URL\n    # This is a simplified implementation; actual logic may depend on the archive structure\n    extracted_dir = os.path.join(tmp_dir, base_name)\n    \n    # If filename is provided, use it as the repo URL\n    if filename:\n        return f\"file://{os.path.join(tmp_dir, filename)}\"\n    \n    # Otherwise, return the path to the extracted directory\n    return f\"file://{extracted_dir}\"", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the base name for the extracted directory\n    if filename is None:\n        filename = archive_path.name\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / filename.split(\".\")[0]\n    extract_dir.mkdir(exist_ok=True)\n    \n    # Use tar to extract the archive\n    try:\n        if archive_path.suffix.lower() in ['.tar.gz', '.tgz']:\n            subprocess.run(['tar', '-xzf', str(archive_path), '-C', str(extract_dir)], check=True)\n        elif archive_path.suffix.lower() in ['.tar.xz']:\n            subprocess.run(['tar', '-xJf', str(archive_path), '-C', str(extract_dir)], check=True)\n        elif archive_path.suffix.lower() in ['.tar.bz2']:\n            subprocess.run(['tar', '-xjf', str(archive_path), '-C', str(extract_dir)], check=True)\n        elif archive_path.suffix.lower() in ['.zip']:\n            subprocess.run(['unzip', '-q', str(archive_path), '-d', str(extract_dir)], check=True)\n        else:\n            raise ValueError(f\"Unsupported archive format: {archive_path.suffix}\")\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as a repository URL\n    # Assuming the extracted directory is the origin URL\n    return str(extract_dir)", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the base name for the extracted directory\n    if filename is None:\n        filename = archive_path.stem\n    \n    extract_dir = tmp_path / filename\n    \n    # Use tar to extract the archive\n    try:\n        subprocess.run(\n            ['tar', '-xzf', str(archive_path), '-C', str(extract_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as a string (file repo url)\n    return str(extract_dir)", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the base name of the archive (without extension)\n    base_name = archive_path.stem\n    \n    # Create the destination directory\n    extract_dir = tmp_path / base_name\n    \n    # Use tar to extract the archive\n    try:\n        subprocess.run(\n            ['tar', '-xzf', str(archive_path), '-C', str(extract_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as a file repo url\n    return str(extract_dir)", "import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    if not os.path.exists(archive_path):\n        raise FileNotFoundError(f\"Archive not found: {archive_path}\")\n\n    tmp_dir = os.path.join(tmp_path, \"repo_extract\")\n    os.makedirs(tmp_dir, exist_ok=True)\n\n    # Determine the base name of the archive without extension\n    base_name = os.path.splitext(os.path.basename(archive_path))[0]\n    \n    # Use tar to extract the archive to the temporary directory\n    try:\n        subprocess.run(\n            [\"tar\", \"-xzf\", archive_path, \"-C\", tmp_dir],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n\n    # Construct the repo URL based on the extracted directory structure\n    # Assuming the extracted content contains a directory with a name that can be used as the repo URL\n    # This is a simplified implementation - actual logic may depend on the archive structure\n    extracted_dir = os.path.join(tmp_dir, base_name)\n    \n    if not os.path.exists(extracted_dir):\n        # Fallback: look for any directory in the extracted content\n        extracted_dir = os.path.join(tmp_dir, next(os.listdir(tmp_dir), \"\"))\n    \n    # Return the path as a string that can be used as a repo URL\n    return os.path.abspath(extracted_dir)"]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1973, "num_retrieved": 3, "inference_time": 0.7852026522159576, "generate_results": ["import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and any files not in the ignored list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and files that are not in the ignored list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and any lines that don't match the ignored pattern\n        filtered_files = [file.strip() for file in ignored_files if file.strip()]\n        \n        # Sort the files alphabetically\n        filtered_files.sort()\n        \n        # Join the files with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter files that are in the ignored list (if ignored is provided)\n        if ignored:\n            ignored_files = [f for f in ignored_files if f in ignored]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Return as a comma-separated string\n        return ','.join(ignored_files) if ignored_files else \"\"\n        \n    except subprocess.CalledProcessError as e:\n        return \"\"\n    except FileNotFoundError:\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter files that match the ignored pattern (if any)\n        # Note: The 'ignored' parameter is not used in the implementation as per the function's description\n        # The function currently returns all ignored files as per git check-ignore\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2119, "num_retrieved": 8, "inference_time": 0.7852026522159576, "generate_results": ["def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini'\n    }\n    \n    # Check if the filename has a supported extension\n    if '.' in filename:\n        file_extension = filename.lower().split('.')[-1]\n        return f'.{file_extension}' in supported_extensions\n    return False", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini', '.log'\n    }\n    \n    # Check if the filename has a supported extension\n    if filename and '.' in filename:\n        ext = filename.lower().split('.')[-1]\n        return '.' + ext in supported_extensions\n    \n    return False", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini', '.log'\n    }\n    \n    # Convert filename to lowercase and extract extension\n    file_lower = filename.lower()\n    file_extension = os.path.splitext(file_lower)[1]\n    \n    # Check if the extension is supported\n    return file_extension in supported_extensions", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini', '.log'\n    }\n    \n    # Convert filename to lowercase and extract extension\n    file_lower = filename.lower()\n    file_extension = os.path.splitext(file_lower)[1]\n    \n    # Check if the extension is supported\n    return file_extension in supported_extensions", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.yaml', '.yml', '.cfg', '.ini', '.log'\n    }\n    \n    # Check if the filename has a supported extension\n    if '.' in filename:\n        file_extension = filename.lower().split('.')[-1]\n        return f'.{file_extension}' in supported_extensions\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2020, "num_retrieved": 5, "inference_time": 0.7852026522159576, "generate_results": ["import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)(\\s+(week|weeks|day|days|hour|hours|minute|minutes|second|seconds))$', frequency, re.IGNORECASE)\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Convert unit to timedelta\n    if unit in ['week', 'weeks']:\n        return datetime.timedelta(weeks=number)\n    elif unit in ['day', 'days']:\n        return datetime.timedelta(days=number)\n    elif unit in ['hour', 'hours']:\n        return datetime.timedelta(hours=number)\n    elif unit in ['minute', 'minutes']:\n        return datetime.timedelta(minutes=number)\n    elif unit in ['second', 'seconds']:\n        return datetime.timedelta(seconds=number)\n    else:\n        raise ValueError(f\"Unsupported time unit: {unit}\")", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Match number and unit (supporting common time units)\n    match = re.match(r'^(\\d+)\\s*(weeks?|days?|hours?|minutes?|seconds?)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return datetime.timedelta(weeks=number)\n    elif unit == 'days':\n        return datetime.timedelta(days=number)\n    elif unit == 'hours':\n        return datetime.timedelta(hours=number)\n    elif unit == 'minutes':\n        return datetime.timedelta(minutes=number)\n    elif unit == 'seconds':\n        return datetime.timedelta(seconds=number)\n    else:\n        raise ValueError(f\"Unsupported time unit: {unit}\")", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)(\\s+)([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(3).lower()\n\n    # Map units to timedelta components\n    unit_map = {\n        'second': datetime.timedelta(seconds=number),\n        'seconds': datetime.timedelta(seconds=number),\n        'minute': datetime.timedelta(minutes=number),\n        'minutes': datetime.timedelta(minutes=number),\n        'hour': datetime.timedelta(hours=number),\n        'hours': datetime.timedelta(hours=number),\n        'day': datetime.timedelta(days=number),\n        'days': datetime.timedelta(days=number),\n        'week': datetime.timedelta(weeks=number),\n        'weeks': datetime.timedelta(weeks=number),\n    }\n\n    if unit not in unit_map:\n        raise ValueError(f\"Unsupported unit: {unit}\")\n\n    return unit_map[unit]", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Match number and unit (supporting common time units)\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Map units to timedelta components\n    unit_map = {\n        'second': datetime.timedelta(seconds=number),\n        'seconds': datetime.timedelta(seconds=number),\n        'minute': datetime.timedelta(minutes=number),\n        'minutes': datetime.timedelta(minutes=number),\n        'hour': datetime.timedelta(hours=number),\n        'hours': datetime.timedelta(hours=number),\n        'day': datetime.timedelta(days=number),\n        'days': datetime.timedelta(days=number),\n        'week': datetime.timedelta(weeks=number),\n        'weeks': datetime.timedelta(weeks=number),\n    }\n\n    if unit not in unit_map:\n        raise ValueError(f\"Unsupported time unit: {unit}\")\n\n    return unit_map[unit]", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Match number and unit (supporting common time units)\n    match = re.match(r'^(\\d+)(\\s+(?:week|weeks|day|days|hour|hours|minute|minutes|second|seconds))$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Invalid frequency format: {frequency}\")\n\n    number = int(match.group(1))\n    unit = match.group(2).lower().strip()\n\n    # Map units to timedelta components\n    unit_map = {\n        'week': 'weeks',\n        'weeks': 'weeks',\n        'day': 'days',\n        'days': 'days',\n        'hour': 'hours',\n        'hours': 'hours',\n        'minute': 'minutes',\n        'minutes': 'minutes',\n        'second': 'seconds',\n        'seconds': 'seconds'\n    }\n\n    if unit not in unit_map:\n        raise ValueError(f\"Unsupported time unit: {unit}\")\n\n    # Create timedelta with the appropriate component\n    try:\n        td = datetime.timedelta(**{unit_map[unit]: number})\n        return td\n    except (ValueError, TypeError) as e:\n        raise ValueError(f\"Invalid frequency value: {frequency}\") from e"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2086, "num_retrieved": 7, "inference_time": 0.7852026522159576, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Localhost has IP address 127.0.0.1\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Localhost has IP address 127.0.0.1\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Localhost has IP address 127.0.0.1\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Localhost has IP address 127.0.0.1\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip = socket.gethostbyname(host)\n        # Localhost has IP address 127.0.0.1\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname cannot be resolved, it's not localhost\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2042, "num_retrieved": 6, "inference_time": 0.8110246360301971, "generate_results": ["import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    if not find_paths:\n        return []\n\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            # Already a pattern, pass through untouched\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a glob pattern that matches any directory containing the path fragment\n            # with the fragment as a file name\n            path = path.strip('/')\n            # Create a glob pattern that matches any directory with the path fragment\n            # as a file name, recursively\n            glob_pattern = f'sh:**/*{path}*/**'\n            result.append(glob_pattern)\n    \n    return result", "import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    if not find_paths:\n        return []\n\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a glob pattern that matches any directory containing the path fragment\n            # with wildcards at the beginning and end\n            # Replace any existing slashes with the glob pattern\n            # Use ** to match any directory path\n            # Use * to match any file name\n            # Use / to match directory separators\n            # Escape any special glob characters\n            escaped_path = re.escape(path)\n            # Create a pattern that matches any directory containing the path fragment\n            # with wildcards at the beginning and end\n            pattern = f\"sh:**/*{escaped_path}*/**\"\n            result.append(pattern)\n    \n    return result", "import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    if not find_paths:\n        return []\n\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a glob pattern that matches any directory containing the path fragment\n            # with the fragment as a leaf\n            path_parts = path.split(os.path.sep)\n            # Create a glob pattern with **/*path*/** to match any directory containing the path\n            # and the path as a leaf\n            glob_pattern = 'sh:**/*' + '*'.join(re.escape(part) for part in path_parts) + '*/**'\n            result.append(glob_pattern)\n    \n    return result", "import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    if not find_paths:\n        return []\n\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a glob pattern that matches any directory containing the path fragment\n            # with the fragment as a leaf\n            path_parts = path.split(os.path.sep)\n            # Create a glob pattern with **/*path*/** to match any directory containing the path\n            # with the path as a leaf\n            glob_pattern = 'sh:**/*' + '*'.join(re.escape(part) for part in path_parts) + '*/**'\n            result.append(glob_pattern)\n    \n    return result", "import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    if not find_paths:\n        return []\n\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a glob pattern that matches any directory containing the path fragment\n            # with the fragment as a leaf\n            path_parts = path.split(os.path.sep)\n            # Create a glob pattern with **/*path*/** to match any directory containing the path\n            # with the path as a leaf\n            glob_pattern = 'sh:**/*' + '*'.join(re.escape(part) for part in path_parts) + '*/**'\n            result.append(glob_pattern)\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2099, "num_retrieved": 7, "inference_time": 0.8110246360301971, "generate_results": ["def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.path.exists(r\"C:\\Program Files\\Git\\git-bash.exe\")", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.path.exists(r\"C:\\Program Files\\Git\\git-bash.exe\")", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.environ.get('TERM', '') == 'xterm'", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.path.exists(r\"C:\\Program Files\\Git\\git-bash.exe\")", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.name == 'nt' and os.path.exists(r\"C:\\Program Files\\Git\\git-bash.exe\")"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1990, "num_retrieved": 4, "inference_time": 0.8110246360301971, "generate_results": ["import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return None\n\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Write the configuration to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set the file permissions\n    os.chmod(config_filename, mode)\n    \n    return None", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return None\n\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Write the configuration to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)\n    \n    return None", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Check if file already exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport errno\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        try:\n            os.makedirs(directory, exist_ok=True)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n\n    # Check if file exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return None\n\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Write the configuration to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set the file permissions\n    os.chmod(config_filename, mode)\n    \n    return None"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1981, "num_retrieved": 3, "inference_time": 0.8110246360301971, "generate_results": ["def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped.startswith(\"#\") or stripped == \"\":\n            continue\n        commands.append(stripped)\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped.startswith(\"#\") or stripped == \"\":\n            continue\n        commands.append(stripped)\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    return seperator.join(commands)"]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2000, "num_retrieved": 4, "inference_time": 0.8110246360301971, "generate_results": ["import subprocess\nimport sys\nimport os\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script file to hold the function code\n    script_dir = Path(__file__).parent\n    script_path = script_dir / f\"temp_script_{hash(func)}_.py\"\n    \n    try:\n        # Generate the code to import and run the function\n        code = f\"\"\"\nimport sys\nimport os\nimport {func.__module__} as module\nsys.path.insert(0, '{script_dir}')\n{func.__name__}(*{args})\n\"\"\"\n        \n        # Write the code to the script file\n        script_path.write_text(code, encoding=\"utf-8\")\n        \n        # Set up environment variables\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script using subprocess\n        result = subprocess.run(\n            [sys.executable, str(script_path)],\n            env=env,\n            timeout=timeout,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n        \n    finally:\n        # Clean up the temporary script file\n        if script_path.exists():\n            script_path.unlink()", "import subprocess\nimport sys\nimport os\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script that imports and calls the function\n    script_path = Path(tempfile.mktemp(suffix='.py'))\n    try:\n        script_content = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n        script_path.write_text(script_content, encoding='utf-8')\n        \n        # Set up environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script with subprocess\n        result = subprocess.run(\n            [sys.executable, str(script_path)],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n    finally:\n        script_path.unlink(missing_ok=True)", "import subprocess\nimport sys\nimport os\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script that imports and calls the function\n    script_path = Path(__file__).parent / f\"temp_script_{hash(func)}_{os.getpid()}.py\"\n    try:\n        script_content = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([f'arg{i}' for i in range(len(args))]) if args else 'None'})  # Placeholder for args\n\"\"\"\n        \n        # Replace the function call with actual function call\n        script_content = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\ndef main():\n    {func.__name__}({', '.join([f'arg{i}' for i in range(len(args))]) if args else 'None'})\nif __name__ == '__main__':\n    main()\n\"\"\"\n        \n        # Write the script to file\n        script_path.write_text(script_content, encoding='utf-8')\n        \n        # Set up environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script with subprocess\n        result = subprocess.run(\n            [sys.executable, str(script_path)],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n        \n    finally:\n        # Clean up the temporary script\n        if script_path.exists():\n            script_path.unlink()", "import subprocess\nimport sys\nimport os\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script that imports and calls the function\n    script = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join(['\"arg\" if i == 0 else \"arg\" for i in range(len(args))])})\n\"\"\"\n    \n    # Prepare environment\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Execute the script using subprocess.run\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-c\", script] + list(args),\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        return result\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Command timed out after {timeout} seconds\")", "import subprocess\nimport sys\nimport os\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script that imports and calls the function\n    script = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([f'arg{i}' for i in range(len(args))]) if args else 'None'})\n\"\"\"\n    \n    # Prepare the environment\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Execute the script using subprocess\n    try:\n        result = subprocess.run(\n            [sys.executable, '-c', script] + list(args),\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        return result\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function {func.__name__} timed out after {timeout} seconds\")"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2021, "num_retrieved": 5, "inference_time": 0.8110246360301971, "generate_results": ["import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value\n    match = matcher.group(0)\n    if '-' in match:\n        # Handle ${FOO-bar} or ${FOO:-bar} format\n        parts = match[1:].split('-')\n        var_name = parts[0]\n        if len(parts) > 1:\n            default_value = parts[1]\n            # Check if the variable exists in environment\n            value = os.environ.get(var_name)\n            if value is not None:\n                return value\n            else:\n                return default_value\n        else:\n            # This shouldn't happen based on the pattern\n            raise ValueError(f\"Invalid format: {match}\")\n    else:\n        # Handle ${FOO} format\n        var_name = match[1:]\n        value = os.environ.get(var_name)\n        if value is not None:\n            return value\n        else:\n            # No default value provided, raise an error\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not set\")", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = matcher.group(0)\n    variable_name = match[1:-1]  # Remove the ${ and } characters\n    \n    # Check for optional default value (e.g., ${FOO:-bar})\n    if ':' in variable_name:\n        var_name, default = variable_name.split(':', 1)\n        default_value = default.strip()\n    else:\n        var_name = variable_name\n        default_value = None\n    \n    # Get the environment variable value\n    env_value = os.environ.get(var_name)\n    \n    # If no value is found and no default is provided, raise an error\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = matcher.group(0)\n    var_name = matcher.group(1)\n    \n    # Extract default value if present (e.g., in ${FOO:-bar})\n    default_value = None\n    if ':' in var_name:\n        var_name, default_value = var_name.split(':', 1)\n        default_value = default_value.strip()\n    \n    # Get environment variable value\n    env_value = os.environ.get(var_name)\n    \n    # If no default value and environment variable is not set, raise an error\n    if default_value is None and env_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value\n    match = matcher.group(0)\n    if '-' in match:\n        # Handle ${FOO-bar} or ${FOO:-bar} format\n        parts = match[1:].split('-')\n        var_name = parts[0]\n        if len(parts) > 1:\n            default_value = parts[1]\n            # Check if the variable exists in environment\n            env_value = os.environ.get(var_name)\n            if env_value is not None:\n                return env_value\n            else:\n                return default_value\n        else:\n            # This shouldn't happen given the pattern\n            raise ValueError(f\"Invalid format: {match}\")\n    else:\n        # Handle ${FOO} format\n        var_name = match[1:]\n        env_value = os.environ.get(var_name)\n        if env_value is not None:\n            return env_value\n        else:\n            # No default value provided, raise an error\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not set\")", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value\n    match = matcher.group(0)\n    if '-' in match:\n        # Handle ${FOO-bar} or ${FOO:-bar} format\n        parts = match[1:].split('-')\n        var_name = parts[0]\n        if len(parts) > 1:\n            default_value = parts[1]\n            # Check if the variable exists in environment\n            env_value = os.environ.get(var_name)\n            if env_value is not None:\n                return env_value\n            else:\n                return default_value\n        else:\n            # This shouldn't happen given the format\n            raise ValueError(f\"Invalid format: {match}\")\n    else:\n        # Handle ${FOO} format\n        var_name = match[1:]\n        env_value = os.environ.get(var_name)\n        if env_value is not None:\n            return env_value\n        else:\n            # No default value provided, raise an error\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not set\")"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2048, "num_retrieved": 6, "inference_time": 0.8110246360301971, "generate_results": ["import urllib\nfrom typing import Tuple\nimport uuidutils\nimport re\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(_(\"Invalid image href: empty string\"))\n\n    # Check if the href starts with http or https\n    if image_href.startswith('http'):\n        use_ssl = image_href.startswith('https')\n        # Extract netloc (network location) and path\n        parsed = urllib.parse.urlparse(image_href)\n        netloc = parsed.netloc\n        path = parsed.path\n        \n        # Extract image_id from the path\n        if not path:\n            raise ValueError(_(\"Invalid image href: missing path\"))\n            \n        # Remove leading slash if present\n        path = path.lstrip('/')\n        \n        # Split by '/' and take the last part as image_id\n        parts = path.split('/')\n        if len(parts) == 0:\n            raise ValueError(_(\"Invalid image href: empty path\"))\n            \n        image_id = parts[-1]\n        \n        # Validate that image_id is a valid UUID or a valid image ID\n        if not image_id:\n            raise ValueError(_(\"Invalid image href: empty image ID\"))\n            \n        # Check if image_id is a valid UUID\n        if uuidutils.is_uuid_like(image_id):\n            pass  # Valid UUID\n        else:\n            # Try to validate as a simple string ID\n            # For now, just accept it if it's not empty\n            pass\n            \n        return (image_id, netloc, use_ssl)\n    else:\n        # If it doesn't start with http, treat it as a local ID\n        # This is a fallback case - assume it's a local image ID\n        # and extract the image_id from the string\n        if '/' in image_href:\n            parts = image_href.split('/', 1)\n            if len(parts) == 2:\n                image_id = parts[1]\n                netloc = ''\n                use_ssl = False\n                return (image_id, netloc, use_ssl)\n        else:\n            # No slash found, assume it's just an image ID\n            image_id = image_href\n            netloc = ''\n            use_ssl = False\n            return (image_id, netloc", "import urllib\nfrom typing import Tuple\nimport uuidutils\nimport re\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(_(\"Invalid image href: empty string\"))\n\n    # Check if the href starts with http or https\n    if image_href.startswith('http'):\n        use_ssl = image_href.startswith('https')\n        # Extract netloc (network location) and path\n        parsed = urllib.parse.urlparse(image_href)\n        netloc = parsed.netloc\n        path = parsed.path\n        \n        # Extract image_id from the path (last part after the base path)\n        if not path:\n            raise ValueError(_(\"Invalid image href: missing path\"))\n            \n        # Split by '/' and take the last part\n        path_parts = path.strip('/').split('/')\n        if not path_parts:\n            raise ValueError(_(\"Invalid image href: empty path\"))\n            \n        image_id = path_parts[-1]\n        \n        # Validate that image_id is a valid UUID or a valid image ID\n        if not image_id:\n            raise ValueError(_(\"Invalid image href: empty image ID\"))\n            \n        # Check if image_id is a valid UUID\n        if uuidutils.is_uuid_like(image_id):\n            return (image_id, netloc, use_ssl)\n        \n        # If not a UUID, try to extract from the full href\n        # For example, if image_href is \"http://fake.server/12345\", image_id is \"12345\"\n        # We already extracted that above, so we just return it\n        return (image_id, netloc, use_ssl)\n    \n    # If it doesn't start with http, assume it's a direct image ID\n    # This is a fallback case\n    if image_href.startswith('http://'):\n        use_ssl = False\n        netloc = image_href.split('://')[1].split('/')[0]\n        path_parts = image_href.split('://')[1].split('/', 1)[1].split('/')\n        image_id = path_parts[-1] if path_parts else ''\n        if not image_id:\n            raise ValueError(_(\"Invalid image href: missing image ID\"))\n        return (image_id, netloc, use_ssl", "import urllib\nfrom typing import Tuple\nimport uuidutils\nimport re\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(_(\"Invalid image href: empty string\"))\n\n    # Check if the href starts with http or https\n    if image_href.startswith('http://'):\n        use_ssl = False\n        netloc = image_href[7:]\n    elif image_href.startswith('https://'):\n        use_ssl = True\n        netloc = image_href[8:]\n    else:\n        # If no protocol is specified, assume it's a local reference\n        # Try to extract the netloc from the path\n        parsed = urllib.parse.urlparse(image_href)\n        if not parsed.netloc:\n            raise ValueError(_(\"Invalid image href: missing netloc\"))\n        netloc = parsed.netloc\n        use_ssl = False  # Default to http if no protocol is specified\n\n    # Extract the image_id from the path\n    parsed = urllib.parse.urlparse(image_href)\n    path = parsed.path\n    if not path:\n        raise ValueError(_(\"Invalid image href: missing path\"))\n\n    # Remove leading slash and split by '/' to get the last part\n    path_parts = path.lstrip('/').split('/')\n    if not path_parts:\n        raise ValueError(_(\"Invalid image href: empty path\"))\n\n    image_id = path_parts[-1]\n\n    # Validate that image_id is a valid UUID or a valid image ID\n    if not image_id:\n        raise ValueError(_(\"Invalid image href: empty image ID\"))\n\n    # Check if image_id is a valid UUID\n    if uuidutils.is_uuid_like(image_id):\n        pass  # Valid UUID\n    else:\n        # Try to validate as a simple ID (e.g., alphanumeric)\n        # If it's not a UUID, we still accept it as long as it's not empty\n        if not image_id or not image_id.strip():\n            raise ValueError(_(\"Invalid image href: invalid image ID\"))\n\n    return (image_id, netloc, use_ssl)", "import urllib\nfrom typing import Tuple\nimport uuidutils\nimport re\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(_(\"Invalid image href: empty string\"))\n\n    # Check if the href starts with http or https\n    if image_href.startswith('http://'):\n        use_ssl = False\n        netloc = image_href[7:]\n    elif image_href.startswith('https://'):\n        use_ssl = True\n        netloc = image_href[8:]\n    else:\n        # If it doesn't start with http/https, assume it's a local ID or path\n        # Try to extract the image ID from the path\n        if '/' in image_href:\n            # Extract the last part of the path as image_id\n            path_parts = image_href.split('/')\n            image_id = path_parts[-1]\n            # Check if it's a valid UUID\n            if uuidutils.is_uuid_like(image_id):\n                netloc = ''\n                use_ssl = False\n                return (image_id, netloc, use_ssl)\n            else:\n                # Try to extract netloc from the path\n                # This is a fallback - assume the path is just the image ID\n                netloc = ''\n                use_ssl = False\n                return (image_id, netloc, use_ssl)\n        else:\n            # No path, just treat as image ID\n            if uuidutils.is_uuid_like(image_href):\n                netloc = ''\n                use_ssl = False\n                return (image_href, netloc, use_ssl)\n            else:\n                raise ValueError(_(\"Invalid image href: not a valid URL or UUID\"))\n\n    # Parse the netloc part\n    try:\n        parsed = urllib.parse.urlparse(netloc)\n        netloc = parsed.netloc\n        if not netloc:\n            raise ValueError(_(\"Invalid image href: missing netloc\"))\n    except Exception:\n        raise ValueError(_(\"Invalid image href: invalid netloc\"))\n\n    # Extract image_id from the path\n    path = parsed.path\n    if path:\n        image_id = path.strip('/').split('/')[-1]\n    else:\n        image_id = ''\n\n    if not image_id:\n        raise ValueError(_(\"Invalid image href: missing", "import urllib\nfrom typing import Tuple\nimport uuidutils\nimport re\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(_(\"Invalid image href: empty string\"))\n\n    # Check if the href starts with http or https\n    if image_href.startswith('http://'):\n        use_ssl = False\n        netloc = image_href[7:]\n    elif image_href.startswith('https://'):\n        use_ssl = True\n        netloc = image_href[8:]\n    else:\n        # If it doesn't start with http/https, assume it's a bare image ID\n        # or a local reference\n        try:\n            # Try to extract image_id from the end of the string\n            # If it's a UUID, it should be at the end\n            parts = image_href.split('/')\n            image_id = parts[-1]\n            if not uuidutils.is_uuid_like(image_id):\n                raise ValueError(_(\"Invalid image href format\"))\n            netloc = \"\"\n            use_ssl = False\n            return (image_id, netloc, use_ssl)\n        except (IndexError, ValueError):\n            raise ValueError(_(\"Invalid image href format\"))\n\n    # Parse the netloc part (host and port)\n    try:\n        parsed = urllib.parse.urlparse(netloc)\n        if not parsed.netloc:\n            raise ValueError(_(\"Invalid netloc in image href\"))\n        host = parsed.netloc\n        # Check if there's a port\n        port = parsed.port\n        # If port is not set, assume default port\n        if port is None:\n            port = 80 if not use_ssl else 443\n    except Exception:\n        raise ValueError(_(\"Invalid netloc in image href\"))\n\n    # Extract image_id from the path\n    path = parsed.path\n    if not path:\n        raise ValueError(_(\"Invalid image href: missing path\"))\n\n    # Extract the last part of the path as image_id\n    image_id_parts = path.strip('/').split('/')\n    if not image_id_parts:\n        raise ValueError(_(\"Invalid image href: missing image ID\"))\n    image_id = image_id_parts[-1]\n\n    # Validate image_id is a valid UUID\n    if not uuidutils.is"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2002, "num_retrieved": 4, "inference_time": 0.8110246360301971, "generate_results": ["import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2142, "num_retrieved": 9, "inference_time": 0.8110246360301971, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    return re.escape(pattern)", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    return escaped_pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    return escaped_pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    return escaped_pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words\n    return r'\\b' + escaped_pattern + r'\\b'"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2060, "num_retrieved": 8, "inference_time": 0.8110246360301971, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n            \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            env=env,\n            text=True,\n            check=False\n        )\n        \n        # Return error code\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n                if result.stderr:\n                    print(f\"Error output: {result.stderr}\")\n            return result.returncode\n            \n        return 0\n        \n    except Exception as e:\n        if verbose:\n            print(f\"Exception occurred: {str(e)}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n            \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env\n        )\n        \n        # Check if command was successful\n        if result.returncode != 0:\n            if not hide_stderr:\n                print(\"Error executing command:\", result.stderr)\n            return result.returncode\n            \n        return 0\n        \n    except Exception as e:\n        print(\"Error running command:\", str(e))\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Return 0 if successful, otherwise return the returncode\n        if result.returncode == 0:\n            return 0\n        else:\n            return result.returncode\n            \n    except Exception as e:\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n            \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Return 0 if successful, otherwise return the returncode\n        return result.returncode\n        \n    except Exception as e:\n        # Return non-zero error code on exception\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Return 0 if successful, otherwise return the returncode\n        if result.returncode == 0:\n            return 0\n        else:\n            return result.returncode\n            \n    except Exception as e:\n        return 1"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 0.8110246360301971, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1909, "num_retrieved": 1, "inference_time": 0.8110246360301971, "generate_results": ["import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., are not pointed to by prop)\n    # by checking which nodes have no incoming triples with the given property\n    all_nodes = set()\n    for s, p, o in graph:\n        if p == prop:\n            all_nodes.add(o)\n    \n    # Find nodes that are not pointed to by prop (i.e., are roots)\n    # A root has no parent in the hierarchy\n    roots = set()\n    for node in all_nodes:\n        # Check if this node has any incoming triples with the given property\n        has_parent = False\n        for s, p, o in graph:\n            if p == prop and o == node:\n                has_parent = True\n                break\n        if not has_parent:\n            roots.add(node)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., are not pointed to by prop)\n    # by checking which nodes have no incoming triples with the given property\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., if there's a triple (subject, prop, parent))\n        # If it does, it's not a root\n        parent = None\n        for obj in graph.objects(subject, prop):\n            parent = obj\n            break\n        \n        # If no parent found, it's a root\n        if parent is None:\n            roots.add(subject)\n    \n    # Recursively find roots by checking if any of the current roots have children\n    # that might be roots themselves\n    # This is a simplified version - we need to ensure we don't miss any roots\n    # by checking all nodes that are not pointed to by prop\n    \n    # Instead, we can do a more direct approach: collect all nodes that are not\n    # the object of any triple with the given property\n    all_objects = set()\n    for s, p, o in graph:\n        if p == prop:\n            all_objects.add(o)\n    \n    # Any node that is not in all_objects is a root\n    all_nodes = set()\n    for s, p, o in graph:\n        all_nodes.add(s)\n        all_nodes.add(o)\n    \n    # Roots are nodes that are not objects of any triple with prop\n    roots = all_nodes - all_objects\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., are not pointed to by prop)\n    # by checking for any triples (x, prop, y) where x is not the subject of any triple (x, prop, y)\n    # We need to find all nodes that are not the object of any triple (child, prop, parent)\n    \n    # First, collect all objects of the given property\n    objects_of_prop = set()\n    for s, p, o in graph:\n        if p == prop:\n            objects_of_prop.add(o)\n    \n    # Then, find all subjects that are not in the objects_of_prop set\n    # These are the roots\n    all_nodes = set()\n    for s, p, o in graph:\n        all_nodes.add(s)\n        all_nodes.add(o)\n    \n    # Roots are those nodes that are not the object of any triple with prop\n    roots = all_nodes - objects_of_prop\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., are not pointed to by prop)\n    # by checking which nodes have no incoming triples with the given property\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., if there's a triple (subject, prop, parent))\n        parent = None\n        for obj in graph.objects(subject, prop):\n            parent = obj\n            break\n        \n        # If no parent found, this is a root\n        if parent is None:\n            roots.add(subject)\n    \n    # Recursively find roots by checking if any of the current roots have children\n    # that might be roots themselves (this is a simplification - we need to find nodes\n    # with no incoming edges for the given property)\n    \n    # Instead, we can use a more direct approach: find all nodes that are not pointed to\n    # by any other node via the given property\n    all_objects = set()\n    for s, p, o in graph.triples((None, prop, None)):\n        all_objects.add(o)\n    \n    # Find all nodes that are not in all_objects (i.e., have no parent)\n    for subject in graph.subjects(None, None):\n        if subject not in all_objects:\n            roots.add(subject)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., are not pointed to by prop)\n    # by checking for any triples (x, prop, y) where x is not the subject of any triple (x, prop, y)\n    # We need to find all nodes that are not the object of any triple (child, prop, parent)\n    \n    # First, collect all objects of the given property\n    objects_of_prop = set()\n    for s, p, o in graph:\n        if p == prop:\n            objects_of_prop.add(o)\n    \n    # Then, find all subjects that are not in the objects_of_prop set\n    # These are the roots\n    all_nodes = set()\n    for s, p, o in graph:\n        all_nodes.add(s)\n        all_nodes.add(o)\n    \n    # Roots are those nodes that are not the object of any triple with prop\n    roots = all_nodes - objects_of_prop\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2240, "num_retrieved": 9, "inference_time": 0.8110246360301971, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2389, "num_retrieved": 19, "inference_time": 0.8110246360301971, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1980, "num_retrieved": 3, "inference_time": 0.8110246360301971, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2098, "num_retrieved": 8, "inference_time": 0.8110246360301971, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2048, "num_retrieved": 6, "inference_time": 1.268451988697052, "generate_results": ["import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration and validate it against the schema\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename not provided in context\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'File not found: {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied: {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs", "import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration and validate it against the schema\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename is not provided in this function\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'File not found: {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied: {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs", "import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration and validate it against the schema\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename is not provided in this function\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file not found: {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Permission denied when reading configuration file: {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration validation error in file {config_filename}: {error}\",\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs", "import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration and validate it against the schema\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename is not provided in this function\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file not found: {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Permission denied when reading configuration file: {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration validation error in file {config_filename}: {error}\",\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs", "import logging\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration and validate it against the schema\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema filename not provided in context\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration file not found: {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Permission denied when reading configuration file: {config_filename}\",\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f\"Configuration validation error in {config_filename}: {error}\",\n                args=(),\n                exc_info=None\n            ))\n\n    return config_dict, logs"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2006, "num_retrieved": 5, "inference_time": 1.268451988697052, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2059, "num_retrieved": 5, "inference_time": 1.268451988697052, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\" Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    nanoseconds_per_second = 1000000000\n    nanoseconds_per_minute = 60000000000\n    nanoseconds_per_hour = 3600000000000\n    \n    hours = ticks // nanoseconds_per_hour\n    remaining = ticks % nanoseconds_per_hour\n    \n    minutes = remaining // nanoseconds_per_minute\n    remaining = remaining % nanoseconds_per_minute\n    \n    seconds = remaining // nanoseconds_per_second\n    microseconds = (remaining % nanoseconds_per_second) // 1000\n    \n    # Create a Time object with the parsed components\n    # Note: The actual Time class implementation is assumed to accept these parameters\n    return cls(hours, minutes, seconds, microseconds, tz=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\" Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    nanoseconds_per_second = 1000000000\n    nanoseconds_per_minute = 60000000000\n    nanoseconds_per_hour = 3600000000000\n    \n    hours = ticks // nanoseconds_per_hour\n    remaining = ticks % nanoseconds_per_hour\n    \n    minutes = remaining // nanoseconds_per_minute\n    remaining = remaining % nanoseconds_per_minute\n    \n    seconds = remaining // nanoseconds_per_second\n    microseconds = (remaining % nanoseconds_per_second) // 1000\n    \n    # Create a Time object with the parsed components\n    # Note: The actual Time class implementation is assumed to accept these parameters\n    return cls(hours, minutes, seconds, microseconds, tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\" Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    nanoseconds_per_second = 1000000000\n    nanoseconds_per_minute = 60000000000\n    nanoseconds_per_hour = 3600000000000\n    \n    hours = ticks // nanoseconds_per_hour\n    remaining = ticks % nanoseconds_per_hour\n    \n    minutes = remaining // nanoseconds_per_minute\n    remaining = remaining % nanoseconds_per_minute\n    \n    seconds = remaining // nanoseconds_per_second\n    microseconds = (remaining % nanoseconds_per_second) // 1000\n    \n    # Create a Time object with the parsed components\n    # Note: The actual Time class implementation is assumed to accept these parameters\n    return cls(hours, minutes, seconds, microseconds, tz=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\" Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    nanoseconds_per_second = 1_000_000_000\n    nanoseconds_per_minute = nanoseconds_per_second * 60\n    nanoseconds_per_hour = nanoseconds_per_minute * 60\n    nanoseconds_per_day = nanoseconds_per_hour * 24\n    \n    seconds = ticks // nanoseconds_per_second\n    remainder = ticks % nanoseconds_per_second\n    \n    minutes = seconds // 60\n    seconds = seconds % 60\n    \n    hours = minutes // 60\n    minutes = minutes % 60\n    \n    # Create a time object with the extracted components\n    time_obj = Time(hours, minutes, seconds, remainder)\n    \n    # If timezone is provided, set it on the time object\n    if tz is not None:\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\" Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and microseconds\n    nanoseconds_per_second = 1000000000\n    nanoseconds_per_minute = 60000000000\n    nanoseconds_per_hour = 3600000000000\n    \n    hours = ticks // nanoseconds_per_hour\n    remaining = ticks % nanoseconds_per_hour\n    \n    minutes = remaining // nanoseconds_per_minute\n    remaining = remaining % nanoseconds_per_minute\n    \n    seconds = remaining // nanoseconds_per_second\n    microseconds = (remaining % nanoseconds_per_second) // 1000\n    \n    # Create a Time object with the parsed components\n    # Note: The actual Time class implementation is assumed to support these parameters\n    return cls(hours, minutes, seconds, microseconds, tz)"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2033, "num_retrieved": 5, "inference_time": 1.268451988697052, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler\n    if (4, 2) == protocol_version or protocol_version is None:\n        handlers[(4, 2)] = Bolt4x2\n\n    # Add Bolt 4.2 async handler\n    if (4, 2) == protocol_version or protocol_version is None:\n        handlers[(4, 2)] = AsyncBolt4x2\n\n    # Add Bolt 3.0 handler (if needed, based on context)\n    # Note: The context does not explicitly define Bolt 3.x handlers,\n    # so we only include what's explicitly defined in the context.\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler\n    if (4, 2) == protocol_version or protocol_version is None:\n        handlers[(4, 2)] = Bolt4x2\n\n    # Add Bolt 4.2 async handler\n    if (4, 2) == protocol_version or protocol_version is None:\n        handlers[(4, 2)] = AsyncBolt4x2\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n    \n    # Define supported protocol handlers based on the context\n    # Bolt 4.2 is supported by Neo4j version 4.2 - 4.4\n    if protocol_version is None or protocol_version == (4, 2):\n        handlers[(4, 2)] = Bolt4x2\n    \n    # Bolt 4.1 is not explicitly defined in context, so we skip it\n    # Bolt 3.0 is referenced in the route method, but no class is defined\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler\n    if (4, 2) == protocol_version or protocol_version is None:\n        handlers[(4, 2)] = Bolt4x2\n\n    # Add Bolt 4.2 async handler\n    if (4, 2) == protocol_version or protocol_version is None:\n        handlers[(4, 2)] = AsyncBolt4x2\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Add Bolt 4.2 handler\n    if (4, 2) == protocol_version or protocol_version is None:\n        handlers[(4, 2)] = Bolt4x2\n\n    # Add Bolt 4.2 async handler\n    if (4, 2) == protocol_version or protocol_version is None:\n        handlers[(4, 2)] = AsyncBolt4x2\n\n    return handlers"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2115, "num_retrieved": 7, "inference_time": 1.268451988697052, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1962, "num_retrieved": 3, "inference_time": 1.268451988697052, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence if possible\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    \n    if isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    \n    if isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    \n    if isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    \n    # Try to convert to a sequence\n    try:\n        seq = lena.core.Sequence(seq)\n        return seq, \"sequence\"\n    except lena.core.LenaTypeError:\n        raise lena.core.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    \n    if isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    \n    if isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    \n    if isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    \n    # Try to convert to sequence if possible\n    try:\n        seq = lena.core.Sequence(seq)\n        return seq, \"sequence\"\n    except lena.core.LenaTypeError:\n        raise lena.core.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    \n    if isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    \n    if isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    \n    if isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    \n    # Try to convert to sequence if possible\n    try:\n        seq = lena.core.Sequence(seq)\n        return seq, \"sequence\"\n    except lena.core.LenaTypeError:\n        raise lena.core.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    \n    if isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    \n    if isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    \n    if isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    \n    # Try to convert to sequence if possible\n    try:\n        seq = lena.core.Sequence(seq)\n        return seq, \"sequence\"\n    except lena.core.LenaTypeError:\n        raise lena.core.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1961, "num_retrieved": 3, "inference_time": 1.268451988697052, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return current scale\n        if hasattr(self, '_scale') and self._scale is not None:\n            return self._scale\n        else:\n            # Compute scale if not already computed and recompute is False\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            # Compute scale (integral of histogram)\n            # This is a placeholder - actual implementation depends on histogram data\n            # For now, assume scale is computed and stored\n            self._scale = self._compute_scale()\n            return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram\n        self._scale = other\n        return other\n\ndef _compute_scale(self):\n    \"\"\"Compute the integral of the histogram (scale).\"\"\"\n    # Placeholder implementation - actual logic depends on histogram data\n    # This should compute the sum of all values in the histogram\n    # For example: sum of all values in the histogram bins\n    # Return the computed scale\n    return 0.0", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return the current scale\n        if hasattr(self, '_scale') and self._scale is not None:\n            return self._scale\n        else:\n            # Scale not computed yet, compute it\n            if recompute:\n                # Recompute scale (integral of histogram)\n                self._scale = self._compute_scale()\n            else:\n                # Don't compute if not asked to\n                raise lena.core.LenaValueError(\"scale not computed before\")\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Compute new scale and update\n        self._scale = other\n    return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return the current scale\n        if not hasattr(self, '_scale') or self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            # Recompute scale (integral of histogram)\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return the current scale\n        if not hasattr(self, '_scale') or self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            # Recompute scale (integral of histogram)\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return the current scale\n        if not hasattr(self, '_scale') or self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            # Recompute scale (integral of histogram)\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram\n        self._scale = other\n        return self._scale"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1939, "num_retrieved": 2, "inference_time": 1.268451988697052, "generate_results": ["def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        return self._scale\n    \n    if self._scale == 0 or self._scale is None:\n        raise lena.core.LenaValueError(\"Cannot rescale graph with unknown or zero scale\")\n    \n    scale_factor = other / self._scale\n    \n    # Only rescale the last coordinate (and its errors)\n    coords = self._coords\n    field_names = self._field_names\n    \n    # Get the last coordinate index\n    last_idx = len(field_names) - 1\n    \n    # Rescale all values in the last coordinate\n    for i, coord in enumerate(coords[last_idx]):\n        coords[last_idx][i] *= scale_factor\n    \n    # If there are errors, rescale them too\n    # Assuming errors are stored in the same position as the coordinate\n    # This implementation assumes the graph structure supports error handling\n    # and that errors are stored in the same array as the coordinate values\n    \n    self._scale = other", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        return self._scale\n    \n    if self._scale == 0 or self._scale is None:\n        raise lena.core.LenaValueError(\"Cannot rescale graph with unknown or zero scale\")\n    \n    scale_factor = other / self._scale\n    \n    # Rescale only the last coordinate (last field name)\n    field_names = self.field_names\n    if len(field_names) == 0:\n        return self._scale\n    \n    last_field = field_names[-1]\n    \n    # Rescale all values in the last coordinate\n    for i, coord in enumerate(self._coords):\n        if i == len(self._coords) - 1:  # Last coordinate\n            self._coords[i] = [val * scale_factor for val in coord]\n    \n    # Rescale errors if they exist (assumed to be part of the last coordinate)\n    # This assumes errors are stored in the same array as the coordinate\n    # and are rescaled together\n    \n    self._scale = other\n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        return self._scale\n    \n    if self._scale == 0 or self._scale is None:\n        raise lena.core.LenaValueError(\"Cannot rescale graph with unknown or zero scale\")\n    \n    scale_factor = other / self._scale\n    \n    # Rescale only the last coordinate (last field)\n    coords = self._coords\n    field_names = self._field_names\n    \n    # Rescale all values in the last coordinate\n    for i, coord in enumerate(coords):\n        if i == len(coords) - 1:  # Last coordinate\n            coords[i] = [val * scale_factor for val in coord]\n    \n    # Rescale errors (if any) in the last coordinate\n    # Assuming errors are stored in the same array as values\n    # This would need to be adjusted based on actual error storage\n    # For now, we assume errors are part of the coordinate values\n    \n    self._scale = other", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        return self._scale\n    \n    if self._scale == 0 or self._scale is None:\n        raise lena.core.LenaValueError(\"Cannot rescale graph with unknown or zero scale\")\n    \n    scale_factor = other / self._scale\n    \n    # Rescale only the last coordinate (last field name)\n    field_names = self.field_names\n    if len(field_names) == 0:\n        return self._scale\n    \n    last_field = field_names[-1]\n    \n    # Rescale all values in the last coordinate\n    for i, coord in enumerate(self._coords):\n        if i == len(self._coords) - 1:  # Last coordinate\n            self._coords[i] = [val * scale_factor for val in coord]\n    \n    # Rescale errors if they exist (assuming errors are stored in the same coordinate)\n    # This part would depend on how errors are stored in the graph structure\n    # For now, we assume errors are part of the coordinate values\n    \n    self._scale = other\n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        return self._scale\n    \n    if self._scale == 0 or self._scale is None:\n        raise lena.core.LenaValueError(\"Cannot rescale graph with unknown or zero scale\")\n    \n    scale_factor = other / self._scale\n    \n    # Rescale only the last coordinate (and its errors)\n    coords = self._coords\n    field_names = self._field_names\n    \n    # Get the index of the last coordinate\n    last_coord_idx = len(field_names) - 1\n    \n    # Rescale each point's last coordinate and any associated errors\n    for coord_list in coords:\n        if len(coord_list) > last_coord_idx:\n            coord_list[last_coord_idx] *= scale_factor\n    \n    # If there are errors, they are stored in the same list as the coordinate\n    # and should be rescaled together\n    # (The exact structure of errors is assumed to be in the same list)\n    \n    self._scale = other"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1936, "num_retrieved": 2, "inference_time": 1.268451988697052, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    points = []\n    for i, (bin_value, bin_edge) in enumerate(hist.bins.items()):\n        # Get the coordinate based on get_coordinate\n        if get_coordinate == \"left\":\n            coord = bin_edge[0]\n        elif get_coordinate == \"right\":\n            coord = bin_edge[1]\n        else:  # \"middle\"\n            coord = (bin_edge[0] + bin_edge[1]) / 2\n        \n        # Apply make_value to get the value\n        value = make_value(bin_value)\n        \n        # Ensure value is numeric and handle context if needed\n        if isinstance(value, (int, float)):\n            points.append((coord, value))\n        else:\n            # If value is a tuple or", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    points = []\n    for i, (bin_value, bin_edge) in enumerate(hist.bins.items()):\n        # Get the coordinate based on get_coordinate\n        if get_coordinate == \"left\":\n            coord = bin_edge[0]\n        elif get_coordinate == \"right\":\n            coord = bin_edge[1]\n        else:  # \"middle\"\n            coord = (bin_edge[0] + bin_edge[1]) / 2\n        \n        # Apply make_value to get the value\n        value = make_value(bin_value)\n        \n        # Ensure value is iterable and matches field_names\n        if isinstance(value, (int, float)):\n            value = (value,)\n        \n        # Create point with coordinate and value\n        points.append((", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    points = []\n    bin_edges = hist.bins\n    bin_contents = hist.contents\n    \n    # Determine coordinate for each bin\n    if get_coordinate == \"left\":\n        coordinates = [edge for edge in bin_edges[:-1]]\n    elif get_coordinate == \"right\":\n        coordinates = [edge for edge in bin_edges[1:]]\n    else:  # \"middle\"\n        coordinates = [(edge + next_edge) / 2 for edge, next_edge in zip(bin_edges[:-1], bin_edges[1:])]\n    \n    # Create points from bin data\n    for coord, content in zip(coordinates, bin_contents):\n        value = make_value(content)\n        points.append((coord, value))\n    \n    # Create graph with specified", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    points = []\n    bin_edges = hist.bins\n    bin_contents = hist.contents\n    \n    # Determine the coordinate for each bin\n    if get_coordinate == \"left\":\n        coordinates = [edge for edge in bin_edges[:-1]]\n    elif get_coordinate == \"right\":\n        coordinates = [edge for edge in bin_edges[1:]]\n    else:  # \"middle\"\n        coordinates = [(edge + next_edge) / 2 for edge, next_edge in zip(bin_edges[:-1], bin_edges[1:])]\n    \n    # Create points from bin data\n    for coord, content in zip(coordinates, bin_contents):\n        value = make_value(content)\n        points.append((coord, value))\n    \n    # Create graph with", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    points = []\n    bin_edges = hist.bins\n    bin_contents = hist.contents\n    \n    # Calculate coordinates based on get_coordinate\n    if get_coordinate == \"left\":\n        coordinates = [edge for edge in bin_edges[:-1]]\n    elif get_coordinate == \"right\":\n        coordinates = [edge for edge in bin_edges[1:]]\n    else:  # \"middle\"\n        coordinates = [(edge + edge_next) / 2 for edge, edge_next in zip(bin_edges[:-1], bin_edges[1:])]\n    \n    # Create points with values from make_value\n    for coord, content in zip(coordinates, bin_contents):\n        value = make_value(content)\n        points.append((coord, value))\n    \n    # Create"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1969, "num_retrieved": 3, "inference_time": 1.268451988697052, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except Exception as e:\n            errors.append(f\"Error checking if candidate provides interface {iface!r}: {e}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else []\n        \n        # Check each method in the interface\n        for method_name in interface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                if not callable(method):\n                    errors.append(f\"Missing method {method_name} or method is not callable\")\n            except AttributeError:\n                errors.append(f\"Missing method {method_name}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        from inspect import signature\n        for method_name in interface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                if callable", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except AttributeError:\n            errors.append(f\"Interface {iface!r} does not have a providedBy method\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            if not hasattr(candidate, method_name):\n                errors.append(f\"Missing required method {method_name!r} in candidate\")\n    except AttributeError:\n        errors.append(f\"Interface {iface!r} does not define required methods\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Check if method signature matches (simplified - actual signature checking would be more complex)\n                # This is a placeholder for actual signature verification\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step 4: Check that candidate defines all", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface, \n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except AttributeError:\n            errors.append(f\"Interface {iface!r} has no providedBy method\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else {}\n    except AttributeError:\n        # If interface doesn't have methods, no need to check\n        pass\n    else:\n        for method_name in interface_methods:\n            try:\n                candidate_method = getattr(candidate, method_name)\n                if callable(candidate_method):\n                    # Check method signature\n                    try:\n                        # Compare method signature (simplified - actual signature checking would be more complex)\n                        # This is a placeholder for actual signature verification\n                        pass\n                    except Exception as e:\n                        errors.append(f\"Method {method_name} has incorrect signature: {e}\")\n                else:\n                    errors.append(f\"Missing required method {method_name}\")\n            except AttributeError:\n                errors.append", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except Exception as e:\n            errors.append(f\"Error checking if candidate provides interface {iface!r}: {e}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else []\n        \n        # Check each method in the interface\n        for method_name in interface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                if not callable(method):\n                    errors.append(f\"Missing method {method_name} or method is not callable\")\n            except AttributeError:\n                errors.append(f\"Missing required method {method_name}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        from inspect import signature\n        for method_name in interface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                if", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except Exception as e:\n            errors.append(f\"Error checking if candidate provides interface {iface!r}: {e}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods and attributes\n        interface_methods = set()\n        interface_attributes = set()\n\n        # We need to inspect the interface to get required methods and attributes\n        # Since we don't have direct access to interface internals, we assume the interface\n        # defines the required methods/attributes through its specification\n        # In practice, this would involve iterating over the interface's methods and attributes\n\n        # Placeholder: In a real implementation, we would extract methods and attributes from the interface\n        # For now, we assume the interface has a way to get its required methods and attributes\n        # This would typically be done via interface introspection\n\n        # Check methods\n        # This is a simplified version - in reality, we'd need to inspect the interface"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1969, "num_retrieved": 3, "inference_time": 1.268451988697052, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface, \n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n        except AttributeError:\n            errors.append(\"Interface %s does not have a providedBy method\" % iface)\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else []\n    except AttributeError:\n        # If interface doesn't have methods, no need to check\n        pass\n    else:\n        for method_name in interface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                if method is None:\n                    errors.append(\"Candidate does not define method %s\" % method_name)\n            except AttributeError:\n                errors.append(\"Candidate does not define method %s\" % method_name)\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        from zope.interface import verify\n        # Use verify to check method signatures\n        verify.verifyObject(iface, candidate)\n    except Exception as e:\n        errors.append", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    # Get the methods required by the interface\n    try:\n        interface_methods = iface._methods  # Assuming interface has a _methods attribute\n    except AttributeError:\n        # If interface doesn't have _methods, assume no methods are required\n        pass\n    else:\n        for method_name in interface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                if not callable(method):\n                    errors.append(f\"Missing method {method_name} or method is not callable\")\n            except AttributeError:\n                errors.append(f\"Missing required method {method_name}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    # This is a simplified check - in practice, signature verification would require \n    # more detailed inspection of method signatures\n    if errors:\n        # Collect all errors and raise them\n        if len(errors) == 1:\n            raise zope.interface.Invalid(errors[0])\n        else:\n            raise", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface, \n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            if not hasattr(candidate, method_name):\n                errors.append(f\"Missing required method {method_name} in object {candidate}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # In a real implementation, we would compare method signatures\n                # Here we just assume signature checking is done at a higher level\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    try:\n        for attr_name in iface:\n            if not hasattr(candidate, attr_name):\n                errors.append(f\"Missing required attribute {attr_name", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            if not hasattr(candidate, method_name):\n                errors.append(f\"Object {candidate} does not define method {method_name}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # In a real implementation, we would compare signatures here\n                # For now, we just note that the method exists\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    try:\n        for attr_name in iface:\n            if not hasattr(candidate, attr_name):\n                errors.append(f\"Object {candidate} does not define attribute {", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface, \n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        # Get the interface's methods\n        interface_methods = iface._methods if hasattr(iface, '_methods') else []\n        \n        # Check each method in the interface\n        for method_name in interface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                # Verify method exists\n                if not callable(method):\n                    errors.append(f\"Missing method {method_name} in {candidate}\")\n            except AttributeError:\n                errors.append(f\"Missing method {method_name} in {candidate}\")\n    except AttributeError:\n        errors.append(\"Interface does not have defined methods\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        from inspect import signature\n        for method_name in interface_methods:\n            try:\n                method = getattr(candidate, method_name)\n                if not callable(method):\n                    errors.append(f\"Method {method_name} is not callable in {candidate}\")\n                else:\n                    #"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2132, "num_retrieved": 8, "inference_time": 1.268451988697052, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # This is a stub implementation based on the context and typical behavior\n    # of zope.interface.verify.verifyClass\n    from zope.interface import implementedBy\n    from zope.interface.common import ABCInterfaceClass\n    \n    # Check if the candidate implements the interface\n    implements = implementedBy(candidate)\n    \n    # If the interface is not in the implemented interfaces, return False\n    if iface not in implements:\n        return False\n    \n    # If tentative is True, we allow for a more lenient check\n    if tentative:\n        # In tentative mode, we only check if the interface is in the implemented interfaces\n        # without strict verification of the inheritance chain\n        return True\n    \n    # Otherwise, perform a strict verification\n    # This involves checking the inheritance chain to ensure the interface is properly\n    # implemented through the class hierarchy\n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Implementation based on the context and standard behavior of zope.interface.verify.verifyClass\n    # This function checks if the candidate class implements the given interface\n    # It uses the interface's implementedBy attribute to determine the interface hierarchy\n    from zope.interface import implementedBy\n    \n    # Get the implemented interfaces of the candidate class\n    implements = implementedBy(candidate)\n    \n    # Check if the interface is in the implemented interfaces\n    if iface not in implements:\n        return False\n    \n    # If tentative is True, we allow for a more lenient check\n    if tentative:\n        # In tentative mode, we only check if the interface is present in the implementation\n        # without strict C3 linearization checks\n        return True\n    \n    # For strict verification, we need to ensure the interface is properly implemented\n    # and that the class's interface hierarchy respects the C3 linearization\n    # This is typically handled by the interface's implementation logic\n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Implementation based on the context and standard zope.interface behavior\n    from zope.interface import implementedBy\n    from zope.interface import _verifyClass\n    \n    # Get the implemented interface specification for the candidate class\n    implements = implementedBy(candidate)\n    \n    # Check if the candidate class implements the given interface\n    # This involves checking if the interface is in the class's implementation chain\n    if tentative:\n        # For tentative verification, we allow the interface to be in the chain\n        # but not necessarily at the top level\n        return iface in implements\n    \n    # For strict verification, we need to ensure the interface is properly implemented\n    # and that the class's implementation chain includes the interface\n    return iface in implements", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # This is a stub implementation based on the context and typical behavior\n    # of zope.interface.verify.verifyClass\n    from zope.interface.common import ABCInterfaceClass\n    from zope.interface.common import ABCInterface\n    \n    # Check if candidate is a class\n    if not isinstance(candidate, type):\n        raise TypeError(\"candidate must be a class\")\n    \n    # Check if iface is an interface\n    if not hasattr(iface, '__sro__'):\n        raise TypeError(\"iface must be an interface\")\n    \n    # Check if the candidate implements the interface\n    # This is a simplified version; actual implementation would use\n    # interface verification logic from zope.interface\n    try:\n        # Use the standard verifyClass logic from zope.interface\n        # which checks the class's MRO against the interface's SRO\n        # and verifies that the interface is implemented\n        return True\n    except Exception:\n        return False", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface import implementedBy\n    from zope.interface.common import ABCInterfaceClass\n    \n    # Get the implemented interfaces of the candidate class\n    implements = implementedBy(candidate)\n    \n    # Check if the interface is in the implemented interfaces\n    if iface not in implements:\n        return False\n    \n    # If tentative is True, we allow for the possibility that the interface\n    # might be provided, even if not directly implemented\n    if tentative:\n        return True\n    \n    # Otherwise, we require that the interface is directly implemented\n    return True"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2152, "num_retrieved": 9, "inference_time": 1.268451988697052, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, check the bases for metaclass information\n    # In Python, if a class has a __metaclass__ attribute, it takes precedence\n    # Otherwise, we fall back to the default metaclass (type)\n    \n    # Check each base class for __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no __metaclass__ is found in bases, return the default metaclass (type)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, check the bases for metaclass information\n    # In Python, the metaclass is typically determined by the metaclass of the first base\n    # or by the metaclass of the class itself if defined\n    \n    # Check if any base has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found in bases, default to type (the standard metaclass)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, check the bases for metaclass information\n    # In Python, the metaclass is typically determined by the first base class that defines a metaclass\n    # or by the default metaclass (type) if none are specified\n    \n    # Check each base class for a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no metaclass is found in bases, default to type (the standard metaclass)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, check the bases for metaclass information\n    # In Python, the metaclass is typically determined by the metaclass of the first base\n    # or by the metaclass of the class itself if defined\n    \n    # Check if any base has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found in bases, default to type (the standard metaclass)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, check the bases for metaclass information\n    # In Python, the metaclass is typically determined by the metaclass of the first base\n    # or by the metaclass of the class itself if it's defined\n    \n    # Check if any base has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found in bases, use the default metaclass\n    # For Python 2, the default is type; for Python 3, it's type as well\n    # But since we're dealing with a general case, we return type as default\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1982, "num_retrieved": 5, "inference_time": 1.268451988697052, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2049, "num_retrieved": 6, "inference_time": 1.268451988697052, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This implementation is not directly available in the provided context\n    # as the LFU (Least Frequently Used) cache implementation is not fully\n    # detailed. However, based on the context, we can infer that the\n    # functionality would require tracking frequency of access.\n    # Since no specific LFU implementation details are provided, \n    # and the context shows LRU, MRU, FIFO, and RR caches but not LFU,\n    # we cannot accurately implement popitem for LFU without additional\n    # information.\n    raise NotImplementedError(\"LFU cache implementation not available in context\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This implementation is not directly available in the provided context\n    # as the LFU (Least Frequently Used) cache implementation is not fully\n    # detailed. However, based on the context, we can infer that the\n    # functionality would require tracking frequency of access.\n    # Since the context does not provide a complete LFU cache implementation,\n    # we cannot accurately implement popitem for LFU without additional data.\n    # Therefore, this function cannot be correctly implemented from the given context.\n    raise NotImplementedError(\"LFU cache implementation not available in context\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This implementation is not directly available in the provided context\n    # as the LFU (Least Frequently Used) cache implementation is not fully shown.\n    # Based on the context, we can infer that LFU caching requires frequency tracking,\n    # which is not present in the provided code snippets.\n    # Therefore, we cannot implement a correct popitem for LFU without additional context.\n    raise NotImplementedError(\"LFU cache implementation not available in context\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This function is not directly available in the provided context for LFU cache\n    # Based on the context, LFU cache is not implemented, and the popitem method\n    # for LFU is not defined. The context shows LRU, MRU, FIFO, and RR caches,\n    # but no LFU cache implementation with popitem.\n    # Since the docstring specifies \"least frequently used\", and no LFU implementation\n    # is provided in the context, we cannot implement this function based on the given context.\n    # Therefore, we return a placeholder that would be appropriate if LFU were implemented.\n    raise NotImplementedError(\"LFU cache implementation not available in context\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    # This function is not directly available in the provided context for LFU cache\n    # As LFU cache implementation is not shown, we cannot implement popitem for LFU\n    # based on the given context. The context shows LRU, MRU, FIFO, and RR caches,\n    # but not LFU.\n    raise NotImplementedError(\"LFU cache popitem implementation not available in context\")"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2049, "num_retrieved": 6, "inference_time": 1.268451988697052, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Find the key with the smallest access count (least recently used)\n    # In LRU, we typically use a different strategy, but based on context,\n    # this appears to be a placeholder for a cache that tracks usage.\n    # Since the actual LRU implementation is not provided, we assume\n    # the cache maintains a structure where we can find the least recently used.\n    # However, the provided context shows that LFU uses a counter and RR uses random choice.\n    # For LRU, the standard implementation would use a list or deque to track order.\n    # Since no specific LRU implementation is given, and the context shows RR and LFU,\n    # we infer that this function is meant to be implemented for a general cache.\n    # But based on the context, we see that the LRU cache is not fully provided.\n    # Therefore, we return a placeholder that would be used in a real LRU implementation.\n    # In a real LRU cache, we would remove the least recently used item.\n    # Since the exact structure is not provided, we cannot implement the full LRU logic.\n    # However, the docstring says \"least recently used\", so we assume a standard LRU.\n    # We'll implement a basic version that removes the first item in the iteration order.\n    # This is a simplified version and assumes the cache maintains a list of keys.\n    # In a real implementation, this would be more sophisticated.\n    keys = list(self)\n    if not keys:\n        raise KeyError('%s is empty' % type(self).__name__)\n    key = keys[0]\n    return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Find the key with the smallest access count (least recently used)\n    # In LRU, we typically use a different mechanism, but since this is a generic\n    # implementation based on the context, we assume the cache maintains access order\n    # However, the provided context shows that LFU uses a counter and RR uses random choice\n    # For LRU, the standard implementation would use a list or deque to track access order\n    # Since the exact LRU implementation is not provided, we assume the cache maintains\n    # a list of keys in order of last access, and we remove the first one.\n    # But based on the context, this function is likely intended to be part of a specific\n    # cache implementation (like LRU), and the provided context shows that RR uses random choice\n    # and LFU uses a counter.\n    # Since no specific LRU implementation is given, and the function is described as\n    # \"least recently used\", we implement it as removing the first key in the iteration\n    # order, which is the least recently used.\n    keys = list(self)\n    key = keys[0]\n    return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Find the key with the smallest access count (least recently used)\n    # In LRU, we typically use a different mechanism, but since this is a generic\n    # implementation based on the context, we assume the cache maintains access\n    # order or uses a different structure. However, the provided context shows\n    # that LFU uses a counter and RR uses random choice.\n    # Since the docstring specifies \"least recently used\", this suggests LRU,\n    # but the provided context does not contain an LRU implementation.\n    # Given the lack of explicit LRU implementation in the context, we return\n    # a placeholder that would be consistent with the pattern in the provided\n    # code.\n    # In a real LRU cache, we would typically use a deque or similar to track\n    # access order.\n    # Since no such structure is provided in the context, we cannot implement\n    # a full LRU popitem without additional data structures.\n    # Therefore, we return a default implementation that would work for a simple\n    # case, though it's not a true LRU.\n    # This is a placeholder based on the pattern in the provided code.\n    # In a real implementation, this would use a list or deque to track order.\n    # For now, we assume the cache maintains a list of keys and remove the first.\n    keys = list(self)\n    if not keys:\n        raise KeyError('%s is empty' % type(self).__name__)\n    key = keys[0]\n    return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Find the key with the smallest access count (least recently used)\n    # In LRU, we typically use a different mechanism, but since this is a generic\n    # implementation based on the context, we assume it's using a counter or\n    # similar structure to track usage.\n    # However, the provided context shows that LFU uses a counter, and LRU\n    # would use a different approach. Since the docstring says \"least recently used\",\n    # and the context shows RR (random replacement), we infer that this is a\n    # generic implementation that may not be fully specified.\n    # Based on the provided context, we implement a fallback that uses the\n    # most common key (which would be least frequently used in LFU).\n    try:\n        (key, _), = self.__counter.most_common(1)\n    except ValueError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    # Find the key with the least recent usage (LRU)\n    # In LRU, we need to find the key that was last accessed the farthest back\n    # Since we don't have explicit LRU tracking in the base Cache class,\n    # we assume the cache maintains items in a way that allows us to access the least recently used\n    # However, based on the context, this function is likely intended to be implemented\n    # in a specific cache subclass that tracks access order.\n    # Since the provided context shows that LRU is implemented in the base Cache class,\n    # and the popitem method in the base class is not fully defined here,\n    # we implement a fallback based on the structure of the cache.\n    \n    # In a typical LRU cache, we would use a list or deque to track access order\n    # But since the base Cache class doesn't expose such a structure,\n    # and the provided context shows that the LRU implementation is in the func.py module,\n    # we assume that the popitem method in the base class is not fully implemented.\n    \n    # Given that the context shows a similar implementation in RRCache using random choice,\n    # and that LRU would remove the least recently used, we implement a version\n    # that removes the first key in the iteration order (which would be the least recently used).\n    \n    # However, without explicit access order tracking, we cannot accurately implement LRU.\n    # Therefore, we return a placeholder that would work if the cache had an access order.\n    \n    # Since the base Cache class does not maintain access order, and the provided context\n    # does not define a clear LRU implementation, we cannot fully implement this function\n    # without additional information.\n    \n    # Based on the context, we assume that the cache maintains a data structure\n    # that allows us to access the least recently used item.\n    \n    # In the absence of such a structure, we fall back to the most common implementation:\n    # remove the first key in the iteration order.\n    \n    keys = list(self)\n    if not keys:\n        raise KeyError('%s is empty' % type(self).__name__)\n    key = keys[0]\n    return (key, self.pop(key"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1995, "num_retrieved": 5, "inference_time": 0.987820953130722, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = list(self)[-1]\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = list(self)[-1]\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = list(self)[-1]\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = list(self)[-1]\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = list(self)[-1]\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2077, "num_retrieved": 7, "inference_time": 0.987820953130722, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Select a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    value = self.__data.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    value = self.__data.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    value = self.__data.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    value = self.__data.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    value = self.__data.pop(key)\n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1919, "num_retrieved": 3, "inference_time": 0.987820953130722, "generate_results": ["def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return self.param_regex", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    return re.compile(self.param_regex)"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2034, "num_retrieved": 5, "inference_time": 0.987820953130722, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1912, "num_retrieved": 1, "inference_time": 0.987820953130722, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1941, "num_retrieved": 2, "inference_time": 0.987820953130722, "generate_results": ["def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1993, "num_retrieved": 4, "inference_time": 0.987820953130722, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone has an is_ambiguous method, use it to determine if the datetime is ambiguous\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            if is_ambiguous_fn(dt):\n                # If ambiguous, we need to determine which fold to use\n                # We'll use fold=0 by default (the first occurrence)\n                # This is consistent with the behavior of the datetime module\n                return dt.replace(tzinfo=self, fold=0)\n            else:\n                return dt.replace(tzinfo=self, fold=0)\n        except Exception:\n            # Fallback to fold=0 if there's an error\n            return dt.replace(tzinfo=self, fold=0)\n\n    # For timezones without is_ambiguous, check if the datetime is ambiguous\n    # by comparing the fold states\n    dt = dt.replace(tzinfo=self)\n    wall_0 = enfold(dt, fold=0)\n    wall_1 = enfold(dt, fold=1)\n\n    # If both have the same offset and DST, then it's not ambiguous\n    same_offset = wall_0.utcoffset() == wall_1.utcoffset()\n    same_dst = wall_0.dst() == wall_1.dst()\n\n    # If not the same, then it's ambiguous and we need to choose fold=0\n    if not (same_offset and same_dst):\n        # Use fold=0 (the first occurrence)\n        return dt.replace(tzinfo=self, fold=0)\n    else:\n        # Not ambiguous, use fold=0\n        return dt.replace(tzinfo=self, fold=0)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone has an is_ambiguous method, use it to determine if the datetime is ambiguous\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            if is_ambiguous_fn(dt):\n                # If ambiguous, we need to determine which fold to use\n                # We'll use fold=0 by default (the first occurrence)\n                return dt.replace(tzinfo=self, fold=0)\n            else:\n                return dt.replace(tzinfo=self, fold=0)\n        except Exception:\n            # Fallback to fold=0 if there's an error\n            return dt.replace(tzinfo=self, fold=0)\n\n    # For timezones without is_ambiguous, check if the datetime is ambiguous\n    # by comparing fold=0 and fold=1 versions\n    dt = dt.replace(tzinfo=self)\n    wall_0 = enfold(dt, fold=0)\n    wall_1 = enfold(dt, fold=1)\n\n    # If both have the same offset and DST, then it's not ambiguous\n    same_offset = wall_0.utcoffset() == wall_1.utcoffset()\n    same_dst = wall_0.dst() == wall_1.dst()\n\n    # If not the same, then it's ambiguous - use fold=0 (first occurrence)\n    if not (same_offset and same_dst):\n        return dt.replace(tzinfo=self, fold=0)\n    else:\n        return dt.replace(tzinfo=self, fold=0)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone has an is_ambiguous method, use it to determine if the datetime is ambiguous\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            if is_ambiguous_fn(dt):\n                # If ambiguous, we need to determine which fold to use\n                # The fold attribute will be set to 0 or 1 based on the fold state\n                # We use fold=0 for the first occurrence (chronologically earlier)\n                # This is the standard behavior for ambiguous times\n                return dt.replace(tzinfo=self, fold=0)\n            else:\n                return dt.replace(tzinfo=self, fold=0)\n        except Exception:\n            # Fallback to fold=0 if there's an error\n            return dt.replace(tzinfo=self, fold=0)\n\n    # For timezones without is_ambiguous, check if the datetime is ambiguous\n    # by comparing the fold states\n    dt = dt.replace(tzinfo=self)\n    wall_0 = enfold(dt, fold=0)\n    wall_1 = enfold(dt, fold=1)\n\n    # If the offsets are different, then the time is ambiguous\n    if wall_0.utcoffset() != wall_1.utcoffset() or wall_0.dst() != wall_1.dst():\n        # Use fold=0 for the first occurrence (chronologically earlier)\n        return dt.replace(tzinfo=self, fold=0)\n    else:\n        # No ambiguity, use fold=0\n        return dt.replace(tzinfo=self, fold=0)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone has an is_ambiguous method, use it to determine if the datetime is ambiguous\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            if is_ambiguous_fn(dt):\n                # If ambiguous, we need to determine which fold to use\n                # We'll use fold=0 by default (the first occurrence)\n                # The fold attribute will be set based on the timezone's behavior\n                return dt.replace(tzinfo=self, fold=0)\n            else:\n                return dt.replace(tzinfo=self, fold=0)\n        except Exception:\n            # Fallback to fold=0 if there's an error\n            return dt.replace(tzinfo=self, fold=0)\n\n    # For timezones without is_ambiguous, check if the fold attribute has any effect\n    # Create two versions of the datetime with fold=0 and fold=1\n    dt_fold_0 = dt.replace(tzinfo=self, fold=0)\n    dt_fold_1 = dt.replace(tzinfo=self, fold=1)\n    \n    # If both have the same offset and DST, then it's not ambiguous\n    if dt_fold_0.utcoffset() == dt_fold_1.utcoffset() and dt_fold_0.dst() == dt_fold_1.dst():\n        # Not ambiguous, use fold=0\n        return dt.replace(tzinfo=self, fold=0)\n    else:\n        # Ambiguous, use fold=0 (first occurrence)\n        return dt.replace(tzinfo=self, fold=0)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone has an is_ambiguous method, use it to determine if the datetime is ambiguous\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            if is_ambiguous_fn(dt):\n                # If ambiguous, we need to determine which fold to use\n                # We use fold=0 by default (the first occurrence)\n                return dt.replace(tzinfo=self, fold=0)\n            else:\n                return dt.replace(tzinfo=self, fold=0)\n        except Exception:\n            # Fall back to fold=0 if there's an error\n            return dt.replace(tzinfo=self, fold=0)\n\n    # For timezones without is_ambiguous, check if the datetime is ambiguous\n    # by comparing fold=0 and fold=1 versions\n    dt = dt.replace(tzinfo=self)\n    wall_0 = enfold(dt, fold=0)\n    wall_1 = enfold(dt, fold=1)\n\n    # If both have the same offset and DST, then it's not ambiguous\n    same_offset = wall_0.utcoffset() == wall_1.utcoffset()\n    same_dst = wall_0.dst() == wall_1.dst()\n\n    # If it's not ambiguous, use fold=0\n    if not (same_offset and same_dst):\n        # It is ambiguous, so we use fold=0 (first occurrence)\n        return dt.replace(tzinfo=self, fold=0)\n    else:\n        return dt.replace(tzinfo=self, fold=0)"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1940, "num_retrieved": 2, "inference_time": 0.987820953130722, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2097, "num_retrieved": 7, "inference_time": 0.987820953130722, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1995, "num_retrieved": 5, "inference_time": 0.987820953130722, "generate_results": ["def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check for public message\n    if request.get('type') == 'public' or request.get('type') == 'diaspora_public':\n        return 'diaspora'\n    \n    # Then check for private message\n    if request.get('type') == 'private' or request.get('type') == 'diaspora_private':\n        return 'diaspora'\n    \n    # Check for legacy payload\n    if request.get('legacy') == True or request.get('legacy'):\n        return 'diaspora'\n    \n    return None", "def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check for Diaspora public message first\n    if request.get('diaspora_public_message'):\n        return 'diaspora'\n    \n    # Check for Diaspora private message\n    if request.get('diaspora_private_message'):\n        return 'diaspora'\n    \n    # Check for legacy payload\n    if request.get('legacy'):\n        return 'diaspora'\n    \n    return None", "def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check for public message\n    if request.get('type') == 'public' or request.get('type') == 'diaspora-public':\n        return 'diaspora'\n    \n    # Then check for private message\n    if request.get('type') == 'private' or request.get('type') == 'diaspora-private':\n        return 'diaspora'\n    \n    # Check for legacy payload\n    if request.get('legacy') == True or request.get('legacy'):\n        return 'diaspora'\n    \n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check for public message\n    if request.get('type') == 'public' or request.get('type') == 'diaspora-public':\n        return 'diaspora'\n    \n    # Then check for private message\n    if request.get('type') == 'private' or request.get('type') == 'diaspora-private':\n        return 'diaspora'\n    \n    # Check for legacy payload\n    if request.get('legacy') == True or request.get('legacy'):\n        return 'diaspora'\n    \n    return None", "def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n    \n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check for Diaspora public message first\n    if request.get('diaspora_public_message'):\n        return 'diaspora'\n    \n    # Check for Diaspora private message\n    if request.get('diaspora_private_message'):\n        return 'diaspora'\n    \n    # Check for legacy payload\n    if request.get('legacy'):\n        return 'diaspora'\n    \n    return None"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2100, "num_retrieved": 9, "inference_time": 0.987820953130722, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL starts with /matrix/ or contains matrix-related paths\n    if request.url.startswith('/matrix/'):\n        return True\n    \n    # Check if the request method is POST and contains matrix-specific headers\n    if request.method == 'POST' and 'X-Matrix-Request' in request.headers:\n        return True\n    \n    # Check if the request body contains matrix-specific content\n    if request.body and b'matrix' in request.body:\n        return True\n    \n    # Check if the request path matches known matrix endpoints\n    matrix_endpoints = ['/matrix/client/r0/', '/matrix/client/1.0/', '/_matrix/']\n    if any(request.url.startswith(endpoint) for endpoint in matrix_endpoints):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or patterns\n    if request.url.startswith('/matrix/'):\n        return True\n    \n    # Check if the request method is a known Matrix method (e.g., POST to /_matrix/federation/v1/send)\n    if request.method in ['POST', 'GET'] and '/_matrix/' in request.url:\n        return True\n    \n    # Check for Matrix-specific headers\n    if request.headers.get('X-Matrix-Client-Id') or request.headers.get('X-Matrix-Access-Token'):\n        return True\n    \n    # Check if the request body contains Matrix-specific content\n    if request.body and b'matrix' in request.body:\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request path indicates a Matrix endpoint\n    if request.url.startswith('/matrix/'):\n        return True\n    \n    # Check if the request contains Matrix-specific headers or parameters\n    if request.headers.get('X-Matrix-Request'):\n        return True\n    \n    # Check if the request method is one typically used by Matrix\n    if request.method in ['POST', 'GET'] and request.url.endswith('/send'):\n        return True\n    \n    # Check if the request body contains Matrix-specific content\n    if request.body and b'matrix' in request.body:\n        return True\n    \n    # Fallback: use protocol identification to determine if it's a Matrix request\n    try:\n        protocol = identify_protocol_by_request(request)\n        return protocol.PROTOCOL_NAME == 'matrix'\n    except NoSuitableProtocolFoundError:\n        return False\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL starts with /matrix/ or contains matrix-related paths\n    if request.url.startswith('/matrix/'):\n        return True\n    \n    # Check if the request method is POST and contains matrix-specific headers\n    if request.method == 'POST' and request.headers.get('X-Matrix-Request'):\n        return True\n    \n    # Check if the request body contains matrix-specific content\n    if request.body and b'matrix' in request.body:\n        return True\n    \n    # Check if the request path matches known matrix endpoints\n    matrix_endpoints = ['/matrix/client/r0/', '/matrix/client/v1/', '/_matrix/']\n    if any(request.url.startswith(endpoint) for endpoint in matrix_endpoints):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request path indicates a Matrix request\n    if request.url.startswith('/matrix/'):\n        return True\n    \n    # Check if the request contains Matrix-specific headers or parameters\n    if request.headers.get('X-Matrix-Request'):\n        return True\n    \n    # Check if the request method is one that Matrix typically uses\n    if request.method in ['POST', 'GET'] and 'matrix' in request.url.lower():\n        return True\n    \n    # Check if the request body contains Matrix-specific content\n    if request.body and b'matrix' in request.body:\n        return True\n    \n    # Fallback: use protocol identification to determine if it's a Matrix request\n    try:\n        protocol = identify_protocol_by_request(request)\n        return protocol.PROTOCOL_NAME == 'matrix'\n    except NoSuitableProtocolFoundError:\n        return False\n    \n    return False"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2067, "num_retrieved": 6, "inference_time": 0.987820953130722, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser, tz\n    import pytz\n    \n    if isinstance(dt, str):\n        dt = parser.parse(dt)\n    \n    if not isinstance(dt, datetime):\n        raise ValueError(\"Expected datetime object or string\")\n    \n    # Convert to UTC if not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.tzutc())\n    \n    # Format according to RFC 1123: \"Mon, 02 Jan 2000 13:45:00 GMT\"\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser, tz\n    import pytz\n    \n    if isinstance(dt, str):\n        dt = parser.parse(dt)\n    \n    if not isinstance(dt, datetime):\n        raise ValueError(\"Expected datetime object or string\")\n    \n    # Convert to UTC if not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.UTC)\n    \n    # Format as RFC 1123 date string\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser, tz\n    import pytz\n    \n    if isinstance(dt, str):\n        dt = parser.parse(dt)\n    \n    if not isinstance(dt, datetime):\n        raise ValueError(\"Expected datetime object or string\")\n    \n    # Ensure timezone is UTC\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=pytz.utc)\n    \n    # Format as RFC 1123 date string\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser, tz\n    import pytz\n    \n    if isinstance(dt, str):\n        dt = parser.parse(dt)\n    \n    if not isinstance(dt, datetime):\n        raise ValueError(\"Expected datetime object or string\")\n    \n    # Convert to UTC if not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.tzutc())\n    \n    # Format as RFC 1123 date string\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import parser, tz\n    import pytz\n    \n    if isinstance(dt, str):\n        dt = parser.parse(dt)\n    \n    if not isinstance(dt, datetime):\n        raise ValueError(\"Expected datetime object or string\")\n    \n    # Convert to UTC if not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.UTC)\n    \n    # Format according to RFC 1123 format: \"Mon, 02 Jan 2020 13:45:00 GMT\"\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1967, "num_retrieved": 4, "inference_time": 0.987820953130722, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Pattern to match tags (e.g., #tag) outside of code blocks\n    # Code blocks are marked by ``` or ```language\n    pattern = r'(?:^|\\s)(?<!`)(#)([\\w\\-]+)(?!\\s*`)'\n    import re\n    \n    # Split text into lines to handle code blocks\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block\n        if line.strip().startswith('```'):\n            # Inside code block, skip tag matching\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers\n        line = line.strip()\n        # Check if line contains code block markers\n        if line.startswith('```') or line.endswith('```'):\n            result_lines.append(line)\n            continue\n            \n        # Find tags in the line\n        matches = re.finditer(pattern, line)\n        for match in matches:\n            tag = match.group(2)\n            tags.add(tag)\n            # Replace tag with replacer function if provided\n            if replacer:\n                replacement = replacer(tag)\n                line = line.replace(f\"#{tag}\", replacement)\n        \n        result_lines.append(line)\n    \n    # Join lines back into text\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Pattern to match tags (e.g., #tag) outside of code blocks\n    # Code blocks are marked by triple backticks (```) or triple quotes (\"\"\") \n    # and we want to ignore tags inside them\n    pattern = r'(?:^|(?<=\\n))(?<!```)(?<!\\n```)(?<!\\n\"\"\"#)(?:#)([a-zA-Z_][a-zA-Z0-9_]*)(?=\\s|$)'\n    \n    # Split text into lines to process each line separately\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block\n        if line.strip().startswith('```') or line.strip().startswith('\"\"\"'):\n            # Inside code block, skip tag matching\n            result_lines.append(line)\n            continue\n            \n        # Find tags in the line (outside code blocks)\n        matches = re.finditer(pattern, line)\n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            if replacer:\n                # Replace the tag with the result of the replacer function\n                line = line.replace(f\"#{tag}\", replacer(tag))\n        result_lines.append(line)\n    \n    # Join lines back into a single string\n    result_text = \"\\n\".join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    lines = text.splitlines()\n    result = []\n    \n    in_code_block = False\n    code_block_indent = 0\n    \n    for line in lines:\n        # Check if line starts a code block (with triple backticks or other code indicators)\n        if line.strip().startswith('```'):\n            in_code_block = not in_code_block\n            code_block_indent = len(line) - len(line.lstrip(' '))\n            continue\n            \n        # Check if line is inside a code block\n        if in_code_block:\n            result.append(line)\n            continue\n            \n        # Process line outside code block\n        # Remove code block indicators\n        stripped_line = line.strip()\n        \n        # Find tags (e.g., #tag) that are not inside code blocks\n        # Use regex to match #tag patterns\n        import re\n        tag_pattern = r'#(\\w+)(?=\\s|$)'\n        matches = re.finditer(tag_pattern, stripped_line)\n        \n        # Extract tags and replace them if replacer is provided\n        current_line = stripped_line\n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            if replacer:\n                current_line = current_line.replace(f'#{tag}', replacer(tag))\n        \n        # Add processed line to result\n        result.append(current_line)\n    \n    # Join lines back into a single string\n    processed_text = '\\n'.join(result)\n    \n    return (tags, processed_text)", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block, skip tag detection\n            result_lines.append(line)\n            continue\n            \n        # Extract tags (e.g., #tag) that are not inside code blocks\n        # Use regex to find #tag patterns\n        import re\n        tag_pattern = r'#(\\w+)(?=[\\s\\W]|$)'\n        matches = re.finditer(tag_pattern, line)\n        \n        # Build new line without tags for processing\n        new_line = line\n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            if replacer:\n                new_line = new_line.replace(f'#{tag}', replacer(tag))\n        \n        result_lines.append(new_line)\n    \n    result_text = '\\n'.join(result_lines)\n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    import re\n    \n    tags = set()\n    # Pattern to match tags (e.g., #tag) that are not inside code blocks\n    # Code blocks are marked by ``` or ```language\n    pattern = r'(?<!```)(?<!\\s)#([a-zA-Z_][a-zA-Z0-9_]*)(?!\\s)(?!\\s*```)'\n    \n    # Split text into parts using code block delimiters\n    parts = []\n    code_block = False\n    current_part = \"\"\n    \n    i = 0\n    while i < len(text):\n        if text[i:i+3] == \"```\":\n            if not code_block:\n                if current_part:\n                    parts.append(current_part)\n                current_part = \"\"\n                code_block = True\n            else:\n                code_block = False\n                if current_part:\n                    parts.append(current_part)\n                current_part = \"\"\n        elif code_block:\n            current_part += text[i]\n        else:\n            # Look for tag pattern outside code blocks\n            match = re.search(pattern, text[i:])\n            if match:\n                tag = match.group(1)\n                tags.add(tag)\n                # Replace the tag with replacer if provided\n                if replacer:\n                    replacement = replacer(tag)\n                    text = text.replace(f\"#{tag}\", replacement)\n                # Remove the tag from the original text to avoid double matching\n                text = text.replace(f\"#{tag}\", \"\")\n            else:\n                current_part += text[i]\n        i += 1\n    \n    # Add the last part if it exists\n    if current_part:\n        parts.append(current_part)\n    \n    # Reconstruct the text from parts\n    result_text = \"\".join(parts)\n    \n    # Re-add the tags that were removed during replacement\n    for tag in tags:\n        result_text = result_text.replace(f\"#{tag}\", \"\")\n    \n    # Re-insert the tags with replacer if provided\n    for tag in tags:\n        if replacer:\n            result_text = result_text.replace(f\"#{tag}\", repl"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1971, "num_retrieved": 3, "inference_time": 0.987820953130722, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Pattern to match URLs in text (including http/https, and various formats)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+\\.[^\\s<>\"{}|\\\\^`[\\]]+|http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)'\n    \n    # Replace matched URLs with HTML links with proper attributes\n    def replace_url(match):\n        url = match.group(0)\n        # Ensure the URL is properly formatted\n        if url.startswith('www.'):\n            url = 'https://' + url\n        # Add target=\"_blank\" and rel=\"noopener noreferrer\" attributes\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{url}</a>'\n    \n    # Replace all URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Regular expression to match URLs in text\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Check if it's a full URL (starts with http/https or www)\n        if url.startswith('www.'):\n            url = 'https://' + url\n        \n        # Replace with a link with target and rel attributes\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{url}</a>'\n    \n    # Replace all URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Regular expression to match URLs in text\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Handle relative URLs by adding protocol if missing\n        if not url.startswith('http'):\n            url = 'http://' + url\n        # Return the URL wrapped in an anchor tag with proper attributes\n        return f'<a href=\"{url}\" rel=\"noopener noreferrer\" target=\"_blank\">{url}</a>'\n    \n    # Replace all URLs in the text with linkified versions\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Regular expression to match URLs in text\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Check if it's already a full URL (starts with http/https)\n        if not url.startswith('http'):\n            url = 'http://' + url\n        \n        # Replace with a link with target=\"_blank\" and rel=\"noopener noreferrer\"\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{url}</a>'\n    \n    # Replace all matched URLs with linkified versions\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Regular expression to match URLs in text\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*(\\.[a-zA-Z]{2,})|www\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?(\\.[a-zA-Z0-9]([a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])?)*(\\.[a-zA-Z]{2,}))'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Add attributes to links\n        return f'<a href=\"{url}\" rel=\"noopener noreferrer\" target=\"_blank\">{url}</a>'\n    \n    # Replace URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2027, "num_retrieved": 5, "inference_time": 0.987820953130722, "generate_results": ["import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=10)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=10)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=10)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=10)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "import requests\nfrom typing import Optional\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=10)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2104, "num_retrieved": 7, "inference_time": 0.987820953130722, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a hash symbol\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the hash symbol and check if the rest is valid\n    tag_name = tag[1:]\n    \n    # Tag name should not be empty\n    if not tag_name:\n        return False\n    \n    # Tag name should only contain alphanumeric characters, underscores, and hyphens\n    # and should not contain consecutive special characters or start/end with special characters\n    import re\n    pattern = r'^[a-zA-Z0-9_\\-]+$'\n    if not re.match(pattern, tag_name):\n        return False\n    \n    # Tag name should not be too long (common limit is 50 characters)\n    if len(tag_name) > 50:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a '#' character\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the '#' and check if the remaining part is valid\n    tag_name = tag[1:]\n    \n    # Tag name should not be empty\n    if not tag_name:\n        return False\n    \n    # Tag name should only contain alphanumeric characters, hyphens, underscores, and spaces\n    # and should not contain consecutive spaces or invalid characters\n    import re\n    pattern = r'^[a-zA-Z0-9\\-_ ]+$'\n    if not re.match(pattern, tag_name):\n        return False\n    \n    # Tag name should not contain consecutive spaces\n    if '  ' in tag_name:\n        return False\n    \n    # Tag name should not start or end with a space\n    if tag_name.startswith(' ') or tag_name.endswith(' '):\n        return False\n    \n    # Tag name should not contain more than 2 consecutive hyphens or underscores\n    if re.search(r'[-_]{3,}', tag_name):\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a hash symbol\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the hash symbol for further validation\n    tag_name = tag[1:]\n    \n    # Tags should not be empty after removing the hash\n    if not tag_name:\n        return False\n    \n    # Tags should only contain alphanumeric characters, underscores, and hyphens\n    # and should not contain spaces or special characters\n    import re\n    pattern = r'^[a-zA-Z0-9_\\-]+$'\n    if not re.match(pattern, tag_name):\n        return False\n    \n    # Tags should not be too long (common limit is 50 characters)\n    if len(tag) > 50:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a hash symbol\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the hash symbol and check if the rest is valid\n    tag_name = tag[1:]\n    \n    # Tag name should not be empty\n    if not tag_name:\n        return False\n    \n    # Tag name should only contain alphanumeric characters, underscores, and hyphens\n    # and should not contain consecutive special characters or start/end with special characters\n    import re\n    pattern = r'^[a-zA-Z0-9_\\-]+$'\n    if not re.match(pattern, tag_name):\n        return False\n    \n    # Tag name should not be too long (common limit is 20 characters)\n    if len(tag_name) > 20:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a hash symbol\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the hash symbol for further validation\n    tag_name = tag[1:]\n    \n    # Tags should not be empty after removing the hash\n    if not tag_name:\n        return False\n    \n    # Tags should only contain alphanumeric characters, underscores, and hyphens\n    # and should not contain spaces or special characters\n    import re\n    pattern = r'^[a-zA-Z0-9_\\-]+$'\n    if not re.match(pattern, tag_name):\n        return False\n    \n    # Tags should not be too long (e.g., max 50 characters)\n    if len(tag) > 50:\n        return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2095, "num_retrieved": 7, "inference_time": 0.8209103792905807, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2006, "num_retrieved": 4, "inference_time": 0.8209103792905807, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n    \n    Basically we've already verified the sender is who they say when receiving the payload. \n    However, the sender might be trying to set another author in the payload itself, since Diaspora has the sender \n    in both the payload headers AND the object. We must ensure they're the same.\n    \"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n    \n    Basically we've already verified the sender is who they say when receiving the payload. \n    However, the sender might be trying to set another author in the payload itself, \n    since Diaspora has the sender in both the payload headers AND the object. \n    We must ensure they're the same.\n    \"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2028, "num_retrieved": 5, "inference_time": 0.8209103792905807, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    path = document_path or '/.well-known/nodeinfo'\n    doc, status_code, error = fetch_document(host=url, path=path)\n    if not doc or error:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    path = document_path or '/.well-known/nodeinfo'\n    doc, status_code, error = fetch_document(host=url, path=path)\n    if not doc or error:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    path = document_path or '/.well-known/nodeinfo'\n    doc, status_code, error = fetch_document(host=url, path=path)\n    if not doc or error:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    path = document_path or '/.well-known/nodeinfo'\n    doc, status_code, error = fetch_document(host=url, path=path)\n    if not doc or error:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    path = document_path or '/.well-known/nodeinfo'\n    doc, status_code, error = fetch_document(host=url, path=path)\n    if not doc or error:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2073, "num_retrieved": 6, "inference_time": 0.8209103792905807, "generate_results": ["from Crypto.PublicKey import RSA\nfrom Crypto.Signature import pkcs1_15\nfrom Crypto.Hash import SHA256\nfrom lxml import etree\nimport base64\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        xml_doc = etree.fromstring(doc)\n        \n        # Create a hash of the XML content\n        xml_content = etree.tostring(xml_doc, encoding='unicode')\n        hash_obj = SHA256.new(xml_content.encode('utf-8'))\n        \n        # Load the public key\n        pub_key = RSA.import_key(public_key)\n        \n        # Verify the signature using PKCS#1 v1.5 padding\n        pkcs1_15.new(pub_key).verify(hash_obj, base64.b64decode(signature))\n        \n        return True\n    except (ValueError, TypeError, Exception):\n        return False", "from Crypto.PublicKey import RSA\nfrom Crypto.Signature import pkcs1_15\nfrom Crypto.Hash import SHA256\nfrom lxml import etree\nimport base64\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        xml_doc = etree.fromstring(doc)\n        \n        # Create a hash of the XML content\n        xml_content = etree.tostring(xml_doc, encoding='unicode')\n        hash_obj = SHA256.new(xml_content.encode('utf-8'))\n        \n        # Load the public key\n        pub_key = RSA.import_key(public_key)\n        \n        # Verify the signature using PKCS#1 v1.5 padding\n        pkcs1_15.new(pub_key).verify(hash_obj, base64.b64decode(signature))\n        \n        return True\n    except (ValueError, TypeError, Exception):\n        return False", "from Crypto.PublicKey import RSA\nfrom Crypto.Signature import pkcs1_15\nfrom Crypto.Hash import SHA256\nfrom lxml import etree\nimport base64\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        xml_doc = etree.fromstring(doc)\n        \n        # Create a hash of the XML content\n        xml_content = etree.tostring(xml_doc, encoding='unicode')\n        hash_obj = SHA256.new(xml_content.encode('utf-8'))\n        \n        # Load the public key\n        pub_key = RSA.import_key(public_key)\n        \n        # Verify the signature using the public key\n        pkcs1_15.new(pub_key).verify(hash_obj, base64.b64decode(signature))\n        \n        return True\n    except (ValueError, TypeError, Exception):\n        return False", "from Crypto.PublicKey import RSA\nfrom Crypto.Signature import pkcs1_15\nfrom Crypto.Hash import SHA256\nfrom lxml import etree\nimport base64\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        xml_doc = etree.fromstring(doc)\n        \n        # Create a hash of the XML content\n        xml_content = etree.tostring(xml_doc, encoding='unicode')\n        hash_obj = SHA256.new(xml_content.encode('utf-8'))\n        \n        # Load the public key\n        pub_key = RSA.import_key(public_key)\n        \n        # Verify the signature using the public key\n        pkcs1_15.new(pub_key).verify(hash_obj, base64.b64decode(signature))\n        \n        return True\n    except (ValueError, TypeError, Exception):\n        return False", "from Crypto.PublicKey import RSA\nfrom Crypto.Signature import pkcs1_15\nfrom Crypto.Hash import SHA256\nfrom lxml import etree\nimport base64\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        xml_doc = etree.fromstring(doc)\n        \n        # Create a hash of the XML content\n        xml_content = etree.tostring(xml_doc, encoding='unicode')\n        hash_obj = SHA256.new(xml_content.encode('utf-8'))\n        \n        # Load the public key\n        pub_key = RSA.import_key(public_key)\n        \n        # Verify the signature using the public key\n        pkcs1_15.new(pub_key).verify(hash_obj, base64.b64decode(signature))\n        \n        return True\n    except (ValueError, TypeError, Exception):\n        return False"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2130, "num_retrieved": 8, "inference_time": 0.8209103792905807, "generate_results": ["import json\nimport re\nfrom lxml import etree\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link['rel'],\n                    'type': link['type'],\n                    'href': link['href']\n                })\n            return result\n    except (json.JSONDecodeError, KeyError):\n        pass\n    \n    # Fall back to parsing XRD (old format)\n    try:\n        root = etree.fromstring(document)\n        # Extract handle from the subject or first link with rel=\"self\"\n        handle = None\n        for link in root.xpath('//link[@rel=\"self\"]'):\n            if link.get('href'):\n                # Extract handle from href if it follows the pattern user@domain.tld\n                match = re.match(r\"(.*?)/([^/]+)\", link.get('href'))\n                if match:\n                    handle = match.group(2)\n        \n        # If no handle found, try to find the handle in the \"Alias\" element\n        if not handle:\n            for elem in root.xpath('//element[@name=\"Alias\"]'):\n                alias = elem.get('value')\n                if alias:\n                    match = re.match(r\"([^/]+)/([^/]+)\", alias)\n                    if match:\n                        handle = match.group(2)\n        \n        # Extract links\n        links = []\n        for link in root.xpath('//link'):\n            links.append({\n                'rel': link.get('rel'),\n                'type': link.get('type'),\n                'href': link.get('href')\n            })\n        \n        if handle:\n            return {\n                'handle': handle,\n                'links': links\n            }\n    except etree.XMLSyntaxError:\n        pass\n    \n    # If parsing fails, return empty dict\n    return {}", "import json\nimport re\nfrom lxml import etree\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link.get('rel'),\n                    'type': link.get('type'),\n                    'href': link.get('href')\n                })\n            return result\n    except (json.JSONDecodeError, KeyError):\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        root = etree.fromstring(document)\n        # Extract handle from the subject or from the first link with rel=\"http://webfinger.net/rel/profile-page\"\n        handle = None\n        for link in root.xpath(\"//link[@rel='http://webfinger.net/rel/profile-page']\"):\n            handle = link.get('href')\n            break\n        \n        if not handle:\n            # Fallback: look for Alias element\n            for elem in root.xpath(\"//element[@name='Alias']\"):\n                handle = elem.get('href')\n                break\n        \n        if not handle:\n            return {}\n        \n        # Extract links\n        links = []\n        for link in root.xpath(\"//link\"):\n            links.append({\n                'rel': link.get('rel'),\n                'type': link.get('type'),\n                'href': link.get('href')\n            })\n        \n        # Extract additional elements like guid, public key, etc.\n        result = {\n            'handle': handle,\n            'links': links\n        }\n        \n        # Look for specific elements in XRD\n        for elem in root.xpath(\"//element[@name='Alias']\"):\n            result['handle'] = elem.get('href')\n        \n        # Look for guid in links with rel=\"http://joindiaspora.com/guid\"\n        guid_link = root.xpath(\"//link[@rel='http://joindiaspora.com/guid']\")\n        if guid_link:\n            result['guid'] = guid_link[0].get('href')\n        \n        # Look for public key in links with rel=\"diaspora", "import json\nimport re\nfrom lxml import etree\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link['rel'],\n                    'type': link['type'],\n                    'href': link['href']\n                })\n            return result\n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        root = etree.fromstring(document)\n        # Extract handle from the subject or first link\n        handle = None\n        for link in root.xpath('//link[@rel=\"http://webfinger.net/rel/profile-page\"]'):\n            handle = link.get('href')\n            break\n        \n        if not handle:\n            # Fallback: look for handle in the alias element\n            for elem in root.xpath('//element[@name=\"Alias\"]'):\n                handle = elem.get('href')\n                break\n        \n        if not handle:\n            return {}\n            \n        # Extract links\n        links = []\n        for link in root.xpath('//link'):\n            links.append({\n                'rel': link.get('rel'),\n                'type': link.get('type'),\n                'href': link.get('href')\n            })\n        \n        return {\n            'handle': handle,\n            'links': links\n        }\n    except etree.XMLSyntaxError:\n        return {}\n    \n    return {}", "import json\nimport re\nfrom lxml import etree\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link.get('rel'),\n                    'type': link.get('type'),\n                    'href': link.get('href')\n                })\n            return result\n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        root = etree.fromstring(document)\n        # Extract handle from the subject or first link\n        handle = None\n        for link in root.xpath('//link[@rel=\"http://webfinger.net/rel/profile-page\"]'):\n            handle = link.get('href')\n            break\n        \n        if not handle:\n            # Try to find handle in the Alias element\n            for elem in root.xpath('//element[@name=\"Alias\"]'):\n                handle = elem.get('href')\n                break\n        \n        if not handle:\n            # Fallback: try to extract from any link with rel=\"http://webfinger.net/rel/profile-page\"\n            for link in root.xpath('//link[@rel=\"http://webfinger.net/rel/profile-page\"]'):\n                handle = link.get('href')\n                break\n        \n        result = {\n            'handle': handle,\n            'links': []\n        }\n        \n        for link in root.xpath('//link'):\n            result['links'].append({\n                'rel': link.get('rel'),\n                'type': link.get('type'),\n                'href': link.get('href')\n            })\n        \n        return result\n    except etree.XMLSyntaxError:\n        # If parsing fails, return empty dict\n        pass\n    \n    # If all parsing attempts fail, return empty dict\n    return {}", "import json\nimport re\nfrom lxml import etree\nfrom typing import Dict, Any\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if 'subject' in data and 'links' in data:\n            result = {\n                'handle': data['subject'],\n                'links': []\n            }\n            for link in data['links']:\n                result['links'].append({\n                    'rel': link.get('rel'),\n                    'type': link.get('type'),\n                    'href': link.get('href')\n                })\n            return result\n    except json.JSONDecodeError:\n        pass\n    \n    # Fall back to parsing XRD (old format)\n    try:\n        root = etree.fromstring(document)\n        # Extract handle from the subject or first link\n        handle = None\n        for link in root.xpath('//link[@rel=\"http://webfinger.net/rel/profile-page\"]'):\n            handle = link.get('href')\n            break\n        \n        if not handle:\n            # Try to find handle in the Alias element\n            for elem in root.xpath('//element[@name=\"Alias\"]'):\n                handle = elem.get('href')\n                break\n        \n        if not handle:\n            # Fallback: try to extract from any link with rel=\"http://webfinger.net/rel/profile-page\"\n            for link in root.xpath('//link[@rel=\"http://webfinger.net/rel/profile-page\"]'):\n                handle = link.get('href')\n                break\n        \n        result = {\n            'handle': handle,\n            'links': []\n        }\n        \n        for link in root.xpath('//link'):\n            result['links'].append({\n                'rel': link.get('rel'),\n                'type': link.get('type'),\n                'href': link.get('href')\n            })\n        \n        return result\n    except etree.XMLSyntaxError:\n        # If parsing fails, return empty dict\n        return {}\n    \n    return {}"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2145, "num_retrieved": 9, "inference_time": 0.8209103792905807, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the handle\n        # The handle format is typically \"username@host\"\n        # We use the host part to construct the webfinger endpoint\n        if \"@\" not in handle:\n            return None\n            \n        user, host = handle.rsplit(\"@\", 1)\n        webfinger_url = f\"http://{host}/.well-known/webfinger?resource=acct:{user}@{host}\"\n        \n        # Fetch the document\n        document, status_code, exception = fetch_document(webfinger_url)\n        \n        # If successful, return the document content\n        if document and status_code == 200:\n            return document\n            \n        # If there's an exception or status code is not 200, return None\n        return None\n        \n    except Exception:\n        # Do not raise if retrieval fails\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Use fetch_document to retrieve the webfinger document\n        # The URL for webfinger is typically <handle>@<domain> where domain is inferred from handle\n        # For example, if handle is \"alice@example.com\", the URL would be \"https://example.com/.well-known/webfinger\"\n        # We extract the domain from the handle\n        if \"@\" in handle:\n            domain = handle.split(\"@\")[1]\n            url = f\"https://{domain}/.well-known/webfinger\"\n        else:\n            # If no @ symbol, assume it's a domain or handle without domain\n            # Default to a simple webfinger URL\n            url = f\"https://{handle}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, code, exception = fetch_document(url)\n        \n        # Return the document if successfully retrieved, otherwise None\n        if document and code == 200:\n            return document\n        else:\n            return None\n    except Exception:\n        # Do not raise if it fails\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Use fetch_document to retrieve the webfinger document\n        # The URL for webfinger is typically <handle>@<domain> where domain is derived from handle\n        # For example, if handle is \"alice@example.com\", the URL would be \"https://example.com/.well-known/webfinger\"\n        domain = handle.split(\"@\")[1] if \"@\" in handle else None\n        if not domain:\n            return None\n            \n        url = f\"https://{domain}/.well-known/webfinger\"\n        \n        document, status_code, exception = fetch_document(url)\n        \n        # Return document if successfully retrieved, otherwise None\n        if document and status_code == 200:\n            return document\n        else:\n            return None\n            \n    except Exception:\n        # Do not raise if retrieval fails\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Use fetch_document to retrieve the webfinger document\n        # The URL for webfinger is typically <handle>@<domain> where domain is derived from handle\n        # For example, if handle is \"alice@example.com\", the URL is \"https://example.com/.well-known/webfinger\"\n        domain = handle.split(\"@\")[1] if \"@\" in handle else None\n        if not domain:\n            return None\n            \n        url = f\"https://{domain}/.well-known/webfinger\"\n        \n        document, status_code, exception = fetch_document(url)\n        \n        # Return the document if successfully retrieved, otherwise None\n        if document and status_code == 200:\n            return document\n        else:\n            return None\n    except Exception:\n        # Do not raise if retrieval fails\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Use fetch_document to retrieve the webfinger document\n        # The URL for webfinger is typically <handle>@<domain> where domain is inferred from handle\n        # For ActivityPub, the webfinger endpoint is usually handled via the handle itself\n        # We use the handle directly as the endpoint for webfinger lookup\n        url = f\"http://webfinger.net/{handle}\"\n        document, status_code, exception = fetch_document(url)\n        if exception:\n            return None\n        return document\n    except Exception:\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2146, "num_retrieved": 9, "inference_time": 0.8209103792905807, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle: str) -> dict:\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # First, try to retrieve the webfinger document using the standard webfinger method\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Parse the links to find the self link with type application/x-diaspora+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/x-diaspora+json\":\n            return {\"url\": link[\"href\"]}\n    \n    # If no Diaspora self link is found, return empty dict\n    return {}", "def retrieve_and_parse_diaspora_webfinger(handle: str) -> dict:\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # First, try to retrieve the webfinger document using the standard webfinger method\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Parse the links to find the self link with type application/x-diaspora+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/x-diaspora+json\":\n            # Return the href as a dictionary with the key \"url\"\n            return {\"url\": link[\"href\"]}\n    \n    # If no Diaspora self link is found, return empty dict\n    return {}", "def retrieve_and_parse_diaspora_webfinger(handle: str) -> dict:\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # First, try to retrieve the webfinger document using the standard webfinger method\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Look for the self link with type application/x-diaspora+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/x-diaspora+json\":\n            # Extract the href from the link\n            webfinger_url = link[\"href\"]\n            # Parse the webfinger document to extract relevant information\n            # For Diaspora, we typically extract the profile URL and other metadata\n            return {\n                \"url\": webfinger_url,\n                \"handle\": handle,\n                \"type\": \"diaspora\"\n            }\n    \n    # If no valid link is found, return empty dict\n    return {}", "def retrieve_and_parse_diaspora_webfinger(handle: str) -> dict:\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # First, try to retrieve the webfinger document using the standard webfinger method\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Look for the self link with type \"application/diaspora+json\"\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            # Extract the href from the link\n            profile_id = link[\"href\"]\n            # Fetch the profile document using the profile ID\n            profile_document, status_code, ex = fetch_document(profile_id, extra_headers={'accept': 'application/diaspora+json'})\n            if profile_document:\n                try:\n                    profile_data = json.loads(decode_if_bytes(profile_document))\n                    # Return the parsed profile data\n                    return profile_data\n                except json.JSONDecodeError:\n                    return {}\n    \n    # If no valid link is found, return empty dict\n    return {}", "def retrieve_and_parse_diaspora_webfinger(handle: str) -> dict:\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    # First, try to retrieve the webfinger document using the standard webfinger method\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Parse the links to find the self link with type application/x-diaspora+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/x-diaspora+json\":\n            return {\"url\": link[\"href\"]}\n    \n    # If no Diaspora self link is found, return empty dict\n    return {}"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2144, "num_retrieved": 9, "inference_time": 0.8209103792905807, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Create a template for Diaspora host-meta with webfinger_host\n    webfinger_host = host\n    hostmeta = generate_host_meta(template=\"diaspora\", webfinger_host=webfinger_host)\n    return hostmeta", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Create a template for Diaspora host-meta with webfinger_host\n    webfinger_host = host\n    hostmeta = generate_host_meta(template=\"diaspora\", webfinger_host=webfinger_host)\n    return hostmeta", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Create a template for Diaspora host-meta with webfinger_host\n    webfinger_host = host\n    hostmeta = generate_host_meta(template=\"diaspora\", webfinger_host=webfinger_host)\n    return hostmeta", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Create a template for Diaspora host-meta with required webfinger_host\n    webfinger_host = host\n    hostmeta = generate_host_meta(template=\"diaspora\", webfinger_host=webfinger_host)\n    return hostmeta", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Create a template for Diaspora host-meta with required webfinger_host\n    webfinger_host = host\n    hostmeta = generate_host_meta(template=\"diaspora\", webfinger_host=webfinger_host)\n    return hostmeta"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1974, "num_retrieved": 3, "inference_time": 0.8209103792905807, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        if method.lower() == \"post\":\n            response = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n        else:\n            response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2009, "num_retrieved": 4, "inference_time": 0.8209103792905807, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val if not keys else {}\n    \n    # Process each subsequent key in the chain\n    for k in keys:\n        if not isinstance(current, dict):\n            raise TypeError(\"Cannot insert into non-dictionary object\")\n        current = current.setdefault(k, {})", "def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val if not keys else {}\n    \n    # Iterate through the remaining keys to create the nested structure\n    for k in keys:\n        if current[k] is None:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value\n    current[key] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    if key:\n        keys_list = [key] + list(keys)\n        for i, k in enumerate(keys_list):\n            if i == len(keys_list) - 1:\n                # Last key - assign the value\n                current[k] = val\n            else:\n                # Intermediate key - create nested dictionary if not exists\n                if k not in current:\n                    current[k] = {}\n                current = current[k]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    if key:\n        key_parts = [key] + list(keys)\n        for i, k in enumerate(key_parts):\n            if i == len(key_parts) - 1:\n                # Last key - assign the value\n                current[k] = val\n            else:\n                # Intermediate key - create the nested structure if needed\n                if k not in current:\n                    current[k] = {}\n                current = current[k]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    if key:\n        keys_list = [key] + list(keys)\n        for i, k in enumerate(keys_list):\n            if i == len(keys_list) - 1:\n                # Last key - assign the value\n                current[k] = val\n            else:\n                # Intermediate key - create nested dictionary if not exists\n                if k not in current:\n                    current[k] = {}\n                current = current[k]"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2105, "num_retrieved": 7, "inference_time": 0.8209103792905807, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class IniType(ComplexType):\n        \"\"\"Complex type that resolves a list of file names.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if not isinstance(value, list):\n                raise exceptions.SpecParserException(\n                    f\"Expected list for 'list_of_file_names', got {type(value).__name__}\")\n            \n            # Validate that each item in the list is a string\n            for item in value:\n                if not isinstance(item, str):\n                    raise exceptions.SpecParserException(\n                        f\"All items in list must be strings, got {type(item).__name__}\")\n            \n            # Return the list as is (no transformation needed)\n            return value\n    \n    return IniType(\n        arg_name=spec_option.get('name'),\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('sub_command_name'),\n        spec_option=spec_option\n    )", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class IniType(ComplexType):\n        \"\"\"Complex type that resolves a list of file names.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if not isinstance(value, list):\n                raise exceptions.SpecParserException(\n                    f\"Expected list for 'list_of_file_names', got {type(value).__name__}\")\n            \n            # Validate that each item in the list is a string\n            for item in value:\n                if not isinstance(item, str):\n                    raise exceptions.SpecParserException(\n                        f\"All items in list must be strings, got {type(item).__name__}\")\n            \n            # Return the list as is (no transformation needed)\n            return value\n    \n    return IniType(\n        arg_name=spec_option.get('name'),\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('sub_command_name'),\n        spec_option=spec_option\n    )", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class IniType(ComplexType):\n        \"\"\"Complex type to handle a list of file names.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if not isinstance(value, list):\n                raise exceptions.SpecParserException(\n                    f\"Expected list for 'list_of_file_names', got {type(value).__name__}\")\n            \n            # Validate that each item in the list is a string\n            for item in value:\n                if not isinstance(item, str):\n                    raise exceptions.SpecParserException(\n                        f\"All items in list_of_file_names must be strings, got {type(item).__name__}\")\n            \n            # Ensure each file name exists in the settings directories\n            for file_name in value:\n                for settings_dir in settings_dirs:\n                    file_path = os.path.join(settings_dir, file_name)\n                    if not os.path.exists(file_path):\n                        raise exceptions.SpecParserException(\n                            f\"File '{file_name}' not found in settings directory '{settings_dir}'\")\n            \n            return value\n    \n    return IniType(\n        arg_name=spec_option.get('name'),\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('sub_command_name'),\n        spec_option=spec_option\n    )", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class IniType(ComplexType):\n        \"\"\"Complex type that resolves a list of file names from a configuration.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if not value:\n                return []\n            \n            # If value is already a list, return it\n            if isinstance(value, list):\n                return value\n            \n            # If value is a string, split by comma and strip whitespace\n            file_names = [fn.strip() for fn in value.split(',') if fn.strip()]\n            return file_names\n    \n    return IniType(\n        arg_name=spec_option.get('name'),\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('sub_command_name'),\n        spec_option=spec_option\n    )", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class IniType(ComplexType):\n        \"\"\"Complex type to handle a list of file names.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if not isinstance(value, list):\n                raise exceptions.SpecParserException(\n                    f\"Expected list for 'list_of_file_names', got {type(value).__name__}\")\n            \n            # Validate that each item in the list is a string\n            for item in value:\n                if not isinstance(item, str):\n                    raise exceptions.SpecParserException(\n                        f\"All items in list_of_file_names must be strings, got {type(item).__name__}\")\n            \n            # Ensure files exist in the specified directories\n            file_paths = []\n            for file_name in value:\n                for settings_dir in settings_dirs:\n                    file_path = os.path.join(settings_dir, file_name)\n                    if os.path.exists(file_path):\n                        file_paths.append(file_path)\n                        break\n                else:\n                    # If no directory contains the file, raise an error\n                    raise exceptions.SpecParserException(\n                        f\"File '{file_name}' not found in any of the settings directories: {settings_dirs}\")\n            \n            return file_paths\n    \n    return IniType(\n        arg_name=spec_option.get('name'),\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('sub_command_name', ''),\n        spec_option=spec_option\n    )"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1971, "num_retrieved": 3, "inference_time": 0.8209103792905807, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is stored as a class attribute or can be retrieved from CoreServices\n    # Based on context, it's likely part of CoreServices\n    from infrared.core.services.ansible_config import AnsibleConfigManager\n    from infrared.core.services import CoreServices\n    \n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(CoreServices.infrared_home())\n    \n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is stored as a class attribute or can be retrieved from CoreServices\n    # Based on context, it's likely part of CoreServices\n    from infrared.core.services.ansible_config import AnsibleConfigManager\n    from infrared.core.services import CoreServices\n    \n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(CoreServices.infrared_home())\n    \n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is stored as a class attribute or can be instantiated\n    # Based on context, this is likely a singleton pattern or static instance\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home=cls.infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is stored as a class attribute or can be retrieved from CoreServices\n    # Based on context, it's likely part of CoreServices\n    from infrared.core.services.ansible_config import AnsibleConfigManager\n    from infrared.core.services import CoreServices\n    \n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(CoreServices.infrared_home())\n    \n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is stored as a class attribute or can be retrieved from CoreServices\n    # Based on context, it's likely part of CoreServices\n    from infrared.core.services.ansible_config import AnsibleConfigManager\n    from infrared.core.services import CoreServices\n    \n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(CoreServices.infrared_home())\n    \n    return cls._ansible_config_manager"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2118, "num_retrieved": 9, "inference_time": 0.8209103792905807, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the workspace manager instance is supposed to be.\n    # In the context provided, it appears to be a class method that\n    # returns an instance of CoreServices.workspace_manager().\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the workspace manager instance is supposed to be.\n    # In the context provided, it appears to be a class method that\n    # returns an instance of CoreServices.workspace_manager().\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the workspace manager instance is supposed to be.\n    # In the context provided, it appears to be a class method that\n    # returns an instance of CoreServices.workspace_manager().\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the workspace manager instance is supposed to be.\n    # Based on the context, it appears to be a class method that returns\n    # an instance of the workspace manager from CoreServices.\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the workspace manager instance is supposed to be.\n    # In the context provided, it's likely that this method would return\n    # an instance of a workspace manager class, possibly from CoreServices.\n    return CoreServices.workspace_manager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2048, "num_retrieved": 6, "inference_time": 0.8209103792905807, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of the class and how the plugin manager\n    # is instantiated or retrieved. In a real scenario, this would return\n    # an instance of the PluginManager class.\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of the class and how the plugin manager\n    # is instantiated or retrieved in the system.\n    # In a real scenario, this would return an instance of PluginManager.\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the plugin manager instance is supposed to be.\n    # In a real scenario, this would return an instance of PluginManager.\n    from infrared.core import plugins_manager\n    return plugins_manager", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of the class and how the plugin manager\n    # is instantiated or retrieved in the system.\n    # In a real scenario, this would return an instance of PluginManager.\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the context of where this method is defined and\n    # what the plugin manager instance is supposed to be.\n    # In a real scenario, this would return an instance of PluginManager.\n    from infrared.core import plugins_manager\n    return plugins_manager"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1938, "num_retrieved": 3, "inference_time": 0.8209103792905807, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        # Load the YAML content into a dictionary\n        yaml_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(f\"Problem with loading YAML file. {err}\")\n\n    # Define required fields (example - actual required fields should be defined based on spec structure)\n    required_fields = [\n        'name', \n        'description', \n        'version', \n        'plugin_type', \n        'commands'\n    ]\n\n    # Validate that all required fields are present\n    for field in required_fields:\n        if field not in yaml_dict:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added based on field types or structure\n    # For example, validate that 'commands' is a list, etc.\n\n    return yaml_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        # Load the YAML content into a dictionary\n        yaml_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(f\"Problem with loading YAML file. {err}\")\n\n    # Define required fields (example - adjust based on actual spec structure)\n    required_fields = [\n        'name', \n        'description', \n        'version', \n        'plugin_type',\n        'commands'\n    ]\n\n    # Validate that all required fields are present\n    for field in required_fields:\n        if field not in yaml_dict:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added here based on specific requirements\n    # For example, validate command structure, required options, etc.\n\n    return yaml_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        # Load the YAML content into a dictionary\n        yaml_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(f\"Problem with loading YAML file. {err}\")\n\n    # Define required fields (these should be present in the spec)\n    required_fields = [\n        'subparsers',\n        'description',\n        'plugin_type',\n        'name'\n    ]\n\n    # Validate that all required fields are present\n    for field in required_fields:\n        if field not in yaml_dict:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added here based on specific requirements\n    # For example, validate subparsers structure, plugin_type values, etc.\n\n    return yaml_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        # Load the YAML content into a dictionary\n        yaml_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(f\"Problem with loading YAML file. {err}\")\n\n    # Define required fields (example - actual required fields should be defined based on spec structure)\n    required_fields = [\n        'name', \n        'description', \n        'version', \n        'plugin_type',\n        'commands'\n    ]\n\n    # Validate that all required fields are present\n    for field in required_fields:\n        if field not in yaml_dict:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added based on field types or structure\n    # For example, validate that 'commands' is a list, etc.\n\n    return yaml_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        # Load the YAML content into a dictionary\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(f\"Problem with loading YAML file. {err}\")\n\n    # Define required fields (example - adjust based on actual spec structure)\n    required_fields = [\n        'name', \n        'description', \n        'version', \n        'plugin_type', \n        'commands'\n    ]\n\n    # Validate that all required fields are present\n    for field in required_fields:\n        if field not in spec_dict:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added here based on specific requirements\n    # For example, validate command structure, required options, etc.\n\n    return spec_dict"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2054, "num_retrieved": 5, "inference_time": 0.8209103792905807, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except Exception as e:\n        raise exceptions.IRValidatorException(f\"Error reading YAML file {yaml_file}: {str(e)}\")\n\n    # Validate that the data is a dictionary (or list, depending on requirements)\n    # If required fields are missing, raise exception\n    # This is a placeholder - actual validation rules should be defined based on context\n    if not data:\n        raise exceptions.IRValidatorException(f\"YAML file {yaml_file} is empty\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except Exception as e:\n        raise exceptions.IRValidatorException(f\"Error reading YAML file {yaml_file}: {str(e)}\")\n\n    # Validate that the data is a dictionary (or has required structure)\n    if data is None:\n        raise exceptions.IRValidatorException(f\"YAML file {yaml_file} is empty or invalid\")\n\n    # Validate required fields (example - customize based on actual requirements)\n    # This is a placeholder - actual validation rules should be defined here\n    required_fields = ['required_field_1', 'required_field_2']  # Replace with actual required fields\n    for field in required_fields:\n        if field not in data:\n            raise exceptions.IRValidatorException(f\"Missing required field '{field}' in {yaml_file}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRFileNotFoundException([yaml_file])\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except Exception as e:\n        raise exceptions.IRValidatorException(f\"Failed to load YAML file: {str(e)}\")\n\n    # Validate that the data is a dictionary (or has required structure)\n    if not isinstance(data, dict):\n        raise exceptions.IRValidatorException(\"YAML file must contain a valid dictionary structure\")\n\n    # Validate required fields (example - customize based on actual requirements)\n    required_fields = ['required_field_1', 'required_field_2']  # Replace with actual required fields\n    for field in required_fields:\n        if field not in data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRFileNotFoundException([yaml_file])\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except Exception as e:\n        raise exceptions.IRValidatorException(f\"Failed to load YAML file: {str(e)}\")\n\n    # Validate that the data is a dictionary (or has required structure)\n    if data is None:\n        raise exceptions.IRValidatorException(\"YAML file is empty or invalid\")\n\n    # Check for required fields (this would depend on specific requirements)\n    # For now, we assume basic validation - in practice, specific required fields should be defined\n    # Example: if required_fields = ['required_field1', 'required_field2']:\n    #   for field in required_fields:\n    #       if field not in data:\n    #           raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except Exception as e:\n        raise exceptions.IRValidatorException(f\"Error loading YAML file {yaml_file}: {str(e)}\")\n\n    # Validate that the data is a dictionary (or has required structure)\n    if not data:\n        raise exceptions.IRValidatorException(f\"YAML file {yaml_file} is empty or invalid structure\")\n\n    # Validate required fields (example - this should be customized based on actual requirements)\n    # For now, we assume the structure is valid if it's a dictionary\n    # In a real implementation, specific field checks would be added here\n\n    return data"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2106, "num_retrieved": 7, "inference_time": 0.8153850436210632, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    # Process each included group\n    for group_name in include_groups:\n        # Assuming group_name is a key in self.spec_helper.spec_dict or can be resolved\n        # This is a simplified implementation - actual logic may depend on context\n        if group_name in self.spec_helper.spec_dict:\n            # Merge the included group into the current parser dict\n            # This is a placeholder - actual merging logic should be defined\n            self.spec_helper.spec_dict.update({group_name: self.spec_helper.spec_dict[group_name]})\n        else:\n            # Handle case where group is not found\n            # Could log warning or raise exception\n            pass", "def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    for group_name in include_groups:\n        # Assuming group_name refers to a group defined in another spec file\n        # This is a placeholder implementation - actual logic would depend on\n        # how group definitions are structured and resolved in the system\n        try:\n            # Look up the group definition in the global groups or spec dictionary\n            # This would typically involve parsing other spec files or accessing\n            # a registry of defined groups\n            group_spec = self.spec_helper.spec_dict.get(group_name)\n            if group_spec:\n                # Merge the group specification into the current parser\n                # This would involve adding arguments or groups from the included spec\n                self.spec_helper.spec_dict.update(group_spec)\n        except Exception as e:\n            # Log or handle error appropriately\n            self.logger.error(f\"Failed to include group {group_name}: {e}\")", "def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    for group_dict in include_groups:\n        group_name = group_dict.get('name')\n        if not group_name:\n            continue\n            \n        # Look for a group with the given name in the base groups\n        base_groups = self.spec_helper.spec_dict.get('groups', {})\n        if group_name in base_groups:\n            # Merge the group into the current parser's groups\n            self.spec_helper.spec_dict['groups'].update(base_groups[group_name])\n        else:\n            # If group not found, try to find it in shared groups\n            shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n            for shared_group in shared_groups:\n                if shared_group.get('name') == group_name:\n                    self.spec_helper.spec_dict['groups'].update(shared_group)\n                    break", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    # Process each included group\n    for group_name in include_groups:\n        # Assuming group_name is a key in self.spec_helper.spec_dict or can be resolved\n        if group_name in self.spec_helper.spec_dict:\n            # Merge the included group into the current parser's groups\n            self.spec_helper.spec_dict['groups'] = self.spec_helper.spec_dict.get('groups', []) + \\\n                self.spec_helper.spec_dict[group_name].get('groups', [])\n        else:\n            # If group not found, log warning or handle appropriately\n            self.logger.warning(f\"Group '{group_name}' not found in spec_dict and cannot be included.\")", "def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    for group_dict in include_groups:\n        group_name = group_dict.get('name')\n        if not group_name:\n            continue\n            \n        # Find the group in the base groups or existing parsers\n        # This is a simplified implementation - actual logic would depend on\n        # how groups are structured and loaded in the system\n        if group_name in self.spec_helper.spec_dict:\n            # Merge the group into the current parser's groups\n            self.spec_helper.spec_dict['groups'].append(self.spec_helper.spec_dict[group_name])\n        else:\n            # If group not found, try to resolve from other sources\n            # This would require additional logic to locate the group\n            pass"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2127, "num_retrieved": 8, "inference_time": 0.8153850436210632, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_option = self.spec_option\n    \n    # Handle complex types with auto propagation\n    if hasattr(self, 'plugin_path') and hasattr(self, 'lookup_dir'):\n        files_path = os.path.join(self.plugin_path, self.lookup_dir)\n        if 'files' in spec_option:\n            allowed_values = list(map((lambda name: os.path.splitext(name)[0]),\n                                      os.listdir(files_path)))\n            allowed_values.sort()\n            defaults['files'] = allowed_values\n    \n    # Handle simple arguments from spec\n    for arg_name, arg_spec in spec_option.get('args', {}).items():\n        if arg_name in spec_option.get('default', {}):\n            defaults[arg_name] = spec_option['default'][arg_name]\n    \n    # Handle nested complex types\n    if hasattr(self, 'resolve'):\n        for arg_name, arg_spec in spec_option.get('args', {}).items():\n            if isinstance(arg_spec, dict) and 'type' in arg_spec:\n                if arg_spec['type'] in ['list_of_files', 'complex_type']:\n                    value = self.resolve(spec_option.get('default', {}).get(arg_name, ''))\n                    if value:\n                        defaults[arg_name] = value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_option = self.spec_option\n    \n    # Handle complex types with auto propagation\n    if hasattr(self, 'plugin_path') and hasattr(self, 'lookup_dir'):\n        files_path = os.path.join(self.plugin_path, self.lookup_dir)\n        allowed_values = list(map((lambda name: os.path.splitext(name)[0]),\n                                  os.listdir(files_path)))\n        allowed_values.sort()\n        if 'files' in spec_option:\n            value = spec_option['files']\n            if value and value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(files_path)\n            defaults['files'] = value\n    \n    # Handle simple arguments from spec\n    for arg_name, arg_spec in spec_option.get('args', {}).items():\n        if arg_name in self.settings_dirs:\n            # Resolve from settings\n            defaults[arg_name] = self.get_setting_value(arg_name)\n        elif arg_name in self.spec_option.get('default', {}):\n            defaults[arg_name] = self.spec_option['default'][arg_name]\n    \n    # Handle complex types\n    for complex_type_name, complex_spec in spec_option.get('complex_types', {}).items():\n        if hasattr(self, complex_type_name):\n            complex_type = getattr(self, complex_type_name)\n            if complex_type.VALUES_AUTO_PROPAGATION:\n                defaults[complex_type_name] = complex_type.get_allowed_values()\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_option = self.spec_option\n    \n    # Handle complex types with auto propagation\n    if hasattr(self, 'plugin_path') and hasattr(self, 'lookup_dir'):\n        files_path = os.path.join(self.plugin_path, self.lookup_dir)\n        allowed_values = list(map((lambda name: os.path.splitext(name)[0]),\n                                  os.listdir(files_path)))\n        allowed_values.sort()\n        if 'files' in spec_option:\n            value = spec_option['files']\n            if value and value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(files_path)\n            defaults['files'] = value\n    \n    # Handle simple arguments from spec\n    for arg_name, arg_spec in spec_option.get('args', {}).items():\n        if arg_name in self.settings_dirs:\n            # Resolve from settings\n            defaults[arg_name] = self.get_setting_value(arg_name)\n        elif arg_name in self.spec_option.get('default', {}):\n            defaults[arg_name] = self.spec_option['default'][arg_name]\n    \n    # Handle complex types\n    for complex_type in self.get_complex_types():\n        if hasattr(complex_type, 'resolve'):\n            resolved_value = complex_type.resolve(spec_option.get('value', ''))\n            if resolved_value:\n                defaults[complex_type.arg_name] = resolved_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get default values from spec if available\n    spec_defaults = self.spec.get('defaults', {})\n    for key, value in spec_defaults.items():\n        if isinstance(value, dict):\n            # Handle nested defaults\n            defaults[key] = value\n        else:\n            defaults[key] = value\n    \n    # Merge with any global defaults from settings\n    settings_defaults = self.settings.get('defaults', {})\n    for key, value in settings_defaults.items():\n        if key not in defaults:\n            defaults[key] = value\n    \n    # Resolve complex types if needed\n    for key, value in defaults.items():\n        if isinstance(value, str) and hasattr(self, 'resolve'):\n            try:\n                resolved_value = self.resolve(value)\n                defaults[key] = resolved_value\n            except Exception as e:\n                # Log or handle error appropriately\n                pass\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Get default values from spec if available\n    spec_defaults = self.spec.get('defaults', {})\n    for key, value in spec_defaults.items():\n        if isinstance(value, dict):\n            # Handle nested defaults\n            defaults[key] = value\n        else:\n            defaults[key] = value\n    \n    # Merge with any global defaults from settings\n    settings_defaults = self.settings.get('defaults', {})\n    for key, value in settings_defaults.items():\n        if key not in defaults:\n            defaults[key] = value\n    \n    # Resolve complex types if needed\n    for key, value in defaults.items():\n        if isinstance(value, str) and hasattr(self, 'resolve'):\n            try:\n                resolved_value = self.resolve(value)\n                defaults[key] = resolved_value\n            except Exception as e:\n                # Log or handle error appropriately\n                pass\n    \n    return defaults"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1925, "num_retrieved": 3, "inference_time": 0.8153850436210632, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1968, "num_retrieved": 4, "inference_time": 0.8153850436210632, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual deprecation logic would depend on\n    # specific deprecated arguments defined in the system\n    deprecated_args = {\n        # Example: 'old_option': 'new_option'\n    }\n    \n    deprecations_found = []\n    \n    # Check CLI arguments for deprecations\n    for arg, value in cli_args.items():\n        if arg in deprecated_args:\n            deprecations_found.append(f\"Argument '{arg}' is deprecated. Use '{deprecated_args[arg]}' instead.\")\n    \n    # Check answer file arguments for deprecations\n    for arg, value in answer_file_args.items():\n        if arg in deprecated_args:\n            deprecations_found.append(f\"Argument '{arg}' in answer file is deprecated. Use '{deprecated_args[arg]}' instead.\")\n    \n    # Print deprecations if any found\n    if deprecations_found:\n        for msg in deprecations_found:\n            print(msg)\n    \n    # Return True if deprecations found, False otherwise\n    return len(deprecations_found) > 0", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on defined deprecation rules\n    deprecated_args = []\n    \n    # Example logic: check for known deprecated arguments in either cli_args or answer_file_args\n    # This is a template and should be customized based on specific deprecation rules\n    \n    # Check if any deprecated arguments are present\n    for arg in cli_args:\n        if arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(arg)\n    \n    for arg in answer_file_args:\n        if arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(arg)\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Warning: The following arguments are deprecated and may be removed in future versions:\")\n        for arg in deprecated_args:\n            print(f\"  - {arg}\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual deprecation logic would depend on\n    # specific deprecated arguments defined in the system\n    deprecated_args = {\n        # Example: 'old_option': 'new_option'\n    }\n    \n    deprecation_warnings = []\n    \n    # Check CLI arguments for deprecated options\n    for arg, value in cli_args.items():\n        if arg in deprecated_args:\n            deprecation_warnings.append(f\"Argument '{arg}' is deprecated. Use '{deprecated_args[arg]}' instead.\")\n    \n    # Check answer file arguments for deprecated options\n    for arg, value in answer_file_args.items():\n        if arg in deprecated_args:\n            deprecation_warnings.append(f\"Argument '{arg}' in answer file is deprecated. Use '{deprecated_args[arg]}' instead.\")\n    \n    # Print warnings if any\n    if deprecation_warnings:\n        for warning in deprecation_warnings:\n            print(warning)\n    \n    return deprecation_warnings", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific deprecation rules\n    deprecated_args = []\n    \n    # Example logic: check for known deprecated arguments in either cli_args or answer_file_args\n    # This is a template and should be customized based on actual deprecation rules\n    \n    # Check for deprecated arguments in cli_args\n    for arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n        if arg in cli_args and cli_args[arg] is not None:\n            deprecated_args.append(f\"CLI argument '{arg}' is deprecated\")\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n        if arg in answer_file_args and answer_file_args[arg] is not None:\n            deprecated_args.append(f\"Answer file argument '{arg}' is deprecated\")\n    \n    # Print deprecated arguments\n    if deprecated_args:\n        for msg in deprecated_args:\n            print(f\"Warning: {msg}\")\n        return True  # Indicate deprecation found\n    \n    return False  # No deprecated arguments found", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual deprecation logic would depend on\n    # specific deprecated arguments defined in the system\n    deprecated_args = {\n        # Example: 'old_option': 'new_option'\n    }\n    \n    deprecation_warnings = []\n    \n    # Check cli_args for deprecated arguments\n    for arg, value in cli_args.items():\n        if arg in deprecated_args:\n            deprecation_warnings.append(f\"Argument '{arg}' is deprecated. Use '{deprecated_args[arg]}' instead.\")\n    \n    # Check answer_file_args for deprecated arguments\n    for arg, value in answer_file_args.items():\n        if arg in deprecated_args:\n            deprecation_warnings.append(f\"Argument '{arg}' is deprecated. Use '{deprecated_args[arg]}' instead.\")\n    \n    # Print deprecation warnings\n    for warning in deprecation_warnings:\n        print(warning)\n    \n    # Return True if any deprecations found, False otherwise\n    return len(deprecation_warnings) > 0"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2017, "num_retrieved": 4, "inference_time": 0.8153850436210632, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2002, "num_retrieved": 4, "inference_time": 0.8153850436210632, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2062, "num_retrieved": 6, "inference_time": 0.8153850436210632, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2071, "num_retrieved": 6, "inference_time": 0.8153850436210632, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise ValueError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise ValueError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    if absent:\n        raise ValueError(\"Expected {} to be specified\".format(absent))"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2098, "num_retrieved": 7, "inference_time": 0.8153850436210632, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    required_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if required_when:\n            # Evaluate the required_when condition\n            condition_met = False\n            for condition in required_when:\n                # Check if the condition is met based on args\n                condition_args = condition.split('==')\n                if len(condition_args) != 2:\n                    continue\n                condition_key, condition_value = condition_args\n                condition_value = condition_value.strip()\n                # Get the value from args\n                value = args.get(command_name, {}).get(condition_key)\n                if value is not None and str(value).strip() == condition_value:\n                    condition_met = True\n                    break\n            if condition_met:\n                required_args.append(option_name)\n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if required_when:\n            # Check if the condition is met\n            condition_met = False\n            for condition in required_when:\n                condition_name = condition['name']\n                condition_value = condition.get('value')\n                condition_action = condition.get('action', '')\n                \n                # Get the value of the condition argument\n                condition_arg_value = args.get(command_name, {}).get(condition_name)\n                \n                # Evaluate the condition\n                if condition_action == 'eq':\n                    if condition_arg_value == condition_value:\n                        condition_met = True\n                        break\n                elif condition_action == 'ne':\n                    if condition_arg_value != condition_value:\n                        condition_met = True\n                        break\n                elif condition_action == 'is':\n                    if condition_arg_value is not None and condition_arg_value == condition_value:\n                        condition_met = True\n                        break\n                elif condition_action == 'not':\n                    if condition_arg_value is None:\n                        condition_met = True\n                        break\n            if condition_met:\n                required_args.append(option_name)\n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if required_when:\n            # Check if the condition is met\n            condition_met = False\n            for condition in required_when:\n                condition_name = condition['name']\n                condition_value = condition.get('value')\n                condition_action = condition.get('action', '')\n                \n                # Get the value of the condition argument\n                condition_arg_value = args.get(command_name, {}).get(condition_name)\n                \n                # Evaluate the condition\n                if condition_action == 'eq':\n                    if condition_arg_value == condition_value:\n                        condition_met = True\n                        break\n                elif condition_action == 'ne':\n                    if condition_arg_value != condition_value:\n                        condition_met = True\n                        break\n                elif condition_action == 'is':\n                    if condition_arg_value is not None and condition_arg_value == condition_value:\n                        condition_met = True\n                        break\n                elif condition_action == 'not':\n                    if condition_arg_value is None:\n                        condition_met = True\n                        break\n            \n            if condition_met:\n                required_args.append(option_name)\n    \n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if required_when:\n            # Check if the condition is met\n            condition_met = False\n            for condition in required_when:\n                condition_name, condition_value = condition.split('=')\n                condition_value = condition_value.strip()\n                # Get the value of the condition argument\n                condition_arg_value = args.get(command_name, {}).get(condition_name)\n                if condition_arg_value == condition_value:\n                    condition_met = True\n                    break\n            if condition_met:\n                required_args.append(option_name)\n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if required_when:\n            # Check if the condition is met\n            condition_met = False\n            for condition in required_when:\n                condition_name = condition['name']\n                condition_value = condition.get('value')\n                condition_action = condition.get('action', '')\n                \n                # Get the value of the condition argument\n                condition_arg_value = args.get(command_name, {}).get(condition_name)\n                \n                # Evaluate the condition\n                if condition_action == 'eq':\n                    if condition_arg_value == condition_value:\n                        condition_met = True\n                        break\n                elif condition_action == 'ne':\n                    if condition_arg_value != condition_value:\n                        condition_met = True\n                        break\n                elif condition_action == 'is_set':\n                    if condition_arg_value is not None:\n                        condition_met = True\n                        break\n                elif condition_action == 'not_set':\n                    if condition_arg_value is None:\n                        condition_met = True\n                        break\n            \n            if condition_met:\n                required_args.append(option_name)\n    \n    return required_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2135, "num_retrieved": 8, "inference_time": 0.8153850436210632, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if the string value exceeds any specified length\n            # Assuming length is stored in a nested structure or passed as part of args\n            # Since no specific length is provided in the context, we'll assume it's stored in args\n            # with a key like 'length' or similar\n            length = args.get(f\"{arg_name}_length\")\n            if length is not None and len(arg_value) > int(length):\n                invalid_options.append((arg_name, arg_value, length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length specification is provided as a separate key or in the args structure\n            # Since context doesn't specify how length is defined, we'll assume it's stored in args with a length key\n            # For example, if args[arg_name] contains a length specification\n            length_spec = args.get(f\"{arg_name}_length\")\n            if length_spec is not None:\n                if len(arg_value) > int(length_spec):\n                    invalid_options.append((arg_name, arg_value, length_spec))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if the string value exceeds any specified length\n            # Assuming length is stored in a nested structure or passed as part of args\n            # Since context doesn't specify how length is defined, we'll assume it's in args\n            # with a key like 'length' or similar\n            length = args.get(f\"{arg_name}_length\")\n            if length is not None and len(arg_value) > int(length):\n                invalid_options.append((arg_name, arg_value, length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if the string value exceeds any specified length\n            # Assuming length is stored in a nested structure or passed as part of args\n            # Since no specific length is provided in the context, we'll assume it's stored in a separate key\n            # For example, if length is stored as args[arg_name + '_length']\n            length_key = arg_name + '_length'\n            if length_key in args:\n                length = args[length_key]\n                if len(arg_value) > length:\n                    invalid_options.append((arg_name, arg_value, length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Assuming length specification is provided as a separate key or in the structure\n            # We need to determine the expected length from context, but it's not provided\n            # In absence of explicit length specification, we'll assume it's not defined\n            # This is a placeholder - actual implementation would require knowing the length rules\n            pass\n    \n    # Since the context does not specify how length is defined or where it's stored,\n    # we cannot fully implement this function without additional information.\n    # The function would typically check against a known length limit for each argument.\n    \n    # Placeholder implementation - in real scenario, length limits would be defined elsewhere\n    # and this function would validate against those limits.\n    return invalid_options"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2106, "num_retrieved": 7, "inference_time": 0.8153850436210632, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_spec in self.get_option_specs().items():\n        for option_name, option_data in option_spec.items():\n            if 'choices' in option_data:\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_data['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_spec in self.get_option_specs().items():\n        for option_name, option_data in option_spec.items():\n            if 'choices' in option_data:\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_data['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_spec in self.get_option_specs().items():\n        for option_name, option_data in option_spec.items():\n            if 'choices' in option_data:\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_data['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_spec in self.get_option_specs().items():\n        for option_name, option_data in option_spec.items():\n            if 'choices' in option_data:\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_data['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, option_spec in self.get_option_specs().items():\n        for option_name, option_data in option_spec.items():\n            if 'choices' in option_data:\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_data['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2129, "num_retrieved": 8, "inference_time": 0.8153850436210632, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            if option_name.startswith('min_') or option_name.startswith('max_'):\n                min_val = option_value.get('min', None)\n                max_val = option_value.get('max', None)\n                if min_val is not None and max_val is not None:\n                    if not (min_val <= max_val):\n                        raise UtilError(f\"Minimum value ({min_val}) cannot be greater than maximum value ({max_val}) for option {option_name}\")\n                elif min_val is not None and max_val is None:\n                    raise UtilError(f\"Maximum value is required for minimum value {min_val} in option {option_name}\")\n                elif min_val is None and max_val is not None:\n                    raise UtilError(f\"Minimum value is required for maximum value {max_val} in option {option_name}\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            if option_name.startswith('min_') or option_name.startswith('max_'):\n                min_val = option_value.get('min', None)\n                max_val = option_value.get('max', None)\n                if min_val is not None and max_val is not None:\n                    if not (min_val <= max_val):\n                        raise UtilError(f\"Minimum value ({min_val}) cannot be greater than maximum value ({max_val}) for option {option_name}\")\n                elif min_val is not None and max_val is None:\n                    raise UtilError(f\"Maximum value is required for minimum value {min_val} in option {option_name}\")\n                elif min_val is None and max_val is not None:\n                    raise UtilError(f\"Minimum value is required for maximum value {max_val} in option {option_name}\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            if option_name.startswith('min_') or option_name.startswith('max_'):\n                min_val = option_value.get('min', None)\n                max_val = option_value.get('max', None)\n                \n                if min_val is not None and max_val is not None:\n                    if min_val > max_val:\n                        raise UtilError(f\"Minimum value {min_val} cannot be greater than maximum value {max_val} for option {option_name}\")\n                \n                # Validate that the value is within the range if both min and max are set\n                if min_val is not None and max_val is not None:\n                    if option_value.get('value') is not None and not (min_val <= option_value['value'] <= max_val):\n                        raise UtilError(f\"Value {option_value['value']} for option {option_name} is not within range [{min_val}, {max_val}]\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            if option_name.endswith('_min') or option_name.endswith('_max'):\n                min_val = option_value.get('min', None)\n                max_val = option_value.get('max', None)\n                if min_val is not None and max_val is not None:\n                    if min_val > max_val:\n                        raise UtilError(f\"Minimum value {min_val} cannot be greater than maximum value {max_val} for option {option_name}\")\n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            if option_name.startswith('min_') or option_name.startswith('max_'):\n                min_val = option_value.get('min', None)\n                max_val = option_value.get('max', None)\n                \n                if min_val is not None and max_val is not None:\n                    if min_val > max_val:\n                        raise UtilError(f\"Minimum value {min_val} cannot be greater than maximum value {max_val} for option {option_name}\")\n                    \n                    # Validate that the actual value is within range\n                    if option_value.get('value') is not None:\n                        value = option_value['value']\n                        if not isinstance(value, (int, float)):\n                            raise UtilError(f\"Value for {option_name} must be a number\")\n                        if value < min_val or value > max_val:\n                            raise UtilError(f\"Value {value} for {option_name} is not within range [{min_val}, {max_val}]\")"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2135, "num_retrieved": 8, "inference_time": 0.8153850436210632, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance using the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Import the ComplexType class from the context\n    from .cli.cli import ComplexType\n    \n    # Create and return an instance of ComplexType with the provided parameters\n    return ComplexType(\n        arg_name=option_name,\n        settings_dirs=None,  # Not specified in context, using None as placeholder\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with required parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance using the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Import the complex type class dynamically based on type_name\n    try:\n        complex_type_class = getattr(self, type_name)\n    except AttributeError:\n        raise ValueError(f\"Unknown complex type: {type_name}\")\n    \n    # Validate that the complex type class inherits from ComplexType\n    if not issubclass(complex_type_class, ComplexType):\n        raise ValueError(f\"Type {type_name} must inherit from ComplexType\")\n    \n    # Create and return the instance with required arguments\n    return complex_type_class(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1907, "num_retrieved": 2, "inference_time": 0.8153850436210632, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Define common argument prefixes\n    control_prefixes = ['debug', 'dry-run', 'generate-answers-file', 'output', 'verbose']\n    nested_prefixes = ['inventory', 'host-pattern', 'tags', 'skip-tags', 'extra-vars', 'ansible-args']\n    \n    for arg in args:\n        if arg.startswith('--'):\n            # Remove the leading --\n            arg_name = arg[2:]\n            \n            # Check if it's a control argument\n            if arg_name in control_prefixes:\n                # Handle control arguments\n                if arg_name == 'debug':\n                    control_args['debug'] = True\n                elif arg_name == 'dry-run':\n                    control_args['dry-run'] = True\n                elif arg_name == 'generate-answers-file':\n                    control_args['generate-answers-file'] = True\n                elif arg_name == 'output':\n                    control_args['output'] = arg_name\n                elif arg_name == 'verbose':\n                    control_args['verbose'] = True\n            elif arg_name.startswith('extra-vars'):\n                # Handle extra-vars as custom args\n                custom_args[arg_name] = arg\n            elif arg_name.startswith('ansible-args'):\n                # Handle ansible-args as custom args\n                custom_args[arg_name] = arg\n            else:\n                # Assume it's a nested argument\n                nested_args[arg_name] = arg\n    \n    # Handle additional arguments that might be in the form of --key=value\n    # This is a simplified version; in practice, you might want to parse them properly\n    for arg in args:\n        if arg.startswith('--'):\n            arg_name = arg[2:]\n            if '=' in arg_name:\n                key, value = arg_name.split('=', 1)\n                if key in control_prefixes:\n                    control_args[key] =", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Define common argument prefixes\n    CONTROL_PREFIXES = [\n        'debug', 'dry-run', 'generate-answers-file', 'output', 'verbose',\n        'extra-vars', 'ansible-args'\n    ]\n\n    for key, value in args.items():\n        if key.startswith('--'):\n            # Remove the leading '--' and split into name and value\n            arg_name = key[2:]\n            if arg_name in CONTROL_PREFIXES:\n                # Handle control arguments\n                control_args[arg_name] = value\n            elif arg_name.startswith('nested-'):\n                # Handle nested arguments (these go into the spec)\n                nested_args[arg_name[7:]] = value\n            elif arg_name.startswith('custom-'):\n                # Handle custom arguments (ansible variables)\n                custom_args[arg_name[7:]] = value\n            else:\n                # Default to nested if not recognized\n                nested_args[arg_name] = value\n\n    return nested_args, control_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Define common argument prefixes\n    control_prefixes = ['debug', 'dry-run', 'generate-answers-file', 'output', 'verbose']\n    nested_prefixes = ['inventory', 'host', 'group', 'vars', 'extra-vars', 'ansible-args']\n    \n    for arg in args:\n        if arg.startswith('--'):\n            # Remove the leading --\n            key = arg[2:]\n            if '=' in key:\n                key, value = key.split('=', 1)\n            else:\n                value = None\n            \n            # Check if it's a control argument\n            if key in control_prefixes:\n                control_args[key] = value\n            # Check if it's a nested argument\n            elif key in nested_prefixes:\n                nested_args[key] = value\n            # Otherwise, treat as custom argument\n            else:\n                custom_args[key] = value\n    \n    return nested_args, control_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through args and classify them\n    for arg in args:\n        if arg.startswith('--'):\n            # Check if it's a control argument (common IR behavior flags)\n            if arg in ['--debug', '--dry-run', '--generate-answers-file', '--output', '--verbose', '--ansible-args']:\n                control_args[arg.replace('--', '')] = True\n            elif arg.startswith('--extra-vars'):\n                # Handle extra-vars as custom arguments\n                custom_args[arg.replace('--', '')] = True\n            else:\n                # Everything else is considered nested argument\n                nested_args[arg.replace('--', '')] = True\n        else:\n            # Handle custom arguments like key=value\n            if '=' in arg:\n                key, value = arg.split('=', 1)\n                if key.startswith('extra-vars'):\n                    custom_args[key.replace('--', '')] = value\n                else:\n                    nested_args[key] = value\n            else:\n                # Default to nested if not recognized\n                nested_args[arg] = True\n\n    return nested_args, control_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through all arguments\n    for arg in args:\n        if arg.startswith('--'):\n            # Check if it's a control argument (common IR behavior flags)\n            if arg in ['--debug', '--dry-run', '--generate-answers-file', '--output', '--verbose', '--ansible-args']:\n                # These are control arguments\n                key = arg.replace('--', '')\n                value = None\n                # If the argument has a value, extract it\n                if '=' in arg:\n                    key, value = arg.split('=', 1)\n                    value = value.strip()\n                control_args[key] = value\n            else:\n                # Otherwise, it's a nested argument (passed to playbook)\n                key = arg.replace('--', '')\n                value = None\n                if '=' in arg:\n                    key, value = arg.split('=', 1)\n                    value = value.strip()\n                nested_args[key] = value\n        else:\n            # Handle case where argument is not prefixed with --\n            # This should not happen in valid input\n            pass\n\n    # Extract custom arguments (custom ansible variables)\n    # These are typically passed as --additional-args option1=value1;option2=value2\n    if '--additional-args' in args:\n        additional_args_arg = args[args.index('--additional-args')]\n        # Parse the additional args\n        if ';' in additional_args_arg:\n            additional_parts = additional_args_arg.split(';')\n            for part in additional_parts:\n                part = part.strip()\n                if '=' in part:\n                    key, value = part.split('=', 1)\n                    custom_args[key] = value\n        else:\n            # If no semicolon, just treat as single key=value\n            if '=' in additional_args_arg:\n                key, value = additional_args_arg.split('=', 1"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1939, "num_retrieved": 2, "inference_time": 0.8153850436210632, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            vars_dict.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # Invalid format, skip\n                pass", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            vars_dict.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            key, value = extra_var.split('=', 1)\n            vars_dict[key] = value", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return\n    \n    for var in extra_vars:\n        if isinstance(var, dict):\n            vars_dict.update(var)\n        else:\n            # Assume var is a string in format \"key=value\"\n            try:\n                key, value = var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # Invalid format, skip\n                pass", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            vars_dict.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # Invalid format, skip\n                pass", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            vars_dict.update(extra_var)\n        elif isinstance(extra_var, str):\n            # Parse string extra-var in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # Invalid format, skip\n                pass"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1957, "num_retrieved": 2, "inference_time": 0.8153850436210632, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars', yaml.safe_dump(extra_vars, default_flow_style=False)])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            if value is not None:\n                cmd.extend(['--' + key, str(value)])\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            env=ir_workspace.env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            timeout=300  # 5 minute timeout\n        )\n        return result.returncode\n    except subprocess.TimeoutExpired:\n        return 1\n    except Exception as e:\n        return 1", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_arg = '--extra-vars'\n        vars_str = yaml.safe_dump(extra_vars, default_flow_style=False)\n        cmd.extend([vars_arg, vars_str])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend([f'--{key}', str(value)])\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            env=ir_workspace.env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code on error", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars', yaml.safe_dump(extra_vars, default_flow_style=False)])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            if value is not None:\n                cmd.extend(['--' + key, str(value)])\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            env=ir_workspace.env,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code on error", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars=\"{}\"'.format(yaml.safe_dump(extra_vars, default_flow_style=False))])\n        cmd.extend(['-e', vars_str])\n    \n    # Add additional ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            if value is not None:\n                cmd.extend(['--{}'.format(key), str(value)])\n    \n    # Execute the playbook using subprocess\n    import subprocess\n    import os\n    \n    # Set environment variables for the playbook execution\n    env = os.environ.copy()\n    env['ANSIBLE_ROLES_PATH'] = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Run the ansible-playbook command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            env=env,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code on error", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Prepare the command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars', yaml.safe_dump(extra_vars, default_flow_style=False)])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            if value is not None:\n                cmd.extend(['--' + key, str(value)])\n    \n    # Execute the playbook\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            env=os.environ.copy(),\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code on error"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 1939, "num_retrieved": 2, "inference_time": 0.9543183247248331, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse command line arguments to extract playbook and other settings\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--playbook', required=True, help='Path to the playbook')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--extra-vars', help='Extra variables to pass to Ansible')\n    parser.add_argument('--ansible-args', help='Additional arguments to pass to Ansible')\n    parser.add_argument('--dry-run', action='store_true', help='Only generate vars dict')\n    parser.add_argument('--generate-answers-file', action='store_true', help='Only generate answers file')\n    parser.add_argument('--output', help='Output file for vars dict')\n\n    args = parser.parse_args(cli_args)\n\n    # Prepare the Ansible execution parameters\n    ansible_args = args.ansible_args if args.ansible_args else []\n    extra_vars = vars_dict if args.extra_vars is None else args.extra_vars\n\n    # Execute Ansible playbook with the provided variables\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=args.playbook,\n        verbose=args.verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n\n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse the CLI arguments to extract playbook path and other options\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--playbook', required=True, help='Path to the playbook')\n    parser.add_argument('--extra-vars', help='Extra vars to pass to ansible')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Only generate vars dict')\n    parser.add_argument('--generate-answers-file', action='store_true', help='Only generate answers file')\n    parser.add_argument('--output', help='Output file for vars dict')\n    \n    # Parse the arguments\n    args = parser.parse_args(cli_args)\n    \n    # If dry-run or generate-answers-file, return early without running ansible\n    if args.dry_run or args.generate_answers_file:\n        return None\n    \n    # Prepare the ansible command\n    ansible_args = []\n    if args.verbose:\n        ansible_args.append('-v')\n    \n    if args.extra_vars:\n        ansible_args.extend(['-e', args.extra_vars])\n    \n    # Execute the ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=args.playbook,\n        verbose=args.verbose,\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse command line arguments to extract ansible-specific options\n    parser = argparse.ArgumentParser()\n    # Add necessary arguments for ansible playbook execution\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--extra-vars', help='Extra vars to pass to ansible')\n    parser.add_argument('--dry-run', action='store_true', help='Only generate vars dict')\n    parser.add_argument('--generate-answers-file', action='store_true', help='Only generate answers file')\n    parser.add_argument('--output', help='Output file for vars dict')\n    \n    # Parse the arguments\n    args = parser.parse_args(cli_args)\n    \n    # If dry-run or generate-answers-file is specified, return early\n    if args.dry_run or args.generate_answers_file:\n        return None\n    \n    # Prepare the ansible playbook execution with the provided vars_dict\n    ansible_args = []\n    if args.verbose:\n        ansible_args.append('--verbose')\n    if args.extra_vars:\n        ansible_args.append('--extra-vars=' + args.extra_vars)\n    \n    # Execute the ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=args.verbose,\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse the CLI arguments to extract playbook path and other options\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--playbook', required=True, help='Path to the playbook')\n    parser.add_argument('--extra-vars', help='Extra variables to pass to Ansible')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Only generate vars dict')\n    parser.add_argument('--generate-answers-file', action='store_true', help='Only generate answers file')\n    parser.add_argument('--output', help='Output file for vars dict')\n    \n    # Parse arguments\n    args = parser.parse_args(cli_args)\n    \n    # Extract relevant arguments\n    playbook_path = args.playbook\n    extra_vars = args.extra_vars\n    verbose = args.verbose\n    dry_run = args.dry_run\n    generate_answers = args.generate_answers_file\n    output_file = args.output\n    \n    # If dry run or generate answers file, return early\n    if dry_run or generate_answers:\n        return None\n    \n    # If output file is specified, write vars dict to file\n    if output_file:\n        with open(output_file, 'w') as f:\n            f.write(yaml.safe_dump(vars_dict, default_flow_style=False))\n    \n    # Set up Ansible environment\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register plugin roles path if defined\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        new_path = ':'.join([ansible_roles_path, roles_path]) if ansible_roles_path else roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute Ans", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse the CLI arguments to extract playbook path and other options\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--playbook', required=True, help='Path to the playbook')\n    parser.add_argument('--extra-vars', help='Extra variables to pass to Ansible')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Only generate vars dict')\n    parser.add_argument('--generate-answers-file', action='store_true', help='Only generate answers file')\n    parser.add_argument('--output', help='Output file for vars dict')\n    \n    # Parse arguments\n    args = parser.parse_args(cli_args)\n    \n    # Extract playbook path from plugin\n    playbook_path = ir_plugin.playbook\n    \n    # Set up Ansible environment\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Prepare extra vars\n    extra_vars = vars_dict if args.extra_vars is None else vars_dict\n    \n    # Execute Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=args.verbose,\n        extra_vars=extra_vars,\n        ansible_args=args\n    )\n    \n    # Restore original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2032, "num_retrieved": 5, "inference_time": 0.9543183247248331, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Map of command names to their expected argument types\n    type_map = {\n        'main': {\n            'node_name': str,\n            'remote_command': str,\n        },\n        'virsh': {\n            'uri': str,\n            'command': str,\n            'args': str,\n        },\n        'ospd': {\n            'host': str,\n            'port': int,\n            'timeout': int,\n        }\n    }\n    \n    # Get the type mapping for the given parser_name\n    if parser_name in type_map:\n        types = type_map[parser_name]\n        for key, value in values_dict.items():\n            if key in types:\n                # Convert value to the expected type\n                if isinstance(value, str):\n                    if key in ['port', 'timeout']:\n                        try:\n                            values_dict[key] = int(value)\n                        except ValueError:\n                            values_dict[key] = 0\n                    elif key in ['node_name', 'remote_command', 'uri', 'command']:\n                        values_dict[key] = value.strip()\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Map of command names to their expected argument types\n    type_mapping = {\n        'main': {\n            'node_name': str,\n            'remote_command': str,\n        },\n        'virsh': {\n            'uri': str,\n            'command': str,\n            'name': str,\n        },\n        'ospd': {\n            'host': str,\n            'port': int,\n            'timeout': int,\n        }\n    }\n    \n    # Get the type mapping for the given parser_name\n    if parser_name in type_mapping:\n        types = type_mapping[parser_name]\n        for key, value in values_dict.items():\n            if key in types:\n                # Convert value to the expected type\n                if isinstance(value, str):\n                    if key in ['port', 'timeout']:\n                        try:\n                            values_dict[key] = int(value)\n                        except ValueError:\n                            pass\n                    elif key in ['node_name', 'remote_command', 'uri', 'command', 'host']:\n                        values_dict[key] = value.strip()\n                # Handle other types as needed\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Map of command names to their expected argument types\n    type_map = {\n        'main': {\n            'node_name': str,\n            'remote_command': str,\n        },\n        'virsh': {\n            'uri': str,\n            'command': str,\n            'args': str,\n        },\n        'ospd': {\n            'host': str,\n            'port': int,\n            'timeout': int,\n        },\n        # Add more command-specific type mappings as needed\n    }\n\n    # Get the type mapping for the given parser_name\n    if parser_name in type_map:\n        for arg_name, value in values_dict.items():\n            if arg_name in type_map[parser_name]:\n                arg_type = type_map[parser_name][arg_name]\n                if arg_type == int and value is not None:\n                    try:\n                        values_dict[arg_name] = int(value)\n                    except (ValueError, TypeError):\n                        pass\n                elif arg_type == str and value is not None:\n                    values_dict[arg_name] = str(value)\n    # Default to string for any unhandled cases\n    for arg_name, value in values_dict.items():\n        if value is not None:\n            values_dict[arg_name] = str(value)", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Map of command names to their expected argument types\n    type_mapping = {\n        'main': {\n            'node_name': str,\n            'remote_command': str,\n        },\n        'virsh': {\n            'uri': str,\n            'command': str,\n            'name': str,\n        },\n        'ospd': {\n            'host': str,\n            'port': int,\n            'timeout': int,\n        }\n    }\n    \n    # Get the type mapping for the given parser_name\n    if parser_name in type_mapping:\n        types = type_mapping[parser_name]\n        for key, value in values_dict.items():\n            if key in types:\n                # Convert value to the expected type\n                if isinstance(value, str):\n                    if types[key] == int:\n                        try:\n                            values_dict[key] = int(value)\n                        except ValueError:\n                            pass  # Keep as string if conversion fails\n                    elif types[key] == bool:\n                        values_dict[key] = value.lower() in ('true', '1', 'yes')\n                # For other types, no specific conversion needed\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get the options for the specified command\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Map option names to their expected types\n    type_map = {}\n    for option in options:\n        if 'type' in option and option['type'] is not None:\n            type_map[option['name']] = option['type']\n    \n    # Convert values based on type specifications\n    for key, value in values_dict.items():\n        if key in type_map:\n            try:\n                if type_map[key] == 'int':\n                    values_dict[key] = int(value)\n                elif type_map[key] == 'bool':\n                    values_dict[key] = value.lower() in ('true', '1', 'yes', 'on')\n                elif type_map[key] == 'float':\n                    values_dict[key] = float(value)\n                elif type_map[key] == 'str':\n                    values_dict[key] = str(value)\n                # Add more types as needed\n            except (ValueError, TypeError):\n                # Keep original value if conversion fails\n                pass"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2080, "num_retrieved": 6, "inference_time": 0.9543183247248331, "generate_results": ["import os\nimport yaml\nfrom collections import OrderedDict\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flat_dict[key] = value\n    \n    return flat_dict", "import os\nimport yaml\nfrom collections import OrderedDict\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flat_dict[key] = value\n    \n    return flat_dict", "import os\nimport yaml\nfrom collections import OrderedDict\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_path):\n        return {}\n\n    with open(spec_path, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n\n    # Flatten the spec dictionary\n    flat_dict = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flat_dict[key] = value\n\n    return flat_dict", "import os\nimport yaml\nfrom collections import OrderedDict\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.exists(spec_file_path):\n        return {}\n    \n    with open(spec_file_path, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the dictionary by recursively converting nested structures\n    def flatten_dict(data, parent_key='', sep='.'):\n        items = []\n        for k, v in data.items():\n            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n            if isinstance(v, dict):\n                items.extend(flatten_dict(v, new_key, sep=sep).items())\n            elif isinstance(v, list):\n                for i, item in enumerate(v):\n                    if isinstance(item, dict):\n                        items.extend(flatten_dict(item, f\"{new_key}{sep}{i}\", sep=sep).items())\n                    else:\n                        items.append((f\"{new_key}{sep}{i}\", item))\n            else:\n                items.append((new_key, v))\n        return OrderedDict(items)\n    \n    return dict(flatten_dict(spec))", "import os\nimport yaml\nfrom collections import OrderedDict\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            flat_dict[key] = value\n    \n    return flat_dict"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2043, "num_retrieved": 5, "inference_time": 0.9543183247248331, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INFRARED_CONFIG_PATH'):\n        os.environ['INFRARED_CONFIG_PATH'] = self.ansible_config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INFRARED_CONFIG_PATH'):\n        os.environ['INFRARED_CONFIG_PATH'] = self.ansible_config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INIR_CONFIG_PATH'):\n        os.environ['INIR_CONFIG_PATH'] = self.ansible_config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INIR_CONFIG_PATH'):\n        os.environ['INIR_CONFIG_PATH'] = self.ansible_config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INFRARED_CONFIG_PATH'):\n        os.environ['INFRARED_CONFIG_PATH'] = self.ansible_config_path"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2091, "num_retrieved": 7, "inference_time": 0.9543183247248331, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation as the actual logic\n    # for extending CLI options is not fully provided in the context.\n    # In a real implementation, this method would add CLI arguments\n    # specific to the spec to the given subparser.\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific spec object\n    # and its defined CLI options\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific spec\n    # and how it defines its CLI options\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This method is part of the SpecObject abstract base class\n    # It should extend the provided subparser with CLI options\n    # based on the spec defined in the object\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation as the full context is incomplete\n    # In a real implementation, this method would add CLI options based on the spec\n    # The actual implementation would depend on the specific spec and its defined options\n    pass"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 2048, "actual_tokens": 2066, "num_retrieved": 8, "inference_time": 0.9543183247248331, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")"]}
