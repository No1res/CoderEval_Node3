{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 116757, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Nanoseconds must be int\")\n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, 1000000000)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Nanoseconds must be int\")\n    \n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, 1000000000)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 110903, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta data.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 110063, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 106039, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' or 'Y' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 101213, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 207106, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for version in [(3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 1), (5, 3)]:\n            if version == protocol_version:\n                handlers[version] = cls._get_handler_for_version(version)\n        return handlers\n    \n    # Return all supported versions\n    handlers = {\n        (3, 5): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5, 3): Bolt5x3\n    }\n    return handlers\n\n@staticmethod\ndef _get_handler_for_version(version):\n    \"\"\"Helper method to get the appropriate handler class for a version.\"\"\"\n    if version == (3, 5):\n        return Bolt3\n    elif version == (4, 0):\n        return Bolt4x0\n    elif version == (4, 1):\n        return Bolt4x1\n    elif version == (4, 2):\n        return Bolt4x2\n    elif version == (4, 3):\n        return Bolt4x3\n    elif version == (4, 4):", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for version in [(3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 1), (5, 3)]:\n            if version == protocol_version:\n                handlers[version] = cls._get_handler_for_version(version)\n        return handlers\n    \n    # Return all available versions if no specific version is provided\n    handlers = {}\n    for version in [(3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 1), (5, 3)]:\n        handlers[version] = cls._get_handler_for_version(version)\n    return handlers\n\n@staticmethod\ndef _get_handler_for_version(version):\n    \"\"\"Helper method to get the appropriate handler class for a version.\"\"\"\n    if version == (3, 5):\n        return Bolt3\n    elif version == (4, 0):\n        return Bolt4x0\n    elif version == (4, 1):\n        return Bolt4x1\n    elif version == (4, 2):\n        return Bolt4x2\n    elif version == (4, 3):\n        return Bolt4x3\n    elif version == (4, 4):\n        return Bolt4x4\n    elif version == (5, 1):\n        return Bolt5x1", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the specific version is supported\n        version_map = {\n            (3, 5): Bolt3,\n            (4, 0): Bolt4x0,\n            (4, 1): Bolt4x1,\n            (4, 2): Bolt4x2,\n            (4, 3): Bolt4x3,\n            (4, 4): Bolt4x4,\n            (5, 0): Bolt5x0,\n            (5, 1): Bolt5x1,\n            (5, 2): Bolt5x2,\n            (5, 3): Bolt5x3,\n        }\n        \n        return {version: version_map[version] for version in version_map.keys() if version == protocol_version}\n    \n    # Return all supported versions if no specific version is provided\n    return {\n        (3, 5): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 0): Bolt5x0,\n        (5, 1): Bolt5x1,\n        (5, 2): Bolt5x2,\n        (5, 3): Bolt5x3,\n    }", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (3, 0):\n            handlers[protocol_version] = Bolt3\n        elif protocol_version == (4, 0):\n            handlers[protocol_version] = Bolt4x0\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 0):\n            handlers[protocol_version] = Bolt5x0\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (5, 4):\n            handlers[protocol_version] = Bolt5x4\n        elif protocol_version == (5, 5):\n            handlers[protocol_version] = Bolt5x5\n        elif protocol_version == (5, 6):\n            handlers[protocol_version] = Bolt5x6\n        elif protocol_version == (5, 7):\n            handlers[protocol_version", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the specific version is supported\n        # This is a simplified implementation - in reality, this would\n        # check against actual supported versions\n        supported_versions = {\n            (3, 5): Bolt3,\n            (4, 0): Bolt4x0,\n            (4, 1): Bolt4x1,\n            (4, 2): Bolt4x2,\n            (4, 3): Bolt4x3,\n            (4, 4): Bolt4x4,\n            (5, 0): Bolt5x0,\n            (5, 1): Bolt5x1,\n            (5, 3): Bolt5x3\n        }\n        \n        return {version: handler for version, handler in supported_versions.items() \n                if version == protocol_version}\n    else:\n        # Return all supported versions\n        supported_versions = {\n            (3, 5): Bolt3,\n            (4, 0): Bolt4x0,\n            (4, 1): Bolt4x1,\n            (4, 2): Bolt4x2,\n            (4, 3): Bolt4x3,\n            (4, 4): Bolt4x4,\n            (5, 0): Bolt5x0,\n            (5, 1): Bolt5x1,\n            (5, 3): Bolt5x3\n        }\n        return supported_versions"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 211369, "num_retrieved": 478, "inference_time": 37.95220559835434, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with metadata and timeout\n            # These will be passed to the transaction when it is executed\n            return func(tx, *args, **kwargs)\n        # Add metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with metadata and timeout\n            # This will be passed to the transaction function\n            return func(tx, *args, **kwargs)\n        \n        # Add metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        \n        return wrapper\n    \n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with metadata and timeout\n            # This will be passed to the transaction function\n            return func(tx, *args, **kwargs)\n        \n        # Add metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        \n        return wrapper\n    \n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with metadata and timeout\n            # These will be passed to the transaction when it is executed\n            return func(tx, *args, **kwargs)\n        # Add metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with metadata and timeout\n            # This will be passed to the transaction function\n            return func(tx, *args, **kwargs)\n        \n        # Add metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 124085, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._list.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._list.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._values.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 175708, "num_retrieved": 497, "inference_time": 37.95220559835434, "generate_results": ["def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[key] for key in self._keys]\n    return [self._values[key] for key in keys]", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[key] for key in self._keys]\n    return [self._values[key] for key in keys]", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values]\n    \n    values = []\n    for key in keys:\n        if isinstance(key, int):\n            values.append(self._values[key])\n        else:\n            values.append(self._values[key])\n    return values", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[key] for key in self._keys]\n    return [self._values[key] for key in keys if key in self._values]", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[key] for key in self._keys]\n    return [self._values[key] for key in keys]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 209974, "num_retrieved": 498, "inference_time": 37.95220559835434, "generate_results": ["def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    keys_list = list(keys) if keys else []\n    \n    # Get the record's keys and values\n    record_keys = self.keys()\n    record_values = self.values()\n    \n    # If no keys are provided, return all values\n    if not keys_list:\n        return dict(zip(record_keys, record_values))\n    \n    # Process each key\n    for key in keys_list:\n        if isinstance(key, int):\n            # Handle index\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            record_dict[record_keys[key]] = record_values[key]\n        else:\n            # Handle key name\n            try:\n                record_dict[key] = record_values[record_keys.index(key)]\n            except ValueError:\n                # Key not found, insert with None\n                record_dict[key] = None\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    keys_list = list(keys) if keys else []\n    \n    # Get the record's keys and values\n    keys_list = list(keys) if keys else []\n    \n    # If no keys are provided, return all values\n    if not keys_list:\n        for key in self.keys():\n            record_dict[key] = self.value(key)\n        return record_dict\n    \n    # Otherwise, include only the specified keys or indexes\n    for key in keys_list:\n        if isinstance(key, int):\n            # Handle index-based access\n            if key < 0 or key >= len(self.keys()):\n                raise IndexError(f\"Index {key} is out of bounds for record with {len(self.keys())} keys\")\n            record_dict[self.keys()[key]] = self.value(self.keys()[key])\n        else:\n            # Handle key-based access\n            record_dict[key] = self.value(key)\n    \n    # For any keys provided that are not in the record, insert with None\n    for key in keys_list:\n        if isinstance(key, int):\n            if key < 0 or key >= len(self.keys()):\n                raise IndexError(f\"Index {key} is out of bounds for record with {len(self.keys())} keys\")\n            if key not in self.keys():\n                record_dict[self.keys()[key]] = None\n        else:\n            if key not in self.keys():\n                record_dict[key] = None\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if keys:\n        # If keys are provided, use them to get values\n        for key in keys:\n            if isinstance(key, int):\n                # If key is an integer, get value by index\n                if key < 0 or key >= len(self._values):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self._keys[key]] = self._values[key]\n            else:\n                # If key is a string, get value by key name\n                if key not in self._keys:\n                    record_dict[key] = None\n                else:\n                    record_dict[key] = self._values[self._keys.index(key)]\n    else:\n        # If no keys are provided, return all values\n        for i, key in enumerate(self._keys):\n            record_dict[key] = self._values[i]\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    record_keys = self.keys()\n    \n    # If no keys are provided, return all values\n    if not keys:\n        for key in record_keys:\n            record_dict[key] = self.value(key)\n        return record_dict\n    \n    # Process each key provided\n    for key in keys:\n        if isinstance(key, int):\n            # Handle index-based access\n            if key < 0 or key >= len(record_keys):\n                raise IndexError(f\"Index {key} is out of bounds for record with {len(record_keys)} fields\")\n            record_dict[record_keys[key]] = self.value(key)\n        else:\n            # Handle key-based access\n            record_dict[key] = self.value(key)\n    \n    # Fill in missing keys with None\n    for key in record_keys:\n        if key not in record_dict:\n            record_dict[key] = None\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    keys_list = list(keys) if keys else []\n    \n    # Get the record's keys and values\n    keys_list = list(keys) if keys else list(self.keys())\n    \n    # If no keys are provided, return all values\n    if not keys_list:\n        for key in self.keys():\n            record_dict[key] = self.value(key)\n        return record_dict\n    \n    # Otherwise, include only the specified keys or indexes\n    for key in keys_list:\n        if isinstance(key, int):\n            # Handle index\n            if key < 0 or key >= len(self.keys()):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            record_dict[self.keys()[key]] = self.value(key)\n        else:\n            # Handle key\n            record_dict[key] = self.value(key)\n    \n    return record_dict"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 152134, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read a 16-bit unsigned integer\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 2 bytes\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 2 bytes\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 2 bytes\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop u16\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 193950, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 200976, "num_retrieved": 486, "inference_time": 37.95220559835434, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    \n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x11\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    \n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x11\", fields,\n                 response=Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    \n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x03\", fields,\n                 response=Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x11\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    \n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x11\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 119984, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def round_half_to_even(n):\n    \"\"\"\n    Rounds a number to the nearest integer, with ties rounded to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even number\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even number\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 183378, "num_retrieved": 472, "inference_time": 37.95220559835434, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base Point class\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None):\n            super().__init__(*coordinates)\n            self.srid = srid or srid_map.get(self.__class__.__name__)\n            \n        def __repr__(self):\n            return f\"{self.__class__.__name__}({self.coordinates}, srid={self.srid})\"\n    \n    # Set the class name and add to the srid_map\n    PointSubclass.__name__ = name\n    srid_map[name] = PointSubclass\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base Point class\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None):\n            super().__init__(*coordinates)\n            self._srid = srid\n            \n        @property\n        def srid(self):\n            return self._srid\n            \n        @srid.setter\n        def srid(self, value):\n            self._srid = value\n            \n        def __repr__(self):\n            return f\"{name}({self._coordinates}, srid={self._srid})\"\n            \n        def __eq__(self, other):\n            if not isinstance(other, PointSubclass):\n                return False\n            return self._coordinates == other._coordinates and self._srid == other._srid\n            \n    # Register the new Point subclass with the srid_map\n    srid_map[PointSubclass.__name__] = (PointSubclass, len(self._coordinates))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointType(Point):\n        def __new__(cls, *coordinates):\n            # Determine the SRID based on the coordinates\n            srid = srid_map.get(len(coordinates))\n            if srid is None:\n                # If no SRID is found, use the default behavior of Point\n                return super().__new__(cls, *coordinates)\n            \n            # Create a new instance with the SRID\n            instance = super().__new__(cls, *coordinates)\n            instance.srid = srid\n            return instance\n        \n        def __init__(self, *coordinates):\n            # Initialize the instance with coordinates\n            super().__init__(*coordinates)\n    \n    # Add the class to the global namespace\n    globals()[name] = PointType\n    \n    return PointType", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointType(Point):\n        def __new__(cls, *coordinates):\n            # Determine the SRID based on the coordinates\n            srid = srid_map.get(len(coordinates))\n            if srid is None:\n                # If no SRID is found, use the default behavior of Point\n                return super().__new__(cls, *coordinates)\n            \n            # Create a new instance with the SRID\n            instance = super().__new__(cls, *coordinates)\n            instance.srid = srid\n            return instance\n        \n        def __init__(self, *coordinates):\n            # Initialize the instance with coordinates\n            super().__init__(*coordinates)\n    \n    # Set the class attributes\n    PointType.__name__ = name\n    PointType.__doc__ = f\"Point subclass for {name}\"\n    \n    return PointType", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base Point class\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None):\n            super().__init__(*coordinates)\n            self.srid = srid or srid_map.get(self.__class__.__name__)\n            \n        @classmethod\n        def from_coordinates(cls, coordinates, srid=None):\n            return cls(*coordinates, srid=srid)\n            \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            return cls(*coordinates, srid=srid)\n            \n        def __repr__(self):\n            return f\"{self.__class__.__name__}({self.coordinates}, srid={self.srid})\"\n            \n    # Set the class name and add to the Point hierarchy\n    PointSubclass.__name__ = name\n    PointSubclass.__module__ = Point.__module__\n    \n    # Add SRID mapping to the class\n    PointSubclass.srid_map = srid_map\n    \n    # Add a method to get the dimension\n    PointSubclass.dimension = len(fields)\n    \n    # Add a method to validate coordinates\n    PointSubclass.validate_coordinates = lambda self: len(self.coordinates) == self.dimension\n    \n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 132203, "num_retrieved": 500, "inference_time": 37.95220559835434, "generate_results": ["def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated: {message}\",\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated: {message}\",\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        func._deprecated = True\n        func._deprecated_message = message\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 52035, "num_retrieved": 224, "inference_time": 3.6509519070386887, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # R options are set using the R option system, which is typically\n    # configured via R's command line or within R scripts using options()\n    # This function would typically inject R options into the R code\n    # to configure behavior that cannot be set through environment variables\n    \n    # Common R options that might be needed for pre-commit hooks\n    # These are set as R options that will be processed when R starts\n    r_options = [\n        # Set R to use a specific working directory\n        \"options(warn = 1)\",\n        # Set R to not show messages about missing packages\n        \"options(show.error.messages = FALSE)\",\n        # Set R to not show warnings about deprecated functions\n        \"options(deprecated = FALSE)\",\n        # Set R to not show messages about missing packages\n        \"options(pkg.load = FALSE)\"\n    ]\n    \n    # Inject the R options into the code\n    # This would typically be done by modifying the R script to include\n    # these options at the beginning\n    return f\"{code}\\n{''.join(r_options)}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # R options are set using the R option system, which is typically\n    # configured via R's command line or within R scripts using options()\n    # Since this function is meant to set R options inline, we would typically\n    # use R's option() function to set these options directly in the R code.\n    # This function takes R code and modifies it to include R options at the\n    # beginning of the script.\n    \n    # Common R options that might need to be set include:\n    # - options(warn=2) to get more warnings\n    # - options(digits=5) to set number of digits\n    # - options(show.signif.stars=TRUE) to show significance stars\n    \n    # Add R options at the beginning of the code\n    r_options = \"\"\"\noptions(warn=2)\noptions(digits=5)\noptions(show.signif.stars=TRUE)\noptions(continue=\" \")\noptions(keep.source=TRUE)\n\"\"\"\n    \n    # Insert the options at the beginning of the code\n    return r_options + \"\\n\" + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be configured via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # R options that need to be set inline\n    r_options = [\n        '--vanilla',  # Disable R startup scripts and options\n        '--no-save',  # Don't save the R session\n        '--no-restore',  # Don't restore the R session\n        '--no-site-file',  # Don't load site-specific configuration\n        '--no-init-file',  # Don't load init files\n        '--no-restore',  # Don't restore the R session\n    ]\n    \n    # Add R options to the code\n    r_code = f\"options({', '.join([f'option={opt}' for opt in r_options])})\"\n    \n    # Return the modified code with R options\n    return f\"{code}\\n{r_code}\""]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 58886, "num_retrieved": 257, "inference_time": 3.6509519070386887, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're not using concurrency, run sequentially\n    if jobs == 1:\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout\n    \n    # For concurrent execution, we would typically use multiprocessing or threading\n    # However, since this is a simplified implementation, we'll just run sequentially\n    # in a way that mimics xargs behavior\n    \n    # In a real implementation, we would spawn multiple processes with the command\n    # and the files, and collect the results\n    result = []\n    for file_arg in varargs:\n        result.append(subprocess.run(cmd, input=file_arg.encode(), capture_output=True))\n    \n    # Combine results\n    final_returncode = 0\n    final_output = b''\n    for proc in result:\n        if proc.returncode != 0:\n            final_returncode = proc.returncode\n        final_output += proc.stdout\n    \n    return final_returncode, final_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're not using concurrency, run sequentially\n    if jobs == 1:\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout\n    \n    # For concurrency, we'd typically use a multiprocessing or threading approach\n    # However, since this is a simplified implementation, we'll just run sequentially\n    # in a way that mimics xargs behavior\n    \n    # In a real implementation, we would:\n    # 1. Split the varargs into chunks based on target_concurrency\n    # 2. Run each chunk in parallel\n    # 3. Combine the results\n    \n    # For now, we'll just run the command with all arguments\n    return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency to use\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're not using concurrency, run sequentially\n    if jobs == 1:\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout\n    \n    # For concurrency, use a simple parallel execution approach\n    # This is a simplified version that doesn't implement full xargs behavior\n    import multiprocessing as mp\n    from concurrent.futures import ProcessPoolExecutor\n    \n    def run_single(file_arg):\n        try:\n            result = subprocess.run(cmd, input=file_arg.encode(), capture_output=True)\n            return result.returncode, result.stdout\n        except Exception as e:\n            return 1, e.__str__.encode()\n    \n    # Use process pool to run commands in parallel\n    with ProcessPoolExecutor(max_workers=jobs) as executor:\n        futures = [executor.submit(run_single, arg) for arg in varargs]\n        results = [future.result() for future in futures]\n    \n    # Combine results\n    overall_returncode = 0\n    overall_output = b''\n    for retcode, output in results:\n        if retcode != 0:\n            overall_returncode = retcode\n        overall_output += output\n    \n    return overall_returncode, overall_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're running in a serial mode, just run one at a time\n    if jobs == 1:\n        result = subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True)\n        return result.returncode, result.stdout\n    \n    # For parallel execution, use a simple approach with subprocesses\n    # This is a simplified version that doesn't handle all edge cases\n    import multiprocessing as mp\n    from concurrent.futures import ProcessPoolExecutor\n    \n    def run_single(file_arg):\n        try:\n            result = subprocess.run(\n                cmd, \n                input=file_arg.encode(), \n                capture_output=True, \n                text=True,\n                timeout=300\n            )\n            return result.returncode, result.stdout.encode()\n        except subprocess.TimeoutExpired:\n            return 1, b'timeout'\n    \n    # Use process pool to run in parallel\n    with ProcessPoolExecutor(max_workers=jobs) as executor:\n        futures = [executor.submit(run_single, arg) for arg in varargs]\n        results = [future.result() for future in futures]\n    \n    # Aggregate results\n    overall_returncode = 0\n    overall_output = b''\n    \n    for retcode, output in results:\n        if retcode != 0:\n            overall_returncode = retcode\n        overall_output += output\n    \n    return overall_returncode, overall_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're running in serial mode, just run the command once\n    if jobs == 1:\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), **kwargs).returncode, b'\\n'.join(varargs).encode()\n    \n    # For parallel execution, use a simple approach with subprocess\n    # This is a simplified version that doesn't fully implement xargs\n    # behavior like input handling or error propagation\n    \n    # Create a temporary file to hold the input\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n        f.write('\\n'.join(varargs))\n        temp_file = f.name\n    \n    try:\n        # Run the command with the temporary file\n        result = subprocess.run(cmd, input=None, text=True, **kwargs)\n        return result.returncode, result.stdout.encode() if result.stdout else b''\n    finally:\n        import os\n        if os.path.exists(temp_file):\n            os.unlink(temp_file)"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 28187, "num_retrieved": 116, "inference_time": 3.6509519070386887, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle every time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 45680, "num_retrieved": 198, "inference_time": 3.6509519070386887, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 62520, "num_retrieved": 288, "inference_time": 3.6509519070386887, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a valid executable path\n    # This handles the case where the first element might be a shebang path that needs to be resolved\n    # and ensures that on Windows, the command is properly formatted for execution\n    \n    # If the command starts with a shebang, we need to ensure it's properly resolved\n    # For Windows, we need to handle the case where the shebang might be in a deep path\n    # and ensure that the command is properly formatted\n    \n    # The implementation follows the pattern of resolving shebangs and ensuring\n    # the command is executable on Windows\n    \n    # If the first element is a shebang, we need to resolve it to a proper executable\n    if cmd and cmd[0].startswith('#!'):\n        # Extract the path after the shebang\n        shebang_path = cmd[0][2:]\n        # Normalize the path to ensure it's in a valid format\n        # This handles deep-path shebangs by ensuring the path is properly formatted\n        # and can be executed on Windows\n        # Replace any forward slashes with backslashes for Windows compatibility\n        shebang_path = shebang_path.replace('/', '\\\\')\n        # Ensure the path is properly formatted for Windows\n        # This ensures that the shebang path is properly resolved\n        # and can be executed on Windows\n        cmd = (shebang_path,) + cmd[1:]\n    \n    # Ensure the command is properly formatted for Windows\n    # This handles the case where the command might be in a deep path\n    # and ensures that the command is properly formatted for execution\n    \n    # Return the normalized command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a valid executable path\n    # This handles the case where the first element might be a shebang path that needs to be resolved\n    # and ensures that on Windows, the command is properly formatted for execution\n    \n    # If the first element is a shebang path (starts with #!), we need to ensure it's properly formatted\n    if cmd and cmd[0].startswith('#!'):\n        # For shebangs, we need to ensure the path is properly resolved\n        # On Windows, we need to handle the path correctly\n        # This is a simplified version that just ensures the path is properly formatted\n        # In practice, this would involve resolving the shebang path to an actual executable\n        # For now, we just return the original command\n        pass\n    \n    # On Windows, we need to ensure that the command is properly formatted\n    # This includes handling paths with backslashes and ensuring they are properly quoted\n    # We also need to ensure that the command is properly formatted for execution\n    # This is a simplified version that just returns the original command\n    # In practice, this would involve more complex path handling and shebang resolution\n    \n    # Return the original command as is\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a proper executable path\n    # This handles the case where the first element might be a shebang or a path\n    # that needs to be resolved properly on Windows\n    if len(cmd) == 0:\n        return cmd\n    \n    # If the first element is a shebang, we need to handle it specially\n    # On Windows, shebangs are not parsed by the system, so we need to ensure\n    # the command is properly formatted\n    if cmd[0].startswith('#!'):\n        # For shebangs, we need to ensure the path is properly resolved\n        # and that the command is properly formatted\n        # We replace the shebang with the actual executable path\n        # This is a simplified version that just replaces the shebang\n        # with the actual executable path\n        # In practice, this would involve resolving the path to the actual executable\n        # For now, we just return the original command\n        pass\n    \n    # On Windows, we need to ensure that the command is properly formatted\n    # and that the path is properly resolved\n    # We normalize the path by ensuring it's in the correct format\n    # This is a simplified version that just returns the original command\n    # In practice, this would involve more complex path resolution\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a proper executable path\n    # This handles the case where the first element might be a shebang or a path\n    # that needs to be resolved properly on Windows\n    if len(cmd) == 0:\n        return cmd\n    \n    # If the first element is a shebang, we need to ensure it's properly handled\n    # On Windows, shebangs are not parsed by default, so we need to ensure the\n    # first element is a valid executable path\n    if cmd[0].startswith('#!'):\n        # For shebangs, we need to ensure the first element is a valid executable\n        # that can be found in the system PATH\n        # We'll replace the shebang with a proper executable path\n        # This is a simplified version - in practice, this would involve\n        # finding the actual executable that matches the shebang\n        # For now, we just return the original command\n        pass\n    \n    # On Windows, we need to ensure that paths are properly formatted\n    # and that the first element is a valid executable\n    # We'll normalize the first element to ensure it's a valid path\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure that the first element is a valid executable\n        # that can be found in the system PATH\n        # We'll ensure that the first element is a valid path\n        # by replacing any forward slashes with backslashes\n        first_element = cmd[0]\n        # Replace forward slashes with backslashes for Windows compatibility\n        first_element = first_element.replace('/', '\\\\')\n        # Ensure the path ends with .exe if it's a Windows executable\n        if first_element.endswith('.py') or first_element.endswith('.sh'):\n            # For Python scripts, we need to ensure the path is properly formatted\n            # For shell scripts, we need to ensure the path is properly formatted\n            pass\n        # Replace the first element with the normalized path\n        cmd = (first_element,) + cmd[1:]\n    \n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a proper executable path\n    # This handles the case where a shebang might be present in a deep path\n    # and ensures that the command is properly formatted for execution\n    if len(cmd) == 0:\n        return cmd\n    \n    # If the first element is a shebang, we need to ensure it's properly handled\n    # For Windows, we need to ensure that the path is properly resolved\n    # and that the shebang is not being misinterpreted\n    if cmd[0].startswith('#!'):\n        # If the first element starts with a shebang, we need to handle it properly\n        # This is to ensure that deep-path shebangs work correctly\n        # We replace the shebang with the actual executable path\n        # This is a workaround for Windows not parsing shebangs properly\n        # and for the issue described in https://bugs.python.org/issue8557\n        # We also ensure that the path is properly formatted\n        # by replacing the shebang with the actual executable path\n        # This ensures that the command is properly formatted for execution\n        # on Windows\n        return (cmd[0].replace('#!', ''),) + cmd[1:]\n    \n    # If the first element is not a shebang, we just return the original command\n    return cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 10175, "num_retrieved": 54, "inference_time": 3.6509519070386887, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, call the function and store the result\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not hashkey:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, call the function and store the result\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, call the function and store the result\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 10517, "num_retrieved": 52, "inference_time": 3.6509519070386887, "generate_results": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    from cachetools.keys import hashkey, typedkey\n    \n    key_func = typedkey if typed else hashkey\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    from cachetools.keys import hashkey, typedkey\n    \n    key_func = typedkey if typed else hashkey\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        update_wrapper(wrapper, func)\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    \n    return decorator"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 3.6509519070386887, "generate_results": ["def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def key_func(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs)\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize)\n        key = typedkey if typed else hashkey\n        \n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        \n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def key_func(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs)\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def key_func(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs)\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 3.6509519070386887, "generate_results": ["def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        # Use a typed key to distinguish between different types of arguments\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = lambda *args, **kwargs: tuple((arg, type(arg)) for arg in args) + tuple((k, type(v)) for k, v in sorted(kwargs.items()))\n    else:\n        # Use a simple hash key\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = lambda *args, **kwargs: tuple(args) + tuple(sorted(kwargs.items()))\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                result = func(*args, **kwargs)\n                cache[key] = result\n                return result\n        return wrapper\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 10041, "num_retrieved": 49, "inference_time": 3.6509519070386887, "generate_results": ["def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(a) for a in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                # Function not in cache, compute and store result\n                result = func(*args, **kwargs)\n                cache[key] = result\n                return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            # Call the original function\n            result = func(*args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU) algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            key = args + tuple(sorted(kwargs.items())) if not typed else args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + tuple(type(arg) for arg in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(a) for a in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                # Function not in cache, compute and store result\n                result = func(*args, **kwargs)\n                cache[key] = result\n                return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            # Call the original function\n            result = func(*args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 8936, "num_retrieved": 45, "inference_time": 3.6509519070386887, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 11144, "num_retrieved": 57, "inference_time": 3.6509519070386887, "generate_results": ["def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 11342, "num_retrieved": 59, "inference_time": 3.6509519070386887, "generate_results": ["def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 11327, "num_retrieved": 60, "inference_time": 3.6509519070386887, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the key for the cache\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss, compute the value\n                    v = func(self, *args, **kwargs)\n                    # Store the value in the cache\n                    try:\n                        with lock:\n                            cache.setdefault(k, v)\n                    except ValueError:\n                        return v  # value too large\n            return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = 0, 0\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    pass\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss, compute the value and store it\n                    v = func(self, *args, **kwargs)\n                    cache[k] = v\n                    return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would require access to the cache's internal state\n                # which is not directly available in this context\n                # For simplicity, we return a placeholder\n                return _CacheInfo(0, 0, cache.maxsize, cache.currsize)\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        # Add cache_info and cache_clear to the wrapper\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the key for the cache\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            # Store the result in the cache\n            with lock:\n                try:\n                    cache.setdefault(k, v)\n                except ValueError:\n                    return v  # value too large\n            return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = 0, 0\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    pass\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        # Create a wrapper function that will handle caching\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the cached value\n                    return cache[k]\n                except KeyError:\n                    # Cache miss - compute the value and store it\n                    result = method(self, *args, **kwargs)\n                    cache[k] = result\n                    return result\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would need to be implemented based on the cache's info\n                # For now, we just return a placeholder\n                return _CacheInfo(0, 0, cache.maxsize, cache.currsize)\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        # Add cache info and clear methods\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        # Update the wrapper with the original method's metadata\n        functools.update_wrapper(wrapper, method)\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        # Create a wrapper function that will handle the caching\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    return cache[k]\n                except KeyError:\n                    # If not found, call the original method and store the result\n                    result = method(self, *args, **kwargs)\n                    cache[k] = result\n                    return result\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would need to be implemented based on the cache's info method\n                # For now, we just return a placeholder\n                return {'hits': 0, 'misses': 0, 'maxsize': cache.maxsize, 'currsize': cache.currsize}\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        # Add cache_info and cache_clear to the wrapper\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        # Update the wrapper with the original method's metadata\n        functools.update_wrapper(wrapper, method)\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 7109, "num_retrieved": 23, "inference_time": 3.6509519070386887, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback information\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_lines = []\n        for frame in tb:\n            tb_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = str(e)\n        \n        # Format the traceback\n        tb_str = \"\\n\".join(tb_lines)\n        \n        # Combine everything\n        result = f\"{exc_type}: {exc_msg}\\nTraceback (most recent call last):\\n{tb_str}\"\n        \n        return result\n        \n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            trace_lines.append(f\"{filename}:{lineno}:{name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            trace_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Return formatted string\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback\n    tb = traceback.extract_tb(e.__traceback__, max_level)\n    \n    # Format the exception\n    exc_type = e.__class__.__name__\n    exc_msg = e.args[0] if e.args else \"No message\"\n    \n    # Build the trace string\n    trace_lines = []\n    for frame in tb:\n        filename = frame.filename\n        line_no = frame.lineno\n        name = frame.name\n        trace_lines.append(f\"{filename}:{line_no}:{name}\")\n    \n    # Limit path level if needed\n    path_lines = []\n    for frame in tb:\n        filename = frame.filename\n        if filename.startswith('/'):\n            path_lines.append(filename)\n    \n    # Format the result\n    result = f\"{exc_type}: {exc_msg}\"\n    if trace_lines:\n        result += \"\\nTraceback (most recent call last):\\n\"\n        for line in trace_lines:\n            result += f\"  {line}\\n\"\n    \n    return result", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            trace_lines.append(f\"{filename}:{lineno}:{name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 11406, "num_retrieved": 43, "inference_time": 3.6509519070386887, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 11524, "num_retrieved": 44, "inference_time": 12.674715131521225, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 11387, "num_retrieved": 44, "inference_time": 12.674715131521225, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 5115, "num_retrieved": 16, "inference_time": 12.674715131521225, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters or local data if needed\n    # Note: ContextFilter uses gevent.local, so we don't need to explicitly reset it\n    # as it's per-thread and managed by the local instance\n    \n    # Optionally, reset any platform-specific state if needed\n    # This is a placeholder - actual implementation depends on what needs to be reset\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state found in context, so no additional reset)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state is referenced in context, so minimal reset)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    cls._reset_platform_state()\n    \n    # Reinitialize any default logging configuration\n    cls._init_default_logging()", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state is referenced in the context, so this is minimal)"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 98613, "num_retrieved": 431, "inference_time": 12.674715131521225, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # of the test being run and the available implementations\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the specific test case\n    class_name = self.__class__.__name__\n    if class_name.endswith('Py'):\n        return class_name[:-2]\n    elif class_name.endswith('Fallback'):\n        return class_name[:-9]\n    return class_name", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the naming convention of the classes being tested\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the specific test case\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # of the test being run and the available implementations\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the specific test case\n    class_name = self.__class__.__name__\n    if class_name.endswith('Py'):\n        return class_name[:-2]\n    elif class_name.endswith('Fallback'):\n        return class_name[:-9]\n    return class_name", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the naming convention of the classes being tested\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the specific test case\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # of the test being run and the available implementations\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the specific test case\n    return self._getTargetClass()"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 93884, "num_retrieved": 418, "inference_time": 12.674715131521225, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and the final merged ordering\n    positions = {}\n    merged = []\n    \n    # Process each ordering\n    for ordering in orderings:\n        # For each element in the ordering, record its position\n        for i, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = []\n            positions[element].append(i)\n    \n    # Sort the orderings by the first element that appears in them\n    # This ensures that the orderings are processed in a way that respects\n    # the constraints\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else None)\n    \n    # Merge the orderings\n    result = []\n    for ordering in sorted_orderings:\n        # For each element in the ordering, add it to the result if it's not already there\n        for element in ordering:\n            if element not in result:\n                result.append(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Sort the orderings by their first element\n    sorted_orderings = sorted(orderings, key=lambda x: x[0])\n    \n    # Build the merged ordering\n    result = []\n    seen = set()\n    \n    # Process each ordering in the sorted order\n    for ordering in sorted_orderings:\n        # Add elements from this ordering that haven't been seen yet\n        for element in ordering:\n            if element not in seen:\n                # Check if this element appears in other orderings\n                # and if the suffix starting with this element is present\n                # in all orderings that contain it\n                valid = True\n                for other_ordering in orderings:\n                    if element in other_ordering:\n                        # Find the position of this element in the other ordering\n                        pos_in_other = other_ordering.index(element)\n                        # Check if the suffix starting with this element is present\n                        suffix = other_ordering[pos_in_other:]\n                        if not all(element in suffix for element in ordering):\n                            valid = False\n                            break\n                \n                if valid:\n                    result.append(element)\n                    seen.add(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Find the first ordering as a base\n    base = orderings[0]\n    \n    # For each subsequent ordering, merge it into the base\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the base\n        # We need to find the longest suffix of the base that matches a prefix of the ordering\n        merged = []\n        base_idx = 0\n        ordering_idx = 0\n        \n        # Process elements in order\n        while base_idx < len(base) and ordering_idx < len(ordering):\n            if base[base_idx] == ordering[ordering_idx]:\n                merged.append(base[base_idx])\n                base_idx += 1\n                ordering_idx += 1\n            else:\n                # If they don't match, we need to insert the ordering element\n                # But we can only do this if it's not already in the merged list\n                # and if it's not a suffix of the base\n                if ordering_idx < len(ordering):\n                    # Insert the ordering element\n                    merged.append(ordering[ordering_idx])\n                    ordering_idx += 1\n                else:\n                    # We've exhausted the ordering, so just append the rest of the base\n                    merged.extend(base[base_idx:])\n                    break\n        else:\n            # If we've processed all elements in the ordering, append the rest of the base\n            if ordering_idx < len(ordering):\n                merged.extend(ordering[ordering_idx:])\n            else:\n                merged.extend(base[base_idx:])\n        \n        # Update the base to be the merged result\n        base = merged\n    \n    return base", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Find the minimum position for each element across all orderings\n    min_positions = {}\n    for element, orderings_positions in positions.items():\n        min_positions[element] = min(orderings_positions.values())\n    \n    # Create a list to store the result\n    result = []\n    \n    # Process each ordering to build the result\n    # We need to ensure that if an element appears in multiple orderings,\n    # the suffix that begins with that element must be in both orderings\n    # This means we need to merge the orderings in a way that respects\n    # the constraints\n    \n    # Sort orderings by the first element that appears in them\n    # This ensures we process orderings in a way that respects the constraints\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else None)\n    \n    # Process each ordering in the sorted order\n    for ordering in sorted_orderings:\n        # Add elements from the ordering that haven't been added yet\n        for element in ordering:\n            if element not in result:\n                result.append(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # We'll use a list of lists to represent the orderings\n    result = []\n    \n    # For each ordering, we need to find the position of each element\n    # and ensure that if an element appears in multiple orderings,\n    # the suffix that begins with that element is preserved\n    \n    # First, create a mapping from each element to the orderings it appears in\n    element_to_orderings = {}\n    for ordering in orderings:\n        for element in ordering:\n            if element not in element_to_orderings:\n                element_to_orderings[element] = []\n            element_to_orderings[element].append(ordering)\n    \n    # Now, we need to merge the orderings\n    # We'll use a list to store the result\n    # We'll iterate through each ordering and add elements to the result\n    # in the order they appear in the orderings\n    \n    # Create a set of all elements that appear in any ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Create a list of all elements in the orderings\n    # We'll use a list to store the result\n    result = []\n    \n    # For each element in the orderings, we need to find its position\n    # in the result\n    # We'll use a dictionary to store the position of each element\n    element_position = {}\n    \n    # For each ordering, we need to find the position of each element\n    # and ensure that if an element appears in multiple orderings,\n    # the suffix that begins with that element is in both orderings\n    \n    # Create a list of all elements in"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 84001, "num_retrieved": 384, "inference_time": 12.674715131521225, "generate_results": ["def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 87068, "num_retrieved": 363, "inference_time": 12.674715131521225, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a simple version that removes duplicates and maintains order\n    unique_classes = []\n    seen = set()\n    \n    for cls in classes:\n        if cls not in seen:\n            unique_classes.append(cls)\n            seen.add(cls)\n    \n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a basic version that would be consistent with the interface\n    # resolution order and C3 linearization principles.\n    \n    if not classes:\n        return ()\n        \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n            \n    # Sort by name to ensure consistent ordering\n    # This is a simplified version; actual implementation may require\n    # more complex resolution logic\n    return tuple(sorted(unique_classes, key=lambda x: (x.__module__, x.__name__)))", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a basic version that would typically be used in interface\n    # resolution to avoid redundant base classes.\n    \n    # Remove duplicates and maintain order\n    seen = set()\n    result = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return result", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # If classes is empty, return empty list\n    if not classes:\n        return []\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by name to ensure consistent ordering\n    unique_classes.sort(key=lambda x: x.__name__)\n    \n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a simple version that removes duplicates and maintains order\n    unique_bases = []\n    seen = set()\n    \n    for cls in classes:\n        if cls not in seen:\n            unique_bases.append(cls)\n            seen.add(cls)\n    \n    return unique_bases"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 81609, "num_retrieved": 348, "inference_time": 12.674715131521225, "generate_results": ["def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, (Attribute, IMethod)):\n                    names.append((name, desc))\n            return names\n        else:\n            # Return all attributes including those defined by base classes\n            names = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, (Attribute, IMethod)):\n                    names.append((name, desc))\n            # Add attributes from base classes\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names.append((name, desc))\n            return names", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base classes\n            names_and_descriptions = []\n            # Get all attributes from the interface and its base interfaces\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names_and_descriptions.append((name, desc))\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, (Attribute, IMethod)):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes defined by the interface and its base interfaces\n            names_and_descriptions = []\n            # Get all base interfaces\n            for base in self.__bases__:\n                if hasattr(base, 'namesAndDescriptions'):\n                    # Recursively get names and descriptions from base interfaces\n                    base_names = base.namesAndDescriptions(all=True)\n                    names_and_descriptions.extend(base_names)\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, (Attribute, IMethod)):\n                    names_and_descriptions.append((name, desc))\n            # Remove duplicates while preserving order\n            seen = set()\n            result = []\n            for name, desc in names_and_descriptions:\n                if name not in seen:\n                    seen.add(name)\n                    result.append((name, desc))\n            return result", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base classes\n            names_and_descriptions = []\n            # Get all attributes from the interface and its base interfaces\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names_and_descriptions.append((name, desc))\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, (Attribute, IMethod)):\n                    names.append((name, desc))\n            return names\n        else:\n            # Return all attributes including those defined by base classes\n            names = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, (Attribute, IMethod)):\n                    names.append((name, desc))\n            # Add attributes from base classes\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names.append((name, desc))\n            return names"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 82798, "num_retrieved": 356, "inference_time": 12.674715131521225, "generate_results": ["def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names_and_descriptions__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names_and_descriptions__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names__', []) + self.__dict__.get('__all_names__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names__', []) + self.__dict__.get('__all_names__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names_and_descriptions__', []))"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 66665, "num_retrieved": 273, "inference_time": 12.674715131521225, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence.flattened())\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 97698, "num_retrieved": 422, "inference_time": 12.674715131521225, "generate_results": ["def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 98931, "num_retrieved": 437, "inference_time": 12.674715131521225, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 89434, "num_retrieved": 361, "inference_time": 12.674715131521225, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # TODO: Implement this function\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # TODO: Implement this function\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # TODO: Implement this function\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # This function is the opposite of _c_optimizations_required\n    # It returns True if C optimizations are not available or ignored\n    # based on the context of the system\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # This function is the opposite of _c_optimizations_required\n    # It returns True if C optimizations are not available or ignored\n    # based on the context of the system\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 98905, "num_retrieved": 437, "inference_time": 12.674715131521225, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 115047, "num_retrieved": 337, "inference_time": 12.674715131521225, "generate_results": ["def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 115760, "num_retrieved": 378, "inference_time": 12.674715131521225, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV instead for converting graphs to CSV\n    raise NotImplementedError(\"to_csv is deprecated and not used in Lena 0.5+\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV instead for converting graphs to CSV\n    raise NotImplementedError(\"to_csv is deprecated and not used in Lena 0.5+\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV for converting graphs to CSV\n    # Instead of implementing this function, we raise a deprecation warning\n    import warnings\n    warnings.warn(\n        \"to_csv is deprecated in Lena 0.5. Use lena.output.ToCSV instead.\",\n        DeprecationWarning\n    )\n    # Since this is a deprecated function, we return an empty string\n    # or raise an error to indicate it should not be used\n    raise DeprecationWarning(\"to_csv is deprecated in Lena 0.5. Use lena.output.ToCSV instead.\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV instead for converting graphs to CSV\n    raise NotImplementedError(\"to_csv is deprecated and not used in Lena 0.5+\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV for converting graphs to CSV\n    # The actual implementation is handled by ToCSV class\n    # which converts histograms and iterables to CSV format\n    pass"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 107439, "num_retrieved": 298, "inference_time": 36.65528753399849, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    for i, (coord, error) in enumerate(zip(self._coords, self._errors)):\n        if coord == coord_name:\n            return i\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    # This is based on the coordinate name in the graph's field names\n    for i, field_name in enumerate(self._field_names):\n        if field_name == coord_name:\n            # Return the error index for this coordinate\n            # This is determined by the error suffixes in the field names\n            # For example, if the field name is \"x\" and there's an error suffix \"x_err\",\n            # the error index is stored in self._error_x_ind\n            if field_name == self._field_names[0]:\n                return self._error_x_ind\n            elif field_name == self._field_names[1]:\n                return self._error_y_ind\n    # If coordinate name not found, return None\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    # This function is likely used in the context of root_graph_errors\n    # where we need to map coordinate names to their error indices\n    \n    # Search through the error suffixes to find the one matching the coordinate name\n    for err in self._parsed_error_names:\n        if err[1] == coord_name:\n            return err[3]\n    \n    # If no error index is found for the coordinate, return None\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    for i, (coord, error) in enumerate(zip(self._coords, self._errors)):\n        if coord == coord_name:\n            return i\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    # This is based on the coordinate name in the graph's field names\n    # and the error indices stored in the graph's error fields\n    for i, field_name in enumerate(self._field_names):\n        if field_name == coord_name:\n            # Check if there's an error field for this coordinate\n            if i < len(self._error_x_ind) and self._error_x_ind[i]:\n                return self._error_x_ind[i]\n            elif i < len(self._error_y_ind) and self._error_y_ind[i]:\n                return self._error_y_ind[i]\n    return None"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 115255, "num_retrieved": 385, "inference_time": 36.65528753399849, "generate_results": ["def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self.field_names:\n        if field.endswith('_low') or field.endswith('_high'):\n            # Extract the coordinate name (x, y, z) from the field name\n            coord_name = field[:-4]  # Remove '_low' or '_high'\n            if coord_name in ['x', 'y', 'z']:\n                error_fields.append((coord_name, field))\n    \n    # Build the error context\n    if error_fields:\n        error_context = {\"error\": {}}\n        for coord_name, field_name in error_fields:\n            # Find the index of the error field in the graph's coordinates\n            # This is a simplified approach - in practice, the actual index\n            # would be determined by the graph's internal structure\n            error_context[\"error\"][coord_name + \"_low\"] = {\"index\": 0}\n        \n        # Update the context with the error information\n        # Existing values are not removed from context.value and its subcontexts\n        if \"error\" not in context:\n            context[\"error\"] = {}\n        context[\"error\"].update(error_context[\"error\"])", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self.field_names:\n        if field.endswith('_low') or field.endswith('_high'):\n            error_fields.append(field)\n    \n    # Create error context\n    error_context = {}\n    for field in error_fields:\n        # Map error field names to x, y, z coordinates\n        if field.startswith('x'):\n            coord = 'x'\n        elif field.startswith('y'):\n            coord = 'y'\n        elif field.startswith('z'):\n            coord = 'z'\n        else:\n            continue\n            \n        # Find the index of the error field in the field names\n        field_index = self.field_names.index(field)\n        error_context[coord + '_low'] = {'index': field_index}\n    \n    # Update context with error information\n    if 'error' not in context:\n        context['error'] = {}\n    \n    # Append error context to existing error context\n    context['error'].update(error_context)", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self.field_names:\n        if field.endswith('_low') or field.endswith('_high'):\n            # Extract the coordinate name (x, y, z) from the field name\n            coord_name = field[:-4]  # Remove '_low' or '_high'\n            if coord_name in ['x', 'y', 'z']:\n                error_fields.append((coord_name, field))\n    \n    # Build the error context\n    if error_fields:\n        error_context = {\"error\": {}}\n        for coord_name, field_name in error_fields:\n            # Find the index of the error field in the graph's coordinates\n            # This is a simplified approach - in practice, the index would be determined\n            # by the field's position in the graph's coordinate system\n            error_context[\"error\"][coord_name + \"_low\"] = {\"index\": 0}  # Placeholder index\n        \n        # Update the context with the error information\n        # Existing values in context.value and its subcontexts are preserved\n        if \"error\" not in context:\n            context[\"error\"] = {}\n        context[\"error\"].update(error_context[\"error\"])", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self.field_names:\n        if field.endswith('_low') or field.endswith('_high'):\n            # Extract the base name (without _low or _high)\n            base_name = field[:-4] if field.endswith('_low') or field.endswith('_high') else field\n            # Map to x, y, z coordinates\n            if base_name in ['x', 'y', 'z']:\n                error_fields.append(base_name)\n    \n    # Create error context\n    if error_fields:\n        error_context = {\"error\": {}}\n        for field in error_fields:\n            # Find the index of the error field\n            # This is a simplified implementation - in practice,\n            # the actual index would be determined by the graph's structure\n            error_context[\"error\"][field] = {\"index\": 0}  # Placeholder index\n        \n        # Update context with error information\n        # Existing values are not removed from context.value and its subcontexts\n        if \"error\" not in context:\n            context[\"error\"] = error_context[\"error\"]\n        else:\n            # Append to existing error context\n            for key, val in error_context[\"error\"].items():\n                if key not in context[\"error\"]:\n                    context[\"error\"][key] = val\n    \n    # Ensure value context is preserved\n    # This is a placeholder - in practice, the value context would be preserved\n    # as specified in the docstring", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self._fields:\n        if field.startswith(\"error_\"):\n            error_fields.append(field)\n    \n    # Create error context\n    error_context = {\"error\": {}}\n    \n    # Map error field names to coordinate names\n    for field in error_fields:\n        # Extract coordinate name from error field (e.g., \"error_x_low\" -> \"x\")\n        coord_name = field[7:]  # Remove \"error_\" prefix\n        if coord_name.startswith(\"x\"):\n            coord_name = \"x\"\n        elif coord_name.startswith(\"y\"):\n            coord_name = \"y\"\n        elif coord_name.startswith(\"z\"):\n            coord_name = \"z\"\n        else:\n            continue\n            \n        # Extract the error type (e.g., \"low\", \"high\")\n        error_type = coord_name + \"_low\" if \"low\" in field else coord_name + \"_high\"\n        \n        # Find the index of this error field in the graph fields\n        try:\n            index = self._fields.index(field)\n            error_context[\"error\"][coord_name] = {\"index\": index}\n        except ValueError:\n            # Field not found in fields\n            pass\n    \n    # Update context with error information\n    if error_context[\"error\"]:\n        context.update(error_context)"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 115695, "num_retrieved": 372, "inference_time": 36.65528753399849, "generate_results": ["def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify 1D and multidimensional edges for integration\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin in the multidimensional mesh\n    for i in range(len(edges)):\n        for j in range(len(edges[i])):\n            # Get the bin content and its edges\n            bin_content = bins[i][j]\n            left_edge = edges[i][j]\n            right_edge = edges[i][j+1] if j+1 < len(edges[i]) else edges[i][-1]\n            \n            # Calculate bin width\n            bin_width = right_edge - left_edge\n            \n            # Add contribution to total integral\n            total += bin_content * bin_width\n    \n    return total", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify 1D and multidimensional edges for integration\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin in the multidimensional mesh\n    for i in range(len(edges)):\n        for j in range(len(edges[i])):\n            # Get the bin value\n            bin_val = bins[i][j]\n            \n            # Calculate the bin volume (area in 2D, length in 1D)\n            # For each dimension, calculate the width of the bin\n            bin_volume = 1.0\n            for dim in range(len(edges)):\n                if dim == 0:\n                    bin_volume *= (edges[dim][j+1] - edges[dim][j])\n                else:\n                    bin_volume *= (edges[dim][j+1] - edges[dim][j])\n            \n            # Add the product of bin value and volume to the total integral\n            total += bin_val * bin_volume\n    \n    return total", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify 1D and multidimensional edges\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # Get bin content\n            bin_val = bins[i][j]\n            \n            # Calculate bin volume (area in 2D, length in 1D)\n            if len(edges) == 1:\n                # 1D case\n                bin_volume = edges[0][j+1] - edges[0][j]\n            else:\n                # 2D case\n                bin_volume = (edges[0][i+1] - edges[0][i]) * (edges[1][j+1] - edges[1][j])\n            \n            # Add contribution to total integral\n            total += bin_val * bin_volume\n    \n    return total", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify 1D and multi-dimensional edges for integration\n    # For 1D, edges is a list of edges\n    # For multi-dimensional, edges is a list of lists of edges\n    if isinstance(edges[0], (list, tuple)):\n        # Multi-dimensional case\n        dim = len(edges)\n        total_integral = 0.0\n        \n        # Iterate over each bin in the multidimensional mesh\n        for i in range(len(edges[0]) - 1):\n            for j in range(len(edges[1]) - 1):\n                # For 2D case, integrate over x and y\n                # For higher dimensions, we need to compute the volume\n                # of each bin and multiply by the bin value\n                if dim == 2:\n                    # 2D case: integrate over x and y\n                    x_edge = edges[0]\n                    y_edge = edges[1]\n                    bin_value = bins[i][j]\n                    bin_area = (x_edge[i+1] - x_edge[i]) * (y_edge[j+1] - y_edge[j])\n                    total_integral += bin_value * bin_area\n                else:\n                    # For higher dimensions, we need to compute the volume\n                    # of each bin and multiply by the bin value\n                    # This is a simplified implementation for the general case\n                    # The actual implementation would depend on the dimensionality\n                    pass\n                    \n        return total_integral\n    else:\n        # 1D case\n        edges = edges\n        total_integral = 0.0\n        for i in range(len(edges) - 1):\n            bin_value = bins[i]\n            bin_width = edges[i+1] - edges[i]\n            total_integral += bin_value * bin_width\n        return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify the 1D and multidimensional edges format\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin in the multidimensional mesh\n    for i in range(len(edges)):\n        for j in range(len(edges[i])):\n            # Get the bin value\n            bin_value = bins[i][j]\n            \n            # Calculate the bin width in each dimension\n            width = edges[i+1][j] - edges[i][j]\n            \n            # Add the contribution of this bin to the total integral\n            total += bin_value * width\n    \n    return total"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 119542, "num_retrieved": 357, "inference_time": 36.65528753399849, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n        \n    if isinstance(seq, tuple):\n        # Check if any element in the tuple is a FillRequestSeq\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n        return False\n    \n    # Check if seq is a sequence with a FillRequest element\n    # This would require checking the internal structure\n    # which is not directly accessible without a specific implementation\n    # For now, we only check for direct FillRequestSeq instances\n    # and tuples containing them\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n        \n    if isinstance(seq, tuple):\n        for element in seq:\n            if is_fill_request_seq(element):\n                return True\n        return False\n    \n    # Check if seq has a fill_request element\n    try:\n        if ct.is_fill_request_el(seq):\n            return True\n    except:\n        pass\n        \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq is a sequence that contains a FillRequest element\n    if hasattr(seq, '_seq') and isinstance(seq, lena_sequence.LenaSequence):\n        for el in seq._seq:\n            if isinstance(el, fill_request_seq.FillRequestSeq):\n                return True\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n        \n    if isinstance(seq, tuple):\n        # Check if any element in the tuple is a FillRequest element\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n        return False\n    \n    # For other types, check if it has a FillRequest element\n    # This is a simplified check based on the context\n    # In a real implementation, we would need to inspect the internal structure\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq is a sequence that contains a FillRequest element\n    if hasattr(seq, '_seq') and isinstance(seq, lena_sequence.LenaSequence):\n        for el in seq._seq:\n            if isinstance(el, fill_request_seq.FillRequestSeq):\n                return True\n    \n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 115801, "num_retrieved": 326, "inference_time": 36.65528753399849, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return callable(getattr(obj, 'fill', None)) and callable(getattr(obj, 'request', None))"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 114340, "num_retrieved": 313, "inference_time": 36.65528753399849, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 115801, "num_retrieved": 326, "inference_time": 36.65528753399849, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 114499, "num_retrieved": 385, "inference_time": 36.65528753399849, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion: check all keys and values\n        result = {}\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n        return result\n    \n    # Level 1: only check direct keys and values\n    result = {}\n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif d1[key] != d2[key]:\n            result[key] = d1[key]\n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Level > 1 - recursive check\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys and values\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # For level > 0, we only go one level deep\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=-1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=0)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n        return result\n    else:\n        # Recursive case - check up to level\n        result = {}\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level-1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys and values\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Recursive case with finite level\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 110010, "num_retrieved": 303, "inference_time": 36.65528753399849, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and dimensions\n    edges = self.edges\n    dim = len(edges)\n    \n    # Check if coordinates are within the valid range for each dimension\n    bin_indices = []\n    for i in range(dim):\n        # Get the current dimension's edges\n        edge_arr = edges[i]\n        # Check if coordinate is within bounds\n        if coord[i] < edge_arr[0] or coord[i] >= edge_arr[-1]:\n            # Coordinate is outside valid range, ignore this fill\n            return\n        # Find the bin index for this coordinate using binary search\n        bin_indices.append(get_bin_on_value_1d(coord[i], edge_arr))\n    \n    # Get the bin content at the calculated indices\n    bin_content = self.bins\n    # Navigate through the nested structure to get the bin content\n    current_bin = bin_content\n    for idx in bin_indices:\n        current_bin = current_bin[idx]\n    \n    # Add the weight to the bin content\n    current_bin += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and check if the coordinate is within bounds\n    edges = self.edges\n    \n    # If the histogram is 1D, check against single edges\n    if len(edges) == 1:\n        low, high = edges[0][0], edges[0][1]\n        if low <= coord <= high:\n            # Get the bin index for the coordinate\n            bin_index = get_bin_on_value(coord, edges)\n            # Fill the bin with weight\n            if bin_index is not None:\n                self.bins[bin_index] += weight\n    # If the histogram is multi-dimensional, check each coordinate\n    else:\n        # For multi-dimensional histograms, we need to check each coordinate\n        # against its corresponding edges\n        bin_indices = []\n        for i, edge in enumerate(edges):\n            low, high = edge[0], edge[1]\n            if low <= coord[i] <= high:\n                bin_indices.append(get_bin_on_value(coord[i], edge))\n            else:\n                # Coordinate is outside the edges, ignore this fill\n                return\n        \n        # If all coordinates are within bounds, fill the bin\n        if bin_indices:\n            # Get the bin content and add the weight\n            bin_content = self.bins[bin_indices]\n            bin_content += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and coordinates\n    edges = self.edges\n    dim = len(edges)\n    \n    # Check if coordinates are within the valid range for each dimension\n    bin_indices = []\n    for i in range(dim):\n        # Get the current dimension's edges\n        edge_arr = edges[i]\n        # Check if coordinate is within bounds\n        if coord[i] < edge_arr[0] or coord[i] >= edge_arr[-1]:\n            # Coordinate is outside valid range, ignore this fill\n            return\n        # Find the bin index for this coordinate\n        bin_idx = get_bin_on_value_1d(coord[i], edge_arr)\n        bin_indices.append(bin_idx)\n    \n    # Get the bin content at the calculated indices\n    bin_content = self.bins\n    # Navigate to the correct bin using the indices\n    current_bin = bin_content\n    for idx in bin_indices:\n        current_bin = current_bin[idx]\n    \n    # Add the weight to the bin content\n    if current_bin is None:\n        current_bin = 0\n    current_bin += weight\n    \n    # Update the bin content\n    # Since we're modifying the bin content directly, we need to ensure\n    # that the bin structure is properly updated\n    # This assumes that the bins are mutable and can be directly modified\n    # In a real implementation, this might involve more complex structure updates\n    # or using a different approach to manage bin content\n    return current_bin", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and dimensions\n    edges = self.edges\n    dim = len(edges)\n    \n    # Check if coordinates are within the valid range for each dimension\n    bin_indices = []\n    for i in range(dim):\n        # Get the current dimension's edges\n        edge_arr = edges[i]\n        # Check if coordinate is within bounds\n        if coord[i] < edge_arr[0] or coord[i] >= edge_arr[-1]:\n            # Coordinate is outside valid range, ignore this fill\n            return\n        # Find the bin index using binary search\n        bin_idx = get_bin_on_value_1d(coord[i], edge_arr)\n        bin_indices.append(bin_idx)\n    \n    # Get the bin content at the calculated indices\n    bin_content = self.bins\n    # Navigate through the nested structure to get the specific bin\n    current_bin = bin_content\n    for idx in bin_indices:\n        current_bin = current_bin[idx]\n    \n    # Add the weight to the bin content\n    current_bin += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and coordinates\n    edges = self.edges\n    dim = len(edges)\n    \n    # Check if coordinates are within the valid range for each dimension\n    bin_indices = []\n    for i in range(dim):\n        # Get the current dimension's edges\n        edge_arr = edges[i]\n        # Check if coordinate is within bounds\n        if coord[i] < edge_arr[0] or coord[i] >= edge_arr[-1]:\n            # Coordinate is outside valid range, ignore this fill\n            return\n        # Find the bin index using binary search\n        bin_indices.append(get_bin_on_value_1d(coord[i], edge_arr))\n    \n    # Get the bin content at the calculated indices\n    bin_content = self.bins\n    # Navigate through the nested structure to get the bin content\n    current_bin = bin_content\n    for idx in bin_indices:\n        current_bin = current_bin[idx]\n    \n    # Add the weight to the bin content\n    current_bin += weight"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 209802, "num_retrieved": 450, "inference_time": 36.65528753399849, "generate_results": ["import re\nfrom typing import Dict, Any, List, Tuple\nfrom validation import ValidationError  # Assuming this is the ValidationError class\n\ndef _validate_labels(labels: Dict[Any, Any]) -> None:\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n    \n    Args:\n        labels (dict): the different labels to validate.\n        \n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define the regex patterns for keys and values\n    # Keys should be strings that match a pattern of alphanumeric characters and hyphens\n    key_pattern = r\"^[a-zA-Z_][a-zA-Z0-9_]*$\"\n    # Values should be strings that match a pattern of alphanumeric characters and hyphens\n    value_pattern = r\"^[a-zA-Z0-9_][a-zA-Z0-9_]*$\"\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the pattern\n        if not re.match(key_pattern, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n            \n    # Validate values\n    for key, value in labels.items():\n        # Check", "import re\nfrom typing import Dict, Any, List, Tuple\nfrom validation import ValidationError  # Assuming this is the ValidationError class\n\ndef _validate_labels(labels: Dict[Any, Any]) -> None:\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n    \n    Args:\n        labels (dict): the different labels to validate.\n        \n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regex patterns for keys and values\n    # Keys should be strings that match the pattern of valid label keys\n    key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    # Values should be strings that match the pattern of valid label values\n    value_pattern = r'^[a-zA-Z0-9_\\-\\.]*$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the pattern\n        if not re.match(key_pattern, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n            \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, str):\n            errors.append({str(value): \"Label value must be a string\"})\n            continue\n            \n        # Check if value matches", "import re\nfrom typing import Dict, Any, List\nfrom validation import ValidationError  # Assuming this is the ValidationError class\n\ndef _validate_labels(labels: Dict[Any, Any]) -> None:\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n    \n    Args:\n        labels (dict): the different labels to validate.\n        \n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regex patterns for keys and values\n    key_pattern = r'^[a-z][a-z0-9_\\-]*$'  # Must start with lowercase letter, followed by alphanumeric, underscore, or hyphen\n    value_pattern = r'^[a-z0-9_\\-]*$'  # Must start with lowercase letter or digit, followed by alphanumeric, underscore, or hyphen\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue", "import re\nfrom typing import Dict, Any, List, Tuple\n\ndef _validate_labels(labels: Dict[Any, Any]) -> None:\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n    \n    Args:\n        labels (dict): the different labels to validate.\n        \n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regex patterns for keys and values\n    # Keys should be strings that match a pattern (e.g., alphanumeric with hyphens)\n    key_pattern = r\"^[a-zA-Z_][a-zA-Z0-9_]*$\"\n    # Values should be strings that match a pattern (e.g., alphanumeric with hyphens)\n    value_pattern = r\"^[a-zA-Z0-9_\\-\\.]*$\"\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the pattern\n        if not re.match(key_pattern, key):\n            errors.append({str(key): f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, str):\n            errors.append({str(value): \"Label value must", "import re\nfrom typing import Dict, Any, List, Tuple\nfrom validation import ValidationError  # Assuming this is the ValidationError class\n\ndef _validate_labels(labels: Dict[Any, Any]) -> None:\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regex patterns for valid labels\n    # Keys should be valid identifiers (alphanumeric, underscore, hyphen)\n    key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    # Values should be valid strings (alphanumeric, underscore, hyphen, dot)\n    value_pattern = r'^[a-zA-Z0-9_\\-\\.]*$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the pattern\n        if not re.match(key_pattern, key):\n            errors.append({str(key): f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 170802, "num_retrieved": 500, "inference_time": 36.65528753399849, "generate_results": ["def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    # Minimum length of 1 character\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    # Minimum length of 1 character\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    # Minimum length of 1 character\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*[a-zA-Z0-9]$\")\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 183665, "num_retrieved": 500, "inference_time": 36.65528753399849, "generate_results": ["def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic would depend on the specific regular expression\n    # that is defined in the context. Since the context does not provide the\n    # specific regex pattern, this implementation assumes a basic validation\n    # that checks if the value is not None and is a string.\n    if value is None:\n        raise ValidationError(\"Value cannot be None\")\n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string\")\n    \n    # If specific validation rules were defined in the context, they would be\n    # implemented here. For now, we only validate that the value is a string.\n    return True", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic is not provided in the context\n    # This is a placeholder implementation based on the function signature\n    # In a real implementation, this would check against a specific regex pattern\n    if not isinstance(value, str):\n        raise ValidationError(f\"Expected string value, got {type(value).__name__}\")\n    \n    # Placeholder: if value is empty, raise ValidationError\n    if not value.strip():\n        raise ValidationError(\"Value cannot be empty\")\n    \n    # Placeholder: if value contains invalid characters, raise ValidationError\n    # This is a generic check - actual validation would depend on the specific regex\n    if any(c in value for c in ['<', '>', '&', '|', '^', '$', '*', '+', '?', '{', '}', '[', ']', '(']):\n        raise ValidationError(\"Value contains invalid characters\")\n    \n    return True", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic would depend on the specific regular expression\n    # that is defined in the context. Since the context does not provide the\n    # specific regex pattern, this implementation would typically check against\n    # a predefined pattern.\n    # For example, if the pattern is for a valid email address:\n    import re\n    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    if not re.match(email_pattern, value):\n        raise ValidationError(f\"Value '{value}' does not match the expected pattern\")", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic is not provided in the context\n    # This is a placeholder implementation based on the function signature\n    # In a real implementation, this would check against a specific regex pattern\n    if not isinstance(value, str):\n        raise ValidationError(f\"Expected string value, got {type(value).__name__}\")\n    \n    # Placeholder: if value is empty, raise ValidationError\n    if not value.strip():\n        raise ValidationError(\"Value cannot be empty\")\n    \n    # Placeholder: if value contains invalid characters, raise ValidationError\n    # This is a generic check - actual validation would depend on the specific regex\n    if any(c in value for c in ['<', '>', '&', '|', '`', '$', '{', '}', '[', ']']):\n        raise ValidationError(\"Value contains invalid characters\")\n    \n    return True", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic is not provided in the context\n    # Based on the function name and typical usage, this likely validates\n    # a value against a predefined regex pattern\n    # Since no specific pattern or context is provided, we'll assume\n    # a basic validation that checks if value is not None and is a string\n    if value is None:\n        raise ValidationError(\"Value cannot be None\")\n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string\")\n    # Additional validation would be based on specific requirements\n    # For now, we only validate that value is a string and not None"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 182714, "num_retrieved": 500, "inference_time": 36.65528753399849, "generate_results": ["def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid Kubernetes resource keys\n    # Must match: [a-z0-9]([a-z0-9.-]*[a-z0-9])?\n    # Must not contain: ., -, or spaces\n    import re\n    pattern = r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid keys: must start with a letter or underscore,\n    # followed by letters, digits, or underscores\n    import re\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid keys: must start with a letter or underscore,\n    # followed by letters, digits, or underscores\n    import re\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid Kubernetes resource keys\n    # Must match: [a-z0-9]([a-z0-9.-]*[a-z0-9])?\n    # Must not contain: ., -, or spaces\n    import re\n    pattern = r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid keys: must start with a letter or underscore,\n    # followed by letters, digits, or underscores\n    import re\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 210727, "num_retrieved": 488, "inference_time": 36.65528753399849, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:`generate_default_observer_schema_list`, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of\n    a Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the result\n    observer_schema = {}\n    \n    # Handle first level dictionary (complete observer schema for a resource)\n    if first_level:\n        # Copy the identifying fields from the manifest\n        try:\n            observer_schema[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        except KeyError:\n            pass\n            \n        try:\n            observer_schema[\"kind\"] = manifest_dict.get(\"kind\")\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict.get(\"metadata\", {})\n            if isinstance(metadata, dict):\n                observer_schema[\"metadata\"] = {}\n                # Copy metadata fields\n                try:\n                    observer_schema[\"metadata\"][\"name\"] = metadata.get(\"name\")\n                except KeyError:\n                    pass\n                try:\n                    observer_schema[\"metadata\"][\"namespace\"] = metadata.get(\"namespace\")\n                except KeyError:\n                    pass\n        except (KeyError, TypeError):\n            pass\n    \n    # Recursively process all values in the manifest dictionary\n    for key, value in manifest_dict.items():\n        # If the value is a dict or list, process it recursively\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level=False)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n    \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # Handle the case where first_level is True (complete observer schema)\n    if first_level:\n        # Copy the identifying fields from the manifest\n        if \"apiVersion\" in manifest_dict:\n            observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        if \"kind\" in manifest_dict:\n            observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n        if \"metadata\" in manifest_dict:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                # Copy metadata fields that are relevant for identification\n                if \"name\" in metadata:\n                    observer_schema[\"metadata\"] = metadata.copy()\n                else:\n                    observer_schema[\"metadata\"] = {}\n    \n    # Recursively process all nested values in the manifest dictionary\n    for key, value in manifest_dict.items():\n        # If the value is a dictionary, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        # If the value is a list, recursively process it\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level=False)\n        # For all other values, set to None\n        else:\n            observer_schema[key] = None\n    \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # Handle the first level dictionary case\n    if first_level:\n        # Copy the identifying fields from the manifest\n        if \"apiVersion\" in manifest_dict:\n            observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        if \"kind\" in manifest_dict:\n            observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n        if \"metadata\" in manifest_dict:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                observer_schema[\"metadata\"] = {}\n                # Copy metadata fields that are relevant for observer schema\n                if \"name\" in metadata:\n                    observer_schema[\"metadata\"][\"name\"] = metadata[\"name\"]\n                if \"namespace\" in metadata:\n                    observer_schema[\"metadata\"][\"namespace\"] = metadata[\"namespace\"]\n    \n    # Recursively process all values in the manifest dictionary\n    for key, value in manifest_dict.items():\n        # If the value is a dictionary, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        # If the value is a list, recursively process it\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level=False)\n        # For all other values, replace with None\n        else:\n            observer_schema[key] = None\n    \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the result\n    result = {}\n    \n    # Handle the first level case where we need to copy identifying fields\n    if first_level:\n        # Copy the identifying fields from the manifest\n        if \"apiVersion\" in manifest_dict:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        if \"kind\" in manifest_dict:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        if \"metadata\" in manifest_dict:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                # Copy metadata fields\n                result[\"metadata\"] = {}\n                if \"name\" in metadata:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                # For other metadata fields, we'll handle them recursively\n                if \"namespace\" in metadata:\n                    result[\"metadata\"][\"namespace\"] = metadata[\"namespace\"]\n        \n        # For all other fields, set to None if not a list or dict\n        for key, value in manifest_dict.items():\n            if key not in [\"apiVersion\", \"kind\", \"metadata\"]:\n                if isinstance(value, dict) or isinstance(value, list):\n                    result[key] = value\n                else:\n                    result[key] = None\n    \n    # For non-first-level cases, just replace non-list and non-dict values with None\n    else:\n        for key, value in manifest_dict.items():\n            if isinstance(value, dict) or isinstance(value, list):\n                if isinstance(value, dict):\n                    result[key] = generate_default_observer_schema_dict(value, first_level=False)", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # Handle the first level dictionary case\n    if first_level:\n        # Copy the identifying fields from the manifest\n        if \"apiVersion\" in manifest_dict:\n            observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        if \"kind\" in manifest_dict:\n            observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n        if \"metadata\" in manifest_dict:\n            metadata = manifest_dict[\"metadata\"]\n            # Copy metadata fields that are relevant for observer schema\n            if \"name\" in metadata:\n                observer_schema[\"metadata\"] = {\n                    \"name\": metadata[\"name\"]\n                }\n            # If metadata has other fields, we'll handle them in the recursive call\n            else:\n                observer_schema[\"metadata\"] = {}\n    \n    # Recursively process the dictionary\n    for key, value in manifest_dict.items():\n        # If the value is a dict or list, process it recursively\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level=False)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n    \n    return observer_schema"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 210528, "num_retrieved": 479, "inference_time": 36.65528753399849, "generate_results": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema):\n        if idx >= len(last_applied_manifest):\n            last_applied_manifest.append(value)\n            continue\n            \n        # If the response is not present, skip\n        if not response or idx >= len(response):\n            continue\n            \n        # If the response is present, update the last_applied_manifest\n        # with the response value\n        last_applied_manifest[idx] = response[idx]", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema):\n        if idx >= len(last_applied_manifest):\n            last_applied_manifest.append(value)\n            continue\n            \n        if idx < len(response):\n            # If the response is not present, initialize with the observer schema\n            if not response[idx]:\n                last_applied_manifest[idx] = value\n            else:\n                # Update the manifest with the response values\n                last_applied_manifest[idx] = response[idx]\n        else:\n            # If response is shorter than observer_schema, append the schema value\n            last_applied_manifest.append(value)", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, observed in enumerate(observer_schema):\n        if idx >= len(last_applied_manifest):\n            last_applied_manifest.append(observed)\n            continue\n            \n        if idx < len(response):\n            # If the observed resource is present in the response, update it\n            # with the response values\n            if isinstance(observed, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observed, response[idx]\n                )\n            elif isinstance(observed, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observed, response[idx]\n                )\n            else:\n                # For scalar values, just assign the response value\n                last_applied_manifest[idx] = response[idx]\n        else:\n            # If the observed resource is not present in the response, add it\n            # as a new entry\n            last_applied_manifest.append(observed)", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema):\n        if idx >= len(last_applied_manifest):\n            last_applied_manifest.append(value)\n            continue\n            \n        if idx < len(response):\n            # If the response is not present, initialize with the observer schema\n            if not response[idx]:\n                last_applied_manifest[idx] = value\n            else:\n                # Update the value in last_applied_manifest with the response value\n                last_applied_manifest[idx] = response[idx]\n        else:\n            # If response is shorter than observer_schema, add the remaining observer schema items\n            last_applied_manifest.append(value)", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema):\n        if idx >= len(last_applied_manifest):\n            last_applied_manifest.append(value)\n            continue\n            \n        if idx >= len(response):\n            # If the response is shorter than the observer_schema, we need to\n            # handle the case where we have more observer_schema entries than\n            # response entries\n            continue\n            \n        # If the response entry is not present in the observer_schema, we need\n        # to initialize the value in last_applied_manifest\n        if value not in response:\n            last_applied_manifest[idx] = value"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 210327, "num_retrieved": 471, "inference_time": 36.65528753399849, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            # For primitive values, just assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 179487, "num_retrieved": 500, "inference_time": 13.745969280600548, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Initialize the default observer schema\n    default_observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Create a default observer schema entry for each resource\n        # The default schema includes the resource's API version, kind, and name\n        observer_schema_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the resource to the default observer schema\n        default_observer_schema.append(observer_schema_entry)\n    \n    # Set the default observer schema in the application status\n    app.status.mangled_observer_schema = default_observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Initialize the default observer schema\n    default_observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Create a default observer schema entry for each resource\n        # The default schema includes the resource's API version, kind, and name\n        observer_schema_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the observer schema entry to the default observer schema list\n        default_observer_schema.append(observer_schema_entry)\n    \n    # Set the default observer schema in the application status\n    app.status.mangled_observer_schema = default_observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Initialize the default observer schema\n    default_observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Create a default observer schema entry for each resource\n        # The default schema includes the resource's API version, kind, and name\n        observer_schema_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the observer schema entry to the default observer schema list\n        default_observer_schema.append(observer_schema_entry)\n    \n    # Set the default observer schema in the application status\n    app.status.mangled_observer_schema = default_observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Initialize the default observer schema\n    default_observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Create a default observer schema entry for each resource\n        # The default schema includes the resource's API version, kind, and name\n        observer_schema_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the observer schema entry to the default observer schema list\n        default_observer_schema.append(observer_schema_entry)\n    \n    # Set the default observer schema in the application status\n    app.status.mangled_observer_schema = default_observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Create a copy of the manifest to avoid modifying the original\n    manifest = app.spec.manifest.copy() if app.spec.manifest else []\n    \n    # Generate the default observer schema for each resource in the manifest\n    default_observer_schema = []\n    \n    for resource in manifest:\n        # Extract the resource metadata\n        resource_metadata = {\n            \"apiVersion\": resource.get(\"apiVersion\"),\n            \"kind\": resource.get(\"kind\"),\n            \"metadata\": {\n                \"name\": resource.get(\"metadata\", {}).get(\"name\"),\n                \"namespace\": resource.get(\"metadata\", {}).get(\"namespace\")\n            }\n        }\n        \n        # Add the resource to the default observer schema\n        default_observer_schema.append(resource_metadata)\n    \n    # Set the default observer schema on the application\n    app.status.mangled_observer_schema = default_observer_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 28364, "num_retrieved": 74, "inference_time": 13.745969280600548, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 28438, "num_retrieved": 74, "inference_time": 13.745969280600548, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 41738, "num_retrieved": 112, "inference_time": 13.745969280600548, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.validate(path)\n            return True\n        except Exception:\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            objdir = path\n            if not self.root_fs.exists(objdir):\n                return False\n            validator.obj_fs = open_fs(objdir, create=False)\n            if validator.validate(objdir):\n                return True\n            else:\n                return False\n        except Exception as e:\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            objdir = path\n            if not self.obj_fs.exists(objdir):\n                raise fs.errors.ResourceNotFound(\"Object directory not found\")\n            validator.obj_fs = self.obj_fs\n            if validator.validate(objdir):\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(\"Validation failed for path %s: %s\", path, str(e))\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.validate(path)\n            return True\n        except Exception:\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            objdir = path\n            if not self.root_fs.exists(objdir):\n                return False\n            validator.obj_fs = open_fs(objdir, create=False)\n            if validator.validate(objdir):\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(\"Validation failed for path %s: %s\", path, str(e))\n            return False"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 50103, "num_retrieved": 122, "inference_time": 13.745969280600548, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()"]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 43124, "num_retrieved": 108, "inference_time": 13.745969280600548, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 49205, "num_retrieved": 120, "inference_time": 13.745969280600548, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 41818, "num_retrieved": 110, "inference_time": 13.745969280600548, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='standalone', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the inventory type value\n            type_value = inventory.get('type')\n            if type_value:\n                # Extract version from type value (e.g., \"https://ocfl.io/1.0/spec/#inventory\")\n                match = re.search(r'/(\\d+\\.\\d+)', type_value)\n                if match:\n                    spec_version = match.group(1)\n                    inv_validator.spec_version = spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the inventory type value\n            if 'type' in inventory:\n                type_value = inventory['type']\n                # Extract version from type value (e.g., \"https://ocfl.io/1.0/spec/#inventory\")\n                m = re.match(r'''https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory''', type_value)\n                if m:\n                    inv_validator.spec_version = m.group(1)\n                else:\n                    # If no valid version found, use self.spec_version\n                    pass\n            else:\n                # No type value, use self.spec_version\n                pass\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='standalone', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 40674, "num_retrieved": 99, "inference_time": 13.745969280600548, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 41982, "num_retrieved": 113, "inference_time": 13.745969280600548, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.warning('W010', where='root', digest_algorithm=prior.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.error('E104', where='root', prior_digest_algorithm=prior.digest_algorithm,\n                       current_digest_algorithm=self.digest_algorithm)\n    # Check that all files in prior manifest are present in current manifest\n    if 'manifest' in prior.inventory and 'manifest' in self.inventory:\n        for digest in prior.inventory['manifest']:\n            if digest not in self.inventory['manifest']:\n                self.log.error('E023b', where='root', missing_filepaths=digest)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the manifest in prior does not contain any files not present in current inventory\n    # This is a simplified check - in practice, we would need to compare the manifest entries\n    # and ensure that all files in prior are also in the current inventory\n    # For now, we just ensure the spec version constraint is met", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.error('E064', root_inv_file='inventory.json', inv_file='inventory.json')", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.error('E064', root_inv_file='inventory.json', inv_file='inventory.json')"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 42172, "num_retrieved": 118, "inference_time": 13.745969280600548, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical files in the state\n        # We need to map each logical file to the actual content files on disk\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # The content files are stored in the manifest under the same digest\n            # We need to find the actual file paths in the manifest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical files in the state\n        # We need to map each logical file to the actual content files on disk\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add the content file path to the set for this logical path\n            # The actual content file path is not directly available in the state\n            # We need to find it in the manifest for this digest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical files in the state\n        # We need to map each logical file to the actual content files on disk\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # The content files are stored in the manifest under the same digest\n            # We need to find the actual file paths in the manifest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical file paths\n        # We need to map each logical path to the actual content files on disk\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            # Add the content files from the manifest for this digest\n            # The content files are stored in the manifest under the digest key\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_path].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical files in the state\n        # We need to map each logical file to the actual content files on disk\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # The content files are stored in the manifest under the same digest\n            # We need to find the actual source files from the manifest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 41181, "num_retrieved": 104, "inference_time": 13.745969280600548, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that all files in fixity are present in manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate structure of fixity block\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that all files in fixity are present in manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that all files in fixity are present in manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all files in fixity are present in manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 120397, "num_retrieved": 500, "inference_time": 13.745969280600548, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isfile(item_path):\n                files.append(item_path)\n        return files\n    return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isfile(item_path):\n                files.append(item_path)\n        return files\n    return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for file in os.listdir(path):\n            file_path = os.path.join(path, file)\n            if os.path.isfile(file_path):\n                files.append(file_path)\n        return files\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isfile(item_path):\n                files.append(item_path)\n        return files\n    return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isfile(item_path):\n                files.append(item_path)\n        return files\n    return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 142482, "num_retrieved": 500, "inference_time": 13.745969280600548, "generate_results": ["def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n    \n    for file in files:\n        file_basename = os.path.basename(file)\n        for xml_basename in grouped_files:\n            if file_basename.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n    \n    for file in files:\n        file_basename = os.path.basename(file)\n        for xml_basename in grouped_files:\n            if file_basename.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n        \n        for file in files:\n            if file.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n        \n        for file in files:\n            if file.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n    \n    for file in files:\n        file_basename = os.path.basename(file)\n        for xml_basename in grouped_files:\n            if file_basename.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 132790, "num_retrieved": 500, "inference_time": 13.745969280600548, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns True for files belonging to a package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns True for files belonging to a package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns True for files belonging to a package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns True for files belonging to a package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 129373, "num_retrieved": 500, "inference_time": 13.745969280600548, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 137836, "num_retrieved": 500, "inference_time": 23.149012058973312, "generate_results": ["def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get the file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract the basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # If this XML file is not already in the data, initialize it\n        if basename not in package_data:\n            package_data[basename] = []\n            \n        # Add the file path to the list for this XML basename\n        package_data[basename].append(file_path)\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # If this XML file is not in the package data yet, initialize it\n        if basename not in package_data:\n            package_data[basename] = {}\n            \n        # Add the file path to the package data\n        package_data[basename]['xml'] = file_path\n        \n        # Look for associated files (renditions and assets) with the same basename\n        # Renditions and assets typically have the same basename as the XML file\n        # with different extensions (e.g., .pdf, .jpg)\n        \n        # Check for renditions (e.g., .pdf)\n        rendition_files = [f for f in files if f.startswith(basename) and f.endswith('.pdf')]\n        for rendition_file in rendition_files:\n            rendition_path = os.path.join(folder, rendition_file)\n            package_data[basename]['renditions'] = package_data[basename].get('renditions', []) + [rendition_path]\n            \n        # Check for assets (e.g., .jpg, .png)\n        asset_files = [f for f in files if f.startswith(basename) and f.endswith(('.jpg', '.jpeg', '.png', '.tiff', '.gif'))]\n        for asset_file in asset_files:\n            asset_path = os.path.join(folder, asset_file)\n            package_data[basename]['assets'] = package_data[basename].get('assets', []) + [asset_path]\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # If this XML file is not in our data yet, initialize it\n        if basename not in package_data:\n            package_data[basename] = {}\n            \n        # Add the file path to the package data\n        package_data[basename]['xml'] = file_path\n        \n        # Look for associated files (renditions and assets) with the same basename\n        # Renditions and assets typically have the same basename as the XML file\n        # with different extensions (e.g., .pdf, .jpg)\n        \n        # Check for renditions (e.g., .pdf)\n        rendition_files = [f for f in files if f.startswith(basename) and f.endswith('.pdf')]\n        for rendition_file in rendition_files:\n            rendition_path = os.path.join(folder, rendition_file)\n            if rendition_file not in package_data[basename]:\n                package_data[basename][rendition_file] = rendition_path\n                \n        # Check for assets (e.g., .jpg, .png)\n        asset_files = [f for f in files if f.startswith(basename) and f.endswith(('.jpg', '.jpeg', '.png', '.tiff', '.gif'))]\n        for asset_file in asset_files:\n            asset_path = os.path.join(folder, asset_file)\n            if asset_file not in package_data[basename]:\n                package_data[basename][asset_file] = asset_path\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename without extension\n        basename = os.path.splitext(filename)[0]\n        \n        # Initialize the package data if not exists\n        if basename not in package_data:\n            package_data[basename] = []\n            \n        # Add the file path to the package data\n        package_data[basename].append(file_path)\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # Initialize the package data if not exists\n        if basename not in package_data:\n            package_data[basename] = []\n            \n        # Add the file path to the package data\n        package_data[basename].append(file_path)\n    \n    return package_data"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 136943, "num_retrieved": 500, "inference_time": 23.149012058973312, "generate_results": ["def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if it's a rendition by comparing with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in ['pt', 'en', 'es', 'fr', 'de']:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if it's a rendition by comparing with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in get_pdf_langs():  # Assuming get_pdf_langs() is defined elsewhere\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if it's a rendition by comparing with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in ['en', 'pt', 'es', 'fr', 'de', 'it', 'ja', 'ko', 'zh']:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if the file is a rendition by comparing the filename with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in ['pt', 'en', 'es']:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if the file is a rendition by comparing with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in ['en', 'pt', 'es', 'fr', 'de', 'it', 'ja', 'ko', 'zh']:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 56190, "num_retrieved": 181, "inference_time": 23.149012058973312, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == 'en':\n        rendition_dict['en'] = file_path\n    else:\n        rendition_dict['original'] = file_path\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == 'en':\n        rendition_dict['en'] = file_path\n    else:\n        rendition_dict['original'] = file_path\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == 'en':\n        rendition_dict['en'] = file_path\n    else:\n        rendition_dict['original'] = file_path\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == \"original\":\n        rendition_dict[\"original\"] = file_path\n    else:\n        rendition_dict[lang] = file_path\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == 'en':\n        rendition_dict['en'] = file_path\n    else:\n        rendition_dict['original'] = file_path\n    return rendition_dict"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 54083, "num_retrieved": 224, "inference_time": 23.149012058973312, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    # Create a dictionary to store the asset mapping\n    asset_map = {}\n    \n    # Extract the base name without extension\n    base_name = basename\n    \n    # Get the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create a key for the asset with the base name and extension\n    asset_key = f\"{base_name}{ext}\"\n    \n    # Add the asset to the map\n    asset_map[asset_key] = file_path\n    \n    return asset_map", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    self[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    self[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    # Create a dictionary to store the asset mapping\n    asset_map = {}\n    \n    # Extract the base name without extension\n    base_name = basename\n    \n    # Get the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create a key for the asset with the base name and extension\n    asset_key = f\"{base_name}.{ext.lower()}\"\n    \n    # Add the asset to the map\n    asset_map[asset_key] = file_path\n    \n    return asset_map", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    self[basename] = file_path"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 134426, "num_retrieved": 500, "inference_time": 23.149012058973312, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    package_data = {}\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        \n        for file_name in file_list:\n            # Extract the basename (without extension) of the XML file\n            file_base_name, file_ext = os.path.splitext(file_name)\n            \n            # Only process XML files\n            if file_ext.lower() == '.xml':\n                # Use the basename as the key in the dictionary\n                basename = file_base_name\n                \n                # Initialize the list if it doesn't exist\n                if basename not in package_data:\n                    package_data[basename] = []\n                \n                # Add the file path to the list\n                package_data[basename].append(file_name)\n    \n    return package_data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        for file_name in file_list:\n            if file_name.endswith(\".xml\"):\n                basename, ext = os.path.splitext(file_name)\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(file_name)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    package_data = {}\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        \n        for file_name in file_list:\n            # Extract the basename of the XML file (without extension)\n            file_base_name, file_ext = os.path.splitext(file_name)\n            \n            # Only process XML files\n            if file_ext.lower() == '.xml':\n                # Use the basename as the key in the dictionary\n                xml_basename = file_base_name\n                \n                # Initialize the list if it doesn't exist\n                if xml_basename not in package_data:\n                    package_data[xml_basename] = []\n                \n                # Add the file path to the list\n                package_data[xml_basename].append(file_name)\n    \n    return package_data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    package_data = {}\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        \n        for file_name in file_list:\n            # Extract the basename of the XML file (without extension)\n            file_base_name, file_ext = os.path.splitext(file_name)\n            \n            # Only process XML files\n            if file_ext.lower() == '.xml':\n                # Use the basename as the key in the dictionary\n                if file_base_name not in package_data:\n                    package_data[file_base_name] = []\n                \n                # Add the file path to the list for this basename\n                package_data[file_base_name].append(file_name)\n    \n    return package_data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    package_data = {}\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        \n        for file_name in file_list:\n            # Extract the basename of the XML file (without extension)\n            file_base_name, file_ext = os.path.splitext(file_name)\n            \n            # Only process XML files\n            if file_ext.lower() == '.xml':\n                # Use the basename as the key in the dictionary\n                package_data[file_base_name] = {\n                    'filename': file_name,\n                    'content': zip_file.read(file_name)\n                }\n    \n    return package_data"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 116756, "num_retrieved": 500, "inference_time": 23.149012058973312, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zf:\n        return zf.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path) as zf:\n        return [item for item in zf.namelist() if not item.endswith(\".xml\")]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zip_file:\n        return [file for file in zip_file.namelist() if not file.endswith('.xml')]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zf:\n        return zf.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zf:\n        return zf.namelist()"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 134895, "num_retrieved": 484, "inference_time": 23.149012058973312, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    replacement = r'w-st=\"\\1\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    return re.sub(pattern, r'w-st=\"\\1\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    return re.sub(pattern, r'w-st=\"\\1\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    replacement = r'w-st=\"'\\1\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    replacement = r'w-st=\"'\\1\"'\n    return re.sub(pattern, replacement, content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 140827, "num_retrieved": 500, "inference_time": 23.149012058973312, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 130764, "num_retrieved": 461, "inference_time": 23.149012058973312, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Try to parse the issue string and extract number and supplement\n    # The issue string might contain a number and a supplement (e.g., \"2(3)\" or \"2-3\")\n    issue_str = issue.strip()\n    \n    # Extract the number part (first number in the string)\n    number_match = re.search(r'^(\\d+)', issue_str)\n    number = 0 if number_match is None else int(number_match.group(1))\n    \n    # Extract the supplement part (everything after the number, if any)\n    supplement_match = re.search(r'(\\d+\\([^)]*\\)|\\d+[-\\s]\\d+)', issue_str)\n    supplement = \"\"\n    if supplement_match:\n        supplement = supplement_match.group(1)\n    \n    # If no supplement is found, try to extract from the rest of the string\n    if not supplement:\n        # Remove the number part from the original string\n        remaining = issue_str[len(str(number)):].strip()\n        # Extract any digits that might be a supplement\n        supplement_match = re.search(r'\\d+', remaining)\n        if supplement_match:\n            supplement = supplement_match.group()\n    \n    # If the supplement is just a number, we need to check if it's a valid supplement\n    # A valid supplement is a number in parentheses or a number after a dash\n    if supplement and supplement.isdigit():\n        # If it's just a number, we need to check if it's a valid supplement\n        # A valid supplement is a number in parentheses or a number after a dash\n        if re.search(r'\\([^)]*\\)', issue_str) or re.search(r'-\\d+$', issue_str):\n            supplement = supplement\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and filter out empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract the number (first numeric part)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if part is a number (including numbers with 's' or 'spe' prefixes)\n        if part.endswith('s') or part.endswith('spe'):\n            # Remove 's' or 'spe' suffix\n            clean_part = part[:-1] if part.endswith('s') else part[:-3] if part.endswith('spe') else part\n            if clean_part.isdigit():\n                number = int(clean_part)\n                break\n        elif part.isdigit():\n            number = int(part)\n            break\n    \n    # If no number found, try to extract supplement from the end\n    if number is None:\n        # Look for supplement at the end of the string\n        for part in reversed(parts):\n            if part.endswith('s') or part.endswith('spe'):\n                supplement = part\n                break\n    \n    # If number is still None, default to 0\n    if number is None:\n        number = 0\n    \n    # Return the extracted values\n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and filter out empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract the number (first numeric part)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if the part is a number (including numbers with 's' or 'spe' prefixes)\n        if part.endswith('s') or part.endswith('spe'):\n            # Remove 's' or 'spe' suffix\n            clean_part = part[:-1] if part.endswith('s') else part[:-3] if part.endswith('spe') else part\n            if clean_part.isdigit():\n                number = int(clean_part)\n                break\n        elif part.isdigit():\n            number = int(part)\n            break\n    \n    # If no number found, try to extract supplement from the end\n    if number is None:\n        # Look for supplement (e.g., 's1', 's2', 's3')\n        for part in parts:\n            if part.startswith('s') and part[1:].isdigit():\n                supplement = part\n                break\n    \n    # If no number found, default to 0\n    if number is None:\n        number = 0\n    \n    # If supplement is not found, default to empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Try to parse the issue string and extract number and supplement\n    # The issue string might contain a number and a supplement (e.g., \"2(3)\" or \"2-3\")\n    issue_str = issue.strip()\n    \n    # Extract the number part (digits)\n    number_match = re.search(r'(\\d+)', issue_str)\n    number = 0 if number_match is None else int(number_match.group(1))\n    \n    # Extract the supplement part (everything after the number, if any)\n    supplement_match = re.search(r'\\((\\d+)\\)|-(\\d+)|(\\d+)$', issue_str)\n    supplement = \"\"\n    \n    if supplement_match:\n        # Check for (number) format\n        if supplement_match.group(1):\n            supplement = supplement_match.group(1)\n        # Check for -number format\n        elif supplement_match.group(2):\n            supplement = supplement_match.group(2)\n        # Check for number at the end\n        elif supplement_match.group(3):\n            supplement = supplement_match.group(3)\n    \n    # If no supplement found, try to extract from the remaining part\n    if not supplement:\n        # Remove the number part from the original string\n        remaining = re.sub(r'\\d+', '', issue_str)\n        # Extract the supplement from the remaining part\n        if remaining and remaining != issue_str:\n            supplement = remaining.strip()\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and remove any empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract the number (first part that looks like a number)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if the part is a number (digits only)\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # If no number was found, try to extract the supplement (everything after a number)\n    if number is None:\n        # Look for a supplement (e.g., \"s1\", \"s2\", etc.)\n        for part in parts:\n            if part.startswith('s'):\n                supplement = part[1:]\n                break\n    \n    # If no number was found, default to 0\n    if number is None:\n        number = 0\n    \n    # If no supplement was found, default to empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 23.149012058973312, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 23.149012058973312, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            simplified = expr.simplify()\n            result.append(simplified)\n        else:\n            result.append(expr)\n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # This is a simplified implementation - actual logic would depend on expression structure\n        # In practice, this would require parsing the expression tree\n        \n        # For now, we return the original expression as a placeholder\n        # A full implementation would need to traverse the expression tree\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        # Absorption: A & (A | B) = A, A | (A & B) = A\n        # Negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(expr, Expression):\n            # Check for absorption cases\n            # Case 1: A & (A | B) = A\n            # Case 2: A | (A & B) = A\n            # Case 3: A & (~A | B) = A & B\n            # Case 4: A | (~A & B) = A | B\n            \n            # We need to evaluate the structure of the expression\n            # This is a simplified implementation assuming we can detect patterns\n            # In a full implementation, we would parse the expression tree\n            \n            # For now, return the expression as is (placeholder)\n            result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we just return the original expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            # This is a simplified implementation - actual logic would depend on expression structure\n            # For now, we return the expression as is since full parsing and simplification is complex\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 165499, "num_retrieved": 500, "inference_time": 23.149012058973312, "generate_results": ["def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        if not hasattr(self, '_handlers'):\n            self._handlers = {}\n        if hook not in self._handlers:\n            self._handlers[hook] = []\n        self._handlers[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        if not hasattr(self, \"_handlers\"):\n            self._handlers = {}\n        if hook not in self._handlers:\n            self._handlers[hook] = []\n        self._handlers[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        if not hasattr(self, '_handlers'):\n            self._handlers = {}\n        if not hasattr(self._handlers, hook):\n            self._handlers[hook] = []\n        self._handlers[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        if not hasattr(self, '_handlers'):\n            self._handlers = {}\n        if not hasattr(self._handlers, hook):\n            self._handlers[hook] = []\n        self._handlers[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        if not hasattr(self, '_handlers'):\n            self._handlers = {}\n        if not hasattr(self._handlers, hook):\n            self._handlers[hook] = []\n        self._handlers[hook].append(handler)\n        return handler\n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 181627, "num_retrieved": 500, "inference_time": 23.149012058973312, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": \"static\",\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": \"static\",\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": \"static\",\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": \"static\",\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": \"static\",\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 53581, "num_retrieved": 195, "inference_time": 23.149012058973312, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 53378, "num_retrieved": 203, "inference_time": 23.149012058973312, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the relative attributes and convert fractional values\n    # to integer equivalents\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if attr == 'days':\n                    days = int(value)\n                    # Adjust the days value to account for fractional days\n                    if value - days != 0:\n                        # Add the fractional part as hours\n                        fractional_days = value - days\n                        hours = int(fractional_days * 24)\n                        # Add the hours to the result\n                        result.hours += hours\n                elif attr == 'hours':\n                    # Convert fractional hours to minutes and seconds\n                    hours = int(value)\n                    fractional_hours = value - hours\n                    minutes = int(fractional_hours * 60)\n                    result.minutes += minutes\n                elif attr == 'minutes':\n                    # Convert fractional minutes to seconds\n                    minutes = int(value)\n                    fractional_minutes = value - minutes\n                    seconds = int(fractional_minutes * 60)\n                    result.seconds += seconds\n                elif attr == 'seconds':\n                    # Convert fractional seconds to microseconds\n                    seconds = int(value)\n                    fractional_seconds = value - seconds\n                    microseconds = int(fractional_seconds * 1000000)\n                    result.microseconds += microseconds\n            else:\n                # For integer values, just assign directly\n                setattr(result, attr, value)\n    \n    # Ensure all values are integers\n    result.days = int(result.days)\n    result.hours = int(result.hours)\n    result.minutes = int(result.minutes)\n    result.seconds = int(result.seconds)\n    result.microseconds = int(result.microseconds)\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the relative attributes and convert fractional values\n    # to integers by rounding up or down appropriately\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Round to nearest integer, but handle the case where fractional\n                # part is exactly 0.5 by rounding up\n                if value == int(value):\n                    # Already an integer\n                    setattr(result, attr, int(value))\n                else:\n                    # Round to nearest integer\n                    setattr(result, attr, int(round(value)))\n            else:\n                # Already an integer\n                setattr(result, attr, value)\n    \n    # Handle the case where we have fractional days that need to be converted\n    # to hours, minutes, and seconds\n    if hasattr(self, 'days') and self.days is not None:\n        days = self.days\n        if isinstance(days, float):\n            days_int = int(round(days))\n            hours = int(round((days - days_int) * 24))\n            # Adjust the days value to account for fractional days\n            if hours > 0:\n                result.days = days_int\n                result.hours = hours\n            else:\n                result.days = days_int\n        else:\n            result.days = days\n    \n    # Ensure all values are integers\n    for attr in self.__slots__:\n        value = getattr(result, attr)\n        if value is not None and isinstance(value, float):\n            setattr(result, attr, int(round(value)))\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the relative attributes and convert fractional values\n    # to integers by rounding up or down appropriately\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Round to nearest integer\n                rounded_value = round(value)\n                # If the value is exactly halfway, round to the nearest even integer\n                if abs(value - rounded_value) == 0.5:\n                    rounded_value = int(round(value))\n                # Ensure we don't lose precision\n                if rounded_value != value:\n                    # Adjust the value to maintain the same relative offset\n                    # This is a simplified approach - in practice, this would need\n                    # to consider the full context of the delta\n                    pass\n            else:\n                rounded_value = value\n                \n            # Set the attribute with the rounded value\n            setattr(result, attr, rounded_value)\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if attr == 'days':\n                    days = int(value)\n                    # Adjust days if there's a fractional part\n                    if value - days != 0:\n                        # Add the fractional part as hours\n                        hours = int((value - days) * 24)\n                        # Add the hours to the result\n                        result.hours += hours\n                elif attr == 'hours':\n                    # Convert fractional hours to minutes and seconds\n                    hours = int(value)\n                    minutes = int((value - hours) * 60)\n                    result.minutes += minutes\n                elif attr == 'minutes':\n                    # Convert fractional minutes to seconds\n                    minutes = int(value)\n                    seconds = int((value - minutes) * 60)\n                    result.seconds += seconds\n                else:\n                    # For other attributes, just convert to integer\n                    setattr(result, attr, int(value))\n            else:\n                # For non-float values, just set the attribute\n                setattr(result, attr, value)\n    \n    # Handle the case where we have fractional days\n    if hasattr(self, 'days') and self.days is not None:\n        days = int(self.days)\n        if self.days - days != 0:\n            # Convert the fractional part to hours\n            fractional_days = self.days - days\n            hours = int(fractional_days * 24)\n            result.hours += hours\n    \n    # Ensure all values are integers\n    for attr in self.__slots__:\n        value = getattr(result, attr)\n        if value is not None and isinstance(value, float):\n            setattr(result, attr, int(value))\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the relative attributes and convert fractional values\n    # to integer equivalents\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if attr == 'days':\n                    days = int(value)\n                    # Adjust the days value to account for fractional days\n                    if value - days != 0:\n                        # Add the fractional part as hours\n                        hours = int((value - days) * 24)\n                        # Add the hours to the result\n                        if hours != 0:\n                            result.hours += hours\n                elif attr == 'hours':\n                    # Convert fractional hours to minutes and seconds\n                    hours = int(value)\n                    minutes = int((value - hours) * 60)\n                    if minutes != 0:\n                        result.minutes += minutes\n                elif attr == 'minutes':\n                    # Convert fractional minutes to seconds\n                    minutes = int(value)\n                    seconds = int((value - minutes) * 60)\n                    if seconds != 0:\n                        result.seconds += seconds\n                elif attr == 'seconds':\n                    # Convert fractional seconds to microseconds\n                    seconds = int(value)\n                    microseconds = int((value - seconds) * 1000000)\n                    if microseconds != 0:\n                        result.microseconds += microseconds\n                else:\n                    # For other attributes, just convert to integer\n                    result.__dict__[attr] = int(value)\n            else:\n                # For non-fractional values, just convert to integer\n                result.__dict__[attr] = int(value)\n    \n    # Ensure all values are positive\n    for attr in self.__slots__:\n        value = getattr(result, attr)\n        if value is not None:\n            if value < 0:\n                # For negative values, we need to adjust the other attributes\n                #"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 51772, "num_retrieved": 185, "inference_time": 23.149012058973312, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 16408, "num_retrieved": 60, "inference_time": 6.5739807933568954, "generate_results": ["def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 13626, "num_retrieved": 43, "inference_time": 6.5739807933568954, "generate_results": ["def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 16253, "num_retrieved": 58, "inference_time": 6.5739807933568954, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 10252, "num_retrieved": 29, "inference_time": 6.5739807933568954, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        if result.returncode == 0:\n            if verbose:\n                print(\"Command succeeded\")\n            return result.stdout.strip() if result.stdout else None\n        else:\n            if not hide_stderr:\n                print(\"Command failed with return code:\", result.returncode)\n                if result.stderr:\n                    print(\"Error output:\", result.stderr)\n            return None\n            \n    except FileNotFoundError:\n        print(f\"Command not found: {' '.join(cmd)}\")\n        return None\n    except Exception as e:\n        print(f\"Error executing command: {str(e)}\")\n        return None", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Return result\n        if result.returncode == 0:\n            return {\n                'returncode': 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr\n            }\n        else:\n            return {\n                'returncode': result.returncode,\n                'stdout': result.stdout,\n                'stderr': result.stderr\n            }\n    except subprocess.SubprocessError as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose\n        if verbose:\n            if result.stdout:\n                print(\"STDOUT:\", result.stdout)\n            if result.stderr:\n                print(\"STDERR:\", result.stderr)\n        \n        # Hide stderr if requested\n        if hide_stderr:\n            if result.stderr:\n                # Only print stderr if it's not empty and we're not hiding it\n                pass\n        \n        return result\n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(\"Command failed with return code:\", e.returncode)\n            if e.stderr:\n                print(\"STDERR:\", e.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(\"Unexpected error:\", str(e))\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT if hide_stderr else subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose or if there's output\n        if verbose and result.stdout:\n            print(result.stdout)\n            \n        # Return the result\n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr if not hide_stderr else None\n        }\n        \n    except subprocess.SubprocessError as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        if result.returncode == 0:\n            if verbose:\n                print(\"Command succeeded\")\n            return result.stdout.strip() if result.stdout else None\n        else:\n            if not hide_stderr:\n                print(\"Command failed with return code:\", result.returncode)\n                if result.stderr:\n                    print(\"Error output:\", result.stderr)\n            else:\n                print(\"Command failed with return code:\", result.returncode)\n            return None\n            \n    except FileNotFoundError:\n        print(f\"Command not found: {' '.join(cmd)}\")\n        return None\n    except Exception as e:\n        print(f\"Error executing command: {str(e)}\")\n        return None"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 16221, "num_retrieved": 59, "inference_time": 6.5739807933568954, "generate_results": ["def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 12606, "num_retrieved": 37, "inference_time": 6.5739807933568954, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 46559, "num_retrieved": 104, "inference_time": 6.5739807933568954, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    entries = self.obj_fs.listdir('')\n    \n    for entry in entries:\n        # Skip directories that are not valid OCFL object roots\n        if entry == 'extensions':\n            # Validate extensions directory\n            self.validate_extensions_dir()\n            continue\n            \n        # Check if this is an OCFL object root\n        if re.match(r'''v\\d+$''', entry):\n            # This is a version directory, skip it\n            continue\n            \n        # Check if this is a valid OCFL object root\n        if entry in ('inventory.json', 'inventory.json.' + self.digest_algorithm):\n            # This is an inventory file, skip it\n            continue\n            \n        # Check if this is a valid OCFL object declaration\n        if entry.startswith('0='):\n            # This is a Namaste file, check if it's a valid object declaration\n            namaste = self.get_namaste(0, '')\n            if namaste is not None:\n                # Validate the object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, check_digests=check_digests)\n                    if validator.validate(self.obj_fs.desc('')):\n                        good_objects += 1\n                num_objects += 1\n                continue\n                \n        # Check if this is a valid OCFL object\n        if entry == 'inventory.json':\n            # This is an inventory file, validate it\n            if validate_objects:\n                validator = Validator(show_warnings=show_warnings, check_digests=check_digests)\n                if validator.validate(self.obj_fs.desc('')):\n                    good_objects += 1\n            num_objects += 1\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    entries = self.obj_fs.listdir('')\n    \n    for entry in entries:\n        # Check if entry is a directory\n        if self.obj_fs.isdir(entry):\n            # Determine if this is an OCFL object\n            path_type = find_path_type(entry)\n            \n            if path_type == 'root':\n                # Validate the OCFL storage root\n                validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests)\n                passed = validator.validate(entry)\n                if passed:\n                    good_objects += 1\n                num_objects += 1\n                \n            elif path_type == 'object':\n                # Validate the OCFL object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests)\n                    passed = validator.validate(entry)\n                    if passed:\n                        good_objects += 1\n                    num_objects += 1\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    for entry in self.obj_fs.scandir(''):\n        if entry.is_dir:\n            # Check if this is an OCFL object directory\n            if entry.name.startswith('v') or entry.name == 'extensions':\n                continue\n                \n            # Check if this is a valid OCFL object root\n            if entry.name == 'inventory.json':\n                # This is a valid object root\n                num_objects += 1\n                # Validate the object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, \n                                        show_errors=True, \n                                        check_digests=check_digests)\n                    try:\n                        validator.validate(entry.name)\n                        if validator.log.num_errors == 0:\n                            good_objects += 1\n                    except Exception as e:\n                        self.log.error('E002', error=str(e))\n            elif entry.name == '0=ocfl_object_1.1' or entry.name == '0=ocfl_object_1.0':\n                # This is a valid object declaration\n                num_objects += 1\n                # Validate the object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, \n                                        show_errors=True, \n                                        check_digests=check_digests)\n                    try:\n                        validator.validate(entry.name)\n                        if validator.log.num_errors == 0:\n                            good_objects += 1\n                    except Exception as e:\n                        self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    for entry in self.obj_fs.scandir(''):\n        if entry.is_dir:\n            # Check if this is an OCFL object directory\n            if entry.name.startswith('v') or entry.name == 'extensions':\n                continue\n                \n            # Check if this is a valid OCFL object root\n            if entry.name == 'inventory.json':\n                # This is a valid object root\n                num_objects += 1\n                # Validate the object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, \n                                         show_errors=True, \n                                         check_digests=check_digests)\n                    try:\n                        validator.validate(entry.name)\n                        if validator.log.num_errors == 0:\n                            good_objects += 1\n                    except Exception as e:\n                        self.log.error('E002', error=str(e))\n            elif entry.name == '0=ocfl_object_1.1' or entry.name == '0=ocfl_object_1.0':\n                # This is a valid object declaration\n                num_objects += 1\n                # Validate the object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, \n                                         show_errors=True, \n                                         check_digests=check_digests)\n                    try:\n                        validator.validate(entry.name)\n                        if validator.log.num_errors == 0:\n                            good_objects += 1\n                    except Exception as e:\n                        self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    entries = self.obj_fs.listdir('')\n    \n    for entry in entries:\n        # Skip directories that are not valid OCFL object directories\n        if entry == 'extensions':\n            # Validate extensions directory as per OCFL spec\n            self.validate_extensions_dir()\n            continue\n            \n        # Check if this is an OCFL object directory\n        if re.match(r'''v\\d+$''', entry):\n            # This is a version directory, skip validation\n            continue\n            \n        # Check if this is a valid OCFL object root\n        if entry in ('0=ocfl_object_1.1', '0=ocfl_object_1.0'):\n            # This is an object declaration file, validate the object\n            if validate_objects:\n                # Create a validator instance for this object\n                validator = Validator(show_warnings=show_warnings, \n                                    show_errors=True, \n                                    check_digests=check_digests)\n                try:\n                    # Validate the object at the root\n                    validator.validate(entry)\n                    if validator.log.num_errors == 0:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n                num_objects += 1\n        elif entry == 'inventory.json':\n            # This is an inventory file, validate it as part of the hierarchy\n            pass\n        elif entry.startswith('0='):\n            # This is a Namaste file, validate it as part of the hierarchy\n            pass\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 45289, "num_retrieved": 92, "inference_time": 6.5739807933568954, "generate_results": ["def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.content_directory is None:\n        self.content_directory = \"content\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Ensure the object filesystem is properly initialized\n    if self.obj_fs is None:\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Set up the initial inventory structure\n    inventory = self.start_inventory()\n    \n    # Write the initial inventory and sidecar files\n    self.write_inventory_and_sidecar(inventory)\n    \n    # Write the object declaration\n    self.write_object_declaration()\n    \n    self.log.info(\"Initialized OCFL storage root with ID %s\", self.id)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.content_directory is None:\n        self.content_directory = \"content\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Set up the filesystem if not already done\n    if self.obj_fs is None:\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the root inventory structure\n    inventory = self.start_inventory()\n    \n    # Create the root directory structure\n    self.obj_fs.makedir('v1')\n    self.obj_fs.makedir('extensions')\n    \n    # Write the initial inventory and sidecar\n    self.write_inventory_and_sidecar(inventory, vdir='v1')\n    \n    # Write the object declaration\n    self.write_object_declaration()\n    \n    # Set the initial state\n    self.log.info(\"Initialized OCFL storage root with ID %s\", self.id)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.content_directory is None:\n        self.content_directory = \"content\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Ensure the object filesystem is properly initialized\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the inventory with default structure\n    self.inventory = self.start_inventory()\n    \n    # Set up the object declaration\n    self.object_declaration = self.object_declaration_object()\n    \n    # Initialize the root inventory with default values\n    self.write_inventory_and_sidecar(self.inventory)\n    \n    # Initialize the object declaration file\n    self.write_object_declaration()\n    \n    # Set up the initial state for the object\n    self.state = {}\n    \n    # Log initialization completion\n    self.log.info(\"OCFL storage root initialized with id=%s, spec_version=%s\", self.id, self.spec_version)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the storage root by creating the necessary directory structure\n    # and setting up the initial state for the OCFL object\n    pass", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.content_directory is None:\n        self.content_directory = \"content\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Ensure the object filesystem is properly initialized\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the root inventory with default values\n    inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            inventory['fixity'][fixity_type] = {}\n    \n    # Write the initial inventory to the root\n    self.write_inventory_and_sidecar(inventory)\n    \n    # Write the object declaration\n    self.write_object_declaration()\n    \n    self.log.info(\"Initialized OCFL storage root with ID %s\", self.id)"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 49588, "num_retrieved": 114, "inference_time": 6.5739807933568954, "generate_results": ["def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d+)$''', version)\n    if not m:\n        raise ValueError(\"Invalid version format: %s\" % version)\n    \n    # Get the version number\n    v = int(m.group(1))\n    \n    # Return the next version number\n    return 'v' + str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d+)$''', version)\n    if not m:\n        raise ValueError(\"Invalid version format: %s\" % version)\n    \n    # Get the version number\n    v = int(m.group(1))\n    \n    # Return the next version number with the same prefix\n    return 'v' + str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d+)$''', version)\n    if not m:\n        raise ValueError(\"Invalid version format: %s\" % version)\n    \n    # Get the version number\n    v = int(m.group(1))\n    \n    # Return the next version number\n    return 'v' + str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d+)$''', version)\n    if not m:\n        raise ValueError(\"Invalid version format: %s\" % version)\n    \n    # Get the version number\n    v = int(m.group(1))\n    \n    # Return the next version number\n    return 'v' + str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d{1,5})$''', version)\n    if not m:\n        raise ValueError(\"Bad version directory name: %s\" % (version))\n    \n    v = int(m.group(1))\n    if v == 0:\n        raise ValueError(\"Bad version directory name: %s, v0 not allowed\" % (version))\n    \n    # Return the next version number\n    return 'v' + str(v + 1)"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 44686, "num_retrieved": 124, "inference_time": 6.5739807933568954, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root inventory.\"\"\"\n    # Initialize a set to track digests that are different from root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if the version directory exists\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E074\", version=vdir)\n            return\n            \n        # Read the inventory for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E075\", version=vdir)\n            return\n            \n        # Parse the inventory\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E076\", version=vdir, error=str(e))\n            return\n            \n        # Validate the inventory structure\n        if 'id' not in version_inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        if 'type' not in version_inventory:\n            self.error(\"E036b\", version=vdir)\n            return\n            \n        if 'digestAlgorithm' not in version_inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # Check if this version has a manifest\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            return\n            \n        # Extract the manifest digests from this version\n        manifest_digests = set(version_inventory['manifest'].keys())\n        \n        # Compare with root inventory manifest to find different digests\n        # (Note: This assumes root inventory is already parsed and available)\n        if hasattr(self, 'root_manifest_digests'):\n            root_manifest_digests = self.root_manifest_digests\n            different_digests.update(manifest_digests - root_manifest_digests)\n            \n    # Return the set of different digests\n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root inventory.\"\"\"\n    # Validate that each version directory exists and has an inventory\n    for vdir in version_dirs:\n        # Check if version directory exists\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E074\", version=vdir)\n            return\n        \n        # Check if inventory file exists in version directory\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E075\", version=vdir)\n            return\n        \n        # Parse the inventory to validate its structure\n        try:\n            inventory = self.parse_inventory(inventory_path)\n            # Validate the inventory structure\n            self.validate(inventory, extract_spec_version=False)\n        except Exception as e:\n            self.error(\"E076\", version=vdir, error=str(e))\n            return\n    \n    # Keep track of content digests different from root inventory\n    # This would typically involve comparing the manifest digests across versions\n    # and recording any differences\n    root_inventory = self.parse_inventory()\n    root_manifest = root_inventory.get('manifest', {})\n    \n    # For each version, compare its manifest to the root manifest\n    for vdir in version_dirs:\n        version_inventory = self.parse_inventory(fs.path.join(vdir, 'inventory.json'))\n        version_manifest = version_inventory.get('manifest', {})\n        \n        # Compare digests between root and version\n        for digest in version_manifest:\n            if digest not in root_manifest:\n                # This digest is new in this version\n                self.different_digests.add(digest)\n    \n    return self.different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep record of content digests different from root inventory.\"\"\"\n    # Initialize a set to track digests that are different from root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        inventory_path = os.path.join(vdir, 'inventory.json')\n        if not os.path.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            return\n            \n        # Parse the inventory for this version\n        try:\n            with open(inventory_path, 'r') as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            return\n            \n        # Check if the inventory has a valid structure\n        if 'id' not in version_inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        # Check if the inventory has a valid digestAlgorithm\n        if 'digestAlgorithm' not in version_inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # Check if the inventory has a valid head version\n        if 'head' not in version_inventory:\n            self.error(\"E036d\", version=vdir)\n            return\n            \n        # Check if the head version is valid (should be the current version)\n        if version_inventory['head'] != vdir:\n            self.warning(\"W002\", version=vdir, expected=vdir, got=version_inventory['head'])\n            \n        # Check if the inventory has a valid manifest\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            return\n            \n        # Check if the manifest has valid digests\n        manifest_digests = set()\n        for digest in version_inventory['manifest']:\n            if not isinstance(version_inventory['manifest'][digest], list):\n                self.error(\"E092\", version=vdir, digest=digest)\n                return\n            manifest_digests.add(digest)\n            \n        # Check if any of these digests are different from root inventory\n        # (This would require access to root inventory, which is not provided in this function)\n        # For now, we just validate the structure and format", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep record of content digests different from root inventory.\"\"\"\n    # Initialize a set to track digests that are different from the root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if the version directory exists and has an inventory\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E074\", version=vdir)\n            continue\n            \n        # Read the inventory for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E075\", version=vdir)\n            continue\n            \n        # Parse the inventory\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E076\", version=vdir, error=str(e))\n            continue\n            \n        # Check if the inventory has a manifest block\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            continue\n            \n        # Extract the manifest digests from this version\n        version_manifest_digests = set(version_inventory['manifest'].keys())\n        \n        # Compare with root inventory manifest (assuming root inventory is available)\n        # For now, we'll assume the root inventory is the first version\n        if hasattr(self, 'root_inventory') and self.root_inventory is not None:\n            root_manifest_digests = set(self.root_inventory['manifest'].keys())\n            # Find digests that are in this version but not in root\n            different_digests.update(version_manifest_digests - root_manifest_digests)\n            \n    # Return the set of digests that are different from the root inventory\n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root inventory.\"\"\"\n    # Initialize a set to track digests that are different from root inventory\n    different_digests = set()\n    \n    # Validate each version directory\n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        inventory_path = os.path.join(vdir, 'inventory.json')\n        if not os.path.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            return\n            \n        # Load the inventory for this version\n        with open(inventory_path, 'r') as fh:\n            version_inventory = json.load(fh)\n            \n        # Check if the inventory has a valid structure\n        if 'id' not in version_inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        # Check if the inventory has a valid digest algorithm\n        if 'digestAlgorithm' not in version_inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # Check if the digest algorithm is supported\n        if version_inventory['digestAlgorithm'] not in ['sha512', 'sha256', 'md5', 'sha1', 'blake2b-512']:\n            self.error(\"E039\", version=vdir, digest_algorithm=version_inventory['digestAlgorithm'])\n            return\n            \n        # Check if the inventory has a manifest\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            return\n            \n        # Extract the manifest digests from this version\n        version_manifest_digests = set(version_inventory['manifest'].keys())\n        \n        # Compare with root inventory manifest to find different digests\n        # Note: This assumes we have access to root inventory manifest\n        # In a real implementation, we would need to track the root inventory\n        # For now, we'll just record the digests from this version\n        different_digests.update(version_manifest_digests)\n        \n        # Validate the manifest structure\n        for digest in version_manifest_digests:\n            if not isinstance(version_inventory['manifest'][digest], list):\n                self.error(\"E092\", version=vdir, digest=digest)\n                return\n    \n    # Return the set of different digests\n    return different_digests"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 50776, "num_retrieved": 127, "inference_time": 6.5739807933568954, "generate_results": ["def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for filename in os.listdir(path):\n            if filename.startswith('0='):\n                namaste_files.append(filename)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if there's exactly one 0=ocfl_object_1.0 file (root)\n        # or if there are multiple 0= files (object)\n        namaste_patterns = []\n        for filename in namaste_files:\n            if filename.startswith('0=ocfl_object_'):\n                namaste_patterns.append(filename)\n        \n        if len(namaste_patterns) == 1:\n            # Check if it's the root declaration (1.0)\n            if namaste_patterns[0] == '0=ocfl_object_1.0':\n                return 'root'\n            else:\n                return 'object'\n        elif len(namaste_patterns) > 1:\n            return 'object'\n        else:\n            # If we have other 0= files, it's likely an object\n            return 'object'\n            \n    except Exception as e:\n        return f\"error determining path type: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for item in os.listdir(path):\n            if item.startswith('0='):\n                namaste_files.append(item)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if there's exactly one 0=ocfl_object_1.0 file (root)\n        # or multiple 0=ocfl_object_x.x files (object)\n        namaste_patterns = []\n        for file in namaste_files:\n            if file == '0=ocfl_object_1.0':\n                namaste_patterns.append('root')\n            elif file.startswith('0=ocfl_object_'):\n                namaste_patterns.append('object')\n            else:\n                namaste_patterns.append('unknown')\n        \n        # If we have a 0=ocfl_object_1.0 file, it's a root\n        if 'root' in namaste_patterns:\n            return 'root'\n        \n        # If we have any 0=ocfl_object_x.x file (not 1.0), it's an object\n        if 'object' in namaste_patterns:\n            return 'object'\n        \n        # If we have any other pattern, it's unknown\n        return 'unknown'\n        \n    except Exception as e:\n        return \"error: \" + str(e)", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for item in os.listdir(path):\n            if item.startswith('0='):\n                namaste_files.append(item)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if any file matches the pattern for a storage root (0=ocfl_object_1.0)\n        root_pattern = '0=ocfl_object_1.0'\n        object_pattern = '0=ocfl_object_'\n        \n        # Check if there's a 0=ocfl_object_1.0 file (storage root)\n        if root_pattern in namaste_files:\n            return 'root'\n        \n        # Check if there's a 0=ocfl_object_x.x file (object)\n        for file in namaste_files:\n            if file.startswith(object_pattern):\n                return 'object'\n        \n        # If no clear pattern found, check if it's a file (inventory)\n        if len(namaste_files) == 1 and len(os.listdir(path)) == 1:\n            return 'file'\n        \n        # Default case - unclear type\n        return 'unknown type'\n        \n    except Exception as e:\n        return f\"error determining path type: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for filename in os.listdir(path):\n            if filename.startswith('0='):\n                namaste_files.append(filename)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if there's exactly one 0=ocfl_object_1.0 file (root)\n        # or if there are multiple 0= files (object)\n        namaste_patterns = []\n        for filename in namaste_files:\n            if filename.startswith('0=ocfl_object_'):\n                namaste_patterns.append(filename)\n        \n        if len(namaste_patterns) == 1:\n            # Check if it's the root declaration\n            if namaste_patterns[0] == '0=ocfl_object_1.0':\n                return 'root'\n            else:\n                return 'object'\n        elif len(namaste_patterns) > 1:\n            return 'object'\n        else:\n            # Check if there are any 0= files at all\n            return 'file'\n            \n    except Exception as e:\n        return f\"error checking path: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for filename in os.listdir(path):\n            if filename.startswith('0='):\n                namaste_files.append(filename)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if there's exactly one 0=ocfl_object_1.0 file (root)\n        # or if there are multiple 0= files (object)\n        namaste_patterns = []\n        for filename in namaste_files:\n            if filename.startswith('0=ocfl_object_'):\n                namaste_patterns.append(filename)\n        \n        if len(namaste_patterns) == 1:\n            # Check if it's the root declaration\n            if namaste_patterns[0] == '0=ocfl_object_1.0':\n                return 'root'\n            else:\n                return 'object'\n        elif len(namaste_patterns) > 1:\n            return 'object'\n        else:\n            # Check if there are any 0= files at all\n            return 'file'\n            \n    except Exception as e:\n        return \"error: \" + str(e)"]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 79408, "num_retrieved": 500, "inference_time": 6.5739807933568954, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(str(bug_id) for bug_id in bug_ids)", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 63340, "num_retrieved": 192, "inference_time": 6.5739807933568954, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 62185, "num_retrieved": 198, "inference_time": 6.5739807933568954, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would parse the arguments and return a Namespace object\n    # For now, we return a dummy parser instance\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would include all the subparsers and arguments defined in the borgmatic codebase\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would include all the subparsers and arguments defined in the borgmatic codebase\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation - the actual implementation would depend on the specific\n    # arguments and options that need to be parsed\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would parse the arguments and return a Namespace object\n    # For now, we return a dummy parser instance\n    return parser.parse_args(arguments)"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 62481, "num_retrieved": 189, "inference_time": 6.5739807933568954, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join(f'--{name}' for name in parser._actions if name.startswith('_') or not name.startswith('_'))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join(f'--{name}' for name in parser._actions if name.startswith('_') or name.replace('_', '-').replace('-', '_') in parser._actions)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join([f'--{name}' for name in parser._actions if name.startswith('_') == False])", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join([f'--{name}' for name in parser._actions if name.startswith('_') == False])", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join([f'--{name}' for name in parser._actions if name.startswith('_') == False])"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 61648, "num_retrieved": 204, "inference_time": 6.5739807933568954, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subcommand', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', '-c', help='Configuration file path')\n    global_parser.add_argument('--version', action='store_true', help='Show version')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Show bash completion script')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables')\n    \n    # Add subparsers for different commands\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Exclude patterns from file')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Archive name or pattern')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    prune_parser = subparsers.add_parser('prune', help='Prune archives')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Keep weekly", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='command')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', '-c', help='Configuration file path')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Output bash completion script')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    \n    # Create arguments\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Exclude patterns from file')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    create_parser.add_argument('--patterns-from', nargs='+', help='Patterns from file')\n    create_parser.add_argument('--compression', choices=['zstd', 'lz4', 'zlib'], default='zstd', help='Compression algorithm')\n    \n    # List arguments\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Archive name or pattern')\n    list_parser.add_argument('--json', action='", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config', '-c', help='configuration file path')\n    global_parser.add_argument('--config-paths', nargs='+', help='configuration file paths')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='set verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true', help='resolve environment variables')\n    global_parser.add_argument('--overrides', nargs='+', help='configuration overrides in format section.option=value')\n    global_parser.add_argument('--bash-completion', action='store_true', help='output bash completion script')\n    \n    # Subparsers for different commands\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--repository', help='repository path')\n    create_parser.add_argument('--archive', help='archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='source directories')\n    create_parser.add_argument('--exclude-from', nargs='+', help='exclude patterns from files')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='exclude nodump')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='archive name or pattern')\n    list_parser.add_argument('--json', action='store_true', help='output", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subcommand', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config', nargs='+', help='configuration files to load')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    global_parser.add_argument('--verbose', action='store_true', help='enable verbose output')\n    global_parser.add_argument('--log-file', help='log to file')\n    global_parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='monitoring verbosity level')\n    global_parser.add_argument('--bash-completion', action='store_true', help='output bash completion script')\n    \n    # Create arguments\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--repository', help='repository path')\n    create_parser.add_argument('--archive', help='archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='exclude patterns from file')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='exclude nodump')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config', nargs='+', help='configuration files to load')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='set verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true', help='resolve environment variables')\n    \n    # Subcommands\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--repository', help='repository path')\n    create_parser.add_argument('--archive', help='archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='exclude patterns from files')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='exclude nodump')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='archive name or pattern')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    info_parser = subparsers.add_parser('info', help='display archive information')\n    info_parser.add_argument('--archive', help='archive name or pattern')\n    info_parser.add_argument('--prefix', help='prefix for archive names')\n    info_parser.add_argument('--json', action"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 61825, "num_retrieved": 205, "inference_time": 18.812931537628174, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, continue to next subparser\n            continue\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, break out of the loop\n            break\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, break out of the loop\n            break\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = []\n    \n    # Process arguments in order\n    for arg in unparsed_arguments:\n        # Try to match each argument with a subparser\n        for subparser_name, subparser in subparsers.items():\n            # Check if the argument can be parsed by this subparser\n            try:\n                # Parse the argument with the subparser\n                parsed_args = subparser.parse_args([arg])\n                # If parsing succeeds, add the parsed arguments to the result\n                arguments[subparser_name] = parsed_args\n                # Remove the argument from the remaining arguments\n                remaining_arguments.remove(arg)\n                break\n            except SystemExit:\n                # If parsing fails, continue to the next subparser\n                continue\n        else:\n            # If no subparser could parse the argument, add it to remaining arguments\n            remaining_arguments.append(arg)\n    \n    return (arguments, remaining_arguments)"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 60165, "num_retrieved": 180, "inference_time": 18.812931537628174, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # The context contains functions related to flags, configuration, and command execution\n    # but does not contain the implementation of make_parsers\n    # Therefore, we return a placeholder implementation\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(description='Borgmatic command-line tool')\n    \n    # Add subparsers for different actions\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be defined in the actual implementation)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    list_parser = subparsers.add_parser('list', help='List archives')\n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    \n    return (top_level_parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # The context contains functions related to flags, arguments, and configuration\n    # but does not contain the implementation of make_parsers\n    # Therefore, we return a placeholder implementation\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(description='Borgmatic command-line tool')\n    \n    # Add subparsers for different actions\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be defined in the actual implementation)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    list_parser = subparsers.add_parser('list', help='List archives')\n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    \n    return (top_level_parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # It would typically build and return a top-level parser and its subparsers\n    # However, no implementation details are provided in the context\n    # Therefore, we return a placeholder tuple with empty parsers\n    return (None, None)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # The context contains functions related to flags, commands, and configuration\n    # but does not contain the implementation of make_parsers\n    # Therefore, we return a placeholder implementation\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(description='Borgmatic command-line tool')\n    \n    # Add subparsers for different actions\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be defined in the actual implementation)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    list_parser = subparsers.add_parser('list', help='List archives')\n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    \n    return (top_level_parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # It would typically build a top-level argument parser with subparsers\n    # for different borgmatic commands\n    # Since no implementation is provided in the context, we return a placeholder\n    # that would need to be filled with actual parser construction logic\n    top_level_parser = None\n    subparsers = None\n    return (top_level_parser, subparsers)"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 90309, "num_retrieved": 286, "inference_time": 18.812931537628174, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 92602, "num_retrieved": 291, "inference_time": 18.812931537628174, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace or parser instance\n    \n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add common arguments here based on the context\n    parser.add_argument('--version', action='store_true', help='Show version information')\n    parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    \n    # Add other arguments as needed based on the borgmatic command structure\n    parser.add_argument('--config', nargs='+', help='Configuration files to load')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--verbosity', choices=['error', 'answer', 'some', 'lots'], default='answer', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace or ArgumentParser instance\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add common arguments here based on the context\n    parser.add_argument('--version', action='store_true', help='Show version information')\n    parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    \n    # Add other arguments as needed based on the borgmatic command structure\n    parser.add_argument('--config', nargs='+', help='Configuration files to load')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--verbosity', choices=['error', 'answer', 'some', 'lots'], default='answer', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the full implementation would require\n    # defining the full argument parser structure which is not provided in the context\n    # The actual implementation would involve creating an ArgumentParser and adding\n    # appropriate arguments based on the borgmatic command structure\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace or ArgumentParser instance\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add common arguments here based on the context\n    parser.add_argument('--version', action='store_true', help='Show version information')\n    parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    \n    # Add other arguments as needed based on the borgmatic command structure\n    parser.add_argument('--config', nargs='+', help='Configuration file paths')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--verbosity', choices=['error', 'answer', 'some', 'lots'], default='answer', help='Set verbosity level')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the full implementation would require\n    # setting up the argument parser with all the necessary subparsers, actions,\n    # and argument definitions which are not provided in the context.\n    # In a real implementation, this would parse the arguments and return\n    # a Namespace or ArgumentParser instance.\n    pass"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 91951, "num_retrieved": 284, "inference_time": 18.812931537628174, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 91946, "num_retrieved": 290, "inference_time": 18.812931537628174, "generate_results": ["def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee $this_filename',\n                        '$this_filename',\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ $1 == \"complete\" ]] && return 0\n                [[ $1 == \"complete\" ]] && return 0\n            }}\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''complete -c borgmatic -", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee $this_filename',\n                        '$this_filename',\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition=\"not __fish_seen_subcommand_from {all_action_parsers}\"\n            _borgmatic_exact_option_condition=\"not __borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''complete -c borgmatic -f -n \"$_borgmatic_action_parser_condition\" -n \"$_borgmatic_exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee $this_filename',\n                        '$this_filename',\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ $1 == \"action\" ]] && echo \"$action_parser_condition\" && echo \"$exact_option_condition\"\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2> /dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            '$this_filename',\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not __borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ -n \"$action_parser_condition\" ]] && [[ -n \"$exact_option_condition\" ]] && eval \"$action_parser_condition\" && eval \"$exact", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee $this_filename',\n                        '$this_filename',\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not __borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ -n \"$action_parser_condition\" ]] && [[ -n \"$exact_option_condition\" ]] && eval \"$action_parser_condition\" && eval \"$exact_option_condition\"\n            }}\n\n            _borgmatic_global_option_condition() {{\n                local exact_option_condition=\"not __borgmatic_current_arg {'"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 92784, "num_retrieved": 294, "inference_time": 18.812931537628174, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=['~/.config/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration file overrides')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=1, choices=[0, 1, 2, 3, 4], help='Verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--log-file-format', help='Log file format')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', help='Repository path')\n    bootstrap_parser.add_argument('--archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source-filename', help='Source configuration file')\n    generate_parser.add_argument('--destination-f", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config-paths', nargs='+', default=[],\n                               help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true',\n                               help='Run in dry-run mode')\n    global_parser.add_argument('--log-file', default=None,\n                               help='Path to log file')\n    global_parser.add_argument('--log-file-format', default=None,\n                               help='Format for log file')\n    global_parser.add_argument('--log-json', action='store_true',\n                               help='Output JSON')\n    global_parser.add_argument('--no-color', action='store_true',\n                               help='Disable color output')\n    global_parser.add_argument('--verbosity', default='info',\n                               choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', default='info',\n                               choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               help='Syslog verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', default='info',\n                               choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               help='Monitoring verbosity level')\n    global_parser.add_argument('--overrides', nargs='+', default=[],\n                               help='Configuration file overrides')\n    global_parser.add_argument('--resolve-env', action='store_true',\n                               help='Resolve environment variables')\n    \n    # Add subparsers for each action\n    # (This is a simplified version - actual implementation would include all subparsers)\n    \n    # Create a namespace for the global arguments\n    global_args = parser.parse_args([])\n    \n    # Parse the actual arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create a dictionary to hold the parsed arguments\n    result = {}\n    \n    # Add", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=['~/.config/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--log-file', help='Log to a file')\n    global_parser.add_argument('--log-file-format', help='Format for log file')\n    global_parser.add_argument('--log-file-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='Verbosity level for log file')\n    global_parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='Verbosity level for syslog')\n    global_parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'info', 'debug'], default='info', help='Verbosity level for monitoring')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration file overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--paths', nargs='+', help='Paths to backup')\n    create_parser.add_argument('--progress', action='store_true', help='Show progress')\n    create_parser.add_argument('--stats', action='store", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=[],\n                               help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', default=[],\n                               help='Configuration file overrides in the form option.suboption=value')\n    global_parser.add_argument('--dry-run', action='store_true',\n                               help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true',\n                               help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Verbosity level')\n    global_parser.add_argument('--log-file', default=None,\n                               help='Path to log file')\n    global_parser.add_argument('--log-file-format', default=None,\n                               help='Format for log file')\n    global_parser.add_argument('--syslog-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Syslog verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Monitoring verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true',\n                               help='Resolve environment variables in configuration')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', required=True,\n                                  help='Repository path')\n    bootstrap_parser.add_argument('--archive', required=True,\n                                  help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', default=os.path.expanduser('~/.config/borgmatic'),\n                                  help='Path to borgmatic source directory')\n    bootstrap_parser.add_argument", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=[],\n                               help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', default=[],\n                               help='Configuration file overrides')\n    global_parser.add_argument('--dry-run', action='store_true',\n                               help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true',\n                               help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Verbosity level')\n    global_parser.add_argument('--log-file', default=None,\n                               help='Path to log file')\n    global_parser.add_argument('--log-file-format', default=None,\n                               help='Format for log file')\n    global_parser.add_argument('--syslog-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Syslog verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Monitoring verbosity level')\n    global_parser.add_argument('--bash-completion', action='store_true',\n                               help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true',\n                               help='Generate fish completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', required=True,\n                                  help='Repository path')\n    bootstrap_parser.add_argument('--archive', required=True,\n                                  help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', default=None,\n                                  help='Borgmatic source directory')"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 92979, "num_retrieved": 295, "inference_time": 18.812931537628174, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse the arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, add them to the parsed_arguments dict\n            if args:\n                parsed_arguments[subparser_name] = args\n                \n                # Remove the parsed arguments from the remaining arguments list\n                remaining_arguments = []\n                for arg in unparsed_arguments:\n                    if arg not in remaining_arguments:\n                        remaining_arguments.append(arg)\n                        \n                # Break out of the loop since we've processed all arguments\n                break\n        except SystemExit:\n            # If the parser raises a SystemExit, it means that the arguments are not valid\n            # for this subparser, so we continue to the next subparser\n            continue\n    \n    # Return the parsed arguments and the remaining arguments\n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse the arguments with the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, store them in the result\n            parsed_arguments[subparser_name] = args\n            \n            # Remove the parsed arguments from the remaining arguments list\n            remaining_arguments = []\n            \n            # If there are still arguments left, continue to the next subparser\n            if remaining_arguments:\n                continue\n                \n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n            \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse the arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were successfully parsed, add them to the parsed_arguments dict\n            if args:\n                parsed_arguments[subparser_name] = args\n                \n                # Remove the parsed arguments from the remaining arguments list\n                # Find the indices of the parsed arguments and remove them\n                indices_to_remove = []\n                for i, arg in enumerate(remaining_arguments):\n                    # Check if the argument is part of the parsed arguments\n                    # This is a simplified check - in practice, you'd need to check if the argument\n                    # matches any of the arguments in the parsed args\n                    if arg in [str(getattr(args, attr)) for attr in dir(args) if not attr.startswith('_')]:\n                        indices_to_remove.append(i)\n                \n                # Remove the arguments from the remaining arguments list\n                for i in sorted(indices_to_remove, reverse=True):\n                    remaining_arguments.pop(i)\n                    \n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n        except Exception:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dict\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse the arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, add them to the parsed_arguments dict\n            if args:\n                parsed_arguments[subparser_name] = args\n                \n                # Remove the parsed arguments from the remaining_arguments list\n                # Find the indices of the parsed arguments and remove them\n                indices_to_remove = []\n                for i, arg in enumerate(remaining_arguments):\n                    # Check if the argument is part of the parsed arguments\n                    # This is a simplified check - in practice, you'd need to track the actual argument names\n                    if hasattr(args, arg):\n                        indices_to_remove.append(i)\n                \n                # Remove the arguments from the list\n                for i in sorted(indices_to_remove, reverse=True):\n                    remaining_arguments.pop(i)\n                    \n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse the arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, store them in the result\n            parsed_arguments[subparser_name] = args\n            \n            # Remove the parsed arguments from the remaining arguments list\n            remaining_arguments = []\n            \n            # Reconstruct the remaining arguments list\n            for arg in unparsed_arguments:\n                if arg not in [str(getattr(args, key)) for key in vars(args).keys()]:\n                    remaining_arguments.append(arg)\n                    \n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n        except Exception:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return (parsed_arguments, remaining_arguments)"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 92623, "num_retrieved": 291, "inference_time": 18.812931537628174, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--version', action='version', version='borgmatic 1.0'\n    )\n    global_parser.add_argument(\n        '--config-paths', nargs='+', default=['~/.borgmatic/config.yaml'],\n        help='Paths to configuration files to load (default: ~/.borgmatic/config.yaml)'\n    )\n    global_parser.add_argument(\n        '--dry-run', action='store_true',\n        help='Run in dry-run mode (do not perform any actual operations)'\n    )\n    global_parser.add_argument(\n        '--log-json', action='store_true',\n        help='Output log messages in JSON format'\n    )\n    global_parser.add_argument(\n        '--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set the verbosity level (default: some)'\n    )\n    global_parser.add_argument(\n        '--no-color', action='store_true',\n        help='Disable color output'\n    )\n    global_parser.add_argument(\n        '--bash-completion', action='store_true',\n        help='Generate bash completion script'\n    )\n    global_parser.add_argument(\n        '--fish-completion', action='store_true',\n        help='Generate fish completion script'\n    )\n    global_parser.add_argument(\n        '--resolve-env', action='store_true',\n        help='Resolve environment variables in configuration files'\n    )\n    global_parser.add_argument(\n        '--overrides', nargs='+',\n        help='Configuration file overrides in the form of \"option.suboption=value\"'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Create parser for 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument(\n        '--repository', help='Repository path to use'\n    )\n    create_parser.add_argument(\n        '--archive', help='Archive name or pattern'\n    )\n    create_parser.add_argument(\n        '--source-directories', nargs='+',\n        help='", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A configuration-driven backup tool for Borg',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--version', action='version', version='borgmatic %(version)s'\n    )\n    global_parser.add_argument(\n        '--config-paths', nargs='+', default=['~/.borgmatic/config.yaml'],\n        help='Paths to configuration files to load (can be directories or files)'\n    )\n    global_parser.add_argument(\n        '--dry-run', action='store_true', default=False,\n        help='Run in dry-run mode (no actual operations)'\n    )\n    global_parser.add_argument(\n        '--log-json', action='store_true', default=False,\n        help='Output log messages in JSON format'\n    )\n    global_parser.add_argument(\n        '--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set the verbosity level (default: some)'\n    )\n    global_parser.add_argument(\n        '--no-color', action='store_true', default=False,\n        help='Disable colorized output'\n    )\n    global_parser.add_argument(\n        '--bash-completion', action='store_true', default=False,\n        help='Print bash completion script'\n    )\n    global_parser.add_argument(\n        '--fish-completion', action='store_true', default=False,\n        help='Print fish completion script'\n    )\n    global_parser.add_argument(\n        '--resolve-env', action='store_true', default=True,\n        help='Resolve environment variables in configuration files'\n    )\n    global_parser.add_argument(\n        '--overrides', nargs='*', default=[],\n        help='Configuration file overrides in the form of \"option.suboption=value\"'\n    )\n    \n    # Add subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add actions\n    create_parser = subparsers.add_parser('create', help='Create a backup archive')\n    create_parser.add_argument(\n        '--repository', help='Repository path to use'\n    )\n    create_parser.add_argument(\n        '--archive', help='Archive name or pattern'\n    )\n    create_parser.add_argument(", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    # Create the top-level parser\n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--version', action='version', version='borgmatic 1.0'\n    )\n    global_parser.add_argument(\n        '--config-paths', nargs='+', default=['~/.config/borgmatic/config.yaml'],\n        help='Paths to configuration files to load (can be directories or files)'\n    )\n    global_parser.add_argument(\n        '--dry-run', action='store_true', default=False,\n        help='Run in dry-run mode (no actual operations)'\n    )\n    global_parser.add_argument(\n        '--log-json', action='store_true', default=False,\n        help='Output log messages in JSON format'\n    )\n    global_parser.add_argument(\n        '--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set the verbosity level'\n    )\n    global_parser.add_argument(\n        '--no-color', action='store_true', default=False,\n        help='Disable color output'\n    )\n    global_parser.add_argument(\n        '--bash-completion', action='store_true', default=False,\n        help='Output bash completion script'\n    )\n    global_parser.add_argument(\n        '--fish-completion', action='store_true', default=False,\n        help='Output fish completion script'\n    )\n    global_parser.add_argument(\n        '--resolve-env', action='store_true', default=True,\n        help='Resolve environment variables in configuration files'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Create subparsers for each action\n    # Note: This is a simplified version - actual implementation would include all actions\n    # with their specific arguments\n    \n    # Example subparser for create\n    create_parser = subparsers.add_parser('create', help='Create a backup archive')\n    create_parser.add_argument(\n        '--repository', help='Repository path to use'\n    )\n    create_parser.add_argument(\n        '--archive', help='Archive name or pattern'", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    # Create the top-level parser\n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--version', action='version', version='borgmatic version 1.0'\n    )\n    global_parser.add_argument(\n        '--config-paths', \n        nargs='+', \n        default=['~/.borgmatic/config.yaml'],\n        help='Paths to configuration files'\n    )\n    global_parser.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode'\n    )\n    global_parser.add_argument(\n        '--log-json', \n        action='store_true', \n        help='Output log messages in JSON format'\n    )\n    global_parser.add_argument(\n        '--verbose', \n        action='store_true', \n        help='Enable verbose output'\n    )\n    global_parser.add_argument(\n        '--no-color', \n        action='store_true', \n        help='Disable color output'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    borgmatic.commands.arguments.add_subparsers(subparsers)\n    \n    # Return the top-level parser, subparsers, and the combined parser with global arguments\n    return (parser, subparsers, parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='some', help='Verbosity level')\n    global_parser.add_argument('--log-json', action='store_true', help='Output log in JSON format')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a backup archive')\n    create_parser.add_argument('--repository', help='Repository to use')\n    create_parser.add_argument('--archive', help='Archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='File containing exclude patterns')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache files')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files from nodump')\n    create_parser.add_argument('--patterns-from', nargs='+', help='File containing patterns')\n    create_parser.add_argument('--progress', action='store"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 198337, "num_retrieved": 377, "inference_time": 18.812931537628174, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Process the response in pages\n    page_size = max_results\n    total_items = len(response)\n    \n    # Calculate the number of pages needed\n    num_pages = (total_items + page_size - 1) // page_size\n    \n    for page in range(num_pages):\n        start_idx = page * page_size\n        end_idx = min((page + 1) * page_size, total_items)\n        \n        # Yield the current page\n        yield response[start_idx:end_idx]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Process response in pages\n    page_size = max_results\n    total_items = len(response)\n    \n    # Calculate number of pages needed\n    num_pages = (total_items + page_size - 1) // page_size\n    \n    for page in range(num_pages):\n        start_idx = page * page_size\n        end_idx = min((page + 1) * page_size, total_items)\n        \n        # Yield the current page\n        yield response[start_idx:end_idx]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # Initialize page_id to None\n    page_id = None\n    \n    # Iterate through the response until no more pages are available\n    while True:\n        # If we have a page_id, include it in the query parameters\n        if page_id is not None:\n            # Create a new response with the page_id parameter\n            new_response = response.copy()\n            # Check if the response has a next_page_id field\n            if 'next_page_id' in new_response:\n                page_id = new_response['next_page_id']\n                # Yield the current page\n                yield new_response\n            else:\n                # If there's no next_page_id, we're done\n                break\n        else:\n            # If we don't have a page_id, yield the first page\n            yield response\n            # If the response has a next_page_id field, set it as the page_id\n            if 'next_page_id' in response:\n                page_id = response['next_page_id']\n            else:\n                break", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Process response in pages\n    page_size = max_results\n    total_items = len(response)\n    \n    # Calculate number of pages needed\n    num_pages = (total_items + page_size - 1) // page_size\n    \n    for page in range(num_pages):\n        start_idx = page * page_size\n        end_idx = min((page + 1) * page_size, total_items)\n        \n        # Yield the current page\n        yield response[start_idx:end_idx]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Process the response in pages\n    page_size = max_results\n    total_items = len(response)\n    \n    # Calculate the number of pages needed\n    num_pages = (total_items + page_size - 1) // page_size\n    \n    for page in range(num_pages):\n        start_idx = page * page_size\n        end_idx = min((page + 1) * page_size, total_items)\n        \n        # Yield the current page\n        yield response[start_idx:end_idx]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 20441, "num_retrieved": 146, "inference_time": 18.812931537628174, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 76049, "num_retrieved": 251, "inference_time": 18.812931537628174, "generate_results": ["def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 23160, "num_retrieved": 60, "inference_time": 18.812931537628174, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 83092, "num_retrieved": 500, "inference_time": 18.812931537628174, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.strip().lower() in ('none', 'null', 'nil', 'n/a', 'na', 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'null', 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'null', 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'none')"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 91951, "num_retrieved": 284, "inference_time": 18.812931537628174, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 165646, "num_retrieved": 500, "inference_time": 18.812931537628174, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 84794, "num_retrieved": 407, "inference_time": 14.660082802176476, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and the two adjacent vertices\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple with the current vertex and its two neighbors\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 1167, "num_retrieved": 5, "inference_time": 14.660082802176476, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        result = alphabet[number % len(alphabet)] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        remainder = number % len(alphabet)\n        result = alphabet[remainder] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        result = alphabet[number % len(alphabet)] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 124218, "num_retrieved": 500, "inference_time": 14.660082802176476, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_number] = flow[register_value]\n            del flow[register_value]"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 37299, "num_retrieved": 244, "inference_time": 14.660082802176476, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 121509, "num_retrieved": 467, "inference_time": 14.660082802176476, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 1161, "num_retrieved": 5, "inference_time": 14.660082802176476, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 20613, "num_retrieved": 149, "inference_time": 14.660082802176476, "generate_results": ["import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path, format=\"tar\") as extracted_files:\n            pass\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path) as extracted_files:\n            pass  # Extracted files are automatically handled by shutil\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path) as extracted_files:\n            pass  # Extract to destination_path\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode to save the downloaded content\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as gz_file:\n        with open(destination_path / \"archive.tar\", \"wb\") as tar_file:\n            shutil.copyfileobj(gz_file, tar_file)\n    \n    # Extract the tar file\n    import tarfile\n    with tarfile.open(destination_path / \"archive.tar\", \"r:gz\") as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as gz_file:\n        with open(destination_path / \"archive.tar\", \"wb\") as tar_file:\n            shutil.copyfileobj(gz_file, tar_file)\n    \n    # Extract the tar file\n    import subprocess\n    subprocess.check_call([\"tar\", \"-xzf\", str(destination_path / \"archive.tar\"), \"-C\", str(destination_path)])\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 121777, "num_retrieved": 486, "inference_time": 14.660082802176476, "generate_results": ["import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\""]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 162218, "num_retrieved": 500, "inference_time": 14.660082802176476, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'.', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'.', regex_key)\n        # Ensure the regex starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'(.|)', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'(.|)', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'(.|)', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 77542, "num_retrieved": 259, "inference_time": 14.660082802176476, "generate_results": ["def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 115448, "num_retrieved": 457, "inference_time": 14.660082802176476, "generate_results": ["import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules than POSIX\n    if platform == 0:\n        # Windows CMD: split on spaces, but preserve quotes and backslashes\n        # Use a regex that handles quoted strings and escaped characters\n        # This is a simplified version that handles basic cases\n        result = []\n        current = \"\"\n        in_quotes = False\n        quote_char = None\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                quote_char = '\"'\n            elif char == '\"' and in_quotes:\n                in_quotes = False\n                quote_char = None\n            elif char == '\\\\' and in_quotes:\n                # Skip escape sequences in quoted strings\n                continue\n            elif char == ' ' and not in_quotes:\n                if current:\n                    result.append(current)\n                    current = \"\"\n            else:\n                current += char\n        \n        if current:\n            result.append(current)\n        \n        # Handle the case where there are no spaces\n        if not result:\n            result = [s]\n            \n        return result\n    \n    # POSIX (Linux, macOS) - use shlex.split as a fallback\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules than POSIX\n    if platform == 0:\n        # Windows CMD: split on spaces, but preserve quotes and backslashes\n        # Use regex to handle quoted strings and escape sequences\n        # This is a simplified version that handles basic cases\n        result = []\n        current = \"\"\n        in_quotes = False\n        quote_char = None\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                quote_char = '\"'\n            elif char == '\"' and in_quotes:\n                in_quotes = False\n                quote_char = None\n            elif char == '\\\\' and in_quotes:\n                # Handle escape sequences in quoted strings\n                current += char\n            elif char == ' ' and not in_quotes:\n                if current:\n                    result.append(current)\n                    current = \"\"\n            else:\n                current += char\n        \n        if current:\n            result.append(current)\n        \n        # Handle the case where there's no space between tokens\n        # This is a fallback for simple cases\n        if not result:\n            result = [s]\n            \n        return result\n    \n    # POSIX (Linux, macOS) - use shlex.split for standard behavior\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Pattern to match shell-like syntax: quoted strings, escaped characters, and word boundaries\n    # This pattern handles quotes, backslashes, and spaces\n    pattern = r'(?:\"([^\"]*)\"|\\'([^\\']*)\\'|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules than POSIX\n    if platform == 0:\n        # Windows CMD: split on spaces, but preserve quotes and backslashes\n        # Use a regex that handles quoted strings and escapes\n        # This is a simplified version that handles basic cases\n        result = []\n        current = \"\"\n        in_quotes = False\n        quote_char = None\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                quote_char = '\"'\n            elif char == '\"' and in_quotes:\n                in_quotes = False\n                quote_char = None\n            elif char == ' ' and not in_quotes:\n                if current:\n                    result.append(current)\n                    current = \"\"\n            else:\n                current += char\n        \n        if current:\n            result.append(current)\n        \n        # Handle quoted strings that might have spaces\n        # This is a simplified version - in practice, we'd need more robust parsing\n        return result\n    \n    # POSIX (Linux, macOS) - use shlex.split as a fallback\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Pattern to match shell-like syntax: quoted strings, escaped characters, and word boundaries\n    # This pattern handles quotes, escaped characters, and spaces\n    pattern = r'(?:\"([^\"]*)\"|\\'([^\\']*)\\'|([^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']+(?:\\s+[^\"\\']"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 87070, "num_retrieved": 370, "inference_time": 14.660082802176476, "generate_results": ["import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path)\n    \n    # Create the destination path\n    dest_path = tmp_dir / filename\n    \n    # Extract the archive to the destination path\n    try:\n        # Use subprocess to run the tar command\n        subprocess.run(\n            ['tar', '-xzf', archive_path, '-C', str(tmp_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        # If tar fails, try with gzip\n        try:\n            subprocess.run(\n                ['gzip', '-d', archive_path],\n                check=True\n            )\n            subprocess.run(\n                ['tar', '-xzf', archive_path, '-C', str(tmp_dir)],\n                check=True\n            )\n        except subprocess.CalledProcessError:\n            raise e\n    \n    # Return the file repo url (assuming it's the path to the extracted file)\n    return str(dest_path)", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).split('.')[0]\n    \n    # Create the destination path\n    dest_path = tmp_dir / filename\n    \n    # Extract the archive using tar\n    try:\n        subprocess.run(['tar', '-xzf', archive_path, '-C', str(tmp_dir)], check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as the origin URL\n    return str(dest_path)", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).split('.')[0]\n    \n    # Create the destination path\n    dest_path = tmp_dir / filename\n    \n    # Extract the archive using tar\n    try:\n        subprocess.run(\n            ['tar', '-xzf', archive_path, '-C', str(tmp_dir)],\n            check=True,\n            capture_output=True\n        )\n    except subprocess.CalledProcessError as e:\n        # Fallback to using gzip if the archive is a gzipped file\n        try:\n            subprocess.run(\n                ['gunzip', '-c', archive_path],\n                check=True,\n                capture_output=True\n            )\n            # If we get here, the file was uncompressed successfully\n            # But we still need to extract it to a directory\n            # This is a simplified fallback - actual extraction logic would depend on the archive format\n            pass\n        except subprocess.CalledProcessError:\n            raise RuntimeError(f\"Failed to extract archive {archive_path}\")\n    \n    # Return the path to the extracted directory as the origin URL\n    # In a real implementation, this would be the actual URL to the repository\n    return str(dest_path)", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path)\n    \n    # Extract the archive to a temporary directory\n    extract_path = tmp_dir / filename\n    \n    # Use subprocess to extract the archive\n    # This handles common archive formats like .tar.gz, .tar.xz, .tar.bz2, etc.\n    try:\n        # Extract the archive using tar\n        cmd = [\"tar\", \"-xzf\", archive_path, \"-C\", str(tmp_dir)]\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        # If tar fails, try other formats\n        # For .tar.gz, .tar.xz, .tar.bz2, etc.\n        pass\n    \n    # Return the path to the extracted directory as the origin URL\n    # This is a placeholder - in a real implementation, this would return\n    # a proper URL based on the extracted files\n    return str(extract_path)", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).split('.')[0]\n    \n    # Create the destination path\n    dest_path = tmp_dir / filename\n    \n    # Use subprocess to extract the archive\n    # This handles common archive formats like .tar.gz, .tar.xz, .tar.bz2, etc.\n    try:\n        # Determine the archive type and extract accordingly\n        if archive_path.endswith('.tar.gz') or archive_path.endswith('.tgz'):\n            cmd = ['tar', '-xzf', archive_path, '-C', str(tmp_dir)]\n        elif archive_path.endswith('.tar.xz'):\n            cmd = ['tar', '-xJf', archive_path, '-C', str(tmp_dir)]\n        elif archive_path.endswith('.tar.bz2'):\n            cmd = ['tar', '-xjf', archive_path, '-C', str(tmp_dir)]\n        elif archive_path.endswith('.tar'):\n            cmd = ['tar', '-xvf', archive_path, '-C', str(tmp_dir)]\n        else:\n            # For other formats, try to extract with generic tools\n            cmd = ['tar', '-xzf', archive_path, '-C', str(tmp_dir)]\n        \n        subprocess.run(cmd, check=True)\n        \n        # Return the path to the extracted directory as the origin URL\n        return str(dest_path)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")"]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 30634, "num_retrieved": 110, "inference_time": 14.660082802176476, "generate_results": ["import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=False)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and any lines that don't match the ignored list\n        filtered_files = [file.strip() for file in ignored_files if file.strip() and file.strip() in ignored]\n        \n        # Sort the files alphabetically\n        filtered_files.sort()\n        \n        # Join the files with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 3282, "num_retrieved": 30, "inference_time": 14.660082802176476, "generate_results": ["def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide specific file types that this module supports\n    # Based on the examples and context, no clear pattern or supported file types are defined\n    # Therefore, return False by default as no supported types are specified\n    return False", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt') or filename.endswith('.ini') or filename.endswith('.json')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide specific file types that this module supports\n    # Based on the examples and context, no clear pattern is defined for supported file types\n    # Therefore, return False by default as no supported types are specified\n    return False", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide specific file types that this module supports\n    # Based on the examples and context, the module appears to handle config files\n    # (INI, JSON) and interactive command-line scripts\n    # Since no explicit file extensions are defined in the context,\n    # we return False by default as no supported types are specified\n    \n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 88443, "num_retrieved": 281, "inference_time": 14.660082802176476, "generate_results": ["import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+(days?|hours?|minutes?|seconds?|weeks?|months?|years?)$', frequency)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency \"{frequency}\"')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    elif unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'months':\n        # Approximate months as 30 days\n        return timedelta(days=number * 30)\n    elif unit == 'years':\n        # Approximate years as 365 days\n        return timedelta(days=number * 365)\n    else:\n        raise ValueError(f'Unknown time unit \"{unit}\"')", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+(days?|hours?|minutes?|seconds?|weeks?|months?|years?)$', frequency)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency \"{frequency}\"')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    elif unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'months':\n        # Approximate months as 30 days\n        return timedelta(days=number * 30)\n    elif unit == 'years':\n        # Approximate years as 365 days\n        return timedelta(days=number * 365)\n    else:\n        raise ValueError(f'Unsupported time unit: {unit}')", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+([a-zA-Z]+)$', frequency)\n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Map units to timedelta\n    unit_map = {\n        'second': timedelta(seconds=number),\n        'seconds': timedelta(seconds=number),\n        'minute': timedelta(minutes=number),\n        'minutes': timedelta(minutes=number),\n        'hour': timedelta(hours=number),\n        'hours': timedelta(hours=number),\n        'day': timedelta(days=number),\n        'days': timedelta(days=number),\n        'week': timedelta(weeks=number),\n        'weeks': timedelta(weeks=number),\n        'month': timedelta(days=number * 30),  # Approximate\n        'months': timedelta(days=number * 30),  # Approximate\n        'year': timedelta(days=number * 365),  # Approximate\n        'years': timedelta(days=number * 365),  # Approximate\n    }\n    \n    if unit not in unit_map:\n        raise ValueError(f\"Unknown unit '{unit}' in frequency '{frequency}'\")\n    \n    return unit_map[unit]", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+(days?|hours?|minutes?|seconds?|weeks?|months?|years?)$', frequency)\n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'days':\n        return datetime.timedelta(days=number)\n    elif unit == 'hours':\n        return datetime.timedelta(hours=number)\n    elif unit == 'minutes':\n        return datetime.timedelta(minutes=number)\n    elif unit == 'seconds':\n        return datetime.timedelta(seconds=number)\n    elif unit == 'weeks':\n        return datetime.timedelta(weeks=number)\n    elif unit == 'months':\n        # Approximate months as 30 days\n        return datetime.timedelta(days=number * 30)\n    elif unit == 'years':\n        # Approximate years as 365 days\n        return datetime.timedelta(days=number * 365)\n    else:\n        raise ValueError(f\"Unknown time unit '{unit}'\")", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+(days?|hours?|minutes?|seconds?|weeks?|months?|years?)$', frequency)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency \"{frequency}\"')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    elif unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'months':\n        # Approximate months as 30 days\n        return timedelta(days=number * 30)\n    elif unit == 'years':\n        # Approximate years as 365 days\n        return timedelta(days=number * 365)\n    else:\n        raise ValueError(f'Unknown time unit \"{unit}\"')"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 116384, "num_retrieved": 466, "inference_time": 14.660082802176476, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP address is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP address is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 90792, "num_retrieved": 286, "inference_time": 19.208111450076103, "generate_results": ["def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:', 'sh:')) else path\n        for path in find_paths\n    ]", "def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:')) else path\n        for path in find_paths\n    ]", "def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:')) else path\n        for path in find_paths\n    ]", "def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:')) else path\n        for path in find_paths\n    ]", "def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:', 'sh:')) else path\n        for path in find_paths\n    ]"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 116433, "num_retrieved": 466, "inference_time": 19.208111450076103, "generate_results": ["import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n:return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM') == 'xterm-256color' and os.environ.get('GIT_BASH') == '1'", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n:return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM') == 'xterm-256color' and os.environ.get('GIT_BASH') == '1'", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n:return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n:return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '').startswith('git')"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 88259, "num_retrieved": 279, "inference_time": 19.208111450076103, "generate_results": ["import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 101627, "num_retrieved": 388, "inference_time": 19.208111450076103, "generate_results": ["import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: a single line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within a reasonable width\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: the one line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within a reasonable width\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join the commands\n    :return: a single line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    command = seperator.join(lines)\n    \n    # Wrap the command to fit within a reasonable width (80 characters)\n    wrapped_command = textwrap.fill(command, width=80)\n    \n    return wrapped_command", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: the one line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within a reasonable width\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: the one line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within a reasonable width\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped"]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 184749, "num_retrieved": 500, "inference_time": 19.208111450076103, "generate_results": ["import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    timeout : int\n        The maximum time in seconds to wait for the subprocess to complete.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_path = f\"/tmp/{module_name}.py\"\n    \n    # Write the function to a temporary file\n    with open(module_path, 'w') as f:\n        f.write(f\"def {func.__name__}(*args, **kwargs):\\n\")\n        f.write(f\"    return {func.__name__}(*args, **kwargs)\\n\")\n    \n    # Import the module\n    try:\n        module = import_module(module_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module {module_name}: {e}\")\n    \n    # Prepare the environment\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Prepare the command\n    command = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}({', '.join([repr(arg) for arg in args])})\"]\n    \n    # Run the subprocess\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            capture_output=True,\n            text=True,\n            env=env\n        )\n        return result.stdout.strip()\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Subprocess timed out after {timeout} seconds\")\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Subprocess returned error code {e.returncode}: {e.stderr}\")\n    finally:\n        # Clean up the temporary file\n        if os.path.exists(module_path):\n            os.remove(module_path)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    timeout : int or float\n        The maximum time in seconds to wait for the process to complete.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_path = f\"/tmp/{module_name}.py\"\n    \n    # Write the function to a temporary file\n    with open(module_path, 'w') as f:\n        f.write(f\"def {func.__name__}(*args, **kwargs):\\n\")\n        f.write(f\"    return {func.__name__}(*args, **kwargs)\\n\")\n    \n    # Import the module\n    try:\n        sys.path.insert(0, os.path.dirname(module_path))\n        module = import_module(module_name)\n        sys.path.pop(0)\n        \n        # Prepare the environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the function in a subprocess\n        cmd = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}({', '.join([repr(arg) for arg in args])})\"]\n        \n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            env=env\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return result.stdout.strip()\n        else:\n            raise RuntimeError(f\"Command failed with return code {result.returncode}: {result.stderr.strip()}\")\n            \n    except Exception as e:\n        raise RuntimeError(f\"Error running subprocess: {str(e)}\")\n    finally:\n        # Clean up the temporary file\n        try:\n            os.remove(module_path)\n        except OSError:\n            pass", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    timeout : int\n        The maximum time in seconds to wait for the subprocess to complete.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_path = f\"/tmp/{module_name}.py\"\n    \n    # Write the function to a temporary file\n    with open(module_path, 'w') as f:\n        f.write(f\"def {func.__name__}(*args, **kwargs):\\n\")\n        f.write(f\"    return {func.__name__}(*args, **kwargs)\\n\")\n    \n    # Import the module\n    try:\n        module = import_module(module_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module {module_name}: {e}\")\n    \n    # Prepare the environment\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Prepare the command\n    command = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}({', '.join([repr(arg) for arg in args])})\"]\n    \n    # Run the subprocess\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            capture_output=True,\n            text=True,\n            env=env\n        )\n        return result.stdout.strip()\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Subprocess timed out after {timeout} seconds\")\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Subprocess failed with return code {e.returncode}: {e.stderr}\")\n    finally:\n        # Clean up the temporary file\n        if os.path.exists(module_path):\n            os.remove(module_path)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    timeout : int\n        The maximum time in seconds to wait for the subprocess to complete.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_code = f\"\"\"\ndef {func.__name__}(*args, **kwargs):\n    return {func.__name__}(*args, **kwargs)\n\"\"\"\n    \n    # Create a temporary module and add the function\n    temp_module = type(module_name, (), {})\n    exec(module_code, temp_module.__dict__)\n    \n    # Prepare the command to run\n    command = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}(*{args})\"]\n    \n    # Add extra environment variables\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the command with timeout\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            capture_output=True,\n            text=True,\n            env=env\n        )\n        return result.returncode, result.stdout, result.stderr\n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Process timed out\"\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    timeout : int\n        The maximum time in seconds to wait for the subprocess to complete.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module_path = f\"/tmp/{module_name}.py\"\n    \n    # Write the function to a temporary file\n    with open(module_path, 'w') as f:\n        f.write(f\"def {func.__name__}(*args, **kwargs):\\n\")\n        f.write(f\"    return {func.__name__}(*args, **kwargs)\\n\")\n    \n    # Import the module\n    try:\n        sys.path.insert(0, os.path.dirname(module_path))\n        module = import_module(module_name)\n        sys.path.pop(0)\n        \n        # Prepare the environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the function in a subprocess\n        cmd = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}({', '.join([repr(arg) for arg in args])})\"]\n        \n        result = subprocess.run(\n            cmd,\n            timeout=timeout,\n            capture_output=True,\n            text=True,\n            env=env\n        )\n        \n        # Return the result\n        return result.returncode, result.stdout, result.stderr\n        \n    except Exception as e:\n        return 1, \"\", str(e)\n    finally:\n        # Clean up the temporary file\n        if os.path.exists(module_path):\n            os.remove(module_path)"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 91501, "num_retrieved": 288, "inference_time": 19.208111450076103, "generate_results": ["import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    var_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(var_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    variable_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(variable_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{variable_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    var_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(var_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Pattern: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    variable_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(variable_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{variable_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    var_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(var_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 100261, "num_retrieved": 500, "inference_time": 19.208111450076103, "generate_results": ["import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after the last slash)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path\")\n            \n        # Get the last part of the path as image_id\n        image_id = path.split('/')[-1]\n        \n        # Validate image_id is not empty\n        if not image_id:\n            raise ValueError(\"Invalid image href: missing image_id\")\n            \n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after the last slash)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path\")\n            \n        # Get the last part of the path as image_id\n        image_id = path.split('/')[-1]\n        \n        # Validate image_id is a valid UUID\n        if not uuidutils.is_uuid_like(image_id):\n            raise ValueError(\"Invalid image ID format\")\n            \n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after the last slash)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path component\")\n            \n        # Get the last part of the path as image_id\n        image_id = path.split('/')[-1]\n        \n        # Validate image_id is not empty\n        if not image_id:\n            raise ValueError(\"Invalid image href: missing image_id\")\n            \n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after any leading path components)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path component\")\n            \n        # Extract the last path component as image_id\n        image_id = path.split('/')[-1]\n        \n        # Validate image_id is not empty\n        if not image_id:\n            raise ValueError(\"Invalid image href: missing image_id in path\")\n            \n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used (https vs http)\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after the last slash)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path component\")\n            \n        # Split path by slashes and take the last part\n        path_parts = path.strip('/').split('/')\n        image_id = path_parts[-1] if path_parts else ''\n        \n        if not image_id:\n            raise ValueError(\"Invalid image href: missing image_id in path\")\n            \n        # Return the parsed components\n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 164038, "num_retrieved": 500, "inference_time": 19.208111450076103, "generate_results": ["def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in os.path.sep:\n            processed_list.append(item[:-1])\n        elif isinstance(item, str):\n            processed_list.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return processed_list", "def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in os.path.sep:\n            processed_list.append(item[:-1])\n        elif isinstance(item, str):\n            processed_list.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return processed_list", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1:\n            # Check if the last character is the OS separator\n            if item.endswith(os.sep):\n                processed_list.append(item[:-1])\n            else:\n                processed_list.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return processed_list", "def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in os.path.sep:\n            processed_list.append(item[:-1])\n        elif isinstance(item, str):\n            processed_list.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return processed_list", "def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.sep):\n            processed_list.append(item[:-1])\n        elif isinstance(item, str):\n            processed_list.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return processed_list"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 2440, "num_retrieved": 10, "inference_time": 19.208111450076103, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern\n    pattern = pattern.replace(' ', r'\\s*')\n    # Replace special regex characters with escaped versions\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literally\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = re.escape(pattern)\n    # Add word boundaries to match exact phrases\n    pattern = r'\\b' + pattern + r'\\b'\n    return pattern"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 140933, "num_retrieved": 387, "inference_time": 19.208111450076103, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env_dict = env or {}\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command execution\n    try:\n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env_dict,\n            shell=False,\n            timeout=None\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode]\n            \n    except subprocess.TimeoutExpired:\n        return [124]\n    except FileNotFoundError:\n        return [127]\n    except PermissionError:\n        return [137]\n    except OSError as error:\n        return [error.errno]", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env_dict = env or {}\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command arguments\n    cmd_args = args if args is not None else []\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd + cmd_args,\n            cwd=cwd,\n            env=env_dict,\n            capture_output=not hide_stderr,\n            text=True,\n            check=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode, result.stderr if result.stderr else result.stdout]\n            \n    except subprocess.SubprocessError as error:\n        return [error.returncode, str(error)]", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env_dict = env or {}\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command arguments\n    cmd_args = args if args is not None else []\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd + cmd_args,\n            cwd=cwd,\n            env=env_dict,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Return the result as a tuple of (returncode, stdout, stderr)\n        return (result.returncode, result.stdout, result.stderr if result.stderr else None)\n        \n    except subprocess.SubprocessError as e:\n        # Return error code and error message\n        return (e.returncode, \"\", str(e))", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Validate inputs\n    if not commands:\n        return []\n    \n    # Prepare the command list\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Prepare environment\n    env = env or os.environ.copy()\n    \n    # Prepare working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare command arguments\n    cmd_args = []\n    for arg in args:\n        if isinstance(arg, str):\n            cmd_args.append(arg)\n        else:\n            cmd_args.extend(arg)\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            capture_output=not hide_stderr,\n            text=True,\n            shell=False,\n            timeout=None\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode]\n            \n    except subprocess.TimeoutExpired:\n        return [124]\n    except FileNotFoundError:\n        return [127]\n    except PermissionError:\n        return [13]\n    except Exception as e:\n        return [1]", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env_dict = env or {}\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command arguments\n    cmd_args = args if args is not None else []\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd + cmd_args,\n            cwd=cwd,\n            env=env_dict,\n            capture_output=not hide_stderr,\n            text=True,\n            check=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode, result.stderr if result.stderr else result.stdout]\n            \n    except subprocess.SubprocessError as e:\n        return [e.returncode, str(e)]"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 19.208111450076103, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 140222, "num_retrieved": 500, "inference_time": 19.208111450076103, "generate_results": ["def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no parent\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object has no parent (i.e., it's not a child of anything), it's a root\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(predicate, object_)):\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no parent\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object has no parent (i.e., it's not a child of anything), it's a root\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(predicate, object_)):\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no parent\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object has no parent (i.e., it's not a child of anything), it's a root\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(predicate=prop)):\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property as their predicate\n    # These are the potential roots (nodes with no parent)\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # Check if this subject has no parent (i.e., no triple with (subject, prop, parent))\n        # If it doesn't have a parent, it's a root\n        has_parent = False\n        for s, p, o in graph.triples((subject, prop, None)):\n            has_parent = True\n            break\n        \n        if not has_parent:\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no parent\n    # These are the roots of the hierarchy\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If this subject has no parent (i.e., no triple with (subject, prop, parent))\n        # then it is a root\n        if not any((subject, prop, parent) in graph for parent in graph.objects(subject, prop)):\n            roots.add(subject)\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 16464, "num_retrieved": 45, "inference_time": 19.208111450076103, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 7074, "num_retrieved": 39, "inference_time": 19.208111450076103, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 20418, "num_retrieved": 67, "inference_time": 19.208111450076103, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 128706, "num_retrieved": 399, "inference_time": 19.208111450076103, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 87523, "num_retrieved": 290, "inference_time": 24.732524782419205, "generate_results": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 92026, "num_retrieved": 500, "inference_time": 24.732524782419205, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 183833, "num_retrieved": 500, "inference_time": 24.732524782419205, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n\n    :rtype: Time\n\n    :raises ValueError: if ticks is out of bounds\n        (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    hours, minutes = map(int, divmod(seconds, 3600))\n    minutes, seconds = map(int, divmod(minutes, 60))\n    \n    # Create a Time object with the parsed components\n    return cls(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\n    def from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not 0 <= ticks < 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a Time object with the calculated hours, minutes, seconds, and nanoseconds\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n\n    :rtype: Time\n\n    :raises ValueError: if ticks is out of bounds\n        (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not 0 <= ticks < 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return cls(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n\n    :rtype: Time\n\n    :raises ValueError: if ticks is out of bounds\n        (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to hours, minutes, seconds, and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    hours, minutes = map(int, divmod(seconds, 3600))\n    minutes, seconds = map(int, divmod(minutes, 60))\n    \n    # Create a Time instance with the calculated components\n    return cls(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\n    def from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not 0 <= ticks < 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a Time object with the parsed hours, minutes, seconds, and nanoseconds\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 207117, "num_retrieved": 500, "inference_time": 24.732524782419205, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the defined protocol handlers\n        handlers = {}\n        for handler_class in cls._get_all_protocol_handlers():\n            if handler_class.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler_class\n                break\n        return handlers\n    \n    # Return all supported protocol versions\n    handlers = {}\n    for handler_class in cls._get_all_protocol_handlers():\n        version = handler_class.PROTOCOL_VERSION\n        handlers[version] = handler_class\n    return handlers\n\ndef _get_all_protocol_handlers(self):\n    \"\"\"Helper method to get all protocol handlers from the class.\"\"\"\n    # This is a placeholder - in a real implementation, this would\n    # return all the protocol handler classes defined in the class\n    # This would typically be defined in the class itself or in a\n    # class-level list\n    return [\n        AsyncBolt3,\n        AsyncBolt4x1,\n        AsyncBolt4x2,\n        AsyncBolt4x3,\n        AsyncBolt4x4,\n        AsyncBolt5x0,\n        AsyncBolt5x1,\n        AsyncBolt5x2,\n        AsyncBolt5x3,\n        AsyncBolt5x4,\n        AsyncBolt5x5,\n        AsyncBolt5x6,\n        AsyncBolt5x7,\n        AsyncBolt5x8,\n        AsyncBolt5x9,\n        AsyncBolt5x10,\n        AsyncBolt5x11,\n        AsyncBolt5x12,\n        AsyncB", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for version in [(3, 0), (3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5)]:\n            if version == protocol_version:\n                handlers[version] = cls._get_handler_for_version(version)\n        return handlers if handlers else {}\n    \n    # Return all supported versions\n    handlers = {}\n    for version in [(3, 0), (3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5)]:\n        handlers[version] = cls._get_handler_for_version(version)\n    return handlers\n\ndef _get_handler_for_version(version):\n    \"\"\"Helper method to map version to appropriate handler class.\"\"\"\n    if version == (3, 0):\n        return Bolt3\n    elif version == (3, 5):\n        return AsyncBolt3\n    elif version == (4, 0):\n        return Bolt4x0\n    elif version == (4, 1):\n        return Bolt4x", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the protocol handlers\n        for version in [(3, 0), (3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5)]:\n            if version == protocol_version:\n                # Return the handler class for the specific version\n                if version == (3, 0):\n                    return {version: Bolt3}\n                elif version == (3, 5):\n                    return {version: Bolt3}\n                elif version == (4, 0):\n                    return {version: Bolt4x0}\n                elif version == (4, 1):\n                    return {version: Bolt4x1}\n                elif version == (4, 2):\n                    return {version: Bolt4x2}\n                elif version == (4, 3):\n                    return {version: Bolt4x3}\n                elif version == (4, 4):\n                    return {version: Bolt4x4}\n                elif version == (5, 0):\n                    return {version: Bolt5x0}\n                elif version == (5, 1):\n                    return {version: Bolt5x1}\n                elif version == (5, 2):\n                    return {version: Bolt5x2}\n                elif version == (5, 3):\n                    return {version:", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for handler_class in cls._get_all_handlers():\n            if handler_class.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler_class\n                break\n        return handlers\n    \n    # Return all supported versions\n    handlers = {}\n    for handler_class in cls._get_all_handlers():\n        version = handler_class.PROTOCOL_VERSION\n        handlers[version] = handler_class\n    return handlers\n\ndef _get_all_handlers():\n    \"\"\"Helper method to get all protocol handler classes.\"\"\"\n    # This is a placeholder - in a real implementation, this would\n    # return all the Bolt handler classes defined in the module\n    # For example:\n    # return [Bolt3, AsyncBolt3, Bolt4x1, AsyncBolt4x1, ...]\n    return []", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for handler_class in cls._handlers.values():\n            if handler_class.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler_class\n                break\n        return handlers\n    \n    # Return all supported versions if no specific version is requested\n    return {version: handler_class for version, handler_class in cls._handlers.items()}"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 179804, "num_retrieved": 500, "inference_time": 24.732524782419205, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if not values:\n        return cls()\n    \n    # Convert the iterable to a set to ensure uniqueness\n    raw_values = set(values)\n    \n    # Create and return a Bookmarks instance with the raw values\n    return cls(raw_values)", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if not values:\n        return cls()\n    \n    # Convert the iterable to a set to ensure uniqueness\n    raw_values = set(values)\n    \n    # Create and return a Bookmarks instance with the raw values\n    return cls(raw_values)", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if isinstance(values, Bookmarks):\n        return values\n    return Bookmarks(tuple(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if isinstance(values, Bookmarks):\n        return values\n    return Bookmarks(tuple(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if isinstance(values, Bookmarks):\n        return values\n    return Bookmarks(tuple(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 118829, "num_retrieved": 376, "inference_time": 24.732524782419205, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if hasattr(seq, \"run\") and callable(seq.run):\n        # seq is already a sequence\n        return (seq, \"sequence\")\n    elif hasattr(seq, \"__iter__\"):\n        # seq is an iterable, convert to a sequence\n        try:\n            # Try to create a Sequence from the iterable\n            from . import Sequence\n            seq_obj = Sequence(seq)\n            return (seq_obj, \"sequence\")\n        except Exception:\n            # If conversion fails, return the original as a sequence\n            return (seq, \"sequence\")\n    else:\n        # seq is a simple element, not a sequence\n        return (seq, \"element\")", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return (seq, \"sequence\")\n    elif isinstance(seq, (tuple, list)):\n        # Convert to a sequence if it's a tuple or list\n        seq_obj = lena.core.Sequence(*seq)\n        return (seq_obj, \"sequence\")\n    elif callable(seq):\n        # If it's callable, it's likely a generator or function\n        return (seq, \"callable\")\n    else:\n        # For any other type, treat it as a simple element\n        return (seq, \"element\")", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return (seq, \"sequence\")\n    elif isinstance(seq, (tuple, list)):\n        # Convert to a sequence if it's a tuple or list\n        seq_obj = lena.core.Sequence(*seq)\n        return (seq_obj, \"sequence\")\n    elif callable(seq):\n        # If it's callable, it's likely a generator or function\n        # Wrap it in a Sequence\n        seq_obj = lena.core.Sequence(seq)\n        return (seq_obj, \"callable\")\n    else:\n        # For other types, try to convert to a sequence\n        # If it's a single element, wrap it in a Sequence\n        seq_obj = lena.core.Sequence(seq)\n        return (seq_obj, \"element\")", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if hasattr(seq, \"run\") and callable(seq.run):\n        # seq is already a sequence\n        seq_type = \"sequence\"\n        return seq, seq_type\n    \n    if isinstance(seq, tuple):\n        # unpack tuple if it's a single element\n        if len(seq) == 1:\n            seq = seq[0]\n        else:\n            # create a sequence from the tuple\n            seq = Sequence(*seq)\n            seq_type = \"sequence\"\n            return seq, seq_type\n    \n    # If seq is a callable, try to convert it to a sequence\n    if callable(seq):\n        # Try to create a sequence from the callable\n        try:\n            seq = Sequence(seq)\n            seq_type = \"sequence\"\n            return seq, seq_type\n        except Exception:\n            # If conversion fails, raise an error\n            raise lena.core.LenaTypeError(\n                \"arguments must implement run method, or be callable generators (convertible to Run), \"\n                \"{} given\".format(seq)\n            )\n    \n    # If seq is not a sequence or callable, raise an error\n    raise lena.core.LenaTypeError(\n        \"arguments must implement run method, or be callable generators (convertible to Run), \"\n        \"{} given\".format(seq)\n    )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if hasattr(seq, \"run\") and callable(seq.run):\n        # seq is already a sequence\n        return (seq, \"sequence\")\n    elif hasattr(seq, \"__iter__\"):\n        # seq is an iterable, convert to a sequence\n        try:\n            # Try to create a Sequence from the iterable\n            from . import Sequence\n            seq_obj = Sequence(seq)\n            return (seq_obj, \"sequence\")\n        except Exception:\n            # If Sequence creation fails, try to determine type\n            # Check if it's a tuple of sequences\n            if isinstance(seq, tuple):\n                return (seq, \"tuple\")\n            else:\n                return (seq, \"iterable\")\n    else:\n        # seq is a single element\n        return (seq, \"element\")"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 116964, "num_retrieved": 378, "inference_time": 24.732524782419205, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # If scale was not computed before, compute it\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute current integral\n        current_integral = self.integral(self.bins, self.edges)\n        # Rescale by multiplying by other/current_integral\n        scale_factor = other / current_integral\n        # Apply scale factor to bins\n        for i, bin_content in enumerate(self.bins):\n            self.bins[i] = bin_content * scale_factor\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale if not computed or if recompute is True\n        if self._scale is None and not recompute:\n            # Compute scale if not already computed\n            if self._scale is None:\n                self._scale = integral(self.bins, self.edges)\n        return self._scale\n    \n    # If other is a float, rescale to that value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Check if scale is zero (can't be rescaled)\n    if self._scale == 0:\n        raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n    \n    # Rescale the histogram\n    scale_factor = other / self._scale\n    # Apply scale factor to bins (assuming bins are values)\n    for bin_content in self.bins:\n        bin_content *= scale_factor\n    \n    # Update the scale\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # Only compute if not already computed\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute current integral\n        current_integral = self.integral(self.bins, self.edges)\n        # Rescale by multiplying by other/current_integral\n        scale_factor = other / current_integral\n        # Apply scale factor to bins\n        for bin_content in self.bins:\n            bin_content *= scale_factor\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # Only compute if not already computed\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute current integral\n        current_integral = self.integral(self.bins, self.edges)\n        # Calculate scaling factor\n        scale_factor = other / current_integral\n        # Apply scaling to bins\n        for bin_content in self.bins:\n            bin_content *= scale_factor\n        # Update scale\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # Only compute if scale was not computed before\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute current integral to get current scale\n        current_scale = self.integral(self.bins, self.edges)\n        # Rescale by multiplying by (other / current_scale)\n        if current_scale != 0:\n            scale_factor = other / current_scale\n            # Apply scale factor to bins\n            for bin_content in self.bins:\n                bin_content *= scale_factor\n            self._scale = other\n        else:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 114494, "num_retrieved": 363, "inference_time": 24.732524782419205, "generate_results": ["def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        # Assuming the graph has a scale attribute or method to get scale\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    # Get the number of dimensions in the graph\n    dim = len(self.field_names) if hasattr(self, 'field_names') else 1\n    \n    # Only rescale the last coordinate\n    last_coord_index = dim - 1\n    \n    # If the graph has a scale attribute, update it\n    if hasattr(self, '_scale'):\n        self._scale = other\n    \n    # If the graph has a method to rescale, use it\n    # Otherwise, assume the scale is stored in a field\n    if hasattr(self, 'scale'):\n        self.scale(other)\n    \n    # If the graph has coordinates, rescale the last coordinate\n    if hasattr(self, 'coords') and hasattr(self, 'coords'):\n        # Rescale the last coordinate\n        if dim > 1:\n            # Rescale the last coordinate\n            last_coord = self.coords[last_coord_index]\n            # Rescale the last coordinate by the factor 'other'\n            if hasattr(last_coord, '__len__'):\n                # If the coordinate is a list, rescale each element\n                for i in range(len(last_coord)):\n                    last_coord[i] *= other\n            else:\n                # If the coordinate is a single value, rescale it\n                last_coord *= other\n    \n    # If the graph has errors, rescale them together with their coordinate\n    if hasattr(self, 'error') and hasattr(self, 'error'):\n        # Rescale the errors\n        if dim > 1:\n            # Rescale the last error coordinate\n            last_error_index =", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaTypeError(\"scale must be a number\")\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) > 0:\n        last_field = field_names[-1]\n        \n        # Rescale the last coordinate and its associated errors\n        # If errors exist, rescale them as well\n        if hasattr(self, 'coords') and len(self.coords) > 0:\n            # Get the index of the last field\n            last_index = field_names.index(last_field)\n            \n            # Rescale the coordinate values\n            for i in range(len(self.coords[0])):\n                self.coords[0][i] = self.coords[0][i] * other\n                \n            # If errors exist, rescale them\n            if len(self.coords) > 1:\n                for i in range(len(self.coords[1])):\n                    self.coords[1][i] = self.coords[1][i] * other\n                    \n        # Update the scale\n        self._scale = other\n    else:\n        raise lena.core.LenaValueError(\"graph has no coordinates to scale\")\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaTypeError(\"scale must be a number\")\n    \n    # Get the field names from the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) > 0:\n        last_field = field_names[-1]\n        \n        # Rescale the last coordinate and its errors\n        if hasattr(self, 'coords') and len(self.coords) > 0:\n            # Get the last coordinate data\n            last_coord = self.coords[-1]\n            \n            # Scale the coordinate values\n            if isinstance(last_coord, (list, tuple)):\n                scaled_values = [val * other for val in last_coord]\n                # Update the coordinates\n                self.coords[-1] = scaled_values\n                \n            # Scale the errors if they exist\n            if hasattr(self, 'errors') and len(self.errors) > 0:\n                error_index = self._error_index if hasattr(self, '_error_index') else -1\n                if error_index >= 0 and error_index < len(self.errors):\n                    scaled_errors = [val * other for val in self.errors[error_index]]\n                    self.errors[error_index] = scaled_errors\n    \n    # Update the scale attribute\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaTypeError(\"scale must be a number\")\n    \n    # Get the field names from the graph\n    field_names = self.field_names\n    \n    # Only rescale the last coordinate\n    last_field = field_names[-1]\n    \n    # Rescale the last coordinate and its errors (if any)\n    if hasattr(self, 'coords') and len(self.coords) > 0:\n        # Get the last coordinate values\n        last_coords = self.coords[-1]\n        \n        # Rescale the last coordinate\n        if hasattr(self, 'errors') and len(self.errors) > 0:\n            # Rescale errors as well\n            last_errors = self.errors[-1]\n            scaled_coords = [val * other for val in last_coords]\n            scaled_errors = [val * other for val in last_errors]\n        else:\n            scaled_coords = [val * other for val in last_coords]\n        \n        # Update the graph with scaled values\n        self.coords[-1] = scaled_coords\n        \n        # If errors exist, update them as well\n        if hasattr(self, 'errors') and len(self.errors) > 0:\n            self.errors[-1] = scaled_errors\n    \n    # Update the scale attribute\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaTypeError(\"scale must be a number\")\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Only rescale the last coordinate\n    last_field = field_names[-1]\n    \n    # Rescale the last coordinate and its errors\n    if hasattr(self, 'coords') and len(self.coords) > 0:\n        # Get the last coordinate values\n        last_coords = self.coords[-1]\n        \n        # Rescale the last coordinate\n        if hasattr(self, 'errors') and len(self.errors) > 0:\n            # Rescale the errors as well\n            last_errors = self.errors[-1]\n            scaled_errors = [other * err for err in last_errors]\n            # Update the errors\n            self.errors[-1] = scaled_errors\n        \n        # Rescale the last coordinate\n        scaled_coords = [other * coord for coord in last_coords]\n        self.coords[-1] = scaled_coords\n    \n    # Update the scale\n    self._scale = other\n    \n    return self._scale"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 117516, "num_retrieved": 395, "inference_time": 24.732524782419205, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(len(edges))]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(len(edges))]\n    elif get_coordinate == \"middle\":\n        bin_coords = [(edges[i][0] + edges[i][-1]) / 2 for i in range(len(edges))]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Create points from histogram bins\n    points = []\n    for i in range(len(edges[0])):\n        for j in range(len(edges[1])):\n            # Get the bin content\n            bin_content = hist.bins[i][j]\n            \n            # Apply make_value to get the point value\n            point_value", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import copy\n    \n    # Default make_value is the bin content\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n    \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    dim = len(edges)\n    \n    # Get bin coordinates based on get_coordinate\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(dim)]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(dim)]\n    elif get_coordinate == \"middle\":\n        bin_coords = [edges[i][len(edges[i]) // 2] for i in range(dim)]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Create points list\n    points = []\n    \n    # Iterate through bins\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n        \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(len(edges))]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(len(edges))]\n    elif get_coordinate == \"middle\":\n        bin_coords = [edges[i][len(edges[i]) // 2] for i in range(len(edges))]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Create points from histogram bins\n    points = []\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # For 2D histogram, use bin coordinates\n            x_coord = edges[0][i]\n            y_coord = edges", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import copy\n    \n    # Default make_value is the bin content\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n    \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    dim = len(edges)\n    \n    # Get bin coordinates based on get_coordinate\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(dim)]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(dim)]\n    elif get_coordinate == \"middle\":\n        bin_coords = [edges[i][len(edges[i]) // 2] for i in range(dim)]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Create points list\n    points = []\n    \n    # Iterate through bins\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Determine the coordinate based on get_coordinate\n    if get_coordinate == \"left\":\n        bin_edges = hist.edges\n        coords = [edges[0] for edges in bin_edges]\n        coords = [edges[0] for edges in bin_edges]\n    elif get_coordinate == \"right\":\n        coords = [edges[-1] for edges in bin_edges]\n    elif get_coordinate == \"middle\":\n        coords = [(edges[0] + edges[-1]) / 2 for edges in bin_edges]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right' or 'middle', got {get_coordinate}\")\n    \n    # Create points from histogram bins\n    points = []\n    for i, bin_content in enumerate(hist.bins):\n        point_value = make_value(bin_content)\n        points.append((coords[i], point_value))\n    \n    # Determine scale\n    if scale is True:\n        scale = hist.scale"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 99881, "num_retrieved": 469, "inference_time": 24.732524782419205, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"method\", \"missing method\", iface, candidate))\n\n    # Step 3: Check if methods have correct signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if hasattr(method, '__func__'):\n                    # Check method signature\n                    try:\n                        method_sig = method.__func__.__code__.co_varnames\n                        # Compare with expected signature\n                        expected_sig = desc.getSignatureString()\n                        # This is a simplified check - in practice, we'd need\n                        # more sophisticated signature comparison\n                    except (AttributeError, TypeError):\n                        errors.append(BrokenMethodImplementation(name,", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"method\", \"missing method\", iface, candidate))\n\n    # Step 3: Check if methods have correct signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if hasattr(method, '__func__'):\n                    # Check signature if possible\n                    try:\n                        sig = method.__func__.__code__.co_varnames\n                        # This is a simplified check - actual signature comparison\n                        # would require more detailed analysis\n                        pass\n                    except (AttributeError, TypeError):\n                        pass\n                else:\n                    # Method is not callable, check if it's a valid attribute\n                    pass", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"method\", \"missing method\", iface, candidate))\n\n    # Step 3: Check that methods have correct signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n                # Check signature if possible\n                try:\n                    if hasattr(method, '__code__'):\n                        # Compare method signature with interface definition\n                        # This is a simplified check - in practice, more detailed\n                        # signature comparison would be needed\n                        pass\n                except AttributeError:\n                    pass", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"method\", \"missing method\", iface, candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n                # Check signature if possible\n                try:\n                    if hasattr(method, '__signature__'):\n                        # Compare signatures if possible\n                        pass\n                except AttributeError:\n                    pass\n    except AttributeError:\n        pass\n\n    # Step 4: Check necessary attributes\n    try:\n        for name, desc in iface", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"methods\", \"missing methods\", iface, candidate))\n\n    # Step 3: Check if methods have correct signature\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__signature__'):\n                    # Try to get signature from function\n                    try:\n                        sig = method.__code__.co_varnames\n                        # Check if signature matches\n                        if not desc.getSignatureString():\n                            errors.append(BrokenMethodImplementation(name, \"incorrect signature\", iface, candidate))\n                    except (AttributeError, TypeError):\n                        errors.append(BrokenMethodImplementation(name, \"incorrect signature\", iface"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 99879, "num_retrieved": 469, "inference_time": 24.732524782419205, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n        else:\n            method = getattr(candidate, name)\n            if not hasattr(method, '__call__'):\n                errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n            try:\n                # Check method signature\n                if hasattr(desc, 'getSignatureString'):\n                    method_sig = desc.getSignatureString()\n                    method_sig_actual = method.__code__.co_varnames\n                    # This is a simplified check - in practice, we'd need to compare\n                    # the actual signature with the expected one\n                    pass\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, \"invalid method signature\", iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate,", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n        else:\n            method = getattr(candidate, name)\n            if not hasattr(method, '__call__'):\n                errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n            try:\n                # Check method signature\n                if hasattr(desc, 'getSignatureString'):\n                    method_sig = method.getSignatureString()\n                    desc_sig = desc.getSignatureString()\n                    if method_sig != desc_sig:\n                        errors.append(BrokenMethodImplementation(name, \"signature mismatch\", iface, candidate))\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, \"signature error\", iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name in iface.names(all=True):\n        method = getattr(candidate, name, None)\n        if method is None:\n            errors.append(BrokenMethodImplementation(name, \"missing\", iface, candidate))\n        else:\n            # Check method signature\n            try:\n                # For methods, we need to check the signature\n                # This is a simplified check - in practice, we'd need to\n                # compare the actual signature with the interface definition\n                pass\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, str(e), iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name in iface.names(all=True):\n        attr = getattr(candidate, name, None)\n        if attr is None:\n            errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Return True if no errors, otherwise raise Invalid with all errors\n    if errors:\n        if len(errors) == 1:\n            raise errors[0]", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name in iface.names(all=True):\n        method = getattr(candidate, name, None)\n        if method is None:\n            errors.append(BrokenMethodImplementation(name, \"missing\", iface, candidate))\n        else:\n            # Check method signature\n            try:\n                # Get the method's signature from the interface\n                method_spec = iface.direct(name)\n                if method_spec is None:\n                    continue\n                \n                # For methods, we need to check the signature\n                # This is a simplified check - in practice, we'd need to\n                # compare the actual method signature with the interface specification\n                pass\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, str(e), iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name in iface.names(all=True):\n        attr = getattr(candidate, name, None)\n        if attr is None:\n            errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Return True if", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n\n    # Step 3: Check method signatures\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if hasattr(candidate, name):\n            method = getattr(candidate, name)\n            if not hasattr(method, '__call__'):\n                errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n            try:\n                # Check if method signature matches\n                method_sig = method.getSignatureString()\n                desc_sig = desc.getSignatureString()\n                if method_sig != desc_sig:\n                    errors.append(BrokenMethodImplementation(name, \"signature mismatch\", iface, candidate))\n            except AttributeError:\n                errors.append(BrokenMethodImplementation(name, \"signature error\", iface, candidate))\n\n    # Step 4: Check all necessary attributes\n    for name, desc in iface.namesAndDescriptions(all=True"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 83123, "num_retrieved": 340, "inference_time": 24.732524782419205, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 80344, "num_retrieved": 319, "inference_time": 24.732524782419205, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 11633, "num_retrieved": 62, "inference_time": 24.732524782419205, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 9729, "num_retrieved": 47, "inference_time": 24.732524782419205, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 24.732524782419205, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 12.381421372294426, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 10009, "num_retrieved": 48, "inference_time": 12.381421372294426, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    value = self[key]\n    del self[key]\n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 28011, "num_retrieved": 72, "inference_time": 12.381421372294426, "generate_results": ["def _create_in_regex(self) -> Pattern[str]:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the in-style parameters\n    # This pattern will match in-style parameters according to the style's param_regex\n    # and escape sequence\n    pattern = self.param_regex\n    \n    # If there's an escape character, we need to account for it in the pattern\n    # The pattern should match the escape sequence and the parameter\n    if self.escape_char:\n        # Escape the escape character in the pattern\n        escape_char = self.escape_char\n        # The pattern should match the escape character followed by the parameter\n        # and the escape sequence should be matched with the escape_regex\n        pattern = self.escape_regex + pattern\n        \n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is defined by the param_regex attribute\n    # of the style instance, which is set during initialization\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is not explicitly defined in the context\n    # However, based on the pattern of parameter matching in the code, we can infer\n    # that it would match named, numeric, or ordinal parameters with their respective formats\n    # This is a placeholder implementation that would typically be defined in the class\n    # based on the specific parameter style being used\n    \n    # Since the exact pattern is not provided in the context, we return a dummy pattern\n    # that would match named parameters in the format {name} or :name\n    # This is a simplified version that would need to be adapted to the actual requirements\n    \n    import re\n    \n    # Pattern to match named parameters (e.g., :name, {name})\n    named_pattern = r'(?P<param>(?P<out_percent>%%)|(?P<escape>[^\\\\]?)\\\\(?P<escape_char>[^\\\\]+)|(?P<param>[a-zA-Z_][a-zA-Z0-9_]*)\\s*[:{]?(?P<param_name>[a-zA-Z_][a-zA-Z0-9_]*)[}])'\n    \n    # Pattern to match numeric parameters (e.g., ?1, ?2)\n    numeric_pattern = r'(?P<param>(?P<out_percent>%%)|(?P<escape>[^\\\\]?)\\\\(?P<escape_char>[^\\\\]+)|(?P<param>\\?(\\d+)))'\n    \n    # Pattern to match ordinal parameters (e.g., %s, %d)\n    ordinal_pattern = r'(?P<param>(?P<out_percent>%%)|(?P<escape>[^\\\\]?)\\\\(?P<escape_char>[^\\\\]+)|(?P<param>%[sd]))'\n    \n    # Combine patterns with alternation\n    pattern = f'({named_pattern}|{numeric_pattern}|{ordinal_pattern})'\n    \n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regex is constructed based on the param_regex attribute\n    # of the style instance, which defines how parameters are matched in the SQL string\n    # This function returns a compiled regular expression pattern for matching in-style parameters\n    import re\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is not explicitly defined in the context\n    # However, based on the pattern of parameter matching in the code, we can infer\n    # that it would match parameter placeholders in the SQL query.\n    # Since the exact pattern is not provided in the context, we return a placeholder\n    # that would match named, numeric, or ordinal parameters based on the style.\n    \n    # This is a generic implementation that would match parameter patterns\n    # based on the style's param_regex attribute\n    return self.param_regex"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 27900, "num_retrieved": 71, "inference_time": 12.381421372294426, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 56503, "num_retrieved": 217, "inference_time": 12.381421372294426, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 50336, "num_retrieved": 190, "inference_time": 12.381421372294426, "generate_results": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 53147, "num_retrieved": 194, "inference_time": 12.381421372294426, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For unambiguous datetimes, we can directly convert from UTC\n    # First, check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine which fold to use\n        fold = self._fold(dt)\n        # Use the fold state to determine which time to return\n        # The fold state indicates whether we're in the first or second occurrence\n        # of an ambiguous time\n        if fold:\n            # Return the second occurrence (fold=1)\n            return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)\n        else:\n            # Return the first occurrence (fold=0)\n            return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)\n    \n    # For non-ambiguous datetimes, simply convert\n    return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine which fold to use\n        # The fold is determined by the fold attribute of the datetime\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence (the \"fold\" version)\n        # Otherwise, we're in the first occurrence\n        # We need to adjust the datetime to the correct fold\n        # This is handled by the _resolve_ambiguous_time function\n        idx = self._find_last_transition(dt)\n        if idx is None or idx == 0:\n            # No transitions found, just return the original datetime\n            return dt\n        \n        # Adjust the index based on fold\n        idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n        adjusted_idx = idx - idx_offset\n        \n        # Create a new datetime with the adjusted index\n        # This effectively resolves the ambiguity\n        new_dt = dt.replace(tzinfo=None)\n        new_dt = new_dt.replace(tzinfo=self)\n        \n        # Apply the transition to get the correct time\n        # This is a simplified version - the actual implementation\n        # would use the transition data to compute the correct offset\n        return new_dt\n    \n    # If not ambiguous, simply return the datetime with the new timezone\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For timezone-aware datetimes, we need to convert from UTC to the local timezone\n    # First, check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine which fold to use\n        fold = self._fold(dt)\n        # Use the fold value to determine which of the two possible times to return\n        # The fold value is 0 for the first occurrence (earlier) and 1 for the second occurrence (later)\n        # We need to adjust the datetime based on the fold\n        # The standard approach is to use the fold value to determine which time to return\n        # If fold is 0, we return the earlier time (standard time)\n        # If fold is 1, we return the later time (daylight saving time)\n        # However, we need to be careful about the actual time calculation\n        # We'll use the _resolve_ambiguous_time method to determine the correct index\n        idx = self._find_last_transition(dt)\n        if idx is None or idx == 0:\n            # No transitions found, return the original datetime\n            return dt\n        # Adjust the index based on fold\n        idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n        # Return the datetime with the adjusted index\n        return dt.replace(tzinfo=self) - self._dst_saved * idx_offset\n    else:\n        # If not ambiguous, simply return the datetime with the local timezone\n        return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine the fold state\n        fold = self._fold(dt)\n        # Return the appropriate time based on fold state\n        # For fold=0 (first occurrence), use the standard time\n        # For fold=1 (second occurrence), use the daylight time\n        if fold == 1:\n            # This is the second occurrence (daylight time)\n            # We need to adjust to the correct offset\n            return dt + self.dst(dt)\n        else:\n            # This is the first occurrence (standard time)\n            return dt\n    else:\n        # Not ambiguous, just return the datetime with the correct offset\n        return dt + self.utcoffset(dt)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For UTC-aware timezones, return the original dt\n    if self is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine the fold state\n        fold = self._fold(dt)\n        # Return the appropriate time based on fold state\n        # The fold state indicates which of the two possible times to use\n        # (the first or second occurrence of the ambiguous time)\n        return dt.replace(fold=not fold)\n    \n    # If not ambiguous, simply return the original datetime\n    return dt"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 60209, "num_retrieved": 230, "inference_time": 12.381421372294426, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 19049, "num_retrieved": 67, "inference_time": 12.381421372294426, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 107425, "num_retrieved": 382, "inference_time": 12.381421372294426, "generate_results": ["def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') or request.headers.get('X-Diaspora-Private-Message'):\n        # Check for legacy Diaspora message headers\n        if request.headers.get('X-Diaspora-Message'):\n            return \"diaspora\"\n    \n    # Check if the request body contains a legacy Diaspora magic envelope\n    body = request.body\n    if isinstance(body, str):\n        # Check for magic envelope signature pattern\n        if b\"application/xml\" in body.encode(\"utf-8\") or b\"magic-env\" in body.encode(\"utf-8\"):\n            return \"diaspora\"\n    \n    # Check if the request URL contains a Diaspora path pattern\n    url = request.url\n    if url and (\"diaspora\" in url.lower() or \"receive\" in url.lower()):\n        return \"diaspora\"\n    \n    # Check if the request has a Diaspora-specific header\n    if request.headers.get('X-Diaspora-Handle'):\n        return \"diaspora\"\n    \n    # Check if the request has a Diaspora-specific content type\n    content_type = request.headers.get('Content-Type', '')\n    if 'diaspora' in content_type.lower():\n        return \"diaspora\"\n    \n    # If none of the above, try to identify based on the request method and path\n    if request.method == 'POST' and ('receive' in request.url.lower() or 'fetch' in request.url.lower()):\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'private':\n        return \"diaspora\"\n    \n    # Check if the request body contains a legacy Diaspora magic envelope signature\n    body = request.body\n    if isinstance(body, str):\n        # Look for the magic envelope signature pattern\n        if 'magic-env' in body.lower() or 'diaspora' in body.lower():\n            return \"diaspora\"\n    \n    # Check if the request URL contains a Diaspora-specific path\n    if request.url and ('diaspora' in request.url.lower() or '/receive/' in request.url):\n        return \"diaspora\"\n    \n    # Check if the request method is a Diaspora-specific method\n    if request.method in ['POST', 'PUT'] and request.headers.get('Content-Type', '').startswith('application/xml'):\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'private':\n        return \"diaspora\"\n    \n    # Check if the request body contains a legacy Diaspora magic envelope\n    body = request.body\n    if isinstance(body, str):\n        # Look for the magic envelope signature pattern\n        if b\"application/xml\" in body.encode(\"utf-8\") or b\"magic-env\" in body.encode(\"utf-8\"):\n            return \"diaspora\"\n    \n    # Check if the request URL contains a Diaspora-specific path\n    url = request.url\n    if url and any(path in url for path in [\"/diaspora/\", \"/receive/users/\", \"/fetch/\", \"/receive/public\"]):\n        return \"diaspora\"\n    \n    # Check if the request has a Diaspora-specific header\n    if request.headers.get('X-Diaspora-Handle') or request.headers.get('X-Diaspora-Handle-Id'):\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'public':\n        return \"diaspora\"\n    \n    # Check if the request has a private message\n    if request.headers.get('X-Diaspora-Message') == 'private':\n        return \"diaspora\"\n    \n    # Check if the request has a legacy payload (public message with specific headers)\n    if request.headers.get('X-Diaspora-Message') == 'legacy':\n        return \"diaspora\"\n    \n    # Check if the request body contains a public message (legacy format)\n    body = request.body\n    if isinstance(body, str) and 'public' in body.lower():\n        return \"diaspora\"\n    \n    # Check if the request body contains a private message (legacy format)\n    if isinstance(body, str) and 'private' in body.lower():\n        return \"diaspora\"\n    \n    # If none of the above conditions are met, return None\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') or request.headers.get('X-Diaspora-Private-Message'):\n        # Legacy Diaspora message headers\n        return \"diaspora\"\n    \n    # Check if the request body contains a public message (legacy format)\n    body = request.body\n    if isinstance(body, str):\n        # Check for legacy public message format\n        if \"public\" in body.lower() or \"diaspora\" in body.lower():\n            return \"diaspora\"\n    \n    # Check if the request has a private message header\n    if request.headers.get('X-Diaspora-Private-Message'):\n        return \"diaspora\"\n    \n    # Check if the request URL contains a diaspora pattern\n    url = request.url\n    if url and ('diaspora' in url.lower() or 'diaspora.' in url.lower()):\n        return \"diaspora\"\n    \n    # Check if the request method is POST and has a specific content type\n    if request.method == 'POST' and request.headers.get('Content-Type', '').startswith('application/diaspora'):\n        return \"diaspora\"\n    \n    # Default to no protocol found\n    return None"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 94313, "num_retrieved": 297, "inference_time": 12.381421372294426, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths\n    if request.url.startswith('/_matrix/'):\n        return True\n    \n    # Check if the request headers contain Matrix-specific headers\n    if request.headers.get('X-Matrix-Server') or request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check if the request method is a Matrix-specific method\n    if request.method in ['POST', 'PUT', 'DELETE'] and request.url.startswith('/_matrix/'):\n        return True\n    \n    # Check if the request contains Matrix-specific parameters\n    if 'matrix' in request.url or 'matrix' in request.headers.get('X-Request-Path', ''):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Check for matrix-specific paths like /_matrix/client/r0\n    if '/_matrix/client/' in url or '/_matrix/federation/' in url:\n        return True\n    \n    # Check for matrix-specific domains or subdomains\n    if 'matrix' in url.lower():\n        return True\n    \n    # Check if the request method is a known Matrix endpoint\n    if request.method in ['POST', 'GET'] and '/_matrix/' in url:\n        return True\n    \n    # Check if the request headers contain matrix-specific information\n    if request.headers.get('X-Matrix-Client-Id'):\n        return True\n    \n    # Check if the request headers contain matrix-specific authentication\n    if request.headers.get('Authorization') and 'matrix' in request.headers.get('Authorization', '').lower():\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Check for common Matrix endpoints like /_matrix/client/r0\n    if '/_matrix/client/' in url or '/_matrix/federation/' in url:\n        return True\n    \n    # Check for Matrix-specific query parameters\n    if 'access_token' in request.headers.get('Authorization', '') or 'matrix' in request.headers.get('X-Matrix-Client', ''):\n        return True\n    \n    # Check if the request method is one that Matrix typically uses\n    if request.method in ['POST', 'GET']:\n        # Matrix typically uses POST for sending messages\n        if request.method == 'POST' and 'content' in request.body.decode('utf-8', errors='ignore'):\n            return True\n    \n    # Check if the request contains Matrix-specific headers\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check if the request URL contains a Matrix server domain\n    matrix_domains = ['matrix.', 'matrix.org', 'matrix.io', 'matrix.hub']\n    for domain in matrix_domains:\n        if domain in url:\n            return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths\n    if request.url.startswith('/_matrix/'):\n        return True\n    \n    # Check if the request headers contain Matrix-specific headers\n    if request.headers.get('X-Matrix-Server') or request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check if the request method is one of the Matrix-specific methods\n    if request.method in ['POST', 'PUT', 'DELETE']:\n        # Matrix often uses these methods for operations\n        # Check if the URL contains matrix-specific endpoints\n        if any(path in request.url for path in [\n            '/_matrix/client/r0/',\n            '/_matrix/federation/v1/',\n            '/_matrix/room/',\n            '/_matrix/media/r0/'\n        ]):\n            return True\n    \n    # Check if the request body contains Matrix-specific content\n    if request.body and b'matrix' in request.body:\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Matrix requests typically have paths like /_matrix/client/r0 or /_matrix/federation\n    matrix_paths = [\n        '/_matrix/client/r0',\n        '/_matrix/federation',\n        '/_matrix/clients',\n        '/_matrix/rooms',\n        '/_matrix/identity',\n        '/_matrix/server'\n    ]\n    \n    # Check if the URL contains any matrix paths\n    for path in matrix_paths:\n        if path in url:\n            return True\n    \n    # Check if the URL contains matrix domains or subdomains\n    matrix_domains = [\n        'matrix.org',\n        'matrix.org',\n        'matrix.example.com',\n        'matrix.domain.tld'\n    ]\n    \n    # Check if the domain matches any matrix domain\n    domain = urlsplit(url).netloc\n    for domain_name in matrix_domains:\n        if domain_name in domain:\n            return True\n    \n    # Check if the request headers contain matrix-related information\n    if request.headers.get('X-Matrix-Client-Id'):\n        return True\n    \n    # Check if the request headers contain matrix-specific authentication\n    if request.headers.get('Authorization') and 'matrix' in request.headers.get('Authorization', ''):\n        return True\n    \n    return False"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 93957, "num_retrieved": 298, "inference_time": 12.381421372294426, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 113699, "num_retrieved": 426, "inference_time": 12.381421372294426, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the rest of the code block\n            continue\n            \n        # Find all hashtags in the line\n        tag_matches = re.finditer(r'#(\\w+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                result = result.replace(f'#{tag}', replacer(tag))\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the code block\n            continue\n            \n        # Find all hashtags in the line\n        # Use a more robust pattern to handle cases like #tag#tag\n        import re\n        tag_matches = re.finditer(r'#([^\\s#]+)', line)\n        for match in tag_matches:\n            tag = match.group(1).lower()\n            tags.add(tag)\n            \n            # If a replacer is provided, replace the tag\n            if replacer:\n                result = result.replace(f'#{tag}', replacer(tag))\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Toggle code block state\n            in_code_block = not in_code_block\n            continue\n            \n        # If inside a code block, skip processing\n        if in_code_block:\n            continue\n            \n        # Find all hashtags in the line\n        tag_matches = re.finditer(r'#(\\w+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                result = result.replace(f'#{tag}', replacer(tag))\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the code block and continue to next line\n            continue\n            \n        # Find all hashtags in the line\n        tag_matches = re.finditer(r'#(\\w+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag with the result of the replacer function\n            if replacer:\n                result = result.replace(f'#{tag}', replacer(tag))\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Pattern to match hashtags and mentions\n    # This pattern will match #tag and @mention, but we need to handle code blocks\n    # Code blocks are enclosed in triple backticks or triple quotes\n    # We'll use a regex to find code blocks and skip them\n    code_block_pattern = r'```(?:\\w+)?\\s*(.*?)\\s*```'\n    code_blocks = []\n    \n    # Find all code blocks\n    matches = re.finditer(code_block_pattern, result, re.DOTALL)\n    for match in matches:\n        code_blocks.append(match.group(1))\n    \n    # Replace code blocks with a placeholder to avoid interference\n    for code in code_blocks:\n        # Replace code block with a placeholder\n        result = result.replace(f\"```{code}```\", \"CODE_BLOCK_PLACEHOLDER\")\n    \n    # Now find tags in the text\n    # We'll use a regex to find hashtags and mentions\n    # Hashtags: #tag\n    # Mentions: @handle\n    tag_pattern = r'(?<!\\w)#(\\w+)|(?<!\\w)@(\\w+)(?!\\w)'\n    \n    # Find all matches\n    matches = re.finditer(tag_pattern, result)\n    for match in matches:\n        tag = match.group(1) if match.group(1) else match.group(2)\n        if tag:\n            tags.add(tag)\n            # If a replacer is provided, replace the tag\n            if replacer:\n                result = result.replace(f\"#{tag}\" if match.group(1) else f\"@{tag}\", replacer(tag))\n    \n    # Restore code blocks\n    for code in code_blocks:\n        result = result.replace(\"CODE_BLOCK_PLACEHOLDER\", f\"```{code}```\")\n    \n    return tags, result"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 88754, "num_retrieved": 280, "inference_time": 12.381421372294426, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in the text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with mailto links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\">\\1</a>', text)\n    \n    # Replace hashtags with HTML links\n    hashtag_pattern = r'#([a-zA-Z0-9_]+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"/tags/\\1\" class=\"hashtag\">#\\1</a>', text)\n    \n    # Replace mentions with HTML links\n    mention_pattern = r'@([a-zA-Z0-9_]+)'\n    text = re.sub(mention_pattern, r'<a href=\"/user/\\1\" class=\"mention\">@\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs in the text with HTML links\n    import re\n    \n    # Pattern to match URLs (including http, https, and other protocols)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    \n    # Replace URLs with HTML anchor tags\n    def replace_url(match):\n        url = match.group(1)\n        # Add attributes like target=\"_blank\" and rel=\"noopener\"\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{url}</a>'\n    \n    # Replace URLs in the text\n    processed_text = re.sub(url_pattern, replace_url, text)\n    \n    return processed_text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Match email addresses\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Match hashtags\n    hashtag_pattern = r'(#\\w+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"/tags/\\1\" class=\"hashtag\">\\1</a>', text)\n    \n    # Match mentions\n    mention_pattern = r'(@\\w+)'\n    text = re.sub(mention_pattern, r'<a href=\"/users/\\1\" class=\"mention\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in the text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with mailto links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\">\\1</a>', text)\n    \n    # Replace any other text that might be a link (e.g., @mentions)\n    # This is a simple pattern to match text that looks like a handle\n    mention_pattern = r'@([a-zA-Z0-9._%+-]+)'\n    text = re.sub(mention_pattern, r'<a href=\"mailto:\\1\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in the text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with mailto links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\">\\1</a>', text)\n    \n    # Replace hashtags with HTML links\n    hashtag_pattern = r'#([a-zA-Z0-9_]+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"/tag/\\1\" class=\"hashtag\">#\\1</a>', text)\n    \n    # Replace mentions with HTML links\n    mention_pattern = r'@([a-zA-Z0-9_]+)'\n    text = re.sub(mention_pattern, r'<a href=\"/user/\\1\" class=\"mention\">@\\1</a>', text)\n    \n    return text"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 83747, "num_retrieved": 322, "inference_time": 12.381421372294426, "generate_results": ["def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except requests.exceptions.RequestException:\n        pass\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 82020, "num_retrieved": 257, "inference_time": 12.381421372294426, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain special characters except for alphanumeric and common punctuation\n    # Remove any non-alphanumeric characters except for hyphens and underscores\n    cleaned_tag = re.sub(r'[^a-zA-Z0-9\\-_]', '', tag.strip())\n    \n    # Tags should not be just a single character (except for single letters)\n    if len(cleaned_tag) == 1:\n        return cleaned_tag.isalpha()\n    \n    # Tags should not start or end with a hyphen or underscore\n    if cleaned_tag.startswith('-') or cleaned_tag.startswith('_') or \\\n       cleaned_tag.endswith('-') or cleaned_tag.endswith('_'):\n        return False\n    \n    # Tags should not contain consecutive hyphens or underscores\n    if '--' in cleaned_tag or '___' in cleaned_tag or '---' in cleaned_tag:\n        return False\n    \n    # Tags should not contain only numbers\n    if cleaned_tag.isdigit():\n        return False\n    \n    # Tags should not be longer than a reasonable length (e.g., 50 characters)\n    if len(cleaned_tag) > 50:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # A tag is typically a single word or a word with a single hashtag prefix\n    cleaned_tag = tag.strip()\n    \n    # Check if it starts with a hashtag (optional)\n    if cleaned_tag.startswith('#'):\n        cleaned_tag = cleaned_tag[1:]\n    \n    # Tags should be alphanumeric or contain only hyphens or underscores\n    # and should not contain spaces or special characters\n    if not cleaned_tag.isalnum() and not all(c in '-_' for c in cleaned_tag):\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(cleaned_tag) > 50:\n        return False\n    \n    # Tags should not be just a single character (except for short tags)\n    if len(cleaned_tag) == 1:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (only alphanumeric and hashtags)\n    # Remove any non-alphanumeric characters except for the hash symbol\n    cleaned_tag = tag.strip().replace('#', '')\n    \n    # Tags should only contain alphanumeric characters\n    if not cleaned_tag.isalnum():\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(tag) > 50:\n        return False\n    \n    # Tags should not start or end with a hash\n    if tag.startswith('#') or tag.endswith('#'):\n        return False\n    \n    # Tags should not contain multiple consecutive hashtags\n    if '##' in tag:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Remove any non-alphanumeric characters except for underscores and hyphens\n    cleaned_tag = re.sub(r'[^a-zA-Z0-9_\\-]', '', tag.strip())\n    \n    # Tags should be at least 1 character long after cleaning\n    if not cleaned_tag:\n        return False\n    \n    # Tags should not start or end with a hyphen or underscore\n    if cleaned_tag.startswith('-') or cleaned_tag.startswith('_') or \\\n       cleaned_tag.endswith('-') or cleaned_tag.endswith('_'):\n        return False\n    \n    # Tags should not be too long (e.g., more than 20 characters)\n    if len(cleaned_tag) > 20:\n        return False\n    \n    # Tags should not be a single character (e.g., \"a\", \"x\")\n    if len(cleaned_tag) == 1:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (only alphanumeric and hyphens)\n    # This is a simplified rule - in practice, tags might allow other characters\n    # but we'll keep it simple for now\n    if not tag.replace('-', '').isalnum():\n        return False\n    \n    # Tags should not start or end with a hyphen\n    if tag.startswith('-') or tag.endswith('-'):\n        return False\n    \n    # Tags should not be too long (arbitrary limit of 50 characters)\n    if len(tag) > 50:\n        return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 107219, "num_retrieved": 406, "inference_time": 18.891021892428398, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    children_dict = {}\n    for child in node:\n        children_dict[child.tag] = child.text\n    return children_dict", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 102132, "num_retrieved": 378, "inference_time": 18.891021892428398, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any extra whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Check if the handles match exactly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles to lowercase and remove any extra whitespace\n    sender_handle = sender_handle.strip().lower()\n    entity_handle = entity_handle.strip().lower()\n    \n    # Check if the handles match\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles to lowercase and remove any extra whitespace\n    sender_handle = sender_handle.strip().lower()\n    entity_handle = entity_handle.strip().lower()\n    \n    # Check if the handles match\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Check if the sender and entity handles match\n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 111389, "num_retrieved": 442, "inference_time": 18.891021892428398, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    # Create the NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Node\",\n            \"adminAccount\": \"admin@domain.tld\"\n        },\n        \"protocols\": [\n            \"activitypub\",\n            \"diaspora\"\n        ],\n        \"openRegistrations\": True,\n        \"services\": {\n            \"inbound\": [\n                \"https://example.com/inbox\"\n            ],\n            \"outbound\": [\n                \"https://example.com/outbox\"\n            ]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 150,\n                \"activeWeek\": 30\n            },\n            \"localPosts\": 5000,\n            \"localComments\": 2000\n        }\n    }\n    \n    # Add custom document path if specified\n    if document_path != \"/.well-known/nodeinfo\":\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    # Default document path if not provided\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    # Create the NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Node\",\n            \"adminAccount\": \"admin@domain.tld\"\n        },\n        \"protocols\": [\n            \"activitypub\",\n            \"diaspora\"\n        ],\n        \"openRegistrations\": True,\n        \"services\": {\n            \"inbound\": [\n                \"https://example.com/inbox\"\n            ],\n            \"outbound\": [\n                \"https://example.com/outbox\"\n            ]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 150,\n                \"activeWeek\": 30\n            },\n            \"localPosts\": 200,\n            \"localComments\": 50\n        }\n    }\n    \n    # Add custom document path if specified\n    if document_path != \"/.well-known/nodeinfo\":\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the context, it appears to be a placeholder or missing implementation\n    # The actual implementation would need to generate a NodeInfo document according to the spec\n    # which includes metadata about the server, software version, protocols supported, etc.\n    \n    # Since no specific implementation details are provided in the context,\n    # we return a minimal valid NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Server\",\n            \"adminAccount\": \"admin@localhost\"\n        },\n        \"protocols\": [\n            \"diaspora\"\n        ],\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": True,\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 100,\n                \"activeWeek\": 20\n            },\n            \"localPosts\": 5000,\n            \"localComments\": 2000\n        }\n    }\n    \n    # If a custom document path is provided, update the path in the document\n    if document_path:\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    # Base NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Node\",\n            \"adminAccount\": \"admin@domain.tld\"\n        },\n        \"protocols\": [\n            \"diaspora\"\n        ],\n        \"openRegistrations\": True,\n        \"services\": {\n            \"inbound\": [\n                \"https://example.com/receive/public\"\n            ],\n            \"outbound\": [\n                \"https://example.com/send/public\"\n            ]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 100,\n                \"activeWeek\": 20\n            },\n            \"localPosts\": 500,\n            \"localComments\": 100\n        }\n    }\n    \n    # Add custom document path if specified\n    if document_path != \"/.well-known/nodeinfo\":\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    # Create the NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Node\",\n            \"adminAccount\": \"admin@domain.tld\"\n        },\n        \"protocols\": [\n            \"activitypub\",\n            \"diaspora\"\n        ],\n        \"openRegistrations\": True,\n        \"services\": {\n            \"inbound\": [\n                \"https://example.com/inbox\"\n            ],\n            \"outbound\": [\n                \"https://example.com/outbox\"\n            ]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 150,\n                \"activeWeek\": 30\n            },\n            \"localPosts\": 5000,\n            \"localComments\": 2000\n        }\n    }\n    \n    # Add custom document path if specified\n    if document_path != \"/.well-known/nodeinfo\":\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 74034, "num_retrieved": 236, "inference_time": 18.891021892428398, "generate_results": ["def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Construct the signature content as per Diaspora protocol\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create the hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Import the public key\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create a PKCS1 v1.5 verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Construct the signature content as per the protocol\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create the hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Import the public key\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create the verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.urlsafe_b64decode(signature))\n        return True\n    except Exception:\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Create the signature content string\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create the hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Import the public key\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create the verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Construct the signature content as per the protocol\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create the hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Import the public key\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create a PKCS1 v1.5 verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Create the signature content string\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create the hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Create the public key object\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create the verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 100693, "num_retrieved": 351, "inference_time": 18.891021892428398, "generate_results": ["def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    import json\n    from lxml import etree\n    \n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        # Extract subject and aliases\n        subject = data.get(\"subject\")\n        aliases = data.get(\"aliases\", [])\n        \n        # Extract links\n        links = []\n        for link in data.get(\"links\", []):\n            links.append({\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            })\n            \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        # Parse the XML document\n        root = etree.fromstring(document)\n        \n        # Extract subject\n        subject_elem = root.find(\".//{http://webfingerprint.org/ns}subject\")\n        subject = None\n        if subject_elem is not None:\n            subject = subject_elem.text\n            \n        # Extract aliases\n        aliases = []\n        for alias in root.findall(\".//{http://webfingerprint.org/ns}alias\"):\n            aliases.append(alias.text)\n            \n        # Extract links\n        links = []\n        for link in root.findall(\".//{http://webfingerprint.org/ns}link\"):\n            link_dict = {\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            }\n            links.append(link_dict)\n            \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except Exception:\n        # If parsing fails, return empty dict\n        pass\n    \n    # If both parsing attempts fail, return empty dict\n    return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Extract subject and aliases from JSON webfinger\n        subject = doc.get(\"subject\")\n        aliases = doc.get(\"aliases\", [])\n        \n        # Extract links from JSON webfinger\n        links = []\n        for link in doc.get(\"links\", []):\n            links.append({\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            })\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except json.JSONDecodeError:\n        # If JSON parsing fails, try parsing as XRD (old format)\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}subject\"):\n                subject = element.text\n                break\n                \n            # Extract aliases from XRD\n            aliases = []\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}alias\"):\n                aliases.append(element.text)\n                \n            # Extract links from XRD\n            links = []\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}link\"):\n                link = {\n                    \"rel\": element.get(\"rel\"),\n                    \"type\": element.get(\"type\"),\n                    \"href\": element.get(\"href\")\n                }\n                links.append(link)\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Extract subject and aliases from JSON webfinger\n        subject = doc.get(\"subject\")\n        aliases = doc.get(\"aliases\", [])\n        \n        # Extract links from JSON webfinger\n        links = []\n        for link in doc.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and type_ and href:\n                links.append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}subject\"):\n                subject = element.text\n                break\n                \n            # Extract aliases from XRD\n            aliases = []\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}alias\"):\n                aliases.append(element.text)\n                \n            # Extract links from XRD\n            links = []\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}link\"):\n                rel = element.get(\"rel\")\n                type_ = element.get(\"type\")\n                href = element.get(\"href\")\n                if rel and type_ and href:\n                    links.append({\n                        \"rel\": rel,\n                        \"type\": type_,\n                        \"href\": href\n                    })\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        # Try to parse as JSON first (new format)\n        data = json.loads(document)\n        \n        # Extract required fields from JSON\n        result = {}\n        \n        # Extract subject (acct:handle)\n        subject = data.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"handle\"] = subject.replace(\"acct:\", \"\")\n        \n        # Extract aliases\n        aliases = data.get(\"aliases\", [])\n        result[\"aliases\"] = []\n        for alias in aliases:\n            if isinstance(alias, str):\n                result[\"aliases\"].append(alias)\n        \n        # Extract links\n        links = data.get(\"links\", [])\n        result[\"links\"] = []\n        for link in links:\n            if isinstance(link, dict):\n                result[\"links\"].append({\n                    \"rel\": link.get(\"rel\"),\n                    \"type\": link.get(\"type\"),\n                    \"href\": link.get(\"href\")\n                })\n        \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            result = {}\n            \n            # Extract subject\n            subject_elem = root.find(\".//{http://webfingerprint.org/ns/xrd}subject\")\n            if subject_elem is not None:\n                subject = subject_elem.text\n                if subject and subject.startswith(\"acct:\"):\n                    result[\"handle\"] = subject.replace(\"acct:\", \"\")\n            \n            # Extract aliases\n            aliases = []\n            for alias in root.findall(\".//{http://webfingerprint.org/ns/xrd}alias\"):\n                aliases.append(alias.text)\n            result[\"aliases\"] = aliases\n            \n            # Extract links\n            links = []\n            for link in root.findall(\".//{http://webfingerprint.org/ns/xrd}link\"):\n                link_dict = {\n                    \"rel\": link.get(\"rel\"),\n                    \"type\": link.get(\"type\"),\n                    \"href\": link.get(\"href\")\n                }\n                links.append(link_dict)\n            result[\"links\"] = links\n            \n            return result\n            \n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Extract subject and aliases from JSON webfinger\n        subject = doc.get(\"subject\")\n        aliases = doc.get(\"aliases\", [])\n        \n        # Extract links from JSON webfinger\n        links = []\n        for link in doc.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and type_ and href:\n                links.append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            for element in root.findall(\".//{http://webfinger.net/rel/profile}subject\"):\n                subject = element.text\n                break\n                \n            # Extract aliases from XRD\n            aliases = []\n            for element in root.findall(\".//{http://webfinger.net/rel/profile}alias\"):\n                aliases.append(element.text)\n                \n            # Extract links from XRD\n            links = []\n            for link in root.findall(\".//{http://webfinger.net/rel/profile}link\"):\n                rel = link.get(\"rel\")\n                type_ = link.get(\"type\")\n                href = link.get(\"href\")\n                if rel and type_ and href:\n                    links.append({\n                        \"rel\": rel,\n                        \"type\": type_,\n                        \"href\": href\n                    })\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 105967, "num_retrieved": 377, "inference_time": 18.891021892428398, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path=f\".well-known/webfinger\", cache=False)\n        if document:\n            return document\n    except Exception:\n        pass\n    return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path='/.well-known/webfinger')\n        if document:\n            return document\n    except Exception:\n        pass\n    return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path=f\"/.well-known/webfinger\")\n        if document:\n            return document\n        # Try with the handle as the resource\n        document, _, _ = fetch_document(host=handle.split(\"@\")[1], path=f\"/.well-known/webfinger\", resource=handle)\n        return document\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path=f\"/.well-known/webfinger\", cache=False)\n        if document:\n            return document\n    except Exception:\n        pass\n    return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path='/.well-known/webfinger')\n        if document:\n            return document\n        return None\n    except Exception:\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 99289, "num_retrieved": 350, "inference_time": 18.891021892428398, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:\" + handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"type\": \"application/activity+json\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:\" + handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"type\": \"application/activity+json\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/activity+json\"\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:\" + handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"type\": \"application/activity+json\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:\" + handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"type\": \"application/activity+json\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 77903, "num_retrieved": 312, "inference_time": 18.891021892428398, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n        \n    try:\n        # Parse the document as XRD\n        xrd = XRD()\n        xrd.from_xml(document)\n        return xrd\n    except Exception as e:\n        logger.error(\"Failed to parse host-meta document from %s: %s\", host, e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    doc, status_code, error = fetch_document(host=host, path='/host-meta')\n    if not doc:\n        return None\n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    xrd = XRD()\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"lrdd\":\n            xrd.links.append(Link(\n                rel='lrdd',\n                type_='application/xrd+xml',\n                template=link.get(\"template\")\n            ))\n    return xrd", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as XRD\n        xrd = XRD()\n        xrd.from_xml(document)\n        return xrd\n    except Exception as ex:\n        logger.warning(\"Failed to parse host-meta document from %s: %s\", host, ex)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n        \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n        \n        # Create an XRD instance and populate it with the retrieved data\n        xrd = XRD()\n        \n        # Add links from the document\n        for link in doc.get('links', []):\n            xrd.links.append(Link(\n                rel=link.get('rel'),\n                type_=link.get('type'),\n                href=link.get('href'),\n                template=link.get('template')\n            ))\n            \n        # Add elements from the document\n        for element in doc.get('elements', []):\n            xrd.elements.append(Element(\n                name=element.get('name'),\n                value=element.get('value')\n            ))\n            \n        return xrd\n        \n    except json.JSONDecodeError:\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    doc, status_code, error = fetch_document(host=host, path='/host-meta')\n    if not doc:\n        return None\n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    xrd = XRD()\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"lrdd\":\n            xrd.links.append(Link(\n                rel=link[\"rel\"],\n                type_=link[\"type_\"],\n                template=link[\"template\"]\n            ))\n    return xrd"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 117249, "num_retrieved": 500, "inference_time": 18.891021892428398, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 104300, "num_retrieved": 352, "inference_time": 18.891021892428398, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual parts\n    parts = [key] + list(keys)\n    \n    for i, part in enumerate(parts):\n        if i == len(parts) - 1:\n            # Last part - assign the value\n            current[part] = val\n        else:\n            # Intermediate part - create nested dictionary if it doesn't exist\n            if part not in current:\n                current[part] = {}\n            current = current[part]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual parts\n    parts = [key] + list(keys)\n    \n    for i, part in enumerate(parts):\n        if i == len(parts) - 1:\n            # Last part - assign the value\n            current[part] = val\n        else:\n            # Intermediate part - create nested dictionary if it doesn't exist\n            if part not in current:\n                current[part] = {}\n            current = current[part]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual components\n    components = [key] + list(keys)\n    \n    for i, component in enumerate(components):\n        if i == len(components) - 1:\n            # Last component - set the value\n            current[component] = val\n        else:\n            # Intermediate component - create nested dictionary if it doesn't exist\n            if component not in current:\n                current[component] = {}\n            current = current[component]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into a list of keys\n    key_parts = [key] + list(keys)\n    \n    # Iterate through each key part\n    for i, k in enumerate(key_parts):\n        if i == len(key_parts) - 1:\n            # Last key - set the value\n            current[k] = val\n        else:\n            # Intermediate key - create the key if it doesn't exist\n            if k not in current:\n                current[k] = {}\n            current = current[k]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual components\n    components = [key] + list(keys)\n    \n    for i, component in enumerate(components):\n        if i == len(components) - 1:\n            # Last component - assign the value\n            current[component] = val\n        else:\n            # Intermediate component - create nested dictionary if needed\n            if component not in current:\n                current[component] = {}\n            current = current[component]"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 90550, "num_retrieved": 250, "inference_time": 18.891021892428398, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n    \n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n    \n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 85281, "num_retrieved": 275, "inference_time": 18.891021892428398, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 77354, "num_retrieved": 249, "inference_time": 18.891021892428398, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and state management\n    # In a real implementation, this would return an instance of WorkspaceManager\n    # that is properly initialized with the workspaces base directory\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # In a real implementation, this might return an instance of WorkspaceManager\n    # or a similar class that manages workspaces.\n    return WorkspaceManager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Initialize workspace manager with workspaces base directory\n    # The base directory is typically defined in the configuration\n    # For now, we'll return a placeholder instance\n    # In a real implementation, this would retrieve the actual workspace manager instance\n    # from a configuration or service registry\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # In a real implementation, this might return an instance of WorkspaceManager\n    # or a similar class that manages workspaces.\n    return WorkspaceManager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # Based on the context, it appears that the workspace manager is\n    # likely a singleton or a service that is initialized once and reused.\n    # The actual implementation would typically involve retrieving an\n    # instance from a service registry or configuration.\n    return WorkspaceManager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 81235, "num_retrieved": 268, "inference_time": 18.891021892428398, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton or instance that can be accessed\n    # through a class-level attribute or service registry\n    # Based on context, this likely returns an instance of InfraredPluginManager\n    # which is initialized in the core services\n    from infrared.core.services.plugins import InfraredPluginManager\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and dependencies\n    # In the context of the retrieved code, this would likely return\n    # an instance of InfraredPluginManager or a similar class\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and should be initialized\n    # based on the context provided in the retrieved context\n    # The context shows that CoreServices.plugins_manager() is used to get the plugin manager\n    # and that it's likely a class method that returns an instance of InfraredPluginManager\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and dependencies\n    # In the context of the retrieved code, this would likely return\n    # an instance of InfraredPluginManager or a similar class\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and dependencies\n    # In the context of the retrieved code, this would likely return\n    # an instance of InfraredPluginManager or a similar class\n    return InfraredPluginManager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 103247, "num_retrieved": 362, "inference_time": 18.891021892428398, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': {'type': dict, 'required': True},\n        'config': {'type': dict, 'required': False},\n        'plugin_type': {'type': str, 'required': True},\n        'description': {'type': str, 'required': True},\n        'plugin.spec': {'type': str, 'required': False}\n    }\n\n    # Validate required fields\n    for field, field_info in required_fields.items():\n        if field_info['required'] and field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate field types\n    for field, field_info in required_fields.items():\n        if field_info['required'] and field in spec_data:\n            if field_info['type'] == dict and not isinstance(spec_data[field], dict):\n                raise IRValidatorException(f\"Field '{field}' must be a dictionary\")\n            elif field_info['type'] == str and not isinstance(spec_data[field], str):\n                raise IRValidatorException(f\"Field '{field}' must be a string\")\n\n    # Validate subparsers structure\n    if 'subparsers' in spec_data and not isinstance(spec_data['subparsers'], dict):\n        raise IRValidatorException(\"Field 'subparsers' must be a dictionary\")\n\n    # Validate plugin_type\n    if 'plugin_type' in spec_data and not isinstance(spec_data['plugin_type'], str):\n        raise IRValidatorException(\"Field 'plugin_type' must be a string\")\n\n    # Validate description\n    if 'description' in spec_data and not isinstance(spec_data['description'], str):\n        raise IRValidatorException(\"Field 'description' must be a string\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': dict,\n        'config': dict,\n        'plugin_type': str,\n        'description': str,\n        'plugin.spec': str,\n        'plugin.path': str,\n        'plugin.entry_point': str,\n        'plugin.playbook': str,\n        'plugin.roles_path': str,\n        'plugin.defaults_dir': str,\n        'plugin.vars_dir': str\n    }\n\n    # Validate required fields\n    for field, expected_type in required_fields.items():\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n        \n        if not isinstance(spec_data[field], expected_type):\n            raise IRValidatorException(f\"Field '{field}' must be of type {expected_type.__name__}, got {type(spec_data[field]).__name__}\")\n\n    # Validate subparsers structure\n    if not spec_data.get('subparsers'):\n        raise IRValidatorException(\"Missing 'subparsers' field in spec file\")\n    \n    # Validate config structure\n    config = spec_data.get('config', {})\n    if not config:\n        raise IRValidatorException(\"Missing 'config' field in spec file\")\n    \n    # Validate plugin_type\n    plugin_type = spec_data.get('plugin_type')\n    if not plugin_type:\n        raise IRValidatorException(\"Missing 'plugin_type' field in spec file\")\n    \n    # Validate description\n    description = spec_data.get('description')\n    if not description:\n        raise IRValidatorException(\"Missing 'description' field in spec file\")\n    \n    # Validate plugin.spec\n    plugin_spec = spec_data.get('plugin.spec')\n    if not plugin_spec:\n        raise IRValidatorException(\"Missing 'plugin.spec' field in spec file\")\n    \n    # Validate plugin.path\n    plugin_path = spec", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': {'type': dict, 'required': True},\n        'config': {'type': dict, 'required': False},\n        'plugin_type': {'type': str, 'required': True},\n        'description': {'type': str, 'required': True},\n        'plugin.spec': {'type': str, 'required': False}\n    }\n\n    # Validate required fields\n    for field, field_info in required_fields.items():\n        if field_info['required'] and field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n        \n        if field_info['required'] and field in spec_data:\n            if field_info['type'] == dict and not isinstance(spec_data[field], dict):\n                raise IRValidatorException(f\"Field '{field}' must be a dictionary\")\n            if field_info['type'] == str and not isinstance(spec_data[field], str):\n                raise IRValidatorException(f\"Field '{field}' must be a string\")\n\n    # Validate subparsers structure\n    if 'subparsers' in spec_data and not isinstance(spec_data['subparsers'], dict):\n        raise IRValidatorException(\"subparsers must be a dictionary\")\n\n    # Validate config structure\n    if 'config' in spec_data and not isinstance(spec_data['config'], dict):\n        raise IRValidatorException(\"config must be a dictionary\")\n\n    # Validate plugin_type\n    if 'plugin_type' in spec_data and not isinstance(spec_data['plugin_type'], str):\n        raise IRValidatorException(\"plugin_type must be a string\")\n\n    # Validate description\n    if 'description' in spec_data and not isinstance(spec_data['description'], str):\n        raise IRValidatorException(\"description must be a string\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': dict,\n        'config': dict,\n        'plugin_type': str,\n        'description': str,\n        'plugin.spec': str,\n        'plugin.path': str,\n        'plugin.entry_point': str,\n        'plugin.playbook': str,\n        'plugin.roles_path': str,\n        'plugin.defaults_dir': str,\n        'plugin.vars_dir': str,\n    }\n\n    # Validate required fields\n    for field, expected_type in required_fields.items():\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n        \n        if not isinstance(spec_data[field], expected_type):\n            raise IRValidatorException(f\"Field '{field}' must be of type {expected_type.__name__}, got {type(spec_data[field]).__name__}\")\n\n    # Validate subparsers structure\n    if not isinstance(spec_data['subparsers'], dict):\n        raise IRValidatorException(\"subparsers must be a dictionary\")\n    \n    # Validate config structure\n    if not isinstance(spec_data['config'], dict):\n        raise IRValidatorException(\"config must be a dictionary\")\n    \n    # Validate plugin_type\n    if not isinstance(spec_data['plugin_type'], str):\n        raise IRValidatorException(\"plugin_type must be a string\")\n    \n    # Validate description\n    if not isinstance(spec_data['description'], str):\n        raise IRValidatorException(\"description must be a string\")\n    \n    # Validate plugin.spec\n    if not isinstance(spec_data['plugin.spec'], str):\n        raise IRValidatorException(\"plugin.spec must be a string\")\n    \n    # Validate plugin.path\n    if not isinstance(spec_data['plugin.path'], str):\n        raise IRValidatorException(\"plugin.path must be a string\")\n    \n    # Validate plugin.entry_point\n    if", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': dict,\n        'config': dict,\n        'plugin_type': str,\n        'description': str,\n        'plugin.spec': str,\n        'plugin.path': str,\n        'plugin.entry_point': str,\n        'plugin.playbook': str,\n        'plugin.roles_path': str,\n        'plugin.defaults_dir': str,\n        'plugin.vars_dir': str,\n    }\n\n    # Validate required fields\n    for field, expected_type in required_fields.items():\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n        \n        if not isinstance(spec_data[field], expected_type):\n            raise IRValidatorException(f\"Field '{field}' must be of type {expected_type.__name__}, got {type(spec_data[field]).__name__}\")\n\n    # Validate subparsers structure\n    if not isinstance(spec_data['subparsers'], dict):\n        raise IRValidatorException(\"subparsers must be a dictionary\")\n    \n    # Validate config structure\n    if not isinstance(spec_data['config'], dict):\n        raise IRValidatorException(\"config must be a dictionary\")\n    \n    # Validate plugin_type\n    if not isinstance(spec_data['plugin_type'], str):\n        raise IRValidatorException(\"plugin_type must be a string\")\n    \n    # Validate description\n    if not isinstance(spec_data['description'], str):\n        raise IRValidatorException(\"description must be a string\")\n    \n    # Validate plugin.spec\n    if not isinstance(spec_data['plugin.spec'], str):\n        raise IRValidatorException(\"plugin.spec must be a string\")\n    \n    # Validate plugin.path\n    if not isinstance(spec_data['plugin.path'], str):\n        raise IRValidatorException(\"plugin.path must be a string\")\n    \n    # Validate plugin.entry_point\n    if"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 103909, "num_retrieved": 369, "inference_time": 18.891021892428398, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 96227, "num_retrieved": 292, "inference_time": 14.211302667856216, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Add the group to the parser dictionary\n        parser_dict[group['name']] = group.get('value', {})\n    \n    # Return the updated parser dictionary\n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Check if the group is already defined in the parser_dict\n        if group not in parser_dict:\n            # Add the group to the parser_dict\n            parser_dict[group] = {}\n    \n    # Merge any additional groups from the spec file\n    for group_name, group_dict in self.spec_helper.spec_dict.get('groups', {}).items():\n        if group_name not in parser_dict:\n            parser_dict[group_name] = group_dict\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Check if the group is already defined in the parser_dict\n        if group not in parser_dict:\n            # Add the group to the parser_dict\n            parser_dict[group] = {}\n    \n    # Merge any existing groups with the new ones\n    # This is a placeholder implementation - actual logic would depend on\n    # the specific requirements of how groups should be included and merged\n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Get the group name and its options\n        group_name = group.get('name')\n        group_options = group.get('options', {})\n        \n        # Add the group to the parser with its options\n        group_parser = parser_dict.get(group_name)\n        if group_parser is None:\n            # Create a new group parser if it doesn't exist\n            group_parser = {}\n            parser_dict[group_name] = group_parser\n            \n        # Add the options from the group to the parser\n        for option_name, option_value in group_options.items():\n            group_parser[option_name] = option_value\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Check if the group is already defined in the parser_dict\n        if group not in parser_dict:\n            # Add the group to the parser_dict\n            parser_dict[group] = {}\n    \n    # Return the updated parser_dict\n    return parser_dict"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 79560, "num_retrieved": 241, "inference_time": 14.211302667856216, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    for parser in self.iterate_parsers():\n        for spec_option in self._get_all_options_spec(parser):\n            option_name = spec_option['name']\n            option_spec = spec_option\n            \n            # Get default value from option spec\n            if 'default' in option_spec:\n                default_value = option_spec['default']\n                # Handle different types of default values\n                if isinstance(default_value, dict):\n                    # Nested default values\n                    defaults[option_name] = default_value\n                else:\n                    defaults[option_name] = default_value\n                    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize defaults dictionary\n    defaults = {}\n    \n    # Get the main spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process the main parser and subparsers to extract default values\n    for parser_name, parser_dict in spec_dict.get('subparsers', {}).items():\n        # Extract default values from options in the parser\n        for group in parser_dict.get('groups', []):\n            for option_name, option_dict in group.get('options', {}).items():\n                # Set default value from option specification\n                if 'default' in option_dict:\n                    defaults[option_name] = option_dict['default']\n        \n        # Extract default values from flat options\n        for option_name, option_dict in parser_dict.get('options', {}).items():\n            if 'default' in option_dict:\n                defaults[option_name] = option_dict['default']\n    \n    # Process shared groups if they exist\n    if 'shared_groups' in spec_dict:\n        for shared_group in spec_dict['shared_groups']:\n            for option_name, option_dict in shared_group.get('options', {}).items():\n                if 'default' in option_dict:\n                    defaults[option_name] = option_dict['default']\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize defaults dictionary\n    defaults = {}\n    \n    # Get the main spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process the main parser and subparsers\n    for parser_name, parser_dict in spec_dict.get('subparsers', {}).items():\n        # Get all option specs for this parser\n        option_specs = self._get_all_options_spec(parser_dict)\n        \n        # Process each option spec\n        for option_spec in option_specs:\n            option_name = option_spec['name']\n            option_type = option_spec.get('type', 'string')\n            \n            # Handle different option types\n            if option_type == 'bool':\n                # Default to False for boolean options\n                defaults[option_name] = False\n            elif option_type == 'int':\n                # Default to 0 for integer options\n                defaults[option_name] = 0\n            elif option_type == 'str':\n                # Default to empty string for string options\n                defaults[option_name] = ''\n            elif option_type == 'list':\n                # Default to empty list for list options\n                defaults[option_name] = []\n            elif option_type == 'dict':\n                # Default to empty dict for dictionary options\n                defaults[option_name] = {}\n    \n    # Add defaults from shared groups\n    for group in spec_dict.get('shared_groups', []):\n        for option_name, option_dict in group.get('options', {}).items():\n            option_type = option_dict.get('type', 'string')\n            if option_type == 'bool':\n                defaults[option_name] = False\n            elif option_type == 'int':\n                defaults[option_name] = 0\n            elif option_type == 'str':\n                defaults[option_name] = ''\n            elif option_type == 'list':\n                defaults[option_name] = []\n            elif option_type == 'dict':\n                defaults[option_name] = {}\n    \n    # Add defaults from main parser options\n    for option_name, option_dict in spec_dict.get('options', {}).items():\n        option_type = option_dict.get('type', 'string')\n        if option_type == 'bool':\n            defaults[option_name] = False\n        elif option_type == 'int':\n            defaults[option_name] = 0\n        elif option_type == 'str':\n            defaults[option_name] = ''\n        elif option_type == 'list", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    for parser in self.iterate_parsers():\n        for spec_option in self._get_all_options_spec(parser):\n            option_name = spec_option['name']\n            option_spec = spec_option\n            \n            # Check if option has a default value defined in spec\n            if 'default' in option_spec:\n                defaults[option_name] = option_spec['default']\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    defaults = {}\n    \n    # Get the spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process the main parser and subparsers\n    for parser_name, parser_dict in spec_dict.get('subparsers', {}).items():\n        # Get the options from the parser\n        options = self._get_all_options_spec(parser_dict)\n        \n        # Process each option\n        for option in options:\n            option_name = option['name']\n            option_spec = option\n            \n            # Get the default value from the option spec\n            if 'default' in option_spec:\n                defaults[option_name] = option_spec['default']\n    \n    # Add any defaults from the main parser\n    main_parser_dict = spec_dict.get('main', {})\n    if 'options' in main_parser_dict:\n        for option in main_parser_dict['options'].values():\n            option_name = option['name']\n            if 'default' in option:\n                defaults[option_name] = option['default']\n    \n    return defaults"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 78342, "num_retrieved": 230, "inference_time": 14.211302667856216, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 90135, "num_retrieved": 305, "inference_time": 14.211302667856216, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append((arg_name, arg_value, 'cli'))\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append((arg_name, arg_value, 'answer_file'))\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Deprecated arguments found:\")\n        for arg_name, arg_value, source in deprecated_args:\n            print(f\"  {source} argument '{arg_name}' with value '{arg_value}' is deprecated\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_arg']:\n            deprecated_args.append((arg_name, arg_value, \"deprecated\"))\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_arg']:\n            deprecated_args.append((arg_name, arg_value, \"deprecated\"))\n    \n    # Print deprecated arguments if any found\n    if deprecated_args:\n        print(\"The following arguments are deprecated and will be removed in a future version:\")\n        for arg_name, arg_value, source in deprecated_args:\n            print(f\"  - {arg_name} (value: {arg_value}) from {source}\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_arg']:\n            deprecated_args.append((arg_name, arg_value, 'cli'))\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_arg']:\n            deprecated_args.append((arg_name, arg_value, 'answer_file'))\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Deprecated arguments found:\")\n        for arg_name, arg_value, source in deprecated_args:\n            print(f\"  {source} argument '{arg_name}' with value '{arg_value}' is deprecated\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_arg1', 'old_arg2', 'old_arg3']:  # Example deprecated arguments\n            deprecated_args.append((arg_name, arg_value))\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_arg1', 'old_arg2', 'old_arg3']:  # Example deprecated arguments\n            deprecated_args.append((arg_name, arg_value))\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Deprecated arguments found:\")\n        for arg_name, arg_value in deprecated_args:\n            print(f\"  Argument '{arg_name}' with value '{arg_value}' is deprecated.\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(f\"--{arg_name}={arg_value}\")\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(f\"--{arg_name}={arg_value}\")\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"The following arguments are deprecated and will be removed in a future version:\")\n        for arg in deprecated_args:\n            print(f\"  {arg}\")\n        return True\n    \n    return False"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 103173, "num_retrieved": 341, "inference_time": 14.211302667856216, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 98501, "num_retrieved": 301, "inference_time": 14.211302667856216, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 90292, "num_retrieved": 304, "inference_time": 14.211302667856216, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # Implementation not found in context\n    # Based on function name and docstring, this function likely returns\n    # a list of argument names that are silenced or not processed\n    # However, no context provides the actual implementation\n    return []"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 88563, "num_retrieved": 275, "inference_time": 14.211302667856216, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self._validate_args(*args)\n    if not required_args:\n        raise UtilError(\"Expected all arguments to be specified\")\n    return required_args"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 93401, "num_retrieved": 319, "inference_time": 14.211302667856216, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    required_args = []\n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if any part of the condition is met\n        condition_met = False\n        for part in condition_parts:\n            # Format: \"arg=value\"\n            if '=' in part:\n                condition_arg, condition_value = part.split('=', 1)\n                # Get the value of the argument from args\n                arg_value = args.get(command_name, {}).get(condition_arg)\n                # Check if the value matches the condition\n                if arg_value == condition_value:\n                    condition_met = True\n                    break\n            else:\n                # Check if the argument exists in args\n                arg_value = args.get(command_name, {}).get(part)\n                if arg_value is not None:\n                    condition_met = True\n                    break\n        \n        # If condition is met, add the option name to required_args\n        if condition_met:\n            required_args.append(option_spec['name'])\n            \n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if any part of the condition is met\n        condition_met = False\n        \n        for part in condition_parts:\n            # Split key=value\n            if '=' in part:\n                key, value = part.split('=', 1)\n            else:\n                key = part\n                value = None\n                \n            # Get the value from args\n            arg_value = args.get(command_name, {}).get(key)\n            \n            # Check if the value matches the required condition\n            if value is None:\n                # If no value specified, check if the key exists\n                if arg_value is not None:\n                    condition_met = True\n                    break\n            else:\n                # Check if the value matches the required value\n                if str(arg_value) == value:\n                    condition_met = True\n                    break\n                    \n        if condition_met:\n            matched_args.append(option_spec['name'])\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if the condition is met\n        condition_met = True\n        \n        for part in condition_parts:\n            # Format: \"arg=value\"\n            if '=' not in part:\n                condition_met = False\n                break\n                \n            condition_arg, condition_value = part.split('=', 1)\n            \n            # Get the value of the condition argument from args\n            condition_arg_value = args.get(command_name, {}).get(condition_arg)\n            \n            # Check if the value matches the condition\n            if condition_arg_value is None:\n                condition_met = False\n                break\n                \n            if condition_value != str(condition_arg_value):\n                condition_met = False\n                break\n                \n        # If condition is met, add the argument name to required_args\n        if condition_met:\n            required_args.append(option_spec['name'])\n            \n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    required_args = []\n    \n    for option_spec in options_spec:\n        option_name = option_spec['name']\n        required_when = option_spec.get('required_when')\n        \n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1, arg2=value2\"\n        condition_parts = required_when.split(',')\n        \n        # Check if the condition is met\n        condition_met = False\n        \n        for part in condition_parts:\n            # Format: \"arg=value\"\n            if '=' not in part:\n                continue\n                \n            condition_arg, condition_value = part.split('=', 1)\n            \n            # Get the value of the condition argument from args\n            condition_value_from_args = args.get(command_name, {}).get(condition_arg)\n            \n            # Check if the value matches\n            if condition_value_from_args == condition_value:\n                condition_met = True\n                break\n                \n        if condition_met:\n            required_args.append(option_name)\n            \n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    required_args = []\n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if required_when:\n            # Extract the condition from required_when\n            # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n            condition_parts = required_when.split(',')\n            condition_matched = False\n            \n            # Check if any part of the condition is met\n            for part in condition_parts:\n                part = part.strip()\n                if '=' in part:\n                    condition_key, condition_value = part.split('=', 1)\n                else:\n                    condition_key = part\n                    condition_value = None\n                \n                # Get the value from args for the condition key\n                condition_value_from_args = args.get(command_name, {}).get(condition_key)\n                \n                # Check if the condition is met\n                if condition_value is None:\n                    # If no value is specified in condition, check if key exists\n                    if condition_value_from_args is not None:\n                        condition_matched = True\n                        break\n                else:\n                    # Check if the value matches\n                    if condition_value_from_args == condition_value:\n                        condition_matched = True\n                        break\n            \n            # If condition is matched, add the option name to required_args\n            if condition_matched:\n                required_args.append(option_spec['name'])\n    \n    return required_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 91657, "num_retrieved": 298, "inference_time": 14.211302667856216, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str):\n                # Check if the value exceeds the specified length\n                # This is a placeholder - actual length validation would depend on context\n                if len(arg_value) > 100:  # Example length limit\n                    raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds length limit of 100 characters\")\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str) and len(arg_value) > 1000:  # Assuming 1000 is the max length\n                raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds the maximum length of 1000 characters.\")", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str) and len(arg_value) > 1000:  # Assuming 1000 is the max length\n                raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds the maximum length of 1000 characters.\")", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str):\n                # Check if the value exceeds the specified length\n                # This is a placeholder - actual length validation would depend on context\n                if len(arg_value) > 100:  # Example length limit\n                    raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds length limit of 100 characters\")\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str):\n                # Check if the value exceeds the specified length\n                # This is a placeholder - actual length validation would depend on context\n                # For example, if there were a length specification in the option spec\n                if len(arg_value) > 100:  # Example length limit\n                    raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' has value '{arg_value}' which exceeds the maximum length of 100 characters.\")"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 91212, "num_retrieved": 294, "inference_time": 14.211302667856216, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        if 'choices' in option_spec:\n            choices = option_spec['choices']\n            if option_name in args:\n                value = args[option_name]\n                if value not in choices:\n                    raise exceptions.IRInvalidChoiceException([\n                        (option_name, value, choices)\n                    ])", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        choices = option_spec.get('choices', [])\n        if not choices:\n            continue\n            \n        # Get the value for this option from args\n        if option_name in args:\n            value = args[option_name]\n            if value not in choices:\n                raise exceptions.IRInvalidChoiceException([\n                    (option_name, value, choices)\n                ])", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        if option_name in args:\n            choices = option_spec.get('choices', [])\n            if choices and args[option_name] not in choices:\n                raise exceptions.IRInvalidChoiceException([\n                    (option_name, args[option_name], choices)\n                ])", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        if 'choices' in option_spec:\n            option_name = option_spec['name']\n            if option_name in args:\n                option_value = args[option_name]\n                if option_value not in option_spec['choices']:\n                    raise exceptions.IRInvalidChoiceException(\n                        [(option_name, option_value, option_spec['choices'])])", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        if option_name not in args:\n            continue\n            \n        option_value = args[option_name]\n        available_choices = option_spec.get('choices', [])\n        \n        if not available_choices:\n            continue\n            \n        if option_value not in available_choices:\n            raise exceptions.IRInvalidChoiceException([\n                (option_name, option_value, available_choices)\n            ])"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 91281, "num_retrieved": 295, "inference_time": 14.211302667856216, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Check if the option has min and max values defined\n            min_value = option_spec.get('min_value')\n            max_value = option_spec.get('max_value')\n            \n            if min_value is not None and max_value is not None:\n                # Convert option_value to int if it's a number\n                try:\n                    value = int(option_value)\n                except (ValueError, TypeError):\n                    continue\n                    \n                # Check if value is within the range\n                if value < min_value or value > max_value:\n                    raise exceptions.IRInvalidMinMaxRangeException([\n                        (command_name, option_name, min_value, max_value, value)\n                    ])", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Check if the option has min and max values defined\n            min_value = option_spec.get('min_value')\n            max_value = option_spec.get('max_value')\n            \n            if min_value is not None and max_value is not None:\n                # Convert option value to int if it's a number\n                try:\n                    value = int(option_value)\n                except (ValueError, TypeError):\n                    continue\n                    \n                # Check if value is within the range\n                if value < min_value or value > max_value:\n                    raise exceptions.IRInvalidMinMaxRangeException([\n                        (command_name, option_name, min_value, max_value, value)\n                    ])", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Extract min and max values from option specification\n            min_value = option_spec.get('min_value')\n            max_value = option_spec.get('max_value')\n            \n            # Skip if no min/max values are defined\n            if min_value is None and max_value is None:\n                continue\n                \n            # Convert option value to int if possible\n            try:\n                value = int(option_value)\n            except (ValueError, TypeError):\n                continue\n                \n            # Check if value is within the specified range\n            if min_value is not None and value < min_value:\n                raise exceptions.IRInvalidMinMaxRangeException([\n                    (command_name, option_name, min_value, value)\n                ])\n                \n            if max_value is not None and value > max_value:\n                raise exceptions.IRInvalidMinMaxRangeException([\n                    (command_name, option_name, max_value, value)\n                ])", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Extract min and max values from option specification\n            min_value = option_spec.get('min', None)\n            max_value = option_spec.get('max', None)\n            \n            # Skip if no min/max values are defined\n            if min_value is None and max_value is None:\n                continue\n                \n            # Convert option value to integer if it's a number\n            try:\n                value = int(option_value)\n            except (ValueError, TypeError):\n                continue\n                \n            # Check if value is within the specified range\n            if min_value is not None and value < min_value:\n                raise exceptions.IRInvalidMinMaxRangeException([\n                    (command_name, option_name, min_value, value)\n                ])\n                \n            if max_value is not None and value > max_value:\n                raise exceptions.IRInvalidMinMaxRangeException([\n                    (command_name, option_name, max_value, value)\n                ])", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Check if the option has min and max values defined\n            min_value = option_spec.get('min_value')\n            max_value = option_spec.get('max_value')\n            \n            if min_value is not None and max_value is not None:\n                # Convert option_value to int if it's a number\n                try:\n                    value = int(option_value)\n                except (ValueError, TypeError):\n                    continue\n                    \n                # Check if value is within the range\n                if value < min_value or value > max_value:\n                    raise exceptions.IRInvalidMinMaxRangeException([\n                        (command_name, option_name, min_value, max_value, value)\n                    ])"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 92770, "num_retrieved": 314, "inference_time": 14.211302667856216, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 96389, "num_retrieved": 347, "inference_time": 14.211302667856216, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Process all arguments\n    for arg_name, arg_value in args.items():\n        # Check if the argument is a control argument (starts with 'extra-vars', 'debug', 'verbose', etc.)\n        # These are typically IR behavior controls\n        if arg_name in ['debug', 'verbose', 'dry-run', 'output', 'generate-answers-file', 'extra-vars']:\n            control_args[arg_name] = arg_value\n        elif arg_name.startswith('extra-vars'):\n            # Handle extra-vars as custom arguments\n            custom_args[arg_name] = arg_value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by Ansible playbooks\n            nested_args[arg_name] = arg_value\n    \n    return control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through all arguments\n    for arg_name, arg_value in args.items():\n        # Check if the argument starts with a control prefix (common control args)\n        # These are typically arguments that control IR behavior\n        if arg_name.startswith('debug') or arg_name.startswith('verbose') or \\\n           arg_name.startswith('dry-run') or arg_name.startswith('output') or \\\n           arg_name.startswith('extra-vars') or arg_name.startswith('ansible-args'):\n            control_args[arg_name] = arg_value\n        elif arg_name.startswith('additional-args'):\n            # Handle additional-args which are passed as a string of key=value pairs\n            additional_args = arg_value.split(';')\n            for additional_arg in additional_args:\n                if '=' in additional_arg:\n                    key, value = additional_arg.split('=', 1)\n                    control_args[key] = value\n        else:\n            # All other arguments are considered nested or custom\n            # Check if it's a custom argument (typically starts with 'custom_' or is a known custom variable)\n            if arg_name.startswith('custom_'):\n                custom_args[arg_name] = arg_value\n            else:\n                # Otherwise treat as nested argument\n                nested_args[arg_name] = arg_value\n\n    return control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through all arguments\n    for arg_name, arg_value in args.items():\n        # Check if the argument starts with a control prefix (commonly '--')\n        # and if it's a known control argument\n        if arg_name.startswith('--'):\n            # Check if it's a control argument (typically those that control IR behavior)\n            # These are arguments that are not nested in the spec file\n            # and are used to control the execution of IR\n            if arg_name in ['--debug', '--verbose', '--dry-run', '--output', '--extra-vars', '--generate-answers-file']:\n                control_args[arg_name] = arg_value\n            else:\n                # For other arguments, we need to determine if they are nested or custom\n                # Nested arguments are typically those that are used by Ansible playbooks\n                # and are nested in the spec file\n                # Custom arguments are custom ansible variables\n                # We'll assume that any argument not explicitly controlled is nested\n                # and will be passed to the playbook\n                nested_args[arg_name] = arg_value\n        else:\n            # For non-argument style keys, treat as nested\n            nested_args[arg_name] = arg_value\n\n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Process arguments to separate control, nested, and custom\n    for arg_name, arg_value in args.items():\n        # Check if argument starts with 'extra-vars' or contains 'extra-vars'\n        if arg_name.startswith('extra-vars') or 'extra-vars' in arg_name:\n            # Handle custom ansible variables\n            custom_args[arg_name] = arg_value\n        elif arg_name.startswith('debug') or arg_name.startswith('verbose'):\n            # Control arguments that affect IR behavior\n            control_args[arg_name] = arg_value\n        else:\n            # Default to nested arguments (used by Ansible playbooks)\n            nested_args[arg_name] = arg_value\n\n    return control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through all arguments\n    for arg_name, arg_value in args.items():\n        # Check if the argument starts with a control prefix (common control args)\n        # These are typically flags or options that control IR behavior\n        if arg_name.startswith('debug') or arg_name.startswith('verbose') or \\\n           arg_name.startswith('dry-run') or arg_name.startswith('output') or \\\n           arg_name.startswith('extra-vars') or arg_name.startswith('ansible-args'):\n            control_args[arg_name] = arg_value\n        elif arg_name.startswith('additional-args'):\n            # Handle additional-args which are passed as key=value pairs\n            # Split by semicolon and process each key=value pair\n            additional_args = arg_value.split(';')\n            for additional_arg in additional_args:\n                if '=' in additional_arg:\n                    key, value = additional_arg.split('=', 1)\n                    control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by Ansible playbooks and stored in nested_args\n            nested_args[arg_name] = arg_value\n\n    return control_args, nested_args"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 94443, "num_retrieved": 285, "inference_time": 14.211302667856216, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder for future implementation\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder or error condition\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder for future expansion\n            vars_dict[extra_var] = None\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder or error case\n            vars_dict[extra_var] = None\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder or error case\n            pass\n    \n    return vars_dict"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 104356, "num_retrieved": 360, "inference_time": 14.211302667856216, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register plugins_dir path otherwise roles introduced by the plugin are not found during the plugin execution\n    plugin_roles_path = ir_plugin.roles_path\n    if plugin_roles_path:\n        role_path = os.path.join(ir_plugin.path, plugin_roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore the original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register the plugin's roles path if provided\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        \n        # Roles path points to the dir which contains installed plugins\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore the original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register plugins_dir path otherwise roles introduced by the plugin\n    # are not found during the plugin execution\n    plugin_roles_path = ir_plugin.roles_path\n    if plugin_roles_path:\n        role_path = os.path.join(ir_plugin.path, plugin_roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\",\n                        role_path)\n        # roles path points to the dir which contains installed plugins\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register plugins_dir path otherwise roles introduced by the plugin\n    # are not found during the plugin execution\n    plugin_roles_path = ir_plugin.roles_path\n    if plugin_roles_path:\n        role_path = os.path.join(ir_plugin.path, plugin_roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\",\n                        role_path)\n        # roles path points to the dir which contains installed plugins\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register the plugin's roles path if provided\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        \n        # Roles path points to the dir which contains installed plugins\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore the original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 105546, "num_retrieved": 371, "inference_time": 13.557366490364075, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 92205, "num_retrieved": 311, "inference_time": 13.557366490364075, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for non-CLI arguments\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Placeholder implementation - actual implementation would depend on\n    # specific type conversions defined in the system\n    # For now, we return the values_dict as-is since no specific conversion\n    # rules are provided in the context\n    return values_dict"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 106267, "num_retrieved": 372, "inference_time": 13.557366490364075, "generate_results": ["def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create a path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Load the plugin spec file\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    # Create a flat dictionary from the spec content\n    # Flatten the nested structure by removing the 'subparsers' key and flattening the rest\n    flat_dict = {}\n    \n    # Extract the plugin type\n    plugin_type = spec_content.get('config', {}).get('plugin_type', spec_content.get('plugin_type'))\n    flat_dict['plugin_type'] = plugin_type\n    \n    # Extract the entry point\n    entry_point = spec_content.get('config', {}).get('entry_point', spec_content.get('entry_point'))\n    flat_dict['entry_point'] = entry_point\n    \n    # Extract the roles path\n    roles_path = spec_content.get('config', {}).get('roles_path', spec_content.get('roles_path'))\n    flat_dict['roles_path'] = roles_path\n    \n    # Extract the description\n    description = spec_content.get('description')\n    flat_dict['description'] = description\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create a path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Load the plugin spec file\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    # Create a flat dictionary from the spec content\n    # Flatten the nested structure by removing the subparsers key\n    flat_dict = {}\n    \n    # Extract the plugin type\n    if 'config' in spec_content:\n        plugin_type = spec_content['config']['plugin_type']\n        flat_dict['plugin_type'] = plugin_type\n    else:\n        plugin_type = spec_content['plugin_type']\n        flat_dict['plugin_type'] = plugin_type\n    \n    # Extract the entry point\n    if 'config' in spec_content:\n        entry_point = spec_content['config']['entry_point']\n        flat_dict['entry_point'] = entry_point\n    else:\n        entry_point = spec_content['entry_point']\n        flat_dict['entry_point'] = entry_point\n    \n    # Extract the roles path\n    if 'config' in spec_content:\n        roles_path = spec_content['config']['roles_path']\n        flat_dict['roles_path'] = roles_path\n    else:\n        roles_path = spec_content['roles_path']\n        flat_dict['roles_path'] = roles_path\n    \n    # Extract the description\n    if 'description' in spec_content:\n        flat_dict['description'] = spec_content['description']\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create plugin instance from the directory path\n    plugin = InfraredPlugin(plugin_dir)\n    \n    # Get the plugin spec file path\n    spec_file = os.path.join(plugin_dir, plugin.PLUGIN_SPEC_FILE)\n    \n    # Load the plugin spec content\n    with open(spec_file, 'r') as spec_file_handle:\n        spec_content = yaml.safe_load(spec_file_handle)\n    \n    # Create a flat dictionary from the spec content\n    # Flatten the nested structure by removing the 'subparsers' key\n    # and using the plugin type as the key for the description\n    flat_dict = {}\n    \n    # Extract plugin type\n    plugin_type = spec_content.get('config', {}).get('plugin_type', spec_content.get('plugin_type'))\n    flat_dict['plugin_type'] = plugin_type\n    \n    # Extract description\n    description = spec_content.get('description', '')\n    flat_dict['description'] = description\n    \n    # Extract entry point\n    entry_point = spec_content.get('config', {}).get('entry_point', spec_content.get('entry_point', 'main.yml'))\n    flat_dict['entry_point'] = entry_point\n    \n    # Extract roles path\n    roles_path = spec_content.get('config', {}).get('roles_path', spec_content.get('roles_path', ''))\n    flat_dict['roles_path'] = roles_path\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create a path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Load the plugin spec file\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    # Create a flat dictionary from the spec content\n    # Flatten the nested structure by iterating through the spec\n    flat_dict = {}\n    \n    # Extract the plugin type\n    plugin_type = spec_content.get('config', {}).get('plugin_type', spec_content.get('plugin_type'))\n    flat_dict['plugin_type'] = plugin_type\n    \n    # Extract the entry point\n    entry_point = spec_content.get('config', {}).get('entry_point', spec_content.get('entry_point'))\n    flat_dict['entry_point'] = entry_point\n    \n    # Extract the roles path\n    roles_path = spec_content.get('config', {}).get('roles_path', spec_content.get('roles_path'))\n    flat_dict['roles_path'] = roles_path\n    \n    # Extract the description\n    description = spec_content.get('description')\n    flat_dict['description'] = description\n    \n    # Extract the subparsers (if any)\n    subparsers = spec_content.get('subparsers', {})\n    flat_dict['subparsers'] = subparsers\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create plugin instance\n    plugin = InfraredPlugin(plugin_dir)\n    \n    # Create a flat dictionary from the plugin's spec\n    spec_dict = {}\n    \n    # Add basic properties\n    spec_dict['name'] = plugin.name()\n    spec_dict['type'] = plugin.type()\n    spec_dict['description'] = plugin.description()\n    spec_dict['entry_point'] = plugin.entry_point()\n    spec_dict['roles_path'] = plugin.roles_path()\n    \n    # Add other properties from the plugin spec\n    try:\n        config = plugin.config\n        for section in config.sections():\n            if section not in ['supported_types', 'git_orgs']:\n                for option in config.options(section):\n                    value = config.get(section, option)\n                    spec_dict[f\"{section}.{option}\"] = value\n    except Exception:\n        pass\n    \n    return spec_dict"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 103033, "num_retrieved": 340, "inference_time": 13.557366490364075, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG', '')\n    if not config_path:\n        # Use the common/ directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        # Use the common/ directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG', '')\n    if not config_path:\n        # Use the common/ directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        # Use the common/ directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG', '')\n    if not config_path:\n        # Use the common/ directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 86437, "num_retrieved": 272, "inference_time": 13.557366490364075, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Implementation based on context\n    # This function is part of the SpecObject abstract base class\n    # It extends the CLI by adding options from the spec to the subparser\n    # The actual implementation would involve parsing the spec and adding\n    # appropriate argparse arguments to the subparser\n    \n    # Since the specific implementation details are not provided in the context,\n    # this is a placeholder that follows the expected pattern\n    # In a real implementation, this would parse the spec and add arguments\n    # to the subparser based on the spec definition\n    \n    # The function is expected to be implemented in the specific spec class\n    # that inherits from SpecObject\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This implementation is based on the context provided\n    # The actual implementation would depend on the specific spec\n    # and how it defines its CLI options\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This implementation is based on the context provided\n    # The actual implementation would depend on the specific spec\n    # and how it defines its CLI options\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a subparser for the plugin\n    plugin_subparser = root_subparsers.add_parser(\n        self.name,\n        help=self.specification.get_help(),\n        description=self.specification.get_description()\n    )\n    \n    # Add all the options from the plugin specification\n    self.specification.add_options(plugin_subparser)\n    \n    # Add any shared groups defined in the plugin specification\n    if hasattr(self.specification, 'shared_groups'):\n        for group in self.specification.shared_groups:\n            plugin_subparser.add_argument_group(group['title'])\n            for opt_name, opt_dict in group.get('options', {}).items():\n                plugin_subparser.add_argument(\n                    opt_name,\n                    **opt_dict\n                )", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This implementation is based on the context provided\n    # The actual implementation would depend on the specific spec\n    # and how it defines its CLI options\n    pass"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 196608, "actual_tokens": 49368, "num_retrieved": 127, "inference_time": 13.557366490364075, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")"]}
