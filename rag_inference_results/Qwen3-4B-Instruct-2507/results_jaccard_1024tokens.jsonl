{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1174, "num_retrieved": 10, "inference_time": 0.6953548192977905, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds since the epoch.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    seconds = nanoseconds // NANO_SECONDS\n    nanoseconds = nanoseconds % NANO_SECONDS\n    clock_time = ClockTime(seconds, nanoseconds)\n    if tz is None:\n        return Time.from_clock_time(clock_time, UnixEpoch)\n    else:\n        return Time.from_clock_time(clock_time, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds since the epoch.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    seconds = nanoseconds // NANO_SECONDS\n    nanoseconds = nanoseconds % NANO_SECONDS\n    clock_time = ClockTime(seconds, nanoseconds)\n    if tz is None:\n        return Time.from_clock_time(clock_time, UnixEpoch)\n    else:\n        return Time.from_clock_time(clock_time, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds since the epoch.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    seconds, nanoseconds = divmod(nanoseconds, NANO_SECONDS)\n    clock_time = ClockTime(seconds, nanoseconds)\n    if tz is None:\n        return Time.from_clock_time(clock_time, UnixEpoch)\n    else:\n        return Time.from_clock_time(clock_time, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds since the epoch.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    seconds = nanoseconds // 1_000_000_000\n    nanoseconds = nanoseconds % 1_000_000_000\n    \n    if tz is not None:\n        # Convert to timezone-aware time\n        from datetime import datetime, timezone, timedelta\n        utc_time = datetime.utcfromtimestamp(seconds)\n        utc_time = utc_time.replace(microsecond=nanoseconds)\n        return Time(utc_time.astimezone(tz))\n    else:\n        # Return local time\n        return Time(seconds, nanoseconds)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds since the epoch.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    seconds = nanoseconds // NANO_SECONDS\n    nanoseconds = nanoseconds % NANO_SECONDS\n    clock_time = ClockTime(seconds, nanoseconds)\n    if tz is None:\n        return Time.from_clock_time(clock_time, UnixEpoch)\n    else:\n        return Time.from_clock_time(clock_time, tz)"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1105, "num_retrieved": 8, "inference_time": 0.6953548192977905, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :returns: A serialized representation of the timedelta value\n    \"\"\"\n    from datetime import timedelta\n    # Convert timedelta to seconds and return as a tuple or structure\n    # This is a simplified representation; actual implementation may depend on packstream format\n    total_seconds = value.total_seconds()\n    return (int(total_seconds),)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :return: A serialized representation of the timedelta value\n    :rtype: object\n    \"\"\"\n    from datetime import timedelta\n    # Convert timedelta to seconds and return as a tuple or appropriate structure\n    # This is a simplified implementation based on typical dehydrator patterns\n    total_seconds = value.total_seconds()\n    return (total_seconds,)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :returns: A serialized representation of the timedelta value\n    \"\"\"\n    from datetime import timedelta\n    # Convert timedelta to seconds and return as a tuple or appropriate structure\n    total_seconds = value.total_seconds()\n    return total_seconds", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :return: A serialized representation of the timedelta value\n    :rtype: object\n    \"\"\"\n    from .._codec.hydration.v1.temporal import dehydrate_timedelta\n    return dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :returns: A packstream-compatible representation of the timedelta\n    :rtype: tuple\n    \"\"\"\n    # Convert timedelta to seconds and return as a tuple\n    # Format: (type_code, seconds)\n    seconds = value.total_seconds()\n    return (b\"t\", seconds)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1105, "num_retrieved": 9, "inference_time": 0.6953548192977905, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"Y\", value.hour, value.minute, value.second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"Y\", value.hour, value.minute, value.second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for time values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"T\", value.hour, value.minute, value.second, nanosecond, value.tzinfo)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"Y\", value.hour, value.minute, value.second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"Y\", value.hour, value.minute, value.second, nanosecond)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1126, "num_retrieved": 8, "inference_time": 0.6953548192977905, "generate_results": ["def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    if isinstance(value, WGS84Point):\n        return Structure(b\"P\", value.latitude, value.longitude)\n    elif isinstance(value, CartesianPoint):\n        return Structure(b\"p\", value.x, value.y)\n    else:\n        raise TypeError(\"Value must be a neo4j.spatial.WGS84Point or neo4j.spatial.CartesianPoint\")", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"P\", value.x, value.y)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"P\", value.x, value.y)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"P\", value.x, value.y)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"P\", value.x, value.y)"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1388, "num_retrieved": 18, "inference_time": 0.6953548192977905, "generate_results": ["def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n:return: list of key names\n    \"\"\"\n    return list(self.keys())"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 913, "num_retrieved": 2, "inference_time": 0.6953548192977905, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Filter to only include if version is supported\n        return {k: v for k, v in supported_versions.items() if v is not None}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) for version in cls._get_supported_versions() if cls._get_handler_for_version(version) is not None}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Filter to only include if version is supported\n        return {k: v for k, v in supported_versions.items() if v is not None}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) for version in cls._get_supported_versions() if cls._get_handler_for_version(version) is not None}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Filter to only include if version is supported\n        return {k: v for k, v in supported_versions.items() if v is not None}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) for version in cls._get_supported_versions() if cls._get_handler_for_version(version) is not None}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Filter to only include supported versions\n        return {k: v for k, v in supported_versions.items() if v is not None}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) for version in cls._get_supported_versions() if cls._get_handler_for_version(version) is not None}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Only include if version is supported\n        if supported_versions[protocol_version] is not None:\n            return supported_versions\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) \n                for version in cls._get_supported_versions() \n                if cls._get_handler_for_version(version) is not None}"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 891, "num_retrieved": 1, "inference_time": 0.6953548192977905, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.session.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # Handle exceptions if needed, though transaction will be rolled back automatically\n                raise e\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # If an exception occurs, rollback the transaction\n                tx.rollback()\n                raise e\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # Re-raise the exception after transaction is committed or rolled back\n                raise e\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # Re-raise the exception after transaction is committed or rolled back\n                raise e\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Begin transaction with provided metadata and timeout\n            tx = tx.session.begin_transaction(metadata=metadata, timeout=timeout)\n            try:\n                result = func(tx, *args, **kwargs)\n                return result\n            except Exception as e:\n                # If an exception occurs, rollback the transaction\n                tx.rollback()\n                raise e\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1347, "num_retrieved": 17, "inference_time": 0.6953548192977905, "generate_results": ["def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\n    \"\"\"\n    return self._keys.index(key)", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\n    \"\"\"\n    return self._keys.index(key)", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\n    \"\"\"\n    return self._keys.index(key)", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\n    \"\"\"\n    return self._keys.index(key)", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int\n    \"\"\"\n    return self._keys.index(key)"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 923, "num_retrieved": 2, "inference_time": 0.6953548192977905, "generate_results": ["def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none are provided, all values will be included\n:return: list of values\n:rtype: list\"\"\"\n    return [record.values(*keys) for record in self]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 953, "num_retrieved": 3, "inference_time": 0.6953548192977905, "generate_results": ["def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_values = self.values(*keys)\n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"index {key} is out of bounds\")\n            result[key] = record_values[key]\n        else:\n            # Assume key is a string (field name)\n            # We need to find the value by key in the record\n            # Since we don't have direct access to the record's keys, \n            # we rely on the fact that values() returns values in order\n            # and that the keys are stored in the record's metadata\n            # This is a simplified implementation assuming keys are valid\n            # and we can map them to values\n            # In a real implementation, we would need to know the field names\n            # This is a placeholder\n            pass\n    # If no keys are provided, return all values as a dictionary\n    if not keys:\n        return dict(zip(self.keys(), record_values))\n    # Otherwise, return only the specified keys/values\n    # This is a simplified version - actual implementation would depend\n    # on how the record's keys are stored and accessed\n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_values = self.values(*keys)\n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"index {key} is out of bounds\")\n            result[key] = record_values[key]\n        else:\n            # Assume key is a string (field name)\n            # We need to find the value by key in the record\n            # Since we don't have direct access to the record's keys,\n            # we'll assume that the values are in the same order as the keys\n            # and that the keys are provided as field names\n            # This is a simplified implementation\n            # In a real implementation, we would need to map keys to values\n            # based on the actual record structure\n            pass\n    # If no keys are provided, return all values\n    if not keys:\n        return dict(zip(self.keys(), record_values))\n    # Otherwise, return only the specified keys/values\n    # This is a simplified version - actual implementation would depend\n    # on how the record's keys and values are structured\n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_values = self.values(*keys)\n    result = {}\n    if not keys:\n        # If no keys are provided, return all values as a dictionary\n        return dict(zip(self.keys(), record_values))\n    \n    # Process each key to extract value\n    for key in keys:\n        if isinstance(key, int):\n            # Index access\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"index {key} is out of bounds\")\n            result[key] = record_values[key]\n        else:\n            # Key access\n            # Assuming the record has a method to get value by key\n            # This is a fallback implementation based on typical behavior\n            try:\n                result[key] = self.get(key)\n            except KeyError:\n                result[key] = None\n    \n    # Fill in missing keys with None\n    for key in self.keys():\n        if key not in result:\n            result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_values = self.values()\n    result = {}\n    \n    if not keys:\n        # If no keys are provided, return all values as a dictionary\n        return dict(zip(self.keys(), record_values))\n    \n    # Process each key provided\n    for key in keys:\n        if isinstance(key, int):\n            # Handle index\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"index {key} is out of bounds\")\n            result[key] = record_values[key]\n        else:\n            # Handle key name\n            # Note: The actual field name lookup would depend on the record's internal structure\n            # This is a simplified implementation assuming the record has a method to get values by key\n            try:\n                result[key] = self.values(key)[0] if key in self.keys() else None\n            except (KeyError, IndexError):\n                result[key] = None\n    \n    # Fill in missing keys with None\n    for field in self.keys():\n        if field not in result:\n            result[field] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_values = self.values(*keys)\n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"index {key} is out of bounds\")\n            result[key] = record_values[key]\n        else:\n            # Assume key is a string (field name)\n            # We need to map key to value, but we don't have direct access to field names\n            # Since we don't have the original field names, we can't reliably map keys\n            # Therefore, we must rely on the fact that values() returns values in order\n            # and keys are either indices or field names. Without field names, we can't\n            # properly map keys to values.\n            # This suggests that the implementation should fall back to using the index\n            # or raise an error if the key is not recognized.\n            # However, based on the context, we assume that keys are either indices or field names\n            # and that the record has a way to map field names to values.\n            # Since we don't have that information, we'll assume the keys are indices\n            # and raise an error if they are out of bounds.\n            pass\n    # Since we don't have access to field names or a way to map keys to values,\n    # and the context doesn't provide the necessary implementation details,\n    # we return a placeholder that would be correct if the record had field names.\n    # In a real implementation, this would use the record's field names.\n    # For now, we return a dictionary with values from the record.\n    # This is a simplified version that assumes keys are indices.\n    if not keys:\n        return dict(zip(range(len(record_values)), record_values))"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1067, "num_retrieved": 7, "inference_time": 0.6953548192977905, "generate_results": ["def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    data = self[-2:]\n    return int.from_bytes(data, byteorder='big')", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    data = self[-2:]\n    return int.from_bytes(data, byteorder='big')", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    data = self[-2:]\n    return int.from_bytes(data, byteorder='big')", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    return (self[-2] << 8) | self[-1]", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian 16-bit unsigned integer.\"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    data = self[-2:]\n    return int.from_bytes(data, byteorder='big')"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 921, "num_retrieved": 2, "inference_time": 0.6953548192977905, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Append a DISCARD message to the output queue with the specified parameters\n    # The actual implementation would depend on the internal message queue or protocol\n    # For now, we just pass through the parameters as per the expected behavior\n    self._append_message(\"DISCARD\", n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n                         hydration_hooks=hydration_hooks, **handlers)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending to output queue\n    # This is a placeholder - actual implementation would depend on the internal\n    # messaging protocol and queue management of the driver\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Append a DISCARD message to the output queue with the specified parameters\n    # The actual implementation would depend on the internal message queue or protocol\n    # This is a placeholder for the actual message construction logic\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Append a DISCARD message to the output queue with the given parameters\n    # The actual implementation would depend on the internal message queue logic\n    # of the Neo4j driver, but based on the pattern of pull and similar methods,\n    # this would involve constructing a DISCARD message with the provided parameters\n    # and adding it to the output queue.\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation of DISCARD message appending logic\n    # This is a placeholder - actual implementation would depend on the internal\n    # message queue and protocol handling\n    pass"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 890, "num_retrieved": 1, "inference_time": 0.6953548192977905, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+.\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    message = {\n        \"type\": \"BEGIN\",\n        \"mode\": mode,\n        \"bookmarks\": bookmarks,\n        \"metadata\": metadata,\n        \"timeout\": timeout,\n        \"db\": db,\n        \"imp_user\": imp_user,\n        \"dehydration_hooks\": dehydration_hooks,\n        \"hydration_hooks\": hydration_hooks\n    }\n    \n    # Append the message to the output queue\n    self._output_queue.append(message)\n    \n    # Return a Response object\n    return self._create_response(handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    message = {\n        'type': 'BEGIN',\n        'mode': mode,\n        'bookmarks': bookmarks,\n        'metadata': metadata,\n        'timeout': timeout,\n        'db': db,\n        'imp_user': imp_user,\n        'dehydration_hooks': dehydration_hooks,\n        'hydration_hooks': hydration_hooks\n    }\n    \n    # Append the message to the output queue\n    self._output_queue.append(message)\n    \n    # Return a Response object for the transaction\n    return self._create_response(handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    message = {\n        \"type\": \"BEGIN\",\n        \"mode\": mode,\n        \"bookmarks\": bookmarks,\n        \"metadata\": metadata,\n        \"timeout\": timeout,\n        \"db\": db,\n        \"imp_user\": imp_user,\n        \"dehydration_hooks\": dehydration_hooks,\n        \"hydration_hooks\": hydration_hooks\n    }\n    \n    # Append the message to the output queue\n    self._queue.append(message)\n    \n    # Return a Response object for the operation\n    return self._response(message, handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    message = {\n        \"type\": \"BEGIN\",\n        \"mode\": mode,\n        \"bookmarks\": bookmarks,\n        \"metadata\": metadata,\n        \"timeout\": timeout,\n        \"db\": db,\n        \"imp_user\": imp_user,\n        \"dehydration_hooks\": dehydration_hooks,\n        \"hydration_hooks\": hydration_hooks\n    }\n    \n    # Append the message to the output queue\n    self._output_queue.append(message)\n    \n    # Return a Response object for the operation\n    return self._create_response(handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # Construct the message to send (BEGIN)\n    # This is a placeholder - actual implementation would involve\n    # sending a BEGIN message via the underlying transport layer\n    # and returning a Response object with appropriate handlers\n    return self._send_message(\"BEGIN\", mode=mode, bookmarks=bookmarks,\n                              metadata=metadata, timeout=timeout,\n                              db=db, imp_user=imp_user,\n                              dehydration_hooks=dehydration_hooks,\n                              hydration_hooks=hydration_hooks, **handlers)"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1105, "num_retrieved": 8, "inference_time": 0.6953548192977905, "generate_results": ["def round_half_to_even(n):\n    \"\"\"Round to nearest even integer, with ties rounding to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"Round to nearest even integer, with ties rounding to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"Round half to even, also known as bankers' rounding.\n    \n    :param n: The number to round\n    :return: The rounded number\n    \"\"\"\n    # Extract the integer and fractional parts\n    integer_part = int(n)\n    fractional_part = abs(n - integer_part)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to the nearest even integer\n    else:\n        return integer_part + (1 if integer_part % 2 == 1 else 0)", "def round_half_to_even(n):\n    \"\"\"Round to nearest even integer, with ties rounding to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"Round half to even, also known as banker's rounding.\n    \n    :param n: The number to round\n    :return: The rounded number\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = abs(n - integer_part)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1098, "num_retrieved": 8, "inference_time": 0.6953548192977905, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"Dynamically create a Point subclass.\"\"\"\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None, **kwargs):\n            super().__init__(*coordinates, **kwargs)\n            self.srid = srid\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            return cls(*coordinates)\n    \n    # Add the class to the namespace\n    globals()[name] = PointSubclass\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"Dynamically create a Point subclass.\"\"\"\n    # Define the metaclass for the new class\n    class PointMeta(type):\n        def __new__(mcs, name, bases, namespace, **kwargs):\n            # Create the new class\n            cls = super().__new__(mcs, name, bases, namespace)\n            # Add the fields to the class\n            for field in fields:\n                setattr(cls, field, property(lambda self, f=field: getattr(self, f)))\n            return cls\n\n    # Create the new Point subclass\n    class PointSubclass(metaclass=PointMeta):\n        def __init__(self, *coordinates):\n            # Initialize with coordinates\n            if len(coordinates) == len(fields):\n                for i, field in enumerate(fields):\n                    setattr(self, field, coordinates[i])\n            else:\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(coordinates)}\")\n\n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            \"\"\"Create an instance from SRID and coordinates.\"\"\"\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            return cls(*coordinates)\n\n        def __repr__(self):\n            return f\"{self.__class__.__name__}({', '.join(f'{getattr(self, f)}' for f in fields)})\"\n\n    # Add the class to the Point hierarchy\n    PointSubclass.__name__ = name\n    PointSubclass.__module__ = \"neo4j.spatial\"\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"Dynamically create a Point subclass.\"\"\"\n    # Define the metaclass for the new class\n    class PointMeta(type):\n        def __new__(mcs, name, bases, namespace, **kwargs):\n            # Create the new class\n            cls = super().__new__(mcs, name, bases, namespace)\n            # Add the fields to the class\n            for field in fields:\n                setattr(cls, field, property(lambda self, f=field: getattr(self, f)))\n            return cls\n\n    # Create the new Point subclass\n    class PointSubclass(metaclass=PointMeta):\n        def __init__(self, *coordinates):\n            # Initialize with coordinates\n            if len(coordinates) == len(fields):\n                for i, field in enumerate(fields):\n                    setattr(self, field, coordinates[i])\n            else:\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(coordinates)}\")\n\n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            \"\"\"Create instance from SRID and coordinates.\"\"\"\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            return cls(*coordinates)\n\n        def __repr__(self):\n            return f\"{self.__class__.__name__}({', '.join(f'{getattr(self, f)}' for f in fields)})\"\n\n    # Add the class to the Point hierarchy\n    PointSubclass.__name__ = name\n    PointSubclass.__module__ = \"neo4j.spatial\"\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"Dynamically create a Point subclass.\"\"\"\n    class PointSubclass(Point):\n        def __init__(self, *coordinates, srid=None, **kwargs):\n            super().__init__(*coordinates, **kwargs)\n            self.srid = srid\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            return cls(*coordinates)\n    \n    # Set the class name and add to the namespace\n    PointSubclass.__name__ = name\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"Dynamically create a Point subclass.\"\"\"\n    # Define the metaclass for the new class\n    class PointMeta(type):\n        def __new__(mcs, name, bases, namespace, **kwargs):\n            # Create the new class\n            cls = super().__new__(mcs, name, bases, namespace)\n            # Add the fields to the class\n            for field in fields:\n                setattr(cls, field, property(lambda self, f=field: getattr(self, f)))\n            return cls\n\n    # Create the new Point subclass\n    class PointSubclass(metaclass=PointMeta):\n        def __init__(self, *coordinates):\n            # Store coordinates in instance attributes\n            for i, coord in enumerate(coordinates):\n                setattr(self, f\"coord_{i}\", coord)\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            # Use srid_map to determine which subclass to use\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            else:\n                # Default to Point if no SRID mapping exists\n                return Point(coordinates)\n        \n        def __repr__(self):\n            return f\"{self.__class__.__name__}({', '.join(str(getattr(self, f'coord_{i}')) for i in range(len(self.__dict__)))})\"\n\n    # Set the class name and add to the Point hierarchy\n    PointSubclass.__name__ = name\n    PointSubclass.__module__ = \"neo4j.spatial\"\n    \n    # Add the srid_map to the class\n    PointSubclass.srid_map = srid_map\n    \n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1136, "num_retrieved": 9, "inference_time": 0.6953548192977905, "generate_results": ["def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    :param message: Deprecation message to be displayed\n    :type message: str\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(f\"{func.__name__} {message}\", DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 972, "num_retrieved": 4, "inference_time": 0.7561962157487869, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Common R options that might need to be set include:\n    # - options(warn=1) to enable warnings\n    # - options(digits=5) to set number of significant digits\n    # - options(scipen=10) to prefer displaying numbers in standard form over scientific notation\n    # - options(stringsAsFactors=FALSE) to prevent string conversion to factors\n    # - options(show.error.messages=TRUE) to show error messages\n    \n    # Example: Set common R options\n    r_options = [\n        \"options(warn=1)\",\n        \"options(digits=5)\",\n        \"options(scipen=10)\",\n        \"options(stringsAsFactors=FALSE)\",\n        \"options(show.error.messages=TRUE)\"\n    ]\n    \n    # Insert R options at the beginning of the code\n    r_options_code = \"\\n\".join(r_options)\n    \n    # If code is empty, just return the options\n    if not code.strip():\n        return r_options_code\n    \n    # Otherwise, prepend the options to the code\n    return f\"{r_options_code}\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Common R options that might need to be set\n    r_options = [\n        \"options(stringsAsFactors = FALSE)\",\n        \"options(scipen = 999)\",  # Prevent scientific notation\n        \"options(warn = 1)\",     # Enable warnings\n        \"options(digits = 5)\"    # Set number of significant digits\n    ]\n    \n    # Insert R options at the beginning of the code\n    return \"\\n\".join(r_options) + \"\\n\" + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Example: setting R options like 'options(stringsAsFactors = FALSE)' or others\n    # This is a placeholder implementation - actual options depend on specific requirements\n    r_options = \"options(stringsAsFactors = FALSE, warn = 1)\\n\"\n    return r_options + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Example: setting R options like max.print, stringsAsFactors, etc.\n    r_options = \"\"\"\n    # Set common R options that cannot be configured via environment variables\n    options(max.print = 1000, stringsAsFactors = FALSE, warn = 1)\n    \"\"\"\n    return code + \"\\n\" + r_options", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # Example: setting R options like max.print, stringsAsFactors, etc.\n    r_options = \"\"\"\n    # Set common R options that cannot be configured via environment variables\n    options(max.print = 1000, stringsAsFactors = FALSE, warn = 1)\n    \"\"\"\n    return code + \"\\n\" + r_options"]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 971, "num_retrieved": 4, "inference_time": 0.7561962157487869, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual number of jobs to use based on target_concurrency\n    jobs = target_concurrency\n    \n    # If color is requested, ensure we have a proper terminal\n    if color:\n        import os\n        if os.isatty(os.fdopen(0).fileno()):\n            # Use a pty if available\n            import pty\n            pty.spawn(cmd)\n            return 0, b''\n    \n    # Use the xargs command with the provided arguments and jobs\n    import subprocess\n    try:\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n',\n            text=True,\n            capture_output=True,\n            timeout=300,\n            encoding='utf-8',\n            shell=False,\n            check=False,\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8') if result.stdout else b''\n    except subprocess.TimeoutExpired:\n        return 1, b\"Command timed out\\n\"\n    except Exception as e:\n        return 1, f\"Error: {str(e)}\\n\".encode('utf-8')", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    \n    # If color is requested, ensure we have a proper terminal\n    if color:\n        import sys\n        if sys.platform == 'win32':\n            # Windows doesn't support pty well, so just skip color\n            pass\n        else:\n            # On Unix-like systems, try to open a pty\n            try:\n                import pty\n                pty.openpty()\n            except (OSError, ImportError):\n                pass\n    \n    # Use the provided command and arguments with the determined concurrency\n    # This is a simplified version - in practice, this would spawn processes\n    # and collect their output and exit codes\n    import subprocess\n    import os\n    \n    # Prepare the command with arguments\n    # We'll use xargs-like behavior: run command once per argument\n    # with concurrency controlled by jobs\n    \n    # If no arguments, return success\n    if not varargs:\n        return 0, b''\n    \n    # Split arguments into chunks based on target_concurrency\n    chunk_size = len(varargs) // jobs if jobs > 0 else 1\n    chunks = [varargs[i:i + chunk_size] for i in range(0, len(varargs), chunk_size)]\n    \n    # Run each chunk in parallel\n    processes = []\n    results = []\n    \n    for chunk in chunks:\n        # Create a subprocess for each chunk\n        process = subprocess.Popen(\n            cmd,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1,\n            universal_newlines=True,\n        )\n        # Send the arguments to the process\n        input_data = '\\n'.join(chunk) + '\\n'\n        process.stdin.write(input_data)\n        process.stdin.close()\n        \n        # Wait for completion and capture output\n        stdout, stderr = process.communicate()\n        results.append((process", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    \n    # If no concurrency is specified, use the default from target_concurrency()\n    if jobs == 1:\n        # Run serially\n        result = subprocess.run(cmd, input='\\n'.join(varargs), text=True, capture_output=True)\n    else:\n        # Use xargs to run commands in parallel\n        # Create a command that passes each file argument to the given command\n        xargs_cmd = ['xargs', '-P', str(jobs)]\n        if color:\n            xargs_cmd.extend(['--color=always'])\n        \n        # Build the command to execute\n        full_cmd = cmd + ['--'] + varargs\n        \n        # Execute using subprocess\n        result = subprocess.run(xargs_cmd, input=' '.join(varargs), text=True, capture_output=True)\n    \n    return result.returncode, result.stdout.encode('utf-8') if result.stdout else b''", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    \n    # If color is requested, ensure we create a pty if supported\n    if color:\n        # Use a pty if available on supported platforms\n        import pty\n        try:\n            # This is a simplified approach - actual pty handling would depend on platform\n            pass\n        except ImportError:\n            pass\n    \n    # Execute xargs with the specified command, arguments, and concurrency\n    # This is a simplified implementation - actual xargs behavior would involve\n    # spawning processes and collecting results\n    import subprocess\n    import os\n    \n    # Prepare the command with arguments\n    # Note: This is a simplified version and does not fully replicate xargs behavior\n    # In a real implementation, this would involve partitioning varargs and running\n    # commands in parallel with proper error handling\n    \n    # For now, we'll just run the command with the arguments\n    # This is a placeholder implementation that would need to be expanded\n    # to properly handle concurrency and argument partitioning\n    \n    try:\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) if varargs else '',\n            text=True,\n            capture_output=True,\n            timeout=300,\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode() if result.stdout else b''\n    except subprocess.TimeoutExpired:\n        return 1, b\"Command timed out\"\n    except Exception as e:\n        return 1, str(e).encode()", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    \n    # If color is requested, try to create a pty if supported\n    if color and _has_pty():\n        # Use pty for color output on supported platforms\n        pass\n    \n    # Execute xargs with the given command, arguments, and concurrency\n    # This is a simplified wrapper around the actual xargs functionality\n    # In a real implementation, this would spawn processes and collect results\n    return xargs.xargs(cmd, varargs, target_concurrency=jobs, color=color)"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1331, "num_retrieved": 16, "inference_time": 0.7561962157487869, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1188, "num_retrieved": 11, "inference_time": 0.7561962157487869, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    # Split the version string by dots and convert each part to integer\n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros if the version has fewer than 3 parts\n    while len(version_tuple) < 3:\n        version_tuple += (0,)\n    \n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    # Split the version string by dots and convert each part to integer\n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros to ensure consistent length (e.g., \"1\" becomes \"1.0.0\")\n    # This is a simple implementation that assumes version strings are in the format \"x.y.z\"\n    while len(version_tuple) < 3:\n        version_tuple += (0,)\n    \n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    # Split the version string by dots and convert each part to integer\n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros to ensure consistent length (e.g., \"1.2\" becomes \"1.2.0\")\n    # This is a simple implementation that assumes version strings are in the format \"x.y.z\"\n    while len(version_tuple) < 3:\n        version_tuple += (0,)\n    \n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    # Split the version string by dots and convert each part to integer\n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros to ensure consistent length (e.g., \"1.2\" becomes (1, 2, 0, 0, ...))\n    # This is a simple implementation that works for basic version comparison\n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    # Split the version string by dots and convert each part to integer\n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros to ensure consistent length (e.g., \"1.2\" becomes (1, 2, 0, 0, ...))\n    # This is a simple implementation that works for basic version comparison\n    return version_tuple"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 909, "num_retrieved": 2, "inference_time": 0.7561962157487869, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command to ensure it works on Windows\n    # Replace backslashes with forward slashes in paths to ensure proper shebang parsing\n    # This handles deep-path shebangs and fixes Windows path parsing issues\n    normalized = []\n    for part in cmd:\n        if os.sep == '\\\\':\n            # Convert backslashes to forward slashes for consistent path handling\n            part = part.replace(os.sep, os.altsep)\n        normalized.append(part)\n    return tuple(normalized)", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command to ensure it works on Windows\n    # Replace backslashes with forward slashes in paths to ensure proper shebang parsing\n    # This handles deep-path shebangs and fixes Windows path parsing issues\n    return tuple(f.replace(os.sep, os.altsep) if os.sep == '\\\\' else f for f in cmd)", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command to ensure it works on Windows\n    # Replace backslashes with forward slashes in paths to ensure proper shebang parsing\n    # This handles deep-path shebangs and fixes Windows path parsing issues\n    normalized = []\n    for arg in cmd:\n        if arg.startswith('#!'):\n            # Handle shebangs by normalizing the path\n            # Replace backslashes with forward slashes for consistency\n            normalized.append(arg.replace('\\\\', '/'))\n        else:\n            # For non-shebang arguments, normalize paths if needed\n            normalized.append(arg.replace('\\\\', '/'))\n    return tuple(normalized)", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command to ensure it works on Windows\n    # Replace backslashes with forward slashes in paths to ensure proper shebang parsing\n    # This helps with deep-path shebangs and compatibility with Windows\n    normalized = []\n    for arg in cmd:\n        if arg.startswith('#!'):\n            # Handle shebangs by normalizing the path\n            # Replace backslashes with forward slashes for consistency\n            normalized.append(arg.replace('\\\\', '/'))\n        else:\n            # For non-shebang arguments, normalize paths if needed\n            normalized.append(arg.replace('\\\\', '/'))\n    return tuple(normalized)", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command to handle Windows-specific issues\n    # Replace backslashes with forward slashes in paths to ensure proper shebang parsing\n    # This helps with deep-path shebangs and compatibility with Windows\n    normalized = []\n    for arg in cmd:\n        if arg.startswith('#!'):\n            # Handle shebangs by normalizing the path\n            # Replace backslashes with forward slashes for consistency\n            normalized.append(arg.replace('\\\\', '/'))\n        else:\n            # For non-shebang arguments, normalize paths if they contain backslashes\n            normalized.append(arg.replace('\\\\', '/'))\n    return tuple(normalized)"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1026, "num_retrieved": 6, "inference_time": 0.7561962157487869, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the result from cache\n            result = cache.get(cache_key)\n            if result is not None:\n                return result\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # If a lock is provided, use it to ensure thread safety\n            if lock is not None:\n                with lock:\n                    cache[cache_key] = result\n            \n            return result\n        return wrapper\n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the result from cache\n            result = cache.get(cache_key)\n            if result is not None:\n                return result\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # If a lock is provided, use it to ensure thread safety\n            if lock is not None:\n                with lock:\n                    cache[cache_key] = result\n            \n            return result\n        return wrapper\n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the result from cache\n            result = cache.get(cache_key)\n            if result is not None:\n                return result\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # If a lock is provided, use it to ensure thread safety\n            if lock is not None:\n                with lock:\n                    cache[cache_key] = result\n            \n            return result\n        return wrapper\n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the value from cache\n            try:\n                result = cache[cache_key]\n                return result\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache[cache_key] = result\n                return result\n        \n        # Optionally add locking if provided\n        if lock is not None:\n            import functools\n            @functools.wraps(func)\n            def locked_wrapper(*args, **kwargs):\n                with lock:\n                    key_args = args + tuple(sorted(kwargs.items()))\n                    cache_key = key(key_args)\n                    try:\n                        result = cache[cache_key]\n                        return result\n                    except KeyError:\n                        result = func(*args, **kwargs)\n                        cache[cache_key] = result\n                        return result\n            return locked_wrapper\n        else:\n            return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a key from args and kwargs using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(key_args)\n            \n            # Try to get the result from cache\n            result = cache.get(cache_key)\n            if result is not None:\n                return result\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache[cache_key] = result\n            \n            # If a lock is provided, use it to ensure thread safety\n            if lock is not None:\n                with lock:\n                    cache[cache_key] = result\n            \n            return result\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 946, "num_retrieved": 3, "inference_time": 0.7561962157487869, "generate_results": ["import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, typed=typed)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = args + tuple(sorted(kwargs.items()))\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.7561962157487869, "generate_results": ["import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        # Create an ordered dictionary to store cache entries\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert kwargs to a tuple for consistent hashing if typed is False\n            if typed:\n                args_tuple = args, tuple(sorted(kwargs.items()))\n            else:\n                args_tuple = args, tuple(sorted(kwargs.items()))\n            \n            try:\n                # Try to get the value from cache\n                result = cache[args_tuple]\n                # Move to end to mark as most recently used\n                cache.move_to_end(args_tuple)\n                return result\n            except KeyError:\n                # If not found, compute the result\n                result = func(*args, **kwargs)\n                # Add to cache and maintain maxsize\n                cache[args_tuple] = result\n                if len(cache) > maxsize:\n                    # Remove the oldest entry (first in OrderedDict)\n                    cache.popitem(last=False)\n                return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = OrderedDict()\n        \n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_tuple = tuple(args) + tuple(sorted(kwargs.items()))\n            else:\n                args_tuple = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                # Get the value from cache if it exists\n                result = cache[args_tuple]\n                # Move to end to mark as most recently used\n                cache.move_to_end(args_tuple)\n                return result\n            except KeyError:\n                # Cache miss - compute and store result\n                result = func(*args, **kwargs)\n                # Add to cache and maintain maxsize\n                cache[args_tuple] = result\n                if len(cache) > maxsize:\n                    # Remove the oldest item (first in OrderedDict)\n                    cache.popitem(last=False)\n                return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        # Create an ordered dictionary to store cache entries\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert arguments to a hashable form if typed is True\n            if typed:\n                args_repr = args\n                kwargs_repr = tuple(sorted(kwargs.items()))\n            else:\n                args_repr = args\n                kwargs_repr = tuple(sorted(kwargs.items()))\n            \n            # Create a key for the cache\n            key = args_repr + tuple(sorted(kwargs_repr))\n            \n            # Check if the key is in the cache\n            if key in cache:\n                # Move to the end to mark as most recently used\n                cache.move_to_end(key)\n                return cache[key]\n            \n            # If not in cache, compute the result\n            result = func(*args, **kwargs)\n            \n            # Add to cache and maintain maxsize\n            cache[key] = result\n            \n            # If cache exceeds maxsize, remove the oldest entry\n            if len(cache) > maxsize:\n                cache.popitem(last=False)\n            \n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = OrderedDict()\n        \n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_tuple = tuple(args) + tuple(sorted(kwargs.items()))\n            else:\n                args_tuple = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                # Get the value from cache if it exists\n                result = cache[args_tuple]\n                # Move to end to mark as most recently used\n                cache.move_to_end(args_tuple)\n                return result\n            except KeyError:\n                # Compute and store the result\n                result = func(*args, **kwargs)\n                # Add to cache and maintain maxsize\n                cache[args_tuple] = result\n                if len(cache) > maxsize:\n                    # Remove the oldest item (first item in OrderedDict)\n                    cache.popitem(last=False)\n                return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = OrderedDict()\n        \n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = args + tuple(sorted(kwargs.items()))\n            else:\n                args_key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                # Get the value from cache if it exists\n                return cache[args_key]\n            except KeyError:\n                # If not found, compute the value\n                value = func(*args, **kwargs)\n                \n                # Add to cache and maintain MRU order\n                cache[args_key] = value\n                \n                # If cache exceeds maxsize, remove the least recently used item\n                if len(cache) > maxsize:\n                    cache.popitem(last=False)\n                \n                return value\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.7561962157487869, "generate_results": ["import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n\n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(sorted(kwargs.items())))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args, tuple(sorted(kwargs.items())))\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Move to end (most recently used)\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # Compute and store result\n            result = func(*args, **kwargs)\n            \n            # Check if cache size exceeds maxsize\n            if len(cache) >= maxsize:\n                # Remove the least recently used item\n                cache.popitem(last=False)\n            \n            # Add the result to cache\n            cache[key] = result\n            \n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator", "import functools\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n    \n    def decorator(func):\n        # Create a cache with LRU behavior\n        cache = functools.LRUCache(maxsize=maxsize, typed=typed)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert kwargs to a tuple for hashing if typed is False\n            if typed:\n                args_repr = args\n                kwargs_repr = tuple(sorted(kwargs.items()))\n            else:\n                args_repr = args\n                kwargs_repr = tuple(sorted(kwargs.items()))\n            \n            # Try to get from cache\n            try:\n                return cache[args_repr]\n            except KeyError:\n                # Compute and cache the result\n                result = func(*args, **kwargs)\n                cache[args_repr] = result\n                return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n    \n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            \n            # Check if key is in cache\n            if key in cache:\n                # Move to end (most recently used)\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # Compute and store result\n            result = func(*args, **kwargs)\n            \n            # If cache is full, remove the least recently used item\n            if len(cache) >= maxsize:\n                cache.popitem(last=False)\n            \n            cache[key] = result\n            return result\n        \n        # Update wrapper with function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n    \n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Move to end to mark as most recently used\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # Compute and store the result\n            result = func(*args, **kwargs)\n            \n            # If cache is full, remove the least recently used item\n            if len(cache) >= maxsize:\n                cache.popitem(last=False)\n            \n            cache[key] = result\n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import OrderedDict\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU) algorithm.\n    \"\"\"\n    if maxsize <= 0:\n        maxsize = 0\n\n    def decorator(func):\n        # Create a cache using OrderedDict to maintain LRU order\n        cache = OrderedDict()\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Handle typed caching\n            if typed:\n                args_key = tuple(args)\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            else:\n                args_key = args\n                kwargs_key = tuple(sorted(kwargs.items()))\n                key = (args_key, tuple(kwargs_key))\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Move to end to mark as most recently used\n                value = cache.pop(key)\n                cache[key] = value\n                return value\n            \n            # If cache is full, remove the least recently used item\n            if len(cache) >= maxsize:\n                cache.popitem(last=False)\n            \n            # Call the original function and store result\n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.7561962157487869, "generate_results": ["import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Access counter for each key\n        access_count = defaultdict(int)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n            \n            # Check if the key exists in cache\n            if key in cache:\n                # Increment access frequency\n                access_count[key] += 1\n                freq[key] += 1\n                # Return cached value\n                return cache[key]\n            \n            # Function call to compute result\n            result = func(*args, **kwargs)\n            \n            # Add to cache with frequency 1\n            cache[key] = result\n            freq[key] = 1\n            access_count[key] = 1\n            \n            # Evict least frequently used item if cache is full\n            if len(cache) > maxsize:\n                # Find the key with minimum frequency\n                min_freq = min(freq.values())\n                # Find keys with minimum frequency\n                min_keys = [k for k, f in freq.items() if f == min_freq]\n                # If multiple keys have same frequency, pick one with lowest access count\n                if min_keys:\n                    evict_key = min(min_keys, key=lambda k: access_count[k])\n                    del cache[evict_key]\n                    del freq[evict_key]\n                    del access_count[evict_key]\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Used to track the order of access (for LFU eviction)\n        access_order = []\n\n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n\n            # Check if key exists in cache\n            if key in cache:\n                # Increment frequency and move to end of access order\n                freq[key] += 1\n                access_order.remove(key)\n                access_order.append(key)\n                return cache[key]\n\n            # Key not in cache, compute value\n            result = func(*args, **kwargs)\n            \n            # If cache is full, evict least frequently used item\n            if len(cache) >= maxsize:\n                # Find the least frequently used key\n                lfu_key = None\n                min_freq = float('inf')\n                for k in access_order:\n                    if freq[k] < min_freq:\n                        min_freq = freq[k]\n                        lfu_key = k\n                \n                # If multiple keys have same frequency, remove the first one in access order\n                if lfu_key is not None:\n                    del cache[lfu_key]\n                    del freq[lfu_key]\n                    access_order.remove(lfu_key)\n            \n            # Add new key-value pair\n            cache[key] = result\n            freq[key] = 1\n            access_order.append(key)\n            \n            return result\n        \n        # Update wrapper metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Access order to maintain LFU order\n        access_order = []\n\n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n\n            # Check if key exists in cache\n            if key in cache:\n                # Increment frequency and move to end of access order\n                freq[key] += 1\n                access_order.remove(key)\n                access_order.append(key)\n                return cache[key]\n\n            # Key not in cache, compute value\n            result = func(*args, **kwargs)\n            \n            # Add to cache if within maxsize\n            if len(cache) >= maxsize:\n                # Remove least frequently used item\n                lfu_key = access_order.pop(0)\n                del cache[lfu_key]\n                del freq[lfu_key]\n            \n            # Add new key-value pair\n            cache[key] = result\n            freq[key] = 1\n            access_order.append(key)\n            \n            return result\n\n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator", "import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Access counter for each key\n        access_count = defaultdict(int)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n            \n            # Check if the key exists in cache\n            if key in cache:\n                # Increment access count and frequency\n                access_count[key] += 1\n                freq[key] += 1\n                # Return cached value\n                return cache[key]\n            \n            # Function call not in cache\n            result = func(*args, **kwargs)\n            \n            # Add to cache with initial frequency of 1\n            cache[key] = result\n            freq[key] = 1\n            access_count[key] = 1\n            \n            # If cache is full, remove the least frequently used item\n            if len(cache) > maxsize:\n                # Find the key with the lowest frequency\n                min_freq = min(freq.values())\n                # Find all keys with minimum frequency\n                candidates = [k for k, f in freq.items() if f == min_freq]\n                # If there are multiple keys with same frequency, pick one with lowest access count\n                if candidates:\n                    candidate = min(candidates, key=lambda k: access_count[k])\n                    del cache[candidate]\n                    del freq[candidate]\n                    del access_count[candidate]\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator", "import functools\nfrom collections import defaultdict\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        # Cache to store (key, value) pairs and their frequencies\n        cache = {}\n        # Frequency counter for each key\n        freq = defaultdict(int)\n        # Access counter for each key\n        access_count = defaultdict(int)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + (type(func),)\n            \n            # Check if the key is in cache\n            if key in cache:\n                # Increment access count\n                access_count[key] += 1\n                # Update frequency\n                freq[key] += 1\n                # Return cached value\n                return cache[key]\n            \n            # Function not in cache, compute result\n            result = func(*args, **kwargs)\n            \n            # Add to cache with initial frequency of 1\n            cache[key] = result\n            freq[key] = 1\n            access_count[key] = 1\n            \n            # If cache is full, remove the least frequently used item\n            if len(cache) > maxsize:\n                # Find the key with minimum frequency\n                min_freq = min(freq.values())\n                # Find keys with minimum frequency\n                candidates = [k for k, f in freq.items() if f == min_freq]\n                # If multiple keys have same frequency, pick one with lowest access count\n                if candidates:\n                    candidate = min(candidates, key=lambda k: access_count[k])\n                    del cache[candidate]\n                    del freq[candidate]\n                    del access_count[candidate]\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 996, "num_retrieved": 5, "inference_time": 0.7561962157487869, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get the first item in the order of insertion\n    key = next(iter(self))\n    value = self.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get the first inserted key (oldest item)\n    key = next(iter(self.__links))\n    value = self.pop(key)\n    # Remove the key from links\n    del self.__links[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get the first key (oldest insertion) from the links\n    key = next(iter(self.__links))\n    value = self.pop(key)\n    # Remove the key from links\n    del self.__links[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get the first key (oldest insertion) from the links\n    key = next(iter(self.__links))\n    value = self.pop(key)\n    # Remove the key from links\n    del self.__links[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair first inserted.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get the first inserted key (oldest item)\n    key = next(iter(self))\n    value = self.pop(key)\n    return (key, value)"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 944, "num_retrieved": 3, "inference_time": 0.7561962157487869, "generate_results": ["def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 931, "num_retrieved": 4, "inference_time": 0.7561962157487869, "generate_results": ["def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    if key in self:\n        return self[key]\n    return default", "def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    if key in self:\n        return self[key]\n    return default", "def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    if key in self:\n        return self[key]\n    return default", "def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    if key in self:\n        return self[key]\n    return default", "def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    if key in self:\n        return self[key]\n    return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1026, "num_retrieved": 6, "inference_time": 0.7561962157487869, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_value = key_func(self, *args, **kwargs)\n            else:\n                key_value = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_instance = self._cache\n            else:\n                cache_instance = cache\n                \n            # Use lock if provided to handle concurrency\n            if lock is not None:\n                with lock:\n                    result = cache_instance.get(key_value)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_instance[key_value] = result\n                    return result\n            else:\n                result = cache_instance.get(key_value)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_instance[key_value] = result\n                return result\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_val = key_func(self, *args, **kwargs)\n            else:\n                key_val = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_instance = self._cache\n            else:\n                cache_instance = cache\n                \n            # Use lock if provided to handle concurrent access\n            if lock is not None:\n                with lock:\n                    result = cache_instance.get(key_val)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_instance[key_val] = result\n                    return result\n            else:\n                result = cache_instance.get(key_val)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_instance[key_val] = result\n                return result\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_val = key_func(self, *args, **kwargs)\n            else:\n                key_val = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_instance = self._cache\n            else:\n                cache_instance = cache\n                \n            # Use lock if provided for thread safety\n            if lock is not None:\n                with lock:\n                    result = cache_instance.get(key_val)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_instance[key_val] = result\n                    return result\n            else:\n                result = cache_instance.get(key_val)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_instance[key_val] = result\n                return result\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_val = key_func(self, *args, **kwargs)\n            else:\n                key_val = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_instance = self._cache\n            else:\n                cache_instance = cache\n                \n            # Use lock if provided to handle concurrent access\n            if lock is not None:\n                with lock:\n                    result = cache_instance.get(key_val)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_instance[key_val] = result\n                    return result\n            else:\n                result = cache_instance.get(key_val)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_instance[key_val] = result\n                return result\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    def decorator(method):\n        import functools\n        \n        @functools.wraps(method)\n        def wrapper(self, *args, **kwargs):\n            # Generate key using the provided key function\n            key_func = key\n            if callable(key_func):\n                key_val = key_func(self, *args, **kwargs)\n            else:\n                key_val = key_func(self, *args, **kwargs)\n            \n            # Use the cache to store and retrieve results\n            if hasattr(self, '_cache'):\n                cache_instance = self._cache\n            else:\n                cache_instance = cache\n                \n            # Use lock if provided for thread safety\n            if lock is not None:\n                with lock:\n                    result = cache_instance.get(key_val)\n                    if result is None:\n                        result = method(self, *args, **kwargs)\n                        cache_instance[key_val] = result\n                    return result\n            else:\n                result = cache_instance.get(key_val)\n                if result is None:\n                    result = method(self, *args, **kwargs)\n                    cache_instance[key_val] = result\n                return result\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 939, "num_retrieved": 3, "inference_time": 0.7561962157487869, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype: str\n    \"\"\"\n    import traceback\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_str = ''.join(traceback.format_list(tb))\n        return f\"{type(e).__name__}: {e}\\n{tb_str}\"\n    except:\n        return f\"{type(e).__name__}: {e}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype: str\n    \"\"\"\n    import traceback\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_str = ''.join(traceback.format_list(tb))\n        return f\"{type(e).__name__}: {e}\\n{tb_str}\"\n    except:\n        return f\"{type(e).__name__}: {e}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype: str\n    \"\"\"\n    import traceback\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_str = ''.join(traceback.format_list(tb))\n        return f\"{type(e).__name__}: {e}\\n{tb_str}\"\n    except:\n        return f\"{type(e).__name__}: {e}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype: str\n    \"\"\"\n    import traceback\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_str = ''.join(traceback.format_list(tb))\n        return f\"{type(e).__name__}: {e}\\n{tb_str}\"\n    except:\n        return f\"{type(e).__name__}: {e}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype: str\n    \"\"\"\n    import traceback\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_str = ''.join(traceback.format_list(tb))\n        return f\"{type(e).__name__}: {e}\\n{tb_str}\"\n    except:\n        return f\"{type(e).__name__}: {e}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1013, "num_retrieved": 6, "inference_time": 0.7561962157487869, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = \"w\" if overwrite else \"a\"\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = \"w\" if overwrite else \"a\"\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 951, "num_retrieved": 5, "inference_time": 0.6332872658967972, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as file:\n            binary_data = file.read()\n        return binary_data.decode(encoding)\n    except Exception as e:\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 958, "num_retrieved": 5, "inference_time": 0.6332872658967972, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    import os\n    if not file_name or not isinstance(file_name, str):\n        return False\n    return os.path.exists(file_name)"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1023, "num_retrieved": 6, "inference_time": 0.6332872658967972, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.root.addHandler(logging.StreamHandler())", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    # Reconfigure logging if needed based on platform or environment\n    # This is a placeholder - actual implementation may depend on context\n    # such as platform, environment variables, or specific logging requirements", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    # Reconfigure logging if needed based on platform or environment\n    # This is a placeholder - actual implementation may depend on context\n    # such as platform, environment variables, or specific logging requirements", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    # Reconfigure logging if needed based on platform or environment\n    # This is a placeholder - actual implementation may depend on context\n    # such as platform, environment variables, or specific logging requirements", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    # Reconfigure logging if needed based on platform or environment\n    # This is a placeholder - actual implementation may depend on context\n    # such as platform, environment variables, or specific logging requirements"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1025, "num_retrieved": 6, "inference_time": 0.6332872658967972, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 943, "num_retrieved": 3, "inference_time": 0.6332872658967972, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and to track the final merged ordering\n    final_order = []\n    seen = set()\n    \n    # For each ordering, we need to ensure that any common suffixes are preserved\n    # We'll build the result by processing orderings in a way that respects all constraints\n    \n    # First, find the common suffixes and merge them\n    # We'll use a greedy approach: process orderings and merge them while preserving order\n    \n    # Group orderings by their common suffixes\n    from collections import defaultdict\n    \n    # For each element, find all orderings that contain it\n    element_orderings = defaultdict(list)\n    for ordering in orderings:\n        for i, elem in enumerate(ordering):\n            element_orderings[elem].append(ordering)\n    \n    # Build the result by processing elements in order\n    # We need to ensure that if an element appears in multiple orderings,\n    # the suffix starting with that element must be in all of them\n    \n    # Create a set of all elements that appear in any ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Sort elements by their first occurrence in any ordering\n    # We'll use the first occurrence of each element in any ordering\n    first_occurrence = {}\n    for elem in all_elements:\n        for ordering in orderings:\n            if elem in ordering:\n                if elem not in first_occurrence:\n                    first_occurrence[elem] = ordering.index(elem)\n                break\n    \n    # Sort elements by their first occurrence\n    sorted_elements = sorted(all_elements, key=lambda x: first_occurrence[x])\n    \n    # Build the final ordering", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # We'll use a list of sets to track which orderings contain each element\n    element_to_orderings = {}\n    for ordering in orderings:\n        for element in ordering:\n            if element not in element_to_orderings:\n                element_to_orderings[element] = []\n            element_to_orderings[element].append(ordering)\n    \n    # Find the common suffix for each element\n    # For each element, we need to find the ordering that contains it and has the longest suffix\n    result = []\n    seen = set()\n    \n    # Sort orderings by length in descending order to process longer ones first\n    sorted_orderings = sorted(orderings, key=len, reverse=True)\n    \n    # For each ordering, add elements that are not yet in the result\n    for ordering in sorted_orderings:\n        for element in ordering:\n            if element not in seen:\n                # Find the position of this element in the current ordering\n                pos = ordering.index(element)\n                # Check if this element appears in any other ordering\n                # If it does, we need to ensure that the suffix starting at this element\n                # is present in all orderings that contain it\n                if element in element_to_orderings:\n                    # Get all orderings that contain this element\n                    orderings_with_element = element_to_orderings[element]\n                    # Find the minimum position where this element appears in any ordering\n                    min_pos = min(ordering.index(element) for ordering in orderings_with_element)\n                    # Find the maximum position where this element appears in any ordering\n                    max_pos = max(ordering.index(element) for ordering in orderings_with_element)\n                    # We need to ensure that the", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Find the common suffix for each prefix\n    result = []\n    used = set()\n    \n    # Sort orderings by length in descending order to handle longer ones first\n    orderings.sort(key=len, reverse=True)\n    \n    for ordering in orderings:\n        # For each element in the ordering, check if it's already in the result\n        # and if it's the start of a suffix that must be preserved\n        for i, item in enumerate(ordering):\n            if item not in used:\n                # Find the longest suffix starting with item that appears in any ordering\n                suffix_found = False\n                for other_ordering in orderings:\n                    if item in other_ordering and other_ordering.index(item) == 0:\n                        # This item is the start of a suffix in another ordering\n                        # We need to preserve the full suffix\n                        suffix_found = True\n                        break\n                \n                # If we found a suffix, we need to include the full suffix\n                if suffix_found:\n                    # Find the full suffix starting with item\n                    for other_ordering in orderings:\n                        if item in other_ordering:\n                            suffix_start = other_ordering.index(item)\n                            suffix = other_ordering[suffix_start:]\n                            # Add the suffix to result if not already present\n                            for s in suffix:\n                                if s not in used:\n                                    result.append(s)\n                                    used.add(s)\n                else:\n                    # Item is not part of any suffix, just add it\n                    if item not in used:\n                        result.append(item)\n                        used.add(item)\n    \n    # Reconstruct the result in the order of the first ordering\n    # This is a simplified approach - the actual logic needs to ensure\n    # that the orderings are merged", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and to track the final merged ordering\n    final_order = []\n    position_map = {}\n    \n    # For each ordering, we need to ensure that if an element appears in multiple\n    # orderings, the suffix starting with that element must be preserved in all\n    # orderings. We'll build the final ordering by merging the orderings in a way\n    # that respects the constraints.\n    \n    # First, collect all unique elements and their positions in each ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Create a list of all elements in the orderings, and for each element,\n    # determine the order in which it appears in each ordering\n    element_positions = {}\n    for element in all_elements:\n        positions = []\n        for ordering in orderings:\n            try:\n                positions.append(ordering.index(element))\n            except ValueError:\n                # Element not found in this ordering\n                pass\n        element_positions[element] = positions\n    \n    # Sort elements by their first occurrence in any ordering\n    # We need to find the earliest position where an element appears\n    # across all orderings\n    sorted_elements = []\n    seen = set()\n    \n    # Process each ordering to build the merged sequence\n    # We'll use a greedy approach: for each ordering, we add elements\n    # that are not yet in the final sequence, but we must respect\n    # the constraint that if an element appears in multiple orderings,\n    # the suffix starting with that element must be in all orderings.\n    \n    # Instead, we can use a simpler approach: find the common suffix\n    #", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # We'll use a list of sets to track which orderings contain each element\n    element_to_orderings = {}\n    for ordering in orderings:\n        for element in ordering:\n            if element not in element_to_orderings:\n                element_to_orderings[element] = []\n            element_to_orderings[element].append(ordering)\n    \n    # Build the result list\n    result = []\n    # Keep track of which orderings we've processed\n    processed_orderings = set()\n    \n    # For each ordering, we need to find the earliest element that appears in all orderings\n    # that contain it\n    while orderings:\n        # Find the first ordering that has an element not yet in result\n        current_ordering = None\n        for ordering in orderings:\n            if not any(elem in result for elem in ordering):\n                current_ordering = ordering\n                break\n        \n        if current_ordering is None:\n            break\n            \n        # Find the first element in current_ordering that is not already in result\n        first_element = None\n        for elem in current_ordering:\n            if elem not in result:\n                first_element = elem\n                break\n                \n        if first_element is None:\n            break\n            \n        # Add the first element to result\n        result.append(first_element)\n        \n        # Remove the current ordering from orderings\n        orderings.remove(current_ordering)\n        \n        # For each element in current_ordering, find the suffix that starts with it\n        # and check if it exists in any other ordering\n        for elem in current_ordering:\n            if elem in result:\n                # Find all orderings that contain elem\n                elem_orderings = element"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1028, "num_retrieved": 6, "inference_time": 0.6332872658967972, "generate_results": ["def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return None\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return None\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return None\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return None\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return None\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1122, "num_retrieved": 10, "inference_time": 0.6332872658967972, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro (method resolution order) to ensure minimal bases\n    # This ensures that if a class is a subclass of another, the subclass comes first\n    # in the minimal base list\n    result = []\n    for cls in unique_classes:\n        # Check if this class is already covered by a previous class in the list\n        # by checking if it's a subclass of any existing class\n        is_covered = False\n        for existing in result:\n            if issubclass(cls, existing):\n                is_covered = True\n                break\n        if not is_covered:\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro order to ensure minimal bases\n    # This is a simplified version assuming we want to preserve the order\n    # and remove redundant bases\n    return tuple(unique_classes)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro (method resolution order) to ensure minimal bases\n    # This ensures that if a class is a subclass of another, the subclass comes first\n    # We use the fact that in Python, the MRO is defined such that a class appears before its parents\n    # So we sort by the class's position in the MRO of the first class\n    from collections import defaultdict\n    \n    # Create a mapping from class to its MRO\n    mro_map = {}\n    for cls in unique_classes:\n        mro_map[cls] = tuple(cls.__mro__)\n    \n    # Sort classes by their MRO depth (shorter MRO first)\n    # This ensures we keep only the minimal set\n    sorted_classes = sorted(unique_classes, key=lambda cls: len(mro_map[cls]))\n    \n    # Remove any class that is a subclass of another class in the list\n    result = []\n    for cls in sorted_classes:\n        is_subclass = False\n        for other in result:\n            if issubclass(cls, other):\n                is_subclass = True\n                break\n        if not is_subclass:\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro (method resolution order) to ensure minimal bases\n    # This ensures that if a class is a subclass of another, the subclass comes first\n    # We use the fact that in Python, if A is a subclass of B, then A appears before B in the MRO\n    # But we need to ensure we don't have redundant bases\n    result = []\n    for cls in unique_classes:\n        # Check if this class is already covered by a previous class in the list\n        # by checking if it's a subclass of any previously added class\n        is_covered = False\n        for existing in result:\n            if issubclass(cls, existing):\n                is_covered = True\n                break\n        if not is_covered:\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro (method resolution order) to ensure minimal bases\n    # This ensures that if a class is a subclass of another, the subclass comes first\n    # in the minimal base list\n    result = []\n    for cls in unique_classes:\n        # Check if this class is already covered by a previous class in the list\n        # by checking if it's a subclass of any existing class\n        is_covered = False\n        for existing in result:\n            if issubclass(cls, existing):\n                is_covered = True\n                break\n        if not is_covered:\n            result.append(cls)\n    \n    return tuple(result)"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1215, "num_retrieved": 13, "inference_time": 0.6332872658967972, "generate_results": ["def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes with their descriptions\n        names = self.names(False)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(True)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes with their descriptions\n        names = self.names(False)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(True)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes with their descriptions\n        names = self.names(False)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(True)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes with their descriptions\n        names = self.names(False)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(True)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes with their descriptions\n        names = self.names(False)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(True)\n        descriptions = []\n        for name in names:\n            attr = getattr(self, name, None)\n            if attr is not None and hasattr(attr, 'description'):\n                descriptions.append((name, attr.description))\n        return descriptions"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1289, "num_retrieved": 15, "inference_time": 0.6332872658967972, "generate_results": ["def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        # Return only the names defined directly by the interface\n        return list(self._names.keys())\n    else:\n        # Return all names defined by the interface directly and indirectly by base interfaces\n        names = set()\n        current = self\n        while current is not None:\n            names.update(current._names.keys())\n            current = current._base\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        # Return only the names defined directly by the interface\n        return list(self.__dict__.keys())\n    else:\n        # Return all names defined by the interface directly and indirectly by base interfaces\n        names = set()\n        current = self\n        while current is not None:\n            names.update(current.__dict__.keys())\n            current = current.__base__\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        # Return only the names defined directly by the interface\n        return list(self.__dict__.keys())\n    else:\n        # Return all names defined by the interface directly and indirectly by base interfaces\n        names = set()\n        current = self\n        while current is not None:\n            names.update(current.__dict__.keys())\n            current = current.__base__\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        # Return only the names defined directly by the interface\n        return list(self.__dict__.keys())\n    else:\n        # Return all names defined by the interface directly and indirectly by base interfaces\n        names = set()\n        current = self\n        while current is not None:\n            names.update(current.__dict__.keys())\n            current = current.__base__\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        # Return only the names defined directly by the interface\n        return list(self.__dict__.keys())\n    else:\n        # Return all names defined by the interface directly and indirectly by base interfaces\n        names = set()\n        current = self\n        while current is not None:\n            names.update(current.__dict__.keys())\n            current = current.__base__\n        return list(names)"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 998, "num_retrieved": 5, "inference_time": 0.6332872658967972, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.append(item)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.append(item)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.append(item)\n        else:\n            output.append(item)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 944, "num_retrieved": 3, "inference_time": 0.6332872658967972, "generate_results": ["def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1004, "num_retrieved": 5, "inference_time": 0.6332872658967972, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python and pure_python.lower() in ('1', 'true', 'yes'):\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python and pure_python.lower() in ('1', 'true', 'yes'):\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.getenv('PURE_PYTHON')\n    if pure_python and pure_python.lower() in ('1', 'true', 'yes'):\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python and pure_python.lower() in ('1', 'true', 'yes'):\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python and pure_python.lower() in ('1', 'true', 'yes'):\n        return False\n    \n    return True"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1204, "num_retrieved": 13, "inference_time": 0.6332872658967972, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1018, "num_retrieved": 6, "inference_time": 0.6332872658967972, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    from zope.interface._compat import _use_c_impl\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    from zope.interface._compat import _use_c_impl\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    from zope.interface._compat import _use_c_impl\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    from zope.interface._compat import _use_c_impl\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    from zope.interface._compat import _use_c_impl\n    return not _use_c_impl()"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 969, "num_retrieved": 4, "inference_time": 0.6332872658967972, "generate_results": ["def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    else:\n        self.bins = self.make_bins()", "def reset(self):\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    else:\n        self.bins = self.make_bins()", "def reset(self):\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    else:\n        self.bins = self.make_bins()", "def reset(self):\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    else:\n        self.bins = self.make_bins()", "def reset(self):\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    else:\n        self.bins = self.make_bins()"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 912, "num_retrieved": 2, "inference_time": 0.6332872658967972, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # It is provided for backward compatibility only.\n    # Actual conversion to CSV should be handled by ToCSV class.\n    \n    if header is not None:\n        result = header + \"\\n\"\n    else:\n        result = \"\"\n    \n    # Assuming self.points contains the data points to be converted\n    # Each point is a tuple of coordinates and values\n    for point in self.points:\n        coord_str = separator.join(str(coord) for coord in point[0])\n        value_str = separator.join(str(value) for value in point[1])\n        result += f\"{coord_str},{value_str}\\n\"\n    \n    return result", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # It is provided for backward compatibility only.\n    # Actual conversion to CSV should use :class:`lena.output.ToCSV`\n    \n    # Placeholder implementation - actual behavior would depend on graph structure\n    # and point data, which is not fully specified in the context.\n    \n    if header is not None:\n        result = header + \"\\n\"\n    else:\n        result = \"\"\n    \n    # Assuming graph points are iterable and each point has coordinates and values\n    # This is a simplified implementation - real implementation would require\n    # access to the graph's point data structure\n    for point in self.points:\n        coords = [str(coord) for coord in point.coords]\n        values = [str(val) for val in point.values]\n        row = separator.join(coords + values)\n        result += row + \"\\n\"\n    \n    return result", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # It is provided for backward compatibility only.\n    # Actual conversion to CSV should be handled by ToCSV class.\n    \n    if header is not None:\n        result = header + \"\\n\"\n    else:\n        result = \"\"\n    \n    # Since the actual implementation of converting graph points to CSV\n    # is not specified in the context and the function is deprecated,\n    # we return an empty string or just the header as per the requirement.\n    # In a real implementation, this would iterate over graph points\n    # and format them into CSV lines.\n    \n    # Placeholder implementation - actual logic would depend on graph structure\n    # and point data access.\n    return result", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # It is provided for backward compatibility only.\n    # Actual conversion to CSV should be handled by ToCSV class.\n    \n    # Placeholder implementation that would convert points to CSV\n    # in a basic form, but note that this is deprecated and not recommended.\n    \n    points = self.points  # Assuming self.points contains the graph points\n    \n    if header is not None:\n        result = [header]\n    \n    for point in points:\n        # Convert coordinates to string, separated by separator\n        coords = [str(coord) for coord in point.coords]\n        coord_str = separator.join(coords)\n        \n        # Convert values to string, separated by separator\n        values = [str(val) for val in point.values]\n        value_str = separator.join(values)\n        \n        result.append(f\"{coord_str},{value_str}\")\n    \n    return \"\\n\".join(result)", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # It is provided for backward compatibility only.\n    # Actual conversion to CSV should be handled by ToCSV class.\n    \n    # Placeholder implementation that simply raises a deprecation warning\n    import warnings\n    warnings.warn(\"to_csv is deprecated in Lena 0.5 and will be removed. Use ToCSV instead.\", DeprecationWarning)\n    \n    # Since the actual conversion logic is not specified in the context,\n    # and the function is deprecated, we return an empty string\n    # or could implement a basic CSV generation if needed.\n    if header is not None:\n        result = header + \"\\n\"\n    else:\n        result = \"\"\n    \n    # In a real implementation, we would iterate over points and format them\n    # as CSV rows based on coordinates and values.\n    # However, without specific data structure details, this is a minimal stub.\n    \n    return result"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1106, "num_retrieved": 9, "inference_time": 0.7247302085161209, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    if coord_name == \"x\":\n        return self._error_x_ind\n    elif coord_name == \"y\":\n        return self._error_y_ind\n    else:\n        return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    if coord_name == \"x\":\n        return self._error_x_ind\n    elif coord_name == \"y\":\n        return self._error_y_ind\n    else:\n        return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    if coord_name == \"x\":\n        return self._error_x_ind\n    elif coord_name == \"y\":\n        return self._error_y_ind\n    else:\n        return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    if coord_name == \"x\":\n        return self._error_x_ind\n    elif coord_name == \"y\":\n        return self._error_y_ind\n    else:\n        return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    if coord_name == \"x\":\n        return self._error_x_ind\n    elif coord_name == \"y\":\n        return self._error_y_ind\n    else:\n        return None"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 877, "num_retrieved": 1, "inference_time": 0.7247302085161209, "generate_results": ["def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph's properties\n    # Assuming the graph has fields like \"E\", \"t\", \"error_E_low\", etc.\n    # Map error fields to x, y, z based on their names\n    error_fields = {}\n    for field in self._fields:\n        if field.startswith(\"error_\"):\n            # Extract the base name (e.g., \"error_E_low\" -> \"E_low\")\n            base_name = field[7:]  # remove \"error_\"\n            # Split by underscore to get components\n            parts = base_name.split(\"_\")\n            if len(parts) >= 2:\n                # Use the first part as the coordinate (x, y, z)\n                coord = parts[0]\n                # Map to x, y, z\n                if coord in [\"x\", \"y\", \"z\"]:\n                    error_fields[coord + \"_\" + parts[1]] = field\n                else:\n                    # Map to x, y, z based on first letter or default\n                    if coord[0].lower() in \"xyz\":\n                        error_fields[coord + \"_\" + parts[1]] = field\n                    else:\n                        error_fields[\"x_\" + parts[1]] = field\n            else:\n                error_fields[\"x_\" + base_name] = field\n    \n    # Append error indices to context.error\n    if not context.get(\"error\"):\n        context[\"error\"] = {}\n    \n    # Create a subcontext for each error field\n    for error_key, field_name in error_fields.items():\n        # Get the index from the field name (e.g., \"error_E", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph's properties\n    # Assuming the graph has fields like \"E\", \"t\", \"error_E_low\", etc.\n    # Map error fields to x, y, z based on their names\n    error_fields = {}\n    for field in self._fields:\n        if field.startswith(\"error_\"):\n            # Extract the base name (e.g., \"error_E_low\" -> \"E_low\")\n            base_name = field[7:]  # remove \"error_\"\n            # Split by underscore to get component names\n            parts = base_name.split(\"_\")\n            if len(parts) >= 2:\n                # Use the first part as coordinate (x, y, z)\n                coord = parts[0]\n                # Map to x, y, z\n                coord_map = {\"x\": \"x\", \"y\": \"y\", \"z\": \"z\"}\n                if coord in coord_map:\n                    error_key = coord_map[coord]\n                    error_subkey = parts[1] if len(parts) > 1 else \"\"\n                    error_fields[f\"{error_key}_{error_subkey}\"] = field\n    \n    # Append indices of errors to context.error\n    if not hasattr(context, 'error'):\n        context.error = {}\n    \n    # Initialize error dictionary if not present\n    if not context.error:\n        context.error = {}\n    \n    # Append error indices (example: index 2 for error_E_low)\n    # This assumes that the graph has an error index stored somewhere\n    # For now, we assume the error index is stored in a field like 'error_index'\n    if hasattr", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph's properties\n    # Assuming the graph has fields like \"E\", \"t\", \"error_E_low\", etc.\n    # We map error fields to x, y, z based on the first three coordinates\n    error_fields = []\n    for field in self._fields:\n        if field.startswith(\"error_\"):\n            # Extract the base name (e.g., \"error_E_low\" -> \"E_low\")\n            base_name = field[7:]  # remove \"error_\"\n            # Split by underscore to get the component (e.g., \"E_low\" -> \"E\", \"low\")\n            parts = base_name.split(\"_\")\n            if len(parts) >= 2:\n                # First part is the coordinate (x, y, z)\n                coord = parts[0]\n                # Map to x, y, z\n                coord_map = {\"x\": \"x\", \"y\": \"y\", \"z\": \"z\"}\n                if coord in coord_map:\n                    error_name = coord_map[coord]\n                    # Append the error field with index\n                    error_fields.append((error_name, base_name))\n    \n    # If there are error fields, update the context.error\n    if error_fields:\n        error_dict = context.get(\"error\", {})\n        for error_name, field_name in error_fields:\n            # Append index of the error field\n            if error_name not in error_dict:\n                error_dict[error_name] = {\"index\": 0}\n            else:\n                error_dict[error_name][\"index\"] = error_dict[error_name].get(\"index\", 0", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph's properties\n    # Assuming the graph has fields like \"E\", \"t\", \"error_E_low\", etc.\n    # We map error fields to x, y, z based on the first three coordinates\n    error_fields = {}\n    \n    # Example: if fields are \"E\", \"t\", \"error_E_low\", \"error_t_low\"\n    # then error_E_low -> x_low, error_t_low -> y_low\n    # We assume the error field names follow the pattern \"error_<field>_low\" or \"error_<field>_high\"\n    \n    # Get all field names from the graph\n    fields = self.get_fields()  # This should be defined in the class\n    \n    for field in fields:\n        if field.startswith(\"error_\"):\n            # Extract the base field name (e.g., \"E\" from \"error_E_low\")\n            base_field = field[7:]  # remove \"error_\"\n            # Check if base_field is one of x, y, z\n            if base_field in [\"x\", \"y\", \"z\"]:\n                # Error field corresponds to x, y, z\n                error_name = base_field + \"_low\"  # or \"_high\", but we assume low\n                error_fields[error_name] = {\"index\": fields.index(field)}\n            else:\n                # Map to x, y, z based on position\n                # e.g., if base_field is \"E\", map to x; \"t\" to y; etc.\n                if base_field == \"E\":\n                    error_name = \"x_low\"", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph's properties\n    # Assuming the graph has fields like \"E\", \"t\", \"error_E_low\", etc.\n    # We map error fields to x, y, z based on the first three coordinates\n    error_fields = {}\n    # Example: if fields are \"E\", \"t\", \"error_E_low\", \"error_t_low\"\n    # then error_E_low -> x_low, error_t_low -> y_low\n    # We assume the graph has a list of fields or attributes\n    \n    # Since we don't have direct access to fields, we assume they are stored\n    # in a way that we can extract them from self._fields or similar\n    # For now, we assume self._fields contains the field names\n    if hasattr(self, '_fields'):\n        fields = self._fields\n        for field in fields:\n            if field.startswith('error_'):\n                # Extract the base name (e.g., 'error_E_low' -> 'E_low')\n                base_name = field[7:]  # remove 'error_'\n                # Split by underscore to get the coordinate and direction\n                parts = base_name.split('_')\n                if len(parts) >= 2:\n                    coord_part = parts[0]\n                    direction_part = parts[1]\n                    # Map coord_part to x, y, z\n                    coord_map = {'E': 'x', 't': 'y', 'x': 'x', 'y': 'y', 'z': 'z'}\n                    coord = coord_map.get(coord_part, 'x')\n                    # Direction: low, high"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 941, "num_retrieved": 3, "inference_time": 0.7247302085161209, "generate_results": ["def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(bins) == 0 or len(edges) < 2:\n        return 0.0\n    \n    integral_value = 0.0\n    for i in range(len(edges) - 1):\n        width = edges[i + 1] - edges[i]\n        bin_value = bins[i]\n        integral_value += bin_value * width\n    \n    return integral_value", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(bins) == 0 or len(edges) < 2:\n        return 0.0\n    \n    integral_value = 0.0\n    for i in range(len(edges) - 1):\n        width = edges[i + 1] - edges[i]\n        bin_value = bins[i]\n        if isinstance(bin_value, (int, float)):\n            integral_value += bin_value * width\n        else:\n            # If bin value is not a number, treat it as 0\n            integral_value += 0.0\n    \n    return integral_value", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(edges) <= 1:\n        return 0.0\n    \n    # Compute the integral by summing the product of bin values and bin widths\n    total_integral = 0.0\n    for i in range(len(edges) - 1):\n        bin_width = edges[i + 1] - edges[i]\n        bin_value = bins[i]\n        total_integral += bin_value * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(bins) == 0 or len(edges) < 2:\n        return 0.0\n    \n    integral_value = 0.0\n    for i in range(len(edges) - 1):\n        width = edges[i + 1] - edges[i]\n        bin_value = bins[i]\n        if isinstance(bin_value, (int, float)):\n            integral_value += bin_value * width\n        else:\n            # If bin value is not a number, treat it as 0\n            integral_value += 0.0\n    \n    return integral_value", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    if len(edges) <= 1:\n        return 0.0\n    \n    integral_value = 0.0\n    for i in range(len(edges) - 1):\n        width = edges[i + 1] - edges[i]\n        integral_value += bins[i] * width\n    \n    return integral_value"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 909, "num_retrieved": 2, "inference_time": 0.7247302085161209, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for element in seq:\n            if isinstance(element, fill_request_seq.FillRequestSeq):\n                return True\n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1152, "num_retrieved": 11, "inference_time": 0.7247302085161209, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request')"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1282, "num_retrieved": 18, "inference_time": 0.7247302085161209, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1152, "num_retrieved": 11, "inference_time": 0.7247302085161209, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute')"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 916, "num_retrieved": 2, "inference_time": 0.7247302085161209, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys in d1\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                # Different values at same key\n                result[key] = d1[key]\n    else:\n        # Limited recursion depth\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys in d1\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                # Different values at same key\n                result[key] = d1[key]\n    else:\n        # Finite recursion: only go one level deep\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries at level 1\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys in d1\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                # Different values at same key\n                result[key] = d1[key]\n    else:\n        # Finite recursion: only go one level deep\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries at level 1\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys in d1\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                # Different values at same key\n                result[key] = d1[key]\n    else:\n        # Finite recursion: only go one level deep\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries at level 1\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys in d1\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                # Different values at same key\n                result[key] = d1[key]\n    else:\n        # Finite recursion: only go one level deep\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                # Recursively process nested dictionaries at level 1\n                nested_diff = difference(d1[key], d2[key], level=level - 1)\n                if nested_diff:\n                    result[key] = nested_diff\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1004, "num_retrieved": 5, "inference_time": 0.7247302085161209, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges of the histogram\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    for i, (lower, upper) in enumerate(zip(edges, edges[1:])):\n        if not (lower <= coord[i] < upper):\n            return  # Ignore coordinates outside the edges\n    \n    # Get the bin index for the given coordinates\n    bin_indices = []\n    for i, (lower, upper) in enumerate(edges):\n        bin_index = int((coord[i] - lower) / (upper - lower) * (len(self.bins[i]) - 1))\n        bin_index = max(0, min(bin_index, len(self.bins[i]) - 2))\n        bin_indices.append(bin_index)\n    \n    # Fill the bin with the given weight\n    bin_content = lena.structures.get_bin_on_index(bin_indices, self.bins)\n    bin_content += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges for the histogram\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    for i, (lower, upper) in enumerate(zip(edges, edges[1:])):\n        if not (lower <= coord[i] < upper):\n            return  # Ignore coordinates outside the edges\n    \n    # Get the bin index for the given coordinates\n    bin_index = []\n    for i, (lower, upper) in enumerate(edges):\n        bin_index.append(int((coord[i] - lower) / (upper - lower) * (len(edges[i]) - 1)))\n    \n    # Fill the histogram at the bin index with the given weight\n    bin_content = lena.structures.get_bin_on_index(bin_index, self.bins)\n    bin_content += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges of the histogram\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    for i, (lower, upper) in enumerate(zip(edges, edges)):\n        if not (lower <= coord[i] < upper):\n            return  # Ignore coordinates outside the edges\n    \n    # Get the bin index for the given coordinates\n    bin_index = []\n    for i, (lower, upper) in enumerate(edges):\n        bin_index.append(int((coord[i] - lower) / (upper - lower) * (len(self.bins[i]) - 1)))\n    \n    # Ensure bin index is within bounds\n    for i, idx in enumerate(bin_index):\n        if idx < 0:\n            bin_index[i] = 0\n        elif idx >= len(self.bins[i]):\n            bin_index[i] = len(self.bins[i]) - 1\n    \n    # Fill the bin with the given weight\n    bin_content = lena.structures.get_bin_on_index(bin_index, self.bins)\n    bin_content += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges of the histogram\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    for i, (lower, upper) in enumerate(zip(edges[0], edges[1])):\n        if not (lower <= coord[i] < upper):\n            return  # Ignore coordinates outside the edges\n    \n    # Get the bin index for the given coordinates\n    bin_indices = []\n    for i, (lower, upper) in enumerate(zip(edges[0], edges[1])):\n        bin_index = int((coord[i] - lower) / (upper - lower) * (len(edges[i]) - 1))\n        bin_indices.append(bin_index)\n    \n    # Fill the bin with the given weight\n    bin_content = lena.structures.get_bin_on_index(bin_indices, self.bins)\n    bin_content += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges of the histogram\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    for i, (lower, upper) in enumerate(zip(edges, edges[1:])):\n        if not (lower <= coord[i] < upper):\n            return  # Ignore coordinates outside the edges\n    \n    # Get the bin index for the given coordinates\n    bin_indices = []\n    for i, (lower, upper) in enumerate(edges):\n        bin_idx = int((coord[i] - lower) / (upper - lower) * (len(self.bins[i]) - 1))\n        bin_idx = max(0, min(bin_idx, len(self.bins[i]) - 2))\n        bin_indices.append(bin_idx)\n    \n    # Fill the bin with the given weight\n    bin_content = lena.structures.get_bin_on_index(bin_indices, self.bins)\n    bin_content += weight"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 914, "num_retrieved": 2, "inference_time": 0.7247302085161209, "generate_results": ["import re\nfrom typing import Dict, Any, List\nfrom validation import ValidationError  # Assuming ValidationError is defined elsewhere\n\ndef _validate_labels(labels):\n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        else:\n            # Check if key matches regex pattern for label keys (e.g., alphanumeric, hyphen, underscore)\n            key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n            if not re.match(key_pattern, key):\n                errors.append({key: \"Label key does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n    \n    if errors:\n        raise ValidationError(messages=errors)", "import re\nfrom typing import Dict, Any, List\nfrom validation import ValidationError  # Assuming ValidationError is defined elsewhere\n\ndef _validate_labels(labels):\n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        else:\n            # Check if key matches regex pattern (e.g., valid label key format)\n            # Common label key regex: alphanumeric, hyphen, underscore, starting with letter or digit\n            key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n            if not re.match(key_pattern, key):\n                errors.append({key: \"Label key does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n    \n    if errors:\n        raise ValidationError(messages=errors)", "import re\nfrom typing import Dict, Any, List\nfrom validation import ValidationError  # Assuming ValidationError is defined elsewhere\n\ndef _validate_labels(labels):\n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        else:\n            # Check if key matches the regex pattern for label keys (e.g., alphanumeric, hyphen, underscore, etc.)\n            # Common label key regex: alphanumeric, hyphen, underscore, starting with letter or digit\n            key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n            if not re.match(key_pattern, key):\n                errors.append({key: \"Label key does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n    \n    if errors:\n        raise ValidationError(messages=errors)", "import re\nfrom typing import Dict, Any, List\nfrom validation import ValidationError  # Assuming ValidationError is defined elsewhere\n\ndef _validate_labels(labels: Dict[Any, Any]):\n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, (str, int, float)):\n            errors.append({str(key): \"Label key does not match the regex [a-zA-Z_][a-zA-Z0-9_]*\"})\n        elif not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', str(key)):\n            errors.append({str(key): \"Label key does not match the regex [a-zA-Z_][a-zA-Z0-9_]*\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n    \n    if errors:\n        raise ValidationError(messages=errors)", "import re\nfrom typing import Dict, Any, List\nfrom validation import ValidationError  # Assuming ValidationError is defined elsewhere\n\ndef _validate_labels(labels):\n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        else:\n            # Check if key matches regex pattern (e.g., valid label key format)\n            # Common label key regex: alphanumeric, hyphens, underscores, starting with letter or digit\n            key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n            if not re.match(key_pattern, key):\n                errors.append({key: \"Label key does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n    \n    if errors:\n        raise ValidationError(messages=errors)"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 988, "num_retrieved": 5, "inference_time": 0.7247302085161209, "generate_results": ["import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow the pattern:\n    # - Start with a letter or number\n    # - Contain only letters, numbers, hyphens, and underscores\n    # - Be 1-253 characters long\n    pattern = r'^[a-zA-Z0-9][a-zA-Z0-9_-]{0,252}$'\n    return re.compile(pattern)", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must:\n    # - start with a letter or number\n    # - contain only letters, numbers, hyphens, and underscores\n    # - be 1-253 characters long\n    pattern = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9_-]{0,252}$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must:\n    # - start with a letter or number\n    # - contain only letters, numbers, hyphens, and underscores\n    # - be 1-253 characters long\n    pattern = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9_-]{0,252}$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow these rules:\n    # - Start with a letter (a-z, A-Z)\n    # - Can contain letters, numbers, hyphens, and underscores\n    # - Cannot start or end with a hyphen or underscore\n    # - Cannot have consecutive hyphens or underscores\n    pattern = re.compile(r'^[a-zA-Z][a-zA-Z0-9_\\-]*[a-zA-Z0-9]$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow these rules:\n    # - Start with a letter or number\n    # - Can contain letters, numbers, hyphens, and underscores\n    # - Cannot start or end with a hyphen or underscore\n    # - Must be 1-253 characters long\n    pattern = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9_-]*[a-zA-Z0-9]$')\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1035, "num_retrieved": 6, "inference_time": 0.7247302085161209, "generate_results": ["def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The function is expected to validate a value against a regular expression\n    # However, no specific regex or validation logic is provided in the context\n    # Since the function signature and docstring imply validation against a regex,\n    # but no regex pattern or configuration is given, we assume a default validation\n    # that checks if the value is a string and non-empty, which is a minimal validation.\n    \n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string.\")\n    \n    if not value:\n        raise ValidationError(\"Value cannot be empty.\")\n    \n    # If more specific regex validation is required, it should be defined here\n    # For now, we only validate that it's a non-empty string\n    return True", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The function is expected to validate a value against a regular expression\n    # However, no specific regex or validation logic is provided in the context\n    # Since the function signature and docstring imply validation against a regex,\n    # but no regex pattern or configuration is given, we assume a default validation\n    # that raises ValidationError for any invalid value (as a placeholder).\n    \n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string.\")\n    \n    # Placeholder: if a specific regex pattern were defined, it would be applied here\n    # For now, we assume any non-empty string is valid, but this is incomplete\n    # In a real implementation, the regex pattern would be defined and used\n    if not value:\n        raise ValidationError(\"Value cannot be empty.\")\n    \n    # If no specific validation is defined, we return without raising\n    # This is a minimal implementation based on the docstring\n    return", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # Placeholder implementation - actual regex validation logic would depend on context\n    # Since no specific regex is provided in the context, this is a minimal stub\n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string.\")\n    \n    # Example validation: check if value matches basic alphanumeric pattern\n    import re\n    if not re.match(r'^[a-zA-Z0-9_\\-\\.]+$', value):\n        raise ValidationError(f\"Value '{value}' does not conform to expected pattern.\")", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The function is expected to validate a value against a regular expression\n    # However, no specific regex or validation logic is provided in the context\n    # Since no regex pattern is defined, we assume a default validation or raise an error\n    # for invalid input based on typical validation patterns.\n    \n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string.\")\n    \n    # Example: validate that value is not empty\n    if not value.strip():\n        raise ValidationError(\"Value cannot be empty.\")\n    \n    # Placeholder: if specific regex rules were defined elsewhere, they would be applied here\n    # For now, we only perform basic validation as per the docstring\n    return True", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The function is expected to validate a value against a regular expression\n    # However, no specific regular expression or context is provided in the retrieved context\n    # Therefore, this implementation assumes a placeholder validation that would typically\n    # be defined in the actual system with specific regex patterns.\n    \n    # Example: if we were to validate a string that matches a simple pattern (e.g., alphanumeric)\n    import re\n    pattern = r'^[a-zA-Z0-9]+$'\n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected pattern.\")"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1005, "num_retrieved": 6, "inference_time": 0.7247302085161209, "generate_results": ["def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid keys (example pattern: alphanumeric, hyphens, dots, and underscores)\n    # This pattern matches keys that start with a letter or underscore, followed by alphanumeric characters, hyphens, dots, or underscores\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9._-]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}.\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid keys (example pattern: alphanumeric, hyphens, dots, and underscores)\n    # This pattern matches keys that are typically used in Kubernetes or similar systems\n    pattern = r'^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid keys (example pattern: alphanumeric, hyphens, dots, and underscores)\n    # This pattern matches keys that start with a letter or underscore, followed by alphanumeric characters, hyphens, dots, or underscores\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9._-]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}.\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid keys (example pattern: alphanumeric, hyphens, dots, and underscores)\n    # This pattern matches keys that start with a letter or digit, followed by alphanumeric, hyphen, dot, or underscore\n    pattern = r'^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}.\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression for valid keys (example pattern: alphanumeric, hyphens, dots, and underscores)\n    # This pattern matches keys that are valid Kubernetes resource names or similar identifiers\n    pattern = r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}. Must match pattern {pattern}\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.7247302085161209, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Keep lists as they are\n            result[key] = value\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Keep lists as they are\n            result[key] = value\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Keep lists as they are\n            result[key] = value\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = value\n        else:\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Keep lists as they are\n            result[key] = value\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n    \n    # If first_level, add apiVersion, kind, and metadata with name\n    if first_level:\n        result[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        result[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        metadata = {}\n        metadata[\"name\"] = manifest_dict.get(\"metadata\", {}).get(\"name\")\n        result[\"metadata\"] = metadata\n    \n    return result"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 897, "num_retrieved": 1, "inference_time": 0.7247302085161209, "generate_results": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for i, (observed_item, desired_item, current_item) in enumerate(zip(observer_schema, last_applied_manifest, response)):\n        if isinstance(observed_item, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        elif isinstance(observed_item, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        else:\n            # For scalar values, simply assign if not already present\n            if not isinstance(last_applied_manifest[i], list):\n                last_applied_manifest[i] = observed_item", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for i, (observed_item, desired_item, current_item) in enumerate(zip(observer_schema, last_applied_manifest, response)):\n        if isinstance(observed_item, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        elif isinstance(observed_item, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        else:\n            # For scalar values, set the value if not already present\n            if not isinstance(last_applied_manifest[i], list):\n                last_applied_manifest[i] = observed_item", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for i, (observed_item, desired_item, current_item) in enumerate(zip(observer_schema, last_applied_manifest, response)):\n        if isinstance(observed_item, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        elif isinstance(observed_item, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        else:\n            # For scalar values, set the value if not already present\n            if i < len(last_applied_manifest) and i < len(response):\n                last_applied_manifest[i] = response[i]", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for i, (observed_item, desired_item, current_item) in enumerate(zip(observer_schema, last_applied_manifest, response)):\n        if isinstance(observed_item, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        elif isinstance(observed_item, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        else:\n            # For scalar values, set the value if not already present\n            if not isinstance(last_applied_manifest[i], list):\n                last_applied_manifest[i] = observed_item", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for i, (observed_item, desired_item, current_item) in enumerate(zip(observer_schema, last_applied_manifest, response)):\n        if isinstance(observed_item, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        elif isinstance(observed_item, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[i], observed_item, current_item\n            )\n        else:\n            # For scalar values, set the value if not already present\n            if i < len(last_applied_manifest) and last_applied_manifest[i] is None:\n                last_applied_manifest[i] = response[i]"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 897, "num_retrieved": 1, "inference_time": 0.7247302085161209, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"{key} not present in response\")\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"{key} not in response {response}\")\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"{key} not in response {response}\")\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"{key} not in response\")\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"{key} not in response {response}\")\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 966, "num_retrieved": 3, "inference_time": 0.7382918298244476, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    schema = {}\n    manifest_resources = app.spec.manifest or []\n    \n    for resource in manifest_resources:\n        resource_name = resource.get(\"name\")\n        if not resource_name:\n            continue\n            \n        # Check if a custom observer schema is already defined for this resource\n        custom_schema = resource.get(\"observer_schema\")\n        if custom_schema:\n            schema[resource_name] = custom_schema\n            continue\n            \n        # Generate default observer schema if no custom schema is provided\n        default_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"status\": {\"type\": \"string\"},\n                \"last_observed\": {\"type\": \"string\", \"format\": \"date-time\"},\n                \"conditions\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"type\": {\"type\": \"string\"},\n                            \"status\": {\"type\": \"string\"},\n                            \"last_transition_time\": {\"type\": \"string\", \"format\": \"date-time\"}\n                        }\n                    }\n                }\n            },\n            \"required\": [\"status\", \"last_observed\", \"conditions\"]\n        }\n        schema[resource_name] = default_schema\n    \n    return schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    default_schema = {}\n    manifest = app.spec.manifest or {}\n    \n    for resource_name, resource_config in manifest.items():\n        # Check if a custom observer schema is already defined\n        if resource_config.get(\"observer_schema\"):\n            continue\n            \n        # Generate default observer schema based on resource type\n        resource_type = resource_config.get(\"kind\", \"unknown\")\n        default_schema[resource_name] = {\n            \"kind\": resource_type,\n            \"api_version\": resource_config.get(\"api_version\", \"v1\"),\n            \"metadata\": {\n                \"name\": resource_name,\n                \"namespace\": resource_config.get(\"namespace\", \"default\")\n            },\n            \"spec\": {}\n        }\n    \n    return default_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    schema = {}\n    manifest = app.spec.manifest or {}\n    \n    for resource_name, resource_config in manifest.items():\n        # Check if a custom observer schema is already defined for this resource\n        if resource_name in app.spec.observers:\n            continue\n            \n        # Generate default observer schema based on resource type\n        resource_type = resource_config.get(\"kind\", \"unknown\")\n        default_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": {\"type\": \"string\"},\n                \"namespace\": {\"type\": \"string\"},\n                \"status\": {\"type\": \"string\"},\n                \"last_observed\": {\"type\": \"string\", \"format\": \"date-time\"}\n            },\n            \"required\": [\"name\", \"namespace\", \"status\"]\n        }\n        \n        schema[resource_name] = default_schema\n    \n    return schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    schema = {}\n    manifest = app.spec.manifest or {}\n    \n    for resource_name, resource_config in manifest.items():\n        # Check if a custom observer schema is already defined for this resource\n        if resource_name in app.spec.observers:\n            continue\n            \n        # Generate default observer schema based on resource type\n        resource_type = resource_config.get(\"kind\", \"unknown\")\n        \n        # Default schema structure for Kubernetes resources\n        default_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": {\"type\": \"string\", \"description\": \"Name of the resource\"},\n                \"namespace\": {\"type\": \"string\", \"description\": \"Namespace where the resource is deployed\"},\n                \"status\": {\"type\": \"string\", \"description\": \"Status of the resource\"},\n                \"last_updated\": {\"type\": \"string\", \"format\": \"date-time\", \"description\": \"Timestamp of last update\"},\n                \"conditions\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"type\": {\"type\": \"string\"},\n                            \"status\": {\"type\": \"string\"},\n                            \"message\": {\"type\": \"string\"}\n                        }\n                    }\n                }\n            },\n            \"required\": [\"name\", \"namespace\", \"status\"],\n            \"additionalProperties\": False\n        }\n        \n        schema[resource_name] = default_schema\n    \n    return schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    schema = {}\n    manifest = app.spec.manifest or {}\n    \n    for resource_name, resource_config in manifest.items():\n        # Check if a custom observer schema is already defined for this resource\n        if resource_name in app.spec.observers:\n            continue\n            \n        # Generate default observer schema based on resource type\n        resource_type = resource_config.get(\"kind\", \"unknown\")\n        default_schema = {\n            \"kind\": resource_type,\n            \"api_version\": resource_config.get(\"api_version\", \"v1\"),\n            \"metadata\": {\n                \"name\": resource_name,\n                \"namespace\": resource_config.get(\"namespace\", \"default\")\n            },\n            \"spec\": {}\n        }\n        \n        # Add default fields based on common Kubernetes resource patterns\n        if resource_type in [\"Deployment\", \"StatefulSet\", \"DaemonSet\"]:\n            default_schema[\"spec\"].update({\n                \"replicas\": resource_config.get(\"replicas\", 1),\n                \"selector\": {\n                    \"match_labels\": resource_config.get(\"selector\", {})\n                }\n            })\n        elif resource_type in [\"Service\", \"Ingress\"]:\n            default_schema[\"spec\"].update({\n                \"ports\": resource_config.get(\"ports\", [])\n            })\n        \n        schema[resource_name] = default_schema\n    \n    return schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 920, "num_retrieved": 2, "inference_time": 0.7382918298244476, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if not isinstance(params, (Mapping, Sequence)):\n        raise TypeError(\"params:{!r} is not a mapping or sequence.\".format(params))\n\n    if isinstance(params, Mapping):\n        out_sql, out_params = self.convert(sql, params)\n    else:\n        out_sql, out_params = self.convert(sql, params)\n\n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_sql, out_params = self.convert(sql, params)\n        return out_sql, out_params\n    else:\n        # params is a sequence, so we assume ordinal style\n        out_sql, out_params = self.convert(sql, params)\n        return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_sql, out_params = self.convert(sql, params)\n        return out_sql, out_params\n    else:\n        # params is a sequence, so we assume ordinal style\n        out_sql, out_params = self.convert(sql, params)\n        return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_sql, out_params = self.convert(sql, params)\n        return out_sql, out_params\n    else:\n        # params is a sequence, so we assume ordinal style\n        out_sql, out_params = self.convert(sql, params)\n        return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_sql, out_params = self.convert(sql, params)\n        return out_sql, out_params\n    else:\n        # params is a sequence, so we assume ordinal style\n        out_sql, out_params = self.convert(sql, params)\n        return out_sql, out_params"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 920, "num_retrieved": 2, "inference_time": 0.7382918298244476, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    - *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n      contains the set of in-style parameters. It maps each parameter\n      (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n      is a named parameter style, then *params* must be a :class:`~collections.abc.Mapping`.\n      If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n      *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    - The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    - A :class:`list` containing each set of converted out-style\n      parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    # Determine the parameter style based on the first params\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if not isinstance(first_params, (Mapping, Sequence)):\n        raise TypeError(\"many_params[0]:{!r} is not a mapping or sequence.\".format(first_params))\n\n    # Convert query using the first params to determine the style\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(\n        partial(self.__regex_replace, first_params, param_conversions, out_format), sql\n    )\n\n    # Convert parameters to out-style\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    - *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n      contains the set of in-style parameters. It maps each parameter\n      (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n      is a named parameter style, then *params* must be a :class:`~collections.abc.Mapping`.\n      If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n      *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    - The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    - A :class:`list` containing each set of converted out-style\n      parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    # Determine the parameter style based on the first params\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if not isinstance(first_params, (Mapping, Sequence)):\n        raise TypeError(\"many_params[0]:{!r} is not a mapping or sequence.\".format(first_params))\n\n    # Convert query using the first params to determine the style\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(\n        partial(self.__regex_replace, first_params, param_conversions, out_format), sql\n    )\n\n    # Convert parameters to out-style\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    - *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n      contains the set of in-style parameters. It maps each parameter\n      (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n      is a named parameter style, then *params* must be a :class:`~collections.abc.Mapping`.\n      If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n      *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    - The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    - A :class:`list` containing each set of converted out-style\n      parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    # Determine the parameter style based on the first params\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if not isinstance(first_params, (Mapping, Sequence)):\n        raise TypeError(\"many_params[0]:{!r} is not a mapping or sequence.\".format(first_params))\n\n    # Convert query using the first params to determine the style\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(\n        partial(self.__regex_replace, first_params, param_conversions, out_format), sql\n    )\n\n    # Convert parameters to out-style\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    - *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n      contains the set of in-style parameters. It maps each parameter\n      (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n      is a named parameter style, then *params* must be a :class:`~collections.abc.Mapping`.\n      If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n      *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    - The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    - A :class:`list` containing each set of converted out-style\n      parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    # Determine the parameter style based on the first params\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if not isinstance(first_params, (Mapping, Sequence)):\n        raise TypeError(\"many_params[0]:{!r} is not a mapping or sequence.\".format(first_params))\n\n    # Convert query using the first params to determine the style\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(\n        partial(self.__regex_replace, first_params, param_conversions, out_format), sql\n    )\n\n    # Convert parameters to out-style\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    - *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n      contains the set of in-style parameters. It maps each parameter\n      (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n      is a named parameter style, then *params* must be a :class:`~collections.abc.Mapping`.\n      If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n      *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    - The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    - A :class:`list` containing each set of converted out-style\n      parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    # Determine the parameter style based on the first params\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if not isinstance(first_params, (Mapping, Sequence)):\n        raise TypeError(\"many_params[0]:{!r} is not a mapping or sequence.\".format(first_params))\n\n    # Convert query using the first params to determine parameter style\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(\n        partial(self.__regex_replace, first_params, param_conversions, out_format), sql\n    )\n\n    # Convert parameters to out-style\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 925, "num_retrieved": 3, "inference_time": 0.7382918298244476, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        (inv_dir, inv_file) = fs.path.split(path)\n        validator.obj_fs = open_fs(inv_dir, create=False)\n        validator.validate_inventory(inv_file, where='standalone')\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    except ValidatorAbortException:\n        return False\n    \n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    \n    self.log.info(\"OCFL object at %s is %s\", path, 'VALID' if passed else 'INVALID')\n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        (inv_dir, inv_file) = fs.path.split(path)\n        validator.obj_fs = open_fs(inv_dir, create=False)\n        validator.validate_inventory(inv_file, where='standalone')\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    except ValidatorAbortException:\n        return False\n    \n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    \n    self.log.info(\"OCFL object at %s is %s\", path, 'VALID' if passed else 'INVALID')\n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        (inv_dir, inv_file) = fs.path.split(path)\n        validator.obj_fs = open_fs(inv_dir, create=False)\n        validator.validate_inventory(inv_file, where='standalone')\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    except ValidatorAbortException:\n        return False\n    \n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    \n    self.log.info(\"OCFL object at %s is %s\", path, 'VALID' if passed else 'INVALID')\n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        (inv_dir, inv_file) = fs.path.split(path)\n        validator.obj_fs = open_fs(inv_dir, create=False)\n        validator.validate_inventory(inv_file, where='standalone')\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    except ValidatorAbortException:\n        return False\n    \n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    \n    self.log.info(\"OCFL object at %s is %s\", path, 'VALID' if passed else 'INVALID')\n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        (inv_dir, inv_file) = fs.path.split(path)\n        validator.obj_fs = open_fs(inv_dir, create=False)\n        validator.validate_inventory(inv_file, where='standalone')\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    except ValidatorAbortException:\n        return False\n    \n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    \n    self.log.info(\"OCFL object at %s is %s\", path, 'VALID' if passed else 'INVALID')\n    return passed"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1025, "num_retrieved": 8, "inference_time": 0.7382918298244476, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for message in self._messages:\n        if prefix:\n            messages.append(f\"{prefix}: {message}\")\n        else:\n            messages.append(message)\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for message in self._messages:\n        if prefix:\n            messages.append(f\"{prefix}: {message}\")\n        else:\n            messages.append(message)\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for message in self._messages:\n        if prefix:\n            messages.append(f\"{prefix}: {message}\")\n        else:\n            messages.append(message)\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for message in self._messages:\n        if prefix:\n            messages.append(f\"{prefix}: {message}\")\n        else:\n            messages.append(message)\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for message in self._messages:\n        if prefix:\n            messages.append(f\"{prefix}: {message}\")\n        else:\n            messages.append(message)\n    return \"\\n\".join(messages) if messages else \"\""]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 951, "num_retrieved": 5, "inference_time": 0.7382918298244476, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"\n    Return string representation of validation log, with optional prefix.\n    \"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"[{prefix}] {msg}\")\n        else:\n            messages.append(msg)\n    return '\\n'.join(messages)", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"[{prefix}] {msg}\")\n        else:\n            messages.append(msg)\n    return '\\n'.join(messages)", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string representation of validation log, with optional prefix.\n    \"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"[{prefix}] {msg}\")\n        else:\n            messages.append(msg)\n    return '\\n'.join(messages)", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"[{prefix}] {msg}\")\n        else:\n            messages.append(msg)\n    return '\\n'.join(messages)", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string representation of validation log, with optional prefix.\n    \"\"\"\n    messages = []\n    for msg in self.messages:\n        if prefix:\n            messages.append(f\"[{prefix}] {msg}\")\n        else:\n            messages.append(msg)\n    return '\\n'.join(messages)"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 962, "num_retrieved": 5, "inference_time": 0.7382918298244476, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 905, "num_retrieved": 2, "inference_time": 0.7382918298244476, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type field (e.g., \"ocfl-1.0\")\n            type_value = inventory['type']\n            import re\n            match = re.match(r'ocfl-(\\d+\\.\\d+)', type_value)\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If type value is invalid or doesn't match ocfl-x.x pattern, fall back to self.spec_version\n                pass\n    \n    # Create an inventory validator with the determined spec_version\n    inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=spec_version)\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type field (e.g., \"ocfl-1.0\")\n            type_value = inventory['type']\n            import re\n            match = re.match(r'ocfl-(\\d+\\.\\d+)', type_value)\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If type value is invalid or doesn't match ocfl-x.x pattern, fall back to self.spec_version\n                pass\n    \n    # Create and use InventoryValidator with the determined spec_version\n    inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=spec_version)\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type field (e.g., \"ocfl-1.0\")\n            type_value = inventory['type']\n            version_match = re.match(r'ocfl-(\\d+\\.\\d+)', type_value)\n            if version_match:\n                spec_version = version_match.group(1)\n            else:\n                # If type doesn't match expected format, fall back to self.spec_version\n                pass\n    \n    # Create inventory validator with the determined spec_version\n    inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=spec_version)\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type field (e.g., \"ocfl-1.0\")\n            type_value = inventory['type']\n            version_match = re.match(r'ocfl-(\\d+\\.\\d+)', type_value)\n            if version_match:\n                spec_version = version_match.group(1)\n            else:\n                # If type is invalid or doesn't match ocfl-x.x pattern, fall back to self.spec_version\n                pass\n    \n    # Create and use InventoryValidator with the determined spec_version\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=spec_version)\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    \n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type field (e.g., \"ocfl-1.0\")\n            type_value = inventory['type']\n            import re\n            match = re.match(r'ocfl-(\\d+\\.\\d+)', type_value)\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If type is invalid or doesn't match ocfl-x.x pattern, fall back to self.spec_version\n                pass\n    \n    # Create an inventory validator with the determined spec_version\n    inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=spec_version)\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 952, "num_retrieved": 4, "inference_time": 0.7382918298244476, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    missing_digests = []\n    for manifest_file in manifest_files:\n        # Extract digests from manifest file (assuming manifest has a 'digests' key)\n        manifest_digests = manifest_file.get('digests', {})\n        for digest_type in manifest_digests:\n            digest_value = manifest_digests[digest_type]\n            # Check if this digest type is in the list of digests used\n            if digest_type not in digests_used:\n                missing_digests.append(digest_type)\n    \n    # Return missing digests if any\n    if missing_digests:\n        return False, missing_digests\n    return True, []", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    missing_digests = []\n    for manifest_file in manifest_files:\n        # Extract digests from manifest file (assuming manifest has a 'digests' field)\n        manifest_digests = manifest_file.get('digests', {})\n        for digest_type, digest_value in manifest_digests.items():\n            if digest_type not in digests_used:\n                missing_digests.append(digest_type)\n    \n    # If any digests are missing, return them\n    if missing_digests:\n        self.log.warning(f\"Digests missing from used list: {missing_digests}\")\n        return False, missing_digests\n    \n    return True, []", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    missing_digests = []\n    for manifest_file in manifest_files:\n        # Extract digests from manifest file (assuming manifest has a 'digests' field)\n        manifest_digests = self._extract_digests_from_manifest(manifest_file)\n        for digest_type in manifest_digests:\n            if digest_type not in digests_used:\n                missing_digests.append(digest_type)\n    \n    # If any digests are missing, return them\n    if missing_digests:\n        self.log.warning(f\"Digests missing from used list: {missing_digests}\")\n        return False, missing_digests\n    \n    return True, []", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    missing_digests = []\n    for manifest_file in manifest_files:\n        # Extract digests from manifest file (assuming manifest has a 'digests' field)\n        manifest_digests = self._extract_digests_from_manifest(manifest_file)\n        for digest_type in manifest_digests:\n            if digest_type not in digests_used:\n                missing_digests.append(digest_type)\n    \n    # Return missing digests if any\n    if missing_digests:\n        self.log.warning(f\"Missing digests in manifest: {missing_digests}\")\n        return False\n    \n    return True", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    missing_digests = []\n    for manifest_file in manifest_files:\n        # Extract digests from manifest file (assuming manifest has a 'digests' field)\n        manifest_digests = self._extract_digests_from_manifest(manifest_file)\n        for digest_type in manifest_digests:\n            if digest_type not in digests_used:\n                missing_digests.append(digest_type)\n    \n    # If any digests are missing, return them\n    if missing_digests:\n        self.log.warning(f\"Digests missing from used list: {missing_digests}\")\n        return False, missing_digests\n    \n    return True, []"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 905, "num_retrieved": 2, "inference_time": 0.7382918298244476, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Validate that the prior version has a valid version number that is less than current\n    if not prior.version or not self.version:\n        return False\n    \n    # Check that prior version is less than current version\n    if prior.version >= self.version:\n        return False\n    \n    # Validate that all objects in prior are present in current\n    prior_objects = set(prior.object_paths())\n    current_objects = set(self.object_paths())\n    \n    # All objects in prior must be present in current\n    for obj_path in prior_objects:\n        if obj_path not in current_objects:\n            return False\n    \n    # Validate that the inventory structure is consistent\n    # Check that prior's expected files are present in current\n    expected_files = ['0=ocfl_object_' + self.spec_version, 'inventory.json',\n                      'inventory.json.' + self.digest_algorithm]\n    \n    # Ensure prior's inventory version is compatible\n    if prior.spec_version != self.spec_version:\n        return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Validate that the prior version has a valid version number that is less than current\n    if not prior.version or not self.version:\n        return False\n    \n    # Check that prior version is strictly less than current version\n    if prior.version >= self.version:\n        return False\n    \n    # Validate that all objects in prior are present in current\n    # This assumes that the inventory.json files contain the list of objects\n    try:\n        prior_objects = set(prior.inventory.get('objects', []))\n        current_objects = set(self.inventory.get('objects', []))\n        \n        # All objects in prior must be present in current\n        if not prior_objects.issubset(current_objects):\n            return False\n            \n        # Validate that the inventory structure is consistent\n        # Prior should have a valid digest algorithm and version\n        if not prior.digest_algorithm or not prior.version:\n            return False\n            \n        # Ensure the digest algorithms match (or are compatible)\n        if prior.digest_algorithm != self.digest_algorithm:\n            # Allow for different algorithms if the version is strictly prior\n            # but this is a soft check\n            pass\n            \n    except (AttributeError, KeyError):\n        return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Validate that the prior version has a valid version number that is less than current\n    if not prior or not hasattr(prior, 'version'):\n        return False\n    \n    if not hasattr(self, 'version'):\n        return False\n    \n    # Check that prior version is less than current version\n    if prior.version >= self.version:\n        return False\n    \n    # Validate that all objects in prior are present in current\n    # This assumes that the inventory.json files contain the list of objects\n    current_objects = set()\n    prior_objects = set()\n    \n    # Extract object names from current inventory\n    if hasattr(self, 'inventory') and self.inventory:\n        current_objects = {obj['name'] for obj in self.inventory.get('objects', [])}\n    \n    # Extract object names from prior inventory\n    if hasattr(prior, 'inventory') and prior.inventory:\n        prior_objects = {obj['name'] for obj in prior.inventory.get('objects', [])}\n    \n    # All objects in prior must be present in current\n    for obj_name in prior_objects:\n        if obj_name not in current_objects:\n            return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Validate that the prior version has a valid version number that is less than current\n    if not prior.version or not self.version:\n        return False\n    \n    # Ensure prior version is strictly less than current version\n    if prior.version >= self.version:\n        return False\n    \n    # Check that all objects in prior are present in current\n    prior_objects = set(prior.object_paths())\n    current_objects = set(self.object_paths())\n    \n    # All objects in prior must be present in current\n    for obj_path in prior_objects:\n        if obj_path not in current_objects:\n            return False\n    \n    # Check that the inventory structure is consistent\n    # Prior should have the same or fewer version directories\n    prior_version_dirs = set(prior.version_dirs)\n    current_version_dirs = set(self.version_dirs)\n    \n    # All prior version directories must be present in current\n    for dir_name in prior_version_dirs:\n        if dir_name not in current_version_dirs:\n            return False\n    \n    return True", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\"\"\"\n    # Validate that the prior version has a valid version number that is less than current\n    if not prior.version or prior.version >= self.version:\n        return False\n    \n    # Check that all objects in the prior version are present in the current version\n    prior_objects = set(prior.object_paths())\n    current_objects = set(self.object_paths())\n    \n    # All objects from prior must be present in current\n    for obj_path in prior_objects:\n        if obj_path not in current_objects:\n            return False\n    \n    # Check that the inventory structure is consistent\n    # Prior version should have the same or fewer version directories\n    prior_version_dirs = set(prior.version_dirs)\n    current_version_dirs = set(self.version_dirs)\n    \n    # All prior version directories must be present in current\n    for dir_name in prior_version_dirs:\n        if dir_name not in current_version_dirs:\n            return False\n    \n    return True"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 905, "num_retrieved": 2, "inference_time": 0.7382918298244476, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    \n    # Get the version data from inventory\n    version_data = inventory.get(version)\n    if not version_data:\n        return logical_path_map\n    \n    # Extract the manifest and versions blocks\n    manifest = version_data.get('manifest', {})\n    versions = version_data.get('versions', {})\n    \n    # Process each file in the manifest\n    for logical_path, content_info in manifest.items():\n        content_files = set()\n        \n        # Add the current version's content file\n        content_files.add(content_info['path'])\n        \n        # Check for duplicates in later versions\n        for later_version in sorted(versions.keys()):\n            if later_version > version:\n                later_version_data = inventory.get(later_version)\n                if later_version_data:\n                    later_manifest = later_version_data.get('manifest', {})\n                    if logical_path in later_manifest:\n                        content_files.add(later_manifest[logical_path]['path'])\n        \n        logical_path_map[logical_path] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    \n    # Get the version data from inventory\n    version_data = inventory.get(version)\n    if not version_data:\n        return logical_path_map\n    \n    # Extract the manifest and content files for this version\n    manifest = version_data.get('manifest', {})\n    \n    # Process each file in the manifest\n    for logical_path, content_info in manifest.items():\n        content_files = set()\n        \n        # Add the content file from this version\n        content_files.add(content_info['path'])\n        \n        # Check if there are any later versions that reference this logical path\n        # and add those references as well\n        for later_version in inventory.get('versions', {}).keys():\n            if later_version > version:\n                later_version_data = inventory.get(later_version)\n                if later_version_data and later_version_data.get('manifest'):\n                    later_manifest = later_version_data['manifest']\n                    if logical_path in later_manifest:\n                        content_files.add(later_manifest[logical_path]['path'])\n        \n        logical_path_map[logical_path] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    \n    # Get the version block from inventory\n    version_block = inventory.get(version)\n    if not version_block:\n        return logical_path_map\n    \n    # Extract the manifest and content files from the version block\n    manifest = version_block.get('manifest', {})\n    \n    # Iterate over each file in the manifest\n    for logical_path, content_info in manifest.items():\n        content_files = set()\n        \n        # Add the content file path from the current version\n        content_files.add(content_info['path'])\n        \n        # Check if there are any later versions that contain the same logical path\n        # and add those to the content_files set\n        for later_version in inventory.get('versions', {}).keys():\n            if later_version > version:\n                later_version_block = inventory.get(later_version)\n                if later_version_block and later_version_block.get('manifest'):\n                    later_manifest = later_version_block['manifest']\n                    if logical_path in later_manifest:\n                        content_files.add(later_manifest[logical_path]['path'])\n        \n        logical_path_map[logical_path] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    \n    # Get the version block from inventory\n    version_block = inventory.get(version)\n    if not version_block:\n        return logical_path_map\n    \n    # Extract the manifest and versions blocks\n    manifest = version_block.get('manifest')\n    versions = version_block.get('versions')\n    \n    # If no manifest or versions, return empty map\n    if not manifest or not versions:\n        return logical_path_map\n    \n    # Process each file in the manifest\n    for logical_path, content_info in manifest.items():\n        content_files = set()\n        \n        # Add the current version's content file\n        content_files.add(content_info['path'])\n        \n        # Check for duplicates in later versions\n        for later_version in versions:\n            if later_version > version:\n                later_version_block = inventory.get(later_version)\n                if later_version_block:\n                    later_manifest = later_version_block.get('manifest')\n                    if later_manifest:\n                        # Check if the same logical path exists in later version\n                        if logical_path in later_manifest:\n                            content_files.add(later_manifest[logical_path]['path'])\n        \n        logical_path_map[logical_path] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    logical_path_map = {}\n    \n    # Get the version data from inventory\n    version_data = inventory.get(version)\n    if not version_data:\n        return logical_path_map\n    \n    # Extract the manifest and content files for this version\n    manifest = version_data.get('manifest', {})\n    \n    # Process each file in the manifest\n    for logical_path, content_info in manifest.items():\n        content_files = set()\n        \n        # Add the content file from this version\n        content_files.add(content_info['path'])\n        \n        # Check if there are later versions that contain the same logical path\n        # and add those as well (if they exist in later versions)\n        for later_version in inventory.get('versions', {}).keys():\n            if later_version > version:\n                later_version_data = inventory.get(later_version)\n                if later_version_data and later_version_data.get('manifest'):\n                    later_manifest = later_version_data['manifest']\n                    if logical_path in later_manifest:\n                        content_files.add(later_manifest[logical_path]['path'])\n        \n        logical_path_map[logical_path] = content_files\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 904, "num_retrieved": 2, "inference_time": 0.7382918298244476, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    # Extract file references from fixity block\n    fixity_files = set()\n    for file_entry in fixity.get('files', []):\n        file_path = file_entry.get('path')\n        if not file_path:\n            self.log.error(\"E015\", where=\"fixity\")\n            return False\n        fixity_files.add(file_path)\n    \n    # Check if all files in fixity are present in manifest_files\n    missing_files = fixity_files - manifest_files\n    if missing_files:\n        for file_path in missing_files:\n            self.log.error(\"E016\", where=\"fixity\", file=file_path)\n        return False\n    \n    # Validate that all files in manifest are referenced in fixity\n    extra_files = manifest_files - fixity_files\n    if extra_files:\n        for file_path in extra_files:\n            self.log.error(\"E017\", where=\"fixity\", file=file_path)\n        return False\n    \n    return True", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    # Extract file references from fixity block\n    fixity_files = set()\n    for file_entry in fixity.get('files', []):\n        file_path = file_entry.get('path')\n        if not file_path:\n            self.log.error(\"E015\", where=\"fixity\", message=\"Missing path in fixity entry\")\n            continue\n        fixity_files.add(file_path)\n    \n    # Check if all files in fixity are present in manifest_files\n    missing_files = fixity_files - manifest_files\n    if missing_files:\n        for file_path in missing_files:\n            self.log.error(\"E016\", where=\"fixity\", message=f\"File {file_path} referenced in fixity but not in manifest\")\n    \n    # Validate that only files in manifest are referenced\n    extra_files = manifest_files - fixity_files\n    if extra_files:\n        for file_path in extra_files:\n            self.log.warning(\"W017\", where=\"fixity\", message=f\"File {file_path} in manifest but not referenced in fixity\")", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    # Extract file references from fixity block\n    fixity_files = set()\n    for file_entry in fixity.get('files', []):\n        file_path = file_entry.get('path')\n        if not file_path:\n            self.log.error(\"E011\", where=\"fixity\")\n            continue\n        fixity_files.add(file_path)\n    \n    # Check if all referenced files are in the manifest\n    missing_files = fixity_files - set(manifest_files)\n    for file_path in missing_files:\n        self.log.error(\"E012\", where=\"fixity\", file=file_path)\n    \n    # Validate that only files in manifest are referenced\n    for file_path in fixity_files:\n        if file_path not in manifest_files:\n            self.log.error(\"E013\", where=\"fixity\", file=file_path)\n    \n    return len(missing_files) == 0", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    # Extract file references from fixity block\n    fixity_files = set()\n    for file_entry in fixity.get('files', []):\n        file_path = file_entry.get('path')\n        if not file_path:\n            self.log.error(\"E015\", where=\"fixity\", message=\"Missing path in fixity entry\")\n            continue\n        fixity_files.add(file_path)\n    \n    # Check that all files in fixity are present in manifest_files\n    missing_files = fixity_files - manifest_files\n    if missing_files:\n        for file_path in missing_files:\n            self.log.error(\"E016\", where=\"fixity\", message=f\"File {file_path} referenced in fixity but not in manifest\")\n    \n    # Validate that all files in manifest are not referenced in fixity\n    # (This is not explicitly stated in the docstring, but implied by \"only files listed in the manifest are referenced\")\n    # If we need to enforce that only manifest files are referenced, the above check covers it.\n    \n    return len(missing_files) == 0", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    # Extract file paths and digests from the fixity block\n    fixity_files = set()\n    for file_entry in fixity.get('files', []):\n        file_path = file_entry.get('path')\n        if not file_path:\n            self.log.error(\"E011\", where=\"fixity\")\n            return False\n        fixity_files.add(file_path)\n    \n    # Check that all files in fixity are present in manifest_files\n    missing_files = fixity_files - set(manifest_files)\n    if missing_files:\n        self.log.error(\"E012\", where=\"fixity\", missing=missing_files)\n        return False\n    \n    # Validate that all files in manifest are not referenced in fixity\n    # (optional: ensure no extra files are included)\n    extra_files = set(manifest_files) - fixity_files\n    if extra_files:\n        self.log.warning(\"W010\", where=\"fixity\", extra=extra_files)\n    \n    return True"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1062, "num_retrieved": 7, "inference_time": 0.7382918298244476, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list_folder(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 943, "num_retrieved": 3, "inference_time": 0.7382918298244476, "generate_results": ["def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    # Extract basename of XML file (without extension)\n    xml_base_name = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Process each file in the list\n    for file in files:\n        file_path = Path(file)\n        file_name = file_path.name\n        \n        # If the file has the same basename as the XML file, group it\n        if file_name.startswith(xml_base_name):\n            # Use the XML basename as key\n            key = xml_base_name\n            \n            if key not in grouped_files:\n                grouped_files[key] = []\n            \n            grouped_files[key].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    # Extract basename of XML file (without extension)\n    xml_base_name = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Process each file in the list\n    for file in files:\n        file_path = Path(file)\n        file_name = file_path.name\n        \n        # If the file name matches the XML basename (or contains it), group it\n        if xml_base_name in file_name:\n            # Use the XML basename as key\n            key = xml_base_name\n            \n            if key not in grouped_files:\n                grouped_files[key] = []\n            \n            grouped_files[key].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    # Extract basename of XML file (without extension)\n    xml_base_name = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Group all files by the XML basename\n    for file in files:\n        file_path = Path(file)\n        file_name = file_path.name\n        \n        # If the file name matches the XML basename (or has the same base name),\n        # group it under the XML basename\n        if file_name.startswith(xml_base_name):\n            # Use the XML basename as key\n            key = xml_base_name\n            if key not in grouped_files:\n                grouped_files[key] = []\n            grouped_files[key].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    # Extract basename of XML file (without extension)\n    xml_base_name = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Process each file to determine if it belongs to the XML file\n    for file in files:\n        file_path = Path(file)\n        file_name = file_path.name\n        \n        # Check if the file name matches the XML basename (with or without extension)\n        if xml_base_name in file_name:\n            # Group files under the XML basename\n            if xml_base_name not in grouped_files:\n                grouped_files[xml_base_name] = []\n            grouped_files[xml_base_name].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    # Extract basename of XML file (without extension)\n    xml_base_name = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Process each file to determine if it belongs to the XML\n    for file in files:\n        file_path = Path(file)\n        file_name = file_path.name\n        \n        # Check if the file name matches the XML basename (case-insensitive)\n        if xml_base_name.lower() in file_name.lower():\n            # If it matches, add it to the group\n            if xml_base_name not in grouped_files:\n                grouped_files[xml_base_name] = []\n            grouped_files[xml_base_name].append(file)\n    \n    return grouped_files"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1018, "num_retrieved": 5, "inference_time": 0.7382918298244476, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 914, "num_retrieved": 2, "inference_time": 0.7382918298244476, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorno os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorno os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorno os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorno os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 875, "num_retrieved": 2, "inference_time": 0.9175620675086975, "generate_results": ["def _explore_folder(folder):\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Iterate over all files in the folder\n    for file_path in folder_path.iterdir():\n        if file_path.is_file():\n            # Extract the filename without extension\n            filename = file_path.name\n            name_without_ext = filename.rsplit('.', 1)[0] if '.' in filename else filename\n            \n            # Check if the file has .xml extension\n            if filename.lower().endswith('.xml'):\n                xml_basename = name_without_ext\n                package_data[xml_basename] = {\n                    'xml': str(file_path),\n                    'renditions': [],\n                    'assets': []\n                }\n                \n                # Look for rendition and asset files with the same basename\n                for other_file in folder_path.iterdir():\n                    if other_file.is_file() and other_file.name.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif')):\n                        other_name = other_file.name.rsplit('.', 1)[0] if '.' in other_file.name else other_file.name\n                        if other_name == xml_basename:\n                            file_type = other_file.suffix.lower()\n                            if file_type in ['.pdf']:\n                                package_data[xml_basename]['renditions'].append(str(other_file))\n                            else:\n                                package_data[xml_basename]['assets'].append(str(other_file))\n            elif filename.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif')):\n                # If it's a rendition or asset, group by XML basename\n                # We need to find the corresponding XML file\n                xml_filename = f\"{name_without_ext}.xml\"\n                xml_file_path = folder_path / xml_filename\n                if xml_file_path.exists():\n                    xml_basename = name_without_ext\n                    if xml_basename not in package_data:\n                        package_data[xml_basename] = {\n                            'xml': str(xml_file_path),\n                            'renditions': [],\n                            'assets': []\n                        }\n                    if file_path.suffix.lower() in ['.pdf']:\n                        package_data[xml_basename]['renditions'].append(str(file_path))\n                    else:\n                        package_data[xml_basename]['assets'].append(str(file_path))\n    \n    return package_data", "def _explore_folder(folder):\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    xml_files = []\n    rendition_files = []\n    asset_files = []\n    \n    # Iterate through all files in the folder\n    for file_path in Path(folder).iterdir():\n        if file_path.is_file():\n            filename = file_path.name\n            file_ext = file_path.suffix.lower()\n            \n            # Check if it's an XML file\n            if file_ext == '.xml':\n                xml_files.append(str(file_path))\n            # Check if it's a rendition (PDF, etc.)\n            elif file_ext in ['.pdf', '.epub', '.mobi', '.djvu', '.png', '.jpg', '.jpeg', '.gif', '.bmp']:\n                rendition_files.append(str(file_path))\n            # Check if it's an asset (image, etc.)\n            elif file_ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg']:\n                asset_files.append(str(file_path))\n    \n    # Extract XML basename for grouping\n    for xml_path in xml_files:\n        xml_name = os.path.basename(xml_path).replace('.xml', '')\n        package_data[xml_name] = {\n            'xml': xml_path,\n            'renditions': [f for f in rendition_files if os.path.basename(f).startswith(xml_name)],\n            'assets': [f for f in asset_files if os.path.basename(f).startswith(xml_name)]\n        }\n    \n    return package_data", "def _explore_folder(folder):\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Iterate over all files in the folder\n    for file_path in folder_path.iterdir():\n        if file_path.is_file():\n            # Extract the filename without extension\n            filename = file_path.name\n            name_without_ext = filename.rsplit('.', 1)[0] if '.' in filename else filename\n            \n            # Check if the file has .xml extension\n            if filename.lower().endswith('.xml'):\n                xml_basename = name_without_ext\n                package_data[xml_basename] = {\n                    'xml': str(file_path),\n                    'renditions': [],\n                    'assets': []\n                }\n                \n                # Look for rendition and asset files with the same basename\n                for other_file in folder_path.iterdir():\n                    if other_file.is_file() and other_file.name.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif')):\n                        other_name = other_file.name.rsplit('.', 1)[0] if '.' in other_file.name else other_file.name\n                        if other_name == xml_basename:\n                            file_ext = other_file.suffix.lower()\n                            if file_ext in ['.pdf']:\n                                package_data[xml_basename]['renditions'].append(str(other_file))\n                            else:\n                                package_data[xml_basename]['assets'].append(str(other_file))\n            elif filename.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif')):\n                # If it's a rendition or asset, group by XML basename\n                # We need to find the corresponding XML file\n                xml_candidate = f\"{name_without_ext}.xml\"\n                xml_file_path = folder_path / xml_candidate\n                if xml_file_path.exists():\n                    xml_basename = name_without_ext\n                    if xml_basename not in package_data:\n                        package_data[xml_basename] = {\n                            'xml': str(xml_file_path),\n                            'renditions': [],\n                            'assets': []\n                        }\n                    if file_path.suffix.lower() in ['.pdf']:\n                        package_data[xml_basename]['renditions'].append(str(file_path))\n                    else:\n                        package_data[xml_basename]['assets'].append(str(file_path))\n    \n    return package_data", "def _explore_folder(folder):\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Iterate through all files in the folder\n    for file_path in folder_path.iterdir():\n        if file_path.is_file():\n            # Extract filename without extension\n            filename = file_path.name\n            name_without_ext = filename.rsplit('.', 1)[0] if '.' in filename else filename\n            \n            # Check if file has .xml extension\n            if filename.lower().endswith('.xml'):\n                xml_basename = name_without_ext\n                package_data[xml_basename] = {\n                    'xml': str(file_path),\n                    'renditions': [],\n                    'assets': []\n                }\n                \n                # Look for rendition and asset files with the same basename\n                for other_file in folder_path.iterdir():\n                    if other_file.is_file() and other_file.name.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif')):\n                        other_name = other_file.name.rsplit('.', 1)[0] if '.' in other_file.name else other_file.name\n                        if other_name == xml_basename:\n                            file_ext = other_file.suffix.lower()\n                            if file_ext in ['.pdf']:\n                                package_data[xml_basename]['renditions'].append(str(other_file))\n                            else:\n                                package_data[xml_basename]['assets'].append(str(other_file))\n            elif filename.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif')):\n                # If it's a rendition or asset, group by XML basename\n                # We need to find the corresponding XML file\n                xml_file_name = filename.rsplit('.', 1)[0] + '.xml'\n                xml_file_path = folder_path / xml_file_name\n                if xml_file_path.is_file():\n                    xml_basename = xml_file_name.rsplit('.', 1)[0]\n                    if xml_basename not in package_data:\n                        package_data[xml_basename] = {\n                            'xml': str(xml_file_path),\n                            'renditions': [],\n                            'assets': []\n                        }\n                    file_ext = other_file.suffix.lower()\n                    if file_ext in ['.pdf']:\n                        package_data[xml_basename]['renditions'].append(str(other_file))\n                    else:\n                        package_data[xml_basename]['assets'].append(str(other_file))\n    \n    return package_data", "def _explore_folder(folder):\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Iterate over all files in the folder\n    for file_path in folder_path.iterdir():\n        if file_path.is_file():\n            # Extract the filename without extension\n            filename = file_path.name\n            name_without_ext = filename.rsplit('.', 1)[0] if '.' in filename else filename\n            \n            # Check if the file has .xml extension\n            if filename.lower().endswith('.xml'):\n                xml_basename = name_without_ext\n                package_data[xml_basename] = {\n                    'xml': str(file_path),\n                    'renditions': [],\n                    'assets': []\n                }\n                \n                # Look for rendition files (e.g., .pdf) and assets (e.g., .jpg, .png)\n                for other_file in folder_path.iterdir():\n                    if other_file.is_file() and other_file.name.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif')):\n                        file_name_without_ext = other_file.name.rsplit('.', 1)[0] if '.' in other_file.name else other_file.name\n                        if file_name_without_ext == xml_basename:\n                            file_type = other_file.suffix.lower()\n                            if file_type in ['.pdf']:\n                                package_data[xml_basename]['renditions'].append(str(other_file))\n                            else:\n                                package_data[xml_basename]['assets'].append(str(other_file))\n            elif filename.lower().endswith(('.pdf', '.jpg', '.jpeg', '.png', '.gif')):\n                # If it's a rendition or asset, check if there's a corresponding XML\n                file_name_without_ext = filename.rsplit('.', 1)[0] if '.' in filename else filename\n                # Look for XML with the same basename\n                xml_file = folder_path / f\"{file_name_without_ext}.xml\"\n                if xml_file.is_file():\n                    xml_basename = file_name_without_ext\n                    if xml_basename not in package_data:\n                        package_data[xml_basename] = {\n                            'xml': str(xml_file),\n                            'renditions': [],\n                            'assets': []\n                        }\n                    if filename.lower().endswith('.pdf'):\n                        package_data[xml_basename]['renditions'].append(str(other_file))\n                    else:\n                        package_data[xml_basename]['assets'].append(str(other_file))\n    \n    return package_data"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 969, "num_retrieved": 4, "inference_time": 0.9175620675086975, "generate_results": ["def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1482, "num_retrieved": 20, "inference_time": 0.9175620675086975, "generate_results": ["def add_rendition(self, lang, file_path):\n    rendition_dict = {}\n    rendition_dict[\"original\"] = file_path\n    if lang:\n        rendition_dict[lang] = f\"{file_path.split('.')[0]}-{lang}.{file_path.split('.')[-1]}\"\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    rendition_dict = {}\n    rendition_dict[\"original\"] = file_path\n    if lang:\n        rendition_dict[lang] = f\"{file_path.split('.')[0]}-{lang}.{file_path.split('.')[-1]}\"\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    rendition_dict = {}\n    rendition_dict[\"original\"] = file_path\n    if lang:\n        rendition_dict[lang] = f\"{file_path.split('.')[0]}-{lang}.{file_path.split('.')[-1]}\"\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    rendition_dict = {}\n    rendition_dict[\"original\"] = file_path\n    if lang:\n        rendition_dict[lang] = f\"{file_path.split('.')[0]}-{lang}.{file_path.split('.')[-1]}\"\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    rendition_dict = {}\n    rendition_dict[\"original\"] = file_path\n    if lang:\n        rendition_dict[lang] = f\"{file_path.split('.')[0]}-{lang}.{file_path.split('.')[-1]}\"\n    return rendition_dict"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1335, "num_retrieved": 15, "inference_time": 0.9175620675086975, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the package with optimized formats if available.\"\"\"\n    # Extract the base name without extension\n    base_name = os.path.splitext(basename)[0]\n    \n    # Define supported image formats and their optimized versions\n    formats = {\n        '.tiff': '.png',\n        '.tif': '.png',\n        '.jpg': '.jpg',\n        '.jpeg': '.jpg',\n        '.png': '.png'\n    }\n    \n    # Determine the optimized format\n    ext = os.path.splitext(basename)[1].lower()\n    optimized_ext = formats.get(ext, ext)\n    \n    # Create optimized asset name\n    optimized_basename = f\"{base_name}.{optimized_ext}\"\n    \n    # Add the original and optimized assets if they exist\n    if os.path.exists(file_path):\n        # Add original asset\n        self.assets.items.append({\n            \"name\": basename,\n            \"uri\": file_path,\n            \"xlink_href\": file_path\n        })\n        \n        # Add optimized asset if it exists\n        optimized_path = file_path.replace(basename, optimized_basename)\n        if os.path.exists(optimized_path):\n            self.assets.items.append({\n                \"name\": optimized_basename,\n                \"uri\": optimized_path,\n                \"xlink_href\": optimized_path\n            })\n    \n    return self.assets.items", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the package with optimized formats if available.\"\"\"\n    # Extract the base name without extension\n    base_name = os.path.splitext(basename)[0]\n    \n    # Define supported image formats and their optimized versions\n    formats = {\n        '.tiff': '.png',\n        '.tif': '.png',\n        '.jpg': '.jpg',\n        '.jpeg': '.jpg',\n        '.png': '.png'\n    }\n    \n    # Determine the optimized format based on the original extension\n    original_ext = os.path.splitext(basename)[1].lower()\n    optimized_ext = formats.get(original_ext, original_ext)\n    \n    # Create optimized asset name\n    optimized_basename = f\"{base_name}.{optimized_ext}\"\n    \n    # Add the original and optimized assets to the package\n    self.assets.items.append({\n        \"name\": basename,\n        \"uri\": file_path,\n        \"xlink_href\": file_path\n    })\n    \n    # If optimized version exists, add it as well\n    if optimized_ext != original_ext:\n        optimized_file_path = file_path.replace(basename, optimized_basename)\n        self.assets.items.append({\n            \"name\": optimized_basename,\n            \"uri\": optimized_file_path,\n            \"xlink_href\": optimized_file_path\n        })\n    \n    return self.assets.items", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the package with optimized formats if available.\"\"\"\n    # Extract the base name without extension\n    base_name = os.path.splitext(basename)[0]\n    \n    # Define supported image formats and their optimized versions\n    formats = {\n        '.tiff': '.png',\n        '.tif': '.png',\n        '.jpg': '.jpg',\n        '.jpeg': '.jpg',\n        '.png': '.png'\n    }\n    \n    # Determine the optimized format based on the original extension\n    original_ext = os.path.splitext(basename)[1].lower()\n    optimized_ext = formats.get(original_ext, original_ext)\n    \n    # Create the optimized filename\n    optimized_basename = f\"{base_name}.{optimized_ext}\"\n    \n    # Add the original and optimized assets if they exist\n    if os.path.exists(file_path):\n        # Add original asset\n        self.assets.items.append({\n            \"name\": basename,\n            \"uri\": file_path,\n            \"xlink_href\": file_path\n        })\n        \n        # If optimized version exists, add it as well\n        optimized_path = file_path.replace(basename, optimized_basename)\n        if os.path.exists(optimized_path):\n            self.assets.items.append({\n                \"name\": optimized_basename,\n                \"uri\": optimized_path,\n                \"xlink_href\": optimized_path\n            })\n    \n    return self.assets.items", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the package with optimized formats if available.\"\"\"\n    # Extract the base name without extension\n    base_name = os.path.splitext(basename)[0]\n    \n    # Define supported image formats and their optimized versions\n    formats = {\n        '.tiff': '.png',\n        '.tif': '.png',\n        '.jpg': '.jpg',\n        '.jpeg': '.jpg',\n        '.png': '.png'\n    }\n    \n    # Determine the optimized format based on the original extension\n    original_ext = os.path.splitext(basename)[1].lower()\n    optimized_ext = formats.get(original_ext, original_ext)\n    \n    # Create optimized asset name\n    optimized_basename = f\"{base_name}.{optimized_ext}\"\n    \n    # Add the original and optimized assets to the package\n    self.assets.items.append({\n        \"name\": basename,\n        \"uri\": file_path,\n        \"xlink_href\": file_path\n    })\n    \n    # If optimized version exists, add it as well\n    if optimized_ext != original_ext:\n        optimized_file_path = file_path.replace(basename, optimized_basename)\n        self.assets.items.append({\n            \"name\": optimized_basename,\n            \"uri\": optimized_file_path,\n            \"xlink_href\": optimized_file_path\n        })\n    \n    return self.assets.items", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the package with optimized formats if available.\"\"\"\n    # Extract the base name without extension\n    base_name = os.path.splitext(basename)[0]\n    \n    # Define supported image formats and their optimized versions\n    formats = {\n        '.tiff': '.png',\n        '.tif': '.png',\n        '.jpg': '.jpg',\n        '.jpeg': '.jpg',\n        '.png': '.png'\n    }\n    \n    # Determine the optimized format\n    ext = os.path.splitext(basename)[1].lower()\n    optimized_ext = formats.get(ext, ext)\n    \n    # Create optimized asset name\n    optimized_basename = f\"{base_name}.{optimized_ext}\"\n    \n    # Add the original and optimized assets if they exist\n    if os.path.exists(file_path):\n        # Add original asset\n        self.assets.items.append({\n            \"name\": basename,\n            \"uri\": file_path,\n            \"xlink_href\": file_path\n        })\n        \n        # Add optimized asset if it exists\n        optimized_path = file_path.replace(basename, optimized_basename)\n        if os.path.exists(optimized_path):\n            self.assets.items.append({\n                \"name\": optimized_basename,\n                \"uri\": optimized_path,\n                \"xlink_href\": optimized_path\n            })\n    \n    return self.assets.items"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1014, "num_retrieved": 5, "inference_time": 0.9175620675086975, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n\n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        for file_info in zf.infolist():\n            filename = file_info.filename\n            if filename.endswith('.xml'):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1006, "num_retrieved": 5, "inference_time": 0.9175620675086975, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return [file_info.filename for file_info in zip_file.infolist()]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return [file_info.filename for file_info in zip_file.infolist()]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\n    Example:\n\n    [\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n        '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n    ]\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return [file_info.filename for file_info in zip_file.infolist()]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return [file_info.filename for file_info in zip_file.infolist()]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return [file_info.filename for file_info in zip_file.infolist()]"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1386, "num_retrieved": 18, "inference_time": 0.9175620675086975, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    return re.sub(r'w:st=\"', 'w-st=\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    return re.sub(r'w:st=\"', 'w-st=\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    return re.sub(r'w:st=\"', 'w-st=\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    return re.sub(r'w:st=\"', 'w-st=\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    return re.sub(r'w:st=\"', 'w-st=\"', content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1216, "num_retrieved": 12, "inference_time": 0.9175620675086975, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        result = node.xpath(xpath)\n        if result:\n            return result[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1091, "num_retrieved": 8, "inference_time": 0.9175620675086975, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a period or space to extract number and supplement\n    # Common patterns: \"2(2)\", \"2(2)\", \"2-3(2)\", etc.\n    issue_str = issue.strip()\n    \n    # Try to find a number and supplement pattern\n    # Look for a number followed by a parenthesis or other separator\n    import re\n    match = re.search(r'(\\d+)(?:\\s*\\((\\d+)\\))?', issue_str)\n    \n    if match:\n        number = int(match.group(1))\n        supplement = match.group(2) if match.group(2) else \"\"\n        return number, supplement\n    \n    # If no clear pattern found, try to extract just the first number\n    number_match = re.search(r'\\d+', issue_str)\n    if number_match:\n        number = int(number_match.group())\n        supplement = \"\"\n        return number, supplement\n    \n    # Default fallback\n    return 0, \"\"", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # The supplement is typically after a dot or in a separate part\n    parts = issue.strip().split('.')\n    if len(parts) == 1:\n        # If no dot, try to extract number and supplement from the whole string\n        # Assume number is the first part, supplement is empty\n        number = parts[0]\n        supplement = \"\"\n    else:\n        # Split into number and supplement\n        number = parts[0]\n        supplement = \".\".join(parts[1:])\n    \n    # Ensure number is numeric\n    try:\n        number = int(number)\n    except ValueError:\n        number = 0\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # Common patterns: \"1(2)\", \"1-2\", \"1(2)\", \"1-2(3)\"\n    issue_str = issue.strip()\n    \n    # Try to extract number (first part before any parentheses or dash)\n    number_match = re.match(r'^(\\d+)(?:\\s*[\\-\\(\\)]\\s*(\\d+))?', issue_str)\n    if number_match:\n        number = int(number_match.group(1))\n        supplement = number_match.group(2) if number_match.group(2) else \"\"\n        return number, supplement\n    \n    # Fallback: if no clear pattern, return 0 and empty string\n    return 0, \"\"", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # The supplement is typically after a dot or in a separate part\n    parts = issue.strip().split('.')\n    number = parts[0] if parts else \"0\"\n    \n    # Check if there's a supplement part (e.g., \"2(2)\", \"2(2a)\", \"2(2b)\")\n    supplement = \"\"\n    if len(parts) > 1:\n        supplement_part = parts[1]\n        # Extract supplement from the part after the dot\n        if supplement_part.startswith('('):\n            supplement = supplement_part[1:-1]  # Remove parentheses\n        else:\n            supplement = supplement_part\n    \n    # If no number is found, default to 0\n    if not number or not number.isdigit():\n        number = \"0\"\n    \n    return int(number), supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by a dot or space to extract number and supplement\n    # The supplement is typically after a dot or in a separate part\n    parts = issue.strip().split('.')\n    number = parts[0] if parts else \"0\"\n    \n    # Check if there's a supplement part (e.g., \"2(2)\" or \"2(2)\")\n    supplement = \"\"\n    if len(parts) > 1:\n        supplement_part = parts[1]\n        # Extract the supplement part (everything after the first dot)\n        # Remove any non-digit characters or parentheses\n        supplement = re.sub(r'[^0-9]', '', supplement_part)\n    \n    # If no supplement found, try to extract from parentheses\n    if not supplement:\n        match = re.search(r'\\(([^)]+)\\)', issue)\n        if match:\n            supplement = match.group(1)\n    \n    # Ensure number is a valid integer\n    try:\n        number = int(number)\n    except ValueError:\n        number = 0\n    \n    return number, supplement"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 0.9175620675086975, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return \"NOT \" + self.children[0].pretty(indent + 1, debug)\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 0.9175620675086975, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it matches absorption or negative absorption patterns\n        # This is a simplified implementation assuming expressions are in a form that can be evaluated\n        \n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using Boolean algebra rules\n        # We'll assume the expressions are already in a form that can be processed\n        # and that the absorption rules can be applied directly\n        \n        # In practice, this would require parsing and evaluating the expression tree\n        # For now, we return the original expression as a placeholder\n        # A full implementation would require recursive evaluation of the expression tree\n        \n        # Apply absorption: if expr is of form A & (A | B) or A | (A & B), reduce to A\n        # Apply negative absorption: if expr is of form A & (~A | B) or A | (~A & B), reduce to A & B or A | B\n        \n        # Since we don't have full expression parsing/evaluation capabilities here,\n        # we return the original expression as a placeholder\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # This is a simplified implementation - actual logic would depend on expression structure\n        if isinstance(expr, Expression):\n            # Simplify using absorption laws\n            # If expr is of the form A & (A | B), reduce to A\n            # If expr is of the form A | (A & B), reduce to A\n            # If expr is of the form A & (~A | B), reduce to A & B\n            # If expr is of the form A | (~A & B), reduce to A | B\n            \n            # This implementation assumes we can detect and simplify these forms\n            # In a full implementation, we would parse the expression tree\n            # For now, we just return the original expression as a placeholder\n            pass\n        \n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        # Absorption: A & (A | B) = A, A | (A & B) = A\n        # Negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(expr, Expression):\n            # Check for absorption cases\n            # Case 1: A & (A | B) = A\n            # Case 2: A | (A & B) = A\n            # Case 3: A & (~A | B) = A & B\n            # Case 4: A | (~A & B) = A | B\n            \n            # We need to evaluate the structure of the expression\n            # This is a simplified implementation assuming we can detect patterns\n            # In a full implementation, we would parse the expression tree\n            \n            # For now, return the expression as is (placeholder)\n            result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we just return the original expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            # This is a simplified implementation that assumes the expression\n            # is in a form where we can apply the rules directly\n            # In a full implementation, we would parse the expression tree\n            # and apply the rules recursively\n            result.append(expr)\n        else:\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 960, "num_retrieved": 4, "inference_time": 0.9175620675086975, "generate_results": ["def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified\n        hook.\n    \"\"\"\n    def decorator(fn):\n        self.register_hook(hook, fn)\n        return fn\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        self.register_hook(hook, fn)\n        return fn\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        self.register_hook(hook, fn)\n        return fn\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        self.registry[hook].append(fn)\n        return fn\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        self.register_hook(hook, fn)\n        return fn\n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 881, "num_retrieved": 3, "inference_time": 0.9175620675086975, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"auth\": {\n            \"static\": user\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"auth\": {\n            \"static\": user\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 904, "num_retrieved": 3, "inference_time": 0.9175620675086975, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous, we can safely determine\n    # if it's in a fold state by checking the fold attribute\n    _fold = self._fold(dt)\n    \n    # If it's in a fold, we need to adjust the index to resolve ambiguity\n    idx = self._find_last_transition(dt)\n    \n    if idx is None or idx == 0:\n        return dt.replace(tzinfo=self)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # If it's ambiguous and in a fold, we need to shift to the correct index\n    idx_offset = int(not _fold and is_ambiguous)\n    \n    # Adjust the index to get the correct time\n    adjusted_idx = idx - idx_offset\n    \n    # Create the new datetime with the adjusted index\n    new_dt = dt.replace(tzinfo=self)\n    \n    return new_dt", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous, we can safely determine\n    # if it's in a fold state by checking the fold attribute\n    _fold = self._fold(dt)\n    \n    # If it's in a fold, we need to adjust the index to resolve the ambiguity\n    idx = self._find_last_transition(dt)\n    \n    if idx is None or idx == 0:\n        return dt.replace(tzinfo=self)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # Adjust index based on fold and ambiguity\n    idx_offset = int(not _fold and is_ambiguous)\n    \n    # Return the resolved datetime with the correct timezone\n    resolved_idx = idx - idx_offset\n    \n    # Create the final datetime with the new timezone\n    return self._localize(dt.replace(tzinfo=None), resolved_idx)", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous, we can safely determine\n    # if it's in a fold state by checking the fold attribute\n    _fold = self._fold(dt)\n    \n    # If it's in a fold, we need to adjust the index to resolve the ambiguity\n    idx = self._find_last_transition(dt)\n    \n    if idx is None or idx == 0:\n        return dt.replace(tzinfo=self)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # Adjust index based on fold and ambiguity\n    idx_offset = int(not _fold and is_ambiguous)\n    \n    # Return the resolved datetime with the appropriate timezone\n    resolved_idx = idx - idx_offset\n    \n    # Create the final datetime with the new timezone\n    return self._dt_from_index(resolved_idx, dt)", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous, we can directly convert it\n    # First, check if the datetime is ambiguous in this timezone\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine if it's in a fold state\n        fold = self._fold(dt)\n        # Adjust the index to resolve the ambiguity\n        idx = self._find_last_transition(dt)\n        if idx is None or idx == 0:\n            return dt.replace(tzinfo=self)\n        \n        # Adjust index based on fold state\n        idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n        resolved_idx = idx - idx_offset\n        \n        # Create a new datetime with the resolved index\n        # This effectively resolves the ambiguity by choosing the correct occurrence\n        # We need to adjust the time based on the transition\n        # Since we're converting from UTC, we need to ensure the correct time zone\n        # representation\n        dt_utc = dt.astimezone(UTC)\n        dt_result = dt_utc.astimezone(self)\n        \n        # Ensure the result is timezone-aware\n        return dt_result\n    \n    # If not ambiguous, simply convert\n    return dt.astimezone(self)", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous, we can safely compute\n    # the local time by converting from UTC to local timezone\n    # First, ensure we're working with a timezone-aware datetime\n    if dt.tzinfo is None:\n        raise ValueError(\"Datetime must be timezone-aware\")\n\n    # Convert from UTC to local timezone\n    local_dt = dt.astimezone(self)\n    \n    # Determine if the datetime is ambiguous or in a fold state\n    # We know it's unambiguous, so we just need to check if it's in a fold\n    fold = self._fold(dt)\n    \n    # If it's in a fold, we need to adjust the index to get the correct time\n    # However, since we're in a known unambiguous state, we don't need to adjust\n    # The fold state is already handled by the timezone's internal logic\n    \n    return local_dt"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 975, "num_retrieved": 4, "inference_time": 0.9175620675086975, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    \n    # Handle each attribute and convert fractional values to integers\n    # For days, convert fractional part to hours\n    if hasattr(self, 'days') and self.days is not None:\n        days = int(self.days)\n        fractional_days = self.days - days\n        if fractional_days > 0:\n            hours = int(fractional_days * 24)\n            result.hours = result.hours + hours\n            # Adjust days if hours exceed 24\n            if result.hours >= 24:\n                result.days += result.hours // 24\n                result.hours = result.hours % 24\n    \n    # Handle hours\n    if hasattr(self, 'hours') and self.hours is not None:\n        result.hours = int(self.hours)\n    \n    # Handle minutes\n    if hasattr(self, 'minutes') and self.minutes is not None:\n        result.minutes = int(self.minutes)\n    \n    # Handle seconds\n    if hasattr(self, 'seconds') and self.seconds is not None:\n        result.seconds = int(self.seconds)\n    \n    # Handle microseconds\n    if hasattr(self, 'microseconds') and self.microseconds is not None:\n        result.microseconds = int(self.microseconds)\n    \n    # Handle other attributes (months, weeks, etc.)\n    if hasattr(self, 'months') and self.months is not None:\n        result.months = int(self.months)\n    \n    if hasattr(self, 'weeks') and self.weeks is not None:\n        result.weeks = int(self.weeks)\n    \n    # Ensure all values are integers\n    result = result.normalize()\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    \n    # Handle each attribute and convert fractional parts to integers\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months']:\n        value = getattr(result, attr, 0)\n        if isinstance(value, (int, float)):\n            if isinstance(value, float):\n                # Convert fractional part to integer\n                int_val = int(value)\n                frac_val = value - int_val\n                if frac_val > 0:\n                    # Add the fractional part to the next unit\n                    if attr == 'days':\n                        # Add fractional days to hours\n                        hours = getattr(result, 'hours', 0)\n                        new_hours = hours + int(frac_val * 24)\n                        result.hours = new_hours\n                    elif attr == 'hours':\n                        # Add fractional hours to minutes\n                        minutes = getattr(result, 'minutes', 0)\n                        new_minutes = minutes + int(frac_val * 60)\n                        result.minutes = new_minutes\n                    elif attr == 'minutes':\n                        # Add fractional minutes to seconds\n                        seconds = getattr(result, 'seconds', 0)\n                        new_seconds = seconds + int(frac_val * 60)\n                        result.seconds = new_seconds\n                    elif attr == 'seconds':\n                        # Add fractional seconds to days (1 day = 86400 seconds)\n                        days = getattr(result, 'days', 0)\n                        new_days = days + int(frac_val / 86400)\n                        result.days = new_days\n            # If it's already an integer, keep it as is\n            setattr(result, attr, int(value))\n    \n    # Ensure all values are integers\n    result.days = int(result.days)\n    result.hours = int(result.hours)\n    result.minutes = int(result.minutes)\n    result.seconds = int(result.seconds)\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    \n    # Handle each attribute and convert fractional values to integers\n    # For days, convert fractional part to hours\n    if hasattr(self, 'days') and self.days is not None:\n        days = int(self.days)\n        fractional_days = self.days - days\n        if fractional_days > 0:\n            hours = int(fractional_days * 24)\n            result.hours = result.hours + hours\n            # Adjust days if hours exceed 24\n            if result.hours >= 24:\n                result.days += result.hours // 24\n                result.hours = result.hours % 24\n    \n    # Handle hours\n    if hasattr(self, 'hours') and self.hours is not None:\n        hours = int(self.hours)\n        fractional_hours = self.hours - hours\n        if fractional_hours > 0:\n            minutes = int(fractional_hours * 60)\n            result.minutes = result.minutes + minutes\n            # Adjust hours if minutes exceed 60\n            if result.minutes >= 60:\n                result.hours += result.minutes // 60\n                result.minutes = result.minutes % 60\n    \n    # Handle minutes\n    if hasattr(self, 'minutes') and self.minutes is not None:\n        minutes = int(self.minutes)\n        fractional_minutes = self.minutes - minutes\n        if fractional_minutes > 0:\n            seconds = int(fractional_minutes * 60)\n            result.seconds = result.seconds + seconds\n            # Adjust minutes if seconds exceed 60\n            if result.seconds >= 60:\n                result.minutes += result.seconds // 60\n                result.seconds = result.seconds % 60\n    \n    # Handle seconds\n    if hasattr(self, 'seconds') and self.seconds is not None:\n        seconds = int(self.seconds)\n        fractional_seconds = self.seconds - seconds\n        if fractional_seconds > 0:\n            microseconds = int(fractional_seconds * 1000000)\n            result.microseconds = result.microseconds + microseconds\n            # Adjust seconds if microseconds exceed 1", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    \n    # Handle each attribute and convert fractional parts to integers\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(result, attr, 0)\n        if isinstance(value, (int, float)):\n            if isinstance(value, float):\n                # Convert fractional part to integer\n                int_part = int(value)\n                frac_part = value - int_part\n                if frac_part > 0:\n                    # Add the fractional part to the next unit\n                    if attr == 'days':\n                        # Add fractional days to hours\n                        hours = getattr(result, 'hours', 0)\n                        hours += frac_part * 24\n                        result.hours = int(hours)\n                    elif attr == 'hours':\n                        # Add fractional hours to minutes\n                        minutes = getattr(result, 'minutes', 0)\n                        minutes += frac_part * 60\n                        result.minutes = int(minutes)\n                    elif attr == 'minutes':\n                        # Add fractional minutes to seconds\n                        seconds = getattr(result, 'seconds', 0)\n                        seconds += frac_part * 60\n                        result.seconds = int(seconds)\n                    elif attr == 'seconds':\n                        # Add fractional seconds to days (very small)\n                        days = getattr(result, 'days', 0)\n                        days += frac_part / (24 * 60 * 60)\n                        result.days = int(days)\n            else:\n                # Already integer\n                pass\n    \n    # Ensure all values are integers\n    result.days = int(result.days)\n    result.hours = int(result.hours)\n    result.minutes = int(result.minutes)\n    result.seconds = int(result.seconds)\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    \n    # Process each attribute to convert fractional values to integers\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(result, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if isinstance(value, float):\n                # Handle fractional days, hours, minutes, seconds\n                if attr == 'days':\n                    days = int(value)\n                    fractional = value - days\n                    # Convert fractional days to hours\n                    fractional_hours = fractional * 24\n                    # Add to hours\n                    hours = getattr(result, 'hours', 0) + int(fractional_hours)\n                    # Adjust days and hours\n                    result.days = days\n                    result.hours = hours\n                elif attr == 'hours':\n                    result.hours = int(value)\n                elif attr == 'minutes':\n                    result.minutes = int(value)\n                elif attr == 'seconds':\n                    result.seconds = int(value)\n    \n    # Ensure all values are integers\n    result.days = int(result.days)\n    result.hours = int(result.hours)\n    result.minutes = int(result.minutes)\n    result.seconds = int(result.seconds)\n    \n    return result"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1007, "num_retrieved": 5, "inference_time": 0.9175620675086975, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    if six.PY2:\n        return lambda: namefunc().encode('ascii')\n    return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    if six.PY2:\n        # In Python 2, convert unicode to bytestring using ASCII encoding\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return as is (already bytes or unicode)\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1003, "num_retrieved": 6, "inference_time": 0.877564087510109, "generate_results": ["def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        # Attempt to get version from _version.py file\n        filename = \"_version.py\"\n        with open(filename) as f:\n            contents = f.read()\n        \n        # Parse version information from the file\n        # This is a simplified version - actual parsing would depend on the format\n        # in _version.py\n        import re\n        match = re.search(r\"__version__\\s*=\\s*['\\\"]([^'\\\"]*)['\\\"]\", contents)\n        if match:\n            return {\"version\": match.group(1), \"long\": match.group(1), \"dirty\": False, \"date\": None}\n        \n        # Fallback to default version if parsing fails\n        return {\"version\": \"0.0.0\", \"long\": \"00000000000000000000\", \"dirty\": False, \"date\": None}\n        \n    except (OSError, IOError, re.error):\n        # If file not found or parsing fails, return default version\n        return {\"version\": \"0.0.0\", \"long\": \"00000000000000000000\", \"dirty\": False, \"date\": None}", "def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        # Attempt to get version from _version.py file\n        filename = \"_version.py\"\n        with open(filename) as f:\n            contents = f.read()\n        \n        # Parse version information from the file\n        # This is a simplified version - actual parsing would depend on the format\n        # in _version.py (likely using regex to extract version string)\n        import re\n        match = re.search(r\"__version__\\s*=\\s*['\\\"]([^'\\\"]*)['\\\"]\", contents)\n        if match:\n            return {\"version\": match.group(1), \"long\": \"\", \"dirty\": False, \"date\": None}\n        \n        # If no version found, return default\n        return {\"version\": \"0.0.0\", \"long\": \"\", \"dirty\": False, \"date\": None}\n        \n    except (OSError, IOError, re.error):\n        # If unable to read or parse the file, return default\n        return {\"version\": \"0.0.0\", \"long\": \"\", \"dirty\": False, \"date\": None}", "def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        # Attempt to get version from _version.py file\n        filename = \"_version.py\"\n        with open(filename) as f:\n            contents = f.read()\n        \n        # Parse version information from the file\n        # This is a simplified version - actual parsing would depend on the format\n        # in _version.py (likely using regex to extract version string)\n        import re\n        match = re.search(r\"__version__\\s*=\\s*['\\\"]([^'\\\"]+)['\\\"]\", contents)\n        if match:\n            return {\"version\": match.group(1), \"long\": \"\", \"dirty\": False, \"date\": None}\n        \n        # If no version found, return default\n        return {\"version\": \"0.0.0\", \"long\": \"\", \"dirty\": False, \"date\": None}\n        \n    except (OSError, IOError, re.error):\n        # If unable to read or parse the file, return default\n        return {\"version\": \"0.0.0\", \"long\": \"\", \"dirty\": False, \"date\": None}", "def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        # Attempt to read version information from _version.py\n        filename = \"_version.py\"\n        with open(filename) as f:\n            contents = f.read()\n        \n        # Parse the version from the file content\n        # This is a simplified version - actual parsing would depend on the format\n        import re\n        match = re.search(r\"__version__\\s*=\\s*['\\\"]([^'\\\"]+)['\\\"]\", contents)\n        if match:\n            return {\"version\": match.group(1), \"long\": \"\", \"dirty\": False, \"date\": None}\n        \n        # Fallback if version not found in the file\n        return {\"version\": \"0.0.0\", \"long\": \"\", \"dirty\": False, \"date\": None}\n        \n    except (OSError, IOError):\n        # If file not found or cannot be read, return default version\n        return {\"version\": \"0.0.0\", \"long\": \"\", \"dirty\": False, \"date\": None}", "def get_versions():\n    \"\"\"\n    Get version information or return default if unable to do so.\n    \"\"\"\n    try:\n        # Attempt to read version information from _version.py\n        filename = \"_version.py\"\n        with open(filename) as f:\n            contents = f.read()\n        \n        # Extract version information using regex (simplified based on context)\n        import re\n        match = re.search(r\"__version__\\s*=\\s*['\\\"]([^'\\\"]+)['\\\"]\", contents)\n        if match:\n            return {\"version\": match.group(1), \"long\": \"\", \"dirty\": False, \"date\": None}\n        \n        # Fallback to default version if not found\n        return {\"version\": \"0.0.0\", \"long\": \"\", \"dirty\": False, \"date\": None}\n        \n    except (OSError, re.error):\n        # If unable to read or parse the file, return default\n        return {\"version\": \"0.0.0\", \"long\": \"\", \"dirty\": False, \"date\": None}"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1000, "num_retrieved": 6, "inference_time": 0.877564087510109, "generate_results": ["def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440_pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440_old\":\n        return render_pep440_old(pieces)\n    elif style == \"pep440_post_branch\":\n        return render_pep440_post_branch(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440_pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440_old\":\n        return render_pep440_old(pieces)\n    elif style == \"pep440_post_branch\":\n        return render_pep440_post_branch(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440_pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440_old\":\n        return render_pep440_old(pieces)\n    elif style == \"pep440_post_branch\":\n        return render_pep440_post_branch(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440_pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440_old\":\n        return render_pep440_old(pieces)\n    elif style == \"pep440_post_branch\":\n        return render_pep440_post_branch(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440_pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440_old\":\n        return render_pep440_old(pieces)\n    elif style == \"pep440_post_branch\":\n        return render_pep440_post_branch(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 927, "num_retrieved": 3, "inference_time": 0.877564087510109, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if '+' in pieces['closest-tag'] or '+' in pieces['short']:\n        return '.'\n    return '+'", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if '+' in pieces.get('version', ''):\n        return '.'\n    return '+'", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if '+' in pieces['version']:\n        return '.'\n    return '+'", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if '+' in pieces['version']:\n        return '.'\n    return '+'", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if '+' in pieces.get('version', ''):\n        return '.'\n    return '+'"]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 977, "num_retrieved": 5, "inference_time": 0.877564087510109, "generate_results": ["import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Ensure commands is a list\n    if isinstance(commands, str):\n        commands = [commands]\n    \n    # Ensure args is a list\n    if args is not None:\n        if isinstance(args, str):\n            args = [args]\n        elif not isinstance(args, list):\n            args = list(args)\n    \n    # Build the full command\n    full_command = commands + args\n    \n    # Set environment\n    env = env or os.environ.copy()\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n        \n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr if not hide_stderr else None\n        }\n        \n    except Exception as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Ensure commands is a list\n    if isinstance(commands, str):\n        commands = [commands]\n    \n    # Ensure args is a list\n    if args is not None:\n        if isinstance(args, str):\n            args = [args]\n        elif not isinstance(args, list):\n            args = list(args)\n    \n    # Build the full command\n    full_command = commands + args\n    \n    # Set environment\n    env = env or os.environ.copy()\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n        \n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr\n        }\n        \n    except Exception as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Ensure commands is a list\n    if isinstance(commands, str):\n        commands = [commands]\n    \n    # Ensure args is a list\n    if args is not None:\n        if isinstance(args, str):\n            args = [args]\n        elif not isinstance(args, list):\n            args = list(args)\n    \n    # Build the full command\n    full_command = commands + args\n    \n    # Set environment\n    env = env or os.environ.copy()\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose\n        if verbose:\n            if result.stdout:\n                print(result.stdout, file=sys.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n        \n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr if not hide_stderr else None\n        }\n        \n    except Exception as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Ensure commands is a list\n    if isinstance(commands, str):\n        commands = [commands]\n    \n    # Ensure args is a list\n    if args is not None:\n        if isinstance(args, str):\n            args = [args]\n        elif not isinstance(args, list):\n            args = list(args)\n    \n    # Build the full command\n    full_command = commands + args\n    \n    # Set environment\n    env = env or os.environ.copy()\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n        \n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr\n        }\n        \n    except Exception as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Ensure commands is a list\n    if isinstance(commands, str):\n        commands = [commands]\n    \n    # Ensure args is a list\n    if args is not None:\n        if isinstance(args, str):\n            args = [args]\n        elif not isinstance(args, list):\n            args = list(args)\n    \n    # Build the full command\n    full_command = commands + args\n    \n    # Set environment\n    env = env or os.environ.copy()\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n        \n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr if not hide_stderr else None\n        }\n        \n    except Exception as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 988, "num_retrieved": 6, "inference_time": 0.877564087510109, "generate_results": ["def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    # Since the VersioneerConfig class is not defined in the context,\n    # and no specific configuration values are provided,\n    # we return a basic configuration object with default values.\n    # In a real scenario, this would initialize and populate the config with\n    # actual project-specific settings.\n    \n    config = type('VersioneerConfig', (), {})()\n    return config", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    # Since the VersioneerConfig class is not defined in the context,\n    # and no specific configuration values are provided,\n    # we return a dummy configuration object.\n    # In a real scenario, this would initialize and populate the config with actual values.\n    return type('VersioneerConfig', (), {})()", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    # Since the VersioneerConfig class is not defined in the context,\n    # and no specific configuration values are provided,\n    # we return a dummy configuration object.\n    # In a real implementation, this would initialize and populate\n    # the VersioneerConfig with appropriate values.\n    return type('VersioneerConfig', (), {})()", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    # Since the VersioneerConfig class is not defined in the context,\n    # and no specific configuration values are provided,\n    # we return a dummy configuration object.\n    # In a real implementation, this would initialize and populate\n    # the VersioneerConfig with appropriate settings.\n    return type('VersioneerConfig', (), {})()", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    # Since the VersioneerConfig class is not defined in the context,\n    # and no specific configuration values are provided,\n    # we return a placeholder configuration object.\n    # In a real implementation, this would initialize and populate\n    # the VersioneerConfig with appropriate settings.\n    return type('VersioneerConfig', (), {})()"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 973, "num_retrieved": 5, "inference_time": 0.877564087510109, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n\n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 954, "num_retrieved": 4, "inference_time": 0.877564087510109, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Args:\n        validate_objects: Whether to validate individual objects within the hierarchy.\n        check_digests: Whether to check digest files for integrity.\n        show_warnings: Whether to show warning messages during validation.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    self.log = self.log  # Ensure log is available\n    \n    # Validate root structure\n    try:\n        root_entries = self.obj_fs.scandir('.')\n        for entry in root_entries:\n            if entry.is_dir:\n                # Check if directory is a valid object directory\n                if entry.name in ['content', 'extensions', 'inventory']:\n                    continue  # Skip known directories\n                \n                # Validate object directory\n                if validate_objects:\n                    try:\n                        # Validate object structure and content\n                        obj_validator = self._create_object_validator(entry.name)\n                        if obj_validator.validate():\n                            good_objects += 1\n                        num_objects += 1\n                    except Exception as e:\n                        self.log.error('E001', entry=entry.name, error=str(e))\n                        num_objects += 1\n                else:\n                    num_objects += 1\n                    \n                # Validate extensions directory\n                if entry.name == 'extensions':\n                    self.validate_extensions_dir()\n                    \n                # Check digest files if requested\n                if check_digests:\n                    self._check_digest_files(entry.name)\n                    \n                # Show warnings if requested\n                if show_warnings:\n                    self._show_warnings(entry.name)\n                    \n    except Exception as e:\n        self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Args:\n        validate_objects: Whether to validate individual objects within the hierarchy.\n        check_digests: Whether to check digest files for integrity.\n        show_warnings: Whether to show warning messages during validation.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Validate root structure and directories\n    try:\n        # Check for required directories\n        required_dirs = ['content', 'inventory', 'extensions']\n        for dir_name in required_dirs:\n            if not self.obj_fs.exists(dir_name):\n                self.log.error('E001', directory=dir_name)\n                continue\n                \n            if not self.obj_fs.isdir(dir_name):\n                self.log.error('E002', directory=dir_name)\n                continue\n                \n        # Validate extensions directory\n        if self.obj_fs.exists('extensions'):\n            self.validate_extensions_dir()\n            \n        # Validate content directory\n        if self.obj_fs.exists('content'):\n            content_entries = self.obj_fs.scandir('content')\n            for entry in content_entries:\n                if entry.is_dir:\n                    num_objects += 1\n                    if validate_objects:\n                        # Validate object within content directory\n                        if self.validate_object(entry.name):\n                            good_objects += 1\n                elif entry.is_file:\n                    # Handle files in content directory if needed\n                    pass\n                    \n        # Check digest files if requested\n        if check_digests:\n            # Validate digest files in inventory\n            inventory_dir = self.obj_fs.path('inventory')\n            if self.obj_fs.exists(inventory_dir):\n                for digest_file in self.obj_fs.listdir(inventory_dir):\n                    if digest_file.endswith('.digest'):\n                        # Validate digest file integrity\n                        try:\n                            self.validate_digest_file(digest_file)\n                        except Exception as e:\n                            self.log.error('E003', file=digest_file, error=str(e))\n        \n        # Show warnings if requested\n        if show_warnings:\n            self.log.show_warnings()\n            \n    except Exception as e:\n        self.log.error('E999', error=str(e))\n        \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Args:\n        validate_objects: Whether to validate the objects themselves.\n        check_digests: Whether to check the integrity of digests.\n        show_warnings: Whether to show warning messages during validation.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Validate root directory structure\n    try:\n        root_entries = self.obj_fs.scandir('.')\n        for entry in root_entries:\n            if entry.is_dir:\n                # Check for valid object directories\n                if entry.name in ['content', 'extensions', 'inventory']:\n                    continue  # Skip known directories\n                else:\n                    # Validate object directory structure\n                    obj_dir = entry.name\n                    if self._validate_object_directory(obj_dir):\n                        num_objects += 1\n                        if validate_objects:\n                            if self._validate_object_contents(obj_dir):\n                                good_objects += 1\n                        if check_digests:\n                            if self._check_digests(obj_dir):\n                                good_objects += 1\n                    else:\n                        if show_warnings:\n                            self.log.warning('W012', directory=obj_dir)\n            elif entry.is_file:\n                # Handle files in root (e.g., metadata)\n                if entry.name in ['manifest.json', 'inventory.json']:\n                    if show_warnings:\n                        self.log.warning('W011', file=entry.name)\n    \n    except Exception as e:\n        self.log.error('E001', error=str(e))\n    \n    # Validate extensions directory\n    if self.obj_fs.exists('extensions'):\n        self.validate_extensions_dir()\n    \n    return num_objects, good_objects\n\ndef _validate_object_directory(self, obj_dir):\n    \"\"\"Validate basic structure of an object directory.\"\"\"\n    try:\n        # Check for required directories\n        required_dirs = ['content', 'inventory']\n        for dir_name in required_dirs:\n            if not self.obj_fs.exists(f'{obj_dir}/{dir_name}'):\n                return False\n        return True\n    except Exception:\n        return False\n\ndef _validate_object_contents(self, obj_dir):\n    \"\"\"Validate contents of an object directory.\"\"\"\n    try:\n        # Check content directory\n        content_dir = f'{obj_dir}/content'\n        if not self.obj_fs.exists(content_dir):\n            return False", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Args:\n        validate_objects: Whether to validate individual objects within the hierarchy.\n        check_digests: Whether to check digest files for integrity.\n        show_warnings: Whether to show warning messages during validation.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    self.log = self.log  # Ensure log is available\n    \n    # Validate root directory structure\n    try:\n        root_entries = self.obj_fs.scandir('.')\n        for entry in root_entries:\n            if entry.is_dir:\n                # Check if directory is a valid object directory\n                if entry.name in ['content', 'extensions', 'inventory']:\n                    continue  # Skip known directories\n                else:\n                    # Validate object directory structure\n                    obj_dir = entry.name\n                    if self._validate_object_directory(obj_dir):\n                        num_objects += 1\n                        if validate_objects:\n                            if self._validate_object_contents(obj_dir, check_digests):\n                                good_objects += 1\n                        if show_warnings:\n                            self.log.warning('W012', directory=obj_dir)\n    except Exception as e:\n        self.log.error('E001', error=str(e))\n    \n    return num_objects, good_objects\n\ndef _validate_object_directory(self, directory):\n    \"\"\"Validate that a directory has the correct structure.\"\"\"\n    try:\n        # Check for required directories\n        required_dirs = ['content', 'inventory']\n        for req_dir in required_dirs:\n            if not self.obj_fs.exists(f'{directory}/{req_dir}'):\n                return False\n        return True\n    except Exception:\n        return False\n\ndef _validate_object_contents(self, directory, check_digests):\n    \"\"\"Validate the contents of an object directory.\"\"\"\n    try:\n        # Validate content directory\n        content_dir = f'{directory}/content'\n        if not self.obj_fs.exists(content_dir):\n            return False\n            \n        # Validate inventory directory\n        inventory_dir = f'{directory}/inventory'\n        if not self.obj_fs.exists(inventory_dir):\n            return False\n            \n        # Validate extensions directory\n        if self.obj_fs.exists(f'{directory}/extensions'):\n            self.validate_extensions_dir()\n            \n        # Check digests if requested\n        if check_digests:\n            self._validate_digests(directory", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Validate the root directory structure\n    try:\n        root_entries = self.obj_fs.scandir('.')\n        for entry in root_entries:\n            if entry.is_dir:\n                # Check if it's a valid object directory\n                if entry.name in ['content', 'extensions', 'inventory']:\n                    continue  # Skip known directories\n                \n                # Validate object directory structure\n                obj_dir = entry.name\n                obj_path = f\"{obj_dir}/\"\n                \n                # Check for required directories\n                required_dirs = ['content', 'inventory']\n                missing_dirs = []\n                for dir_name in required_dirs:\n                    dir_path = f\"{obj_path}{dir_name}\"\n                    if not self.obj_fs.exists(dir_path):\n                        missing_dirs.append(dir_name)\n                \n                if missing_dirs:\n                    if show_warnings:\n                        self.log.warning('W012', obj=obj_dir, missing=missing_dirs)\n                    continue\n                \n                # Validate content directory\n                if validate_objects:\n                    content_entries = self.obj_fs.scandir(f\"{obj_path}content\")\n                    for content_entry in content_entries:\n                        if content_entry.is_file:\n                            num_objects += 1\n                            if check_digests:\n                                # Validate digest files (simplified)\n                                digest_file = f\"{obj_path}inventory/{obj_dir}.json\"\n                                if self.obj_fs.exists(digest_file):\n                                    try:\n                                        # Placeholder for digest validation\n                                        pass\n                                    except Exception as e:\n                                        if show_warnings:\n                                            self.log.warning('W014', obj=obj_dir, error=str(e))\n                                        continue\n                            good_objects += 1\n                \n                # Validate extensions directory\n                if entry.name == 'extensions':\n                    self.validate_extensions_dir()\n                \n    except Exception as e:\n        if show_warnings:\n            self.log.warning('W015', error=str(e))\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1010, "num_retrieved": 7, "inference_time": 0.877564087510109, "generate_results": ["def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the OCFL object structure\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default values for validation\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, show_errors=self.show_errors, lang=self.lang)\n    \n    # Initialize the storage root structure\n    # This would typically involve creating directories, setting up metadata, etc.\n    # For now, just ensure all required attributes are set\n    pass", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the OCFL object structure\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default values for validation\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, show_errors=self.show_errors, lang=self.lang)\n    \n    # Initialize the storage root structure\n    # This would typically involve creating the necessary directory structure\n    # and setting up the initial metadata files\n    pass", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the OCFL object structure\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default values for validation\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, show_errors=self.show_errors, lang=self.lang)\n    \n    # Initialize the storage root structure\n    # This would typically involve creating directories, setting up metadata, etc.\n    # For now, just set up the required attributes\n    self.id = \"ocfl-root\"\n    self.spec_version = \"1.0\"\n    self.digest_algorithm = \"sha256\"\n    self.content_directory = \"content\"\n    self.inventory_digest_files = [\"inventory.json\", \"digests.json\"]\n    \n    # Initialize filesystem and validation components\n    self.obj_fs = ObjectFileSystem()\n    self.root_inv_validator = Validator()", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the OCFL object structure\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default values for validation\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, show_errors=self.show_errors, lang=self.lang)\n    \n    # Initialize the storage root structure\n    # This would typically involve creating the necessary directory structure\n    # and setting up the initial metadata files\n    pass", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the OCFL object structure\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    self.obj_fs = None\n    \n    # Set up default values for validation\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=self.show_warnings, show_errors=self.show_errors, lang=self.lang)\n    \n    # Initialize the storage root structure\n    # This would typically involve creating the necessary directories and files\n    # such as the root directory, inventory, and content directory\n    # The actual implementation would depend on the specific OCFL storage requirements\n    pass"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 998, "num_retrieved": 6, "inference_time": 0.877564087510109, "generate_results": ["def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Convert version to integer for arithmetic\n    try:\n        v = int(version)\n    except ValueError:\n        raise ValueError(\"Version must be a valid integer\")\n    \n    # Return the next integer version\n    return str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Convert version to integer for arithmetic\n    try:\n        v = int(version)\n    except ValueError:\n        raise ValueError(\"Version must be a valid integer\")\n    \n    # Return the next version number\n    return str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Convert version to integer for arithmetic\n    try:\n        v = int(version)\n    except ValueError:\n        raise ValueError(\"Version must be a valid integer\")\n    \n    # Return the next version number\n    return str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Convert version to integer for comparison\n    try:\n        version_int = int(version)\n    except ValueError:\n        raise ValueError(\"Version must be a valid integer\")\n    \n    # Return the next integer version\n    return str(version_int + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Convert version to integer for comparison\n    try:\n        version_int = int(version)\n    except ValueError:\n        raise ValueError(\"Version must be a valid integer\")\n    \n    # Return the next integer version\n    return str(version_int + 1)"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.877564087510109, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Validate that each version has an inventory\n    missing_inventory = []\n    content_digests = {}\n    \n    for version_dir in version_dirs:\n        # Check if version directory name is valid (should be v1, v2, etc.)\n        if not version_dir.startswith('v'):\n            self.error(\"E009\")\n            continue\n            \n        version_num = int(version_dir[1:])\n        if version_num < 1:\n            self.error(\"E009\")\n            continue\n            \n        # Validate that each version has an inventory\n        # This would typically involve checking for the presence of an inventory file\n        # For now, we assume the inventory exists and just record content digests\n        # In a real implementation, we would check for the existence of inventory files\n        # and extract content digests from them\n        \n        # For this function, we'll just record the content digests\n        # In a real implementation, we would parse the inventory and extract digests\n        # and compare them to the root inventory\n        \n        # Placeholder: record content digests (in a real implementation, this would be populated)\n        content_digests[version_dir] = {}\n    \n    # Check for missing inventories - in a real implementation, this would be based on actual file checks\n    # For now, we just return the recorded digests\n    return content_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Check that each version directory has an inventory\n    missing_inventories = []\n    content_digest_diffs = {}\n    \n    for version_dir in version_dirs:\n        # Validate version directory name format\n        if not version_dir.startswith('v'):\n            self.error(\"E009\")\n            continue\n            \n        version_num = int(version_dir[1:])\n        if version_num < 1:\n            self.error(\"E009\")\n            continue\n            \n        # Check if inventory exists for this version\n        # In a real implementation, this would check for the presence of an inventory file\n        # For now, we assume the inventory is present and validate content digests\n        # If we had access to the actual inventory data, we would compare digests\n        \n        # For the purpose of this function, we'll just record any differences\n        # from the root inventory (this would be implemented with actual inventory data)\n        # Placeholder: In a real implementation, we would parse the inventory and compare digests\n        # For now, we just note that we're tracking differences\n        content_digest_diffs[version_dir] = {}\n    \n    # If any version is missing an inventory, report error\n    # This would be implemented with actual inventory checks\n    # For now, we just return the recorded differences\n    return content_digest_diffs", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Validate that each version directory has an inventory\n    missing_inventories = []\n    content_digest_diffs = {}\n    \n    for version_dir in version_dirs:\n        # Check if version directory has an inventory\n        # This is a simplified check; actual implementation would depend on\n        # how inventory is accessed and validated\n        if not self.has_inventory(version_dir):\n            missing_inventories.append(version_dir)\n            self.error(f\"E012: Missing inventory for version {version_dir}\")\n    \n    # If any inventories are missing, return early\n    if missing_inventories:\n        return\n    \n    # Collect content digest differences from each version compared to root\n    root_inventory = self.get_root_inventory()\n    if not root_inventory:\n        self.error(\"E013: Root inventory not found\")\n        return\n    \n    for version_dir in version_dirs:\n        version_inventory = self.get_inventory(version_dir)\n        if not version_inventory:\n            continue\n            \n        # Compare content digests between root and current version\n        for content_path, digest in version_inventory.get('content_digests', {}).items():\n            root_digest = root_inventory.get('content_digests', {}).get(content_path)\n            if root_digest and root_digest != digest:\n                content_digest_diffs[content_path] = {\n                    'root': root_digest,\n                    'version': digest,\n                    'version_dir': version_dir\n                }\n    \n    # Return the record of content digest differences\n    return content_digest_diffs", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Validate that each version directory has an inventory\n    missing_inventories = []\n    content_digest_diffs = {}\n    \n    for version_dir in version_dirs:\n        # Check if version directory name is valid (should be v1, v2, etc.)\n        if not version_dir.startswith('v'):\n            self.error(\"E009\")\n            continue\n            \n        version_num = int(version_dir[1:])\n        if version_num < 1:\n            self.error(\"E009\")\n            continue\n            \n        # Validate that each version has an inventory\n        # This is a placeholder - actual inventory check would depend on the inventory structure\n        # For now, we just record any differences in content digests\n        # In a real implementation, we would parse the inventory and compare digests\n        \n        # Example: if we had access to the inventory data, we would compare digests\n        # For now, we just record that we're checking for differences\n        content_digest_diffs[version_dir] = {}\n    \n    # If any version is missing an inventory, report error\n    # This would be more specific in a full implementation\n    if len(content_digest_diffs) != len(version_dirs):\n        self.error(\"E010\")\n    \n    # Return the record of content digest differences\n    return content_digest_diffs", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n\n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume version_dirs is in sequence (1, 2, 3...)\n    # Check that each version directory has an inventory\n    missing_inventories = []\n    content_digests = {}\n    \n    for version_dir in version_dirs:\n        # Version directories should be in sequence, so we expect v1, v2, v3, etc.\n        if not version_dir.startswith('v'):\n            self.error(\"E009\")\n            continue\n            \n        version_num = int(version_dir[1:])\n        if version_num < 1:\n            self.error(\"E009\")\n            continue\n            \n        # Check if inventory exists for this version\n        # In a real implementation, this would check for the presence of an inventory file\n        # For now, we assume the inventory is present and validate content digests\n        inventory_path = f\"{version_dir}/inventory.json\"  # Example path\n        # Placeholder for actual inventory file check\n        \n        # For content digests, compare with root inventory\n        # This would typically involve reading the inventory files\n        # and comparing digests\n        # For now, we just record differences\n        content_digests[version_dir] = {}\n        \n        # Example: if we had a root inventory with known digests\n        # and found differences, we would record them\n        # This is a simplified version\n        \n    # Record any differences from root inventory\n    # This would be expanded with actual digest comparison logic\n    self.content_digests = content_digests"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 927, "num_retrieved": 3, "inference_time": 0.877564087510109, "generate_results": ["def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists and is accessible\n        # Assuming we have a filesystem interface to read files\n        fs = get_filesystem(path)  # This would be defined in context\n        entries = fs.scandir(path)\n        \n        # Look for \"0=*\" Namaste files\n        namaste_files = []\n        for entry in entries:\n            if entry.is_file and entry.name.startswith('0='):\n                namaste_files.append(entry.name)\n        \n        if not namaste_files:\n            return \"no_namaste_files\"\n        \n        # Check the first namaste file to determine type\n        first_namaste = namaste_files[0]\n        if first_namaste == '0=ocfl_object_' + self.spec_version:\n            return 'root'\n        elif first_namaste.startswith('0=ocfl_object_'):\n            return 'object'\n        elif first_namaste.startswith('0='):\n            return 'file'\n        else:\n            return 'invalid_namaste_format'\n            \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists and is accessible\n        # Assuming we have a filesystem interface to read files\n        fs = get_filesystem(path)  # This would be defined in context\n        entries = fs.scandir(path)\n        \n        # Look for \"0=*\" Namaste files\n        namaste_files = []\n        for entry in entries:\n            if entry.is_file and entry.name.startswith('0='):\n                namaste_files.append(entry.name)\n        \n        if not namaste_files:\n            return \"no_namaste_files\"\n        \n        # Check the first namaste file to determine type\n        first_namaste = namaste_files[0]\n        if first_namaste == '0=ocfl_object_' + self.spec_version:\n            return 'root'\n        elif first_namaste.startswith('0=ocfl_object_'):\n            return 'object'\n        elif first_namaste.startswith('0='):\n            return 'file'\n        else:\n            return 'invalid_namaste_format'\n            \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists and is accessible\n        if not path:\n            return \"error: empty path\"\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, pyfs=None)  # Note: This assumes find_namastes can be called with path context\n        \n        # Since we're only looking at \"0=*\" files, we need to check if any exist in the given path\n        # We'll simulate the behavior based on the context\n        # In a real implementation, we'd use the filesystem to scan the path\n        \n        # For now, we assume find_namastes is called with the path context\n        # This is a simplified version based on the context provided\n        \n        # If no 0=* files found, it's not a valid OCFL structure\n        if not namastes:\n            return \"error: no 0=* namaste file found\"\n        \n        # Check the first 0=* file to determine type\n        first_namaste = namastes[0]\n        namaste_name = first_namaste.name\n        \n        # Check if it's a root (0=ocfl_object_xxx)\n        if namaste_name.startswith('0=ocfl_object'):\n            return 'root'\n        \n        # Check if it's an object (0=ocfl_object_xxx)\n        if namaste_name.startswith('0=ocfl_object'):\n            return 'object'\n        \n        # Check if it's a file (inventory.json, etc.)\n        if namaste_name in ['inventory.json', 'inventory.json.' + 'sha256']:\n            return 'file'\n        \n        # Otherwise, it's an unknown type\n        return 'unknown: ' + namaste_name\n        \n    except Exception as e:\n        return \"error: \" + str(e)", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists and is accessible\n        # Assuming we have a filesystem interface to read files\n        fs = get_filesystem(path)  # This would be defined in context\n        entries = fs.scandir(path)\n        \n        # Look for \"0=*\" Namaste files\n        namaste_files = []\n        for entry in entries:\n            if entry.name.startswith('0=') and entry.is_file:\n                namaste_files.append(entry.name)\n        \n        if not namaste_files:\n            return \"no_namaste_files\"\n        \n        # Check the first namaste file to determine type\n        first_namaste = namaste_files[0]\n        \n        # If it's 0=ocfl_object_<version>, it's an object\n        if first_namaste.startswith('0=ocfl_object_'):\n            return 'object'\n        \n        # If it's 0=ocfl_storage_root or similar, it's a root\n        if first_namaste.startswith('0=ocfl_storage_root'):\n            return 'root'\n        \n        # Otherwise, it might be an inventory file\n        if first_namaste.endswith('inventory.json') or first_namaste.endswith('inventory.json.' + 'sha256'):\n            return 'file'\n        \n        # For any other case, return an error description\n        return \"unknown_namaste_file: \" + first_namaste\n        \n    except Exception as e:\n        return \"error_reading_path: \" + str(e)", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists and is accessible\n        # Assuming we have a filesystem interface to read files\n        fs = get_filesystem(path)  # This would be defined in context\n        entries = fs.scandir('')\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for entry in entries:\n            if entry.is_file and entry.name.startswith('0='):\n                namaste_files.append(entry.name)\n        \n        if not namaste_files:\n            return \"no_namaste_files\"\n        \n        # Check the first namaste file to determine type\n        first_namaste = namaste_files[0]\n        if first_namaste == '0=ocfl_object_' + self.spec_version:\n            return 'root'\n        elif first_namaste.startswith('0=ocfl_object_'):\n            return 'object'\n        elif first_namaste.startswith('0='):\n            return 'file'\n        else:\n            return 'invalid_namaste_format'\n            \n    except Exception as e:\n        return f\"error: {str(e)}\""]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1470, "num_retrieved": 20, "inference_time": 0.877564087510109, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if not already present\n    if 'ids' not in params:\n        params['ids'] = bug_ids\n    elif isinstance(params['ids'], list):\n        params['ids'].extend(bug_ids)\n    else:\n        params['ids'] = [params['ids']] + bug_ids\n    \n    # Ensure the params are properly formatted for Bugzilla queries\n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if not already present\n    if 'bug_ids' not in params:\n        params['bug_ids'] = bug_ids\n    \n    # Ensure bug_ids is a list\n    if not isinstance(bug_ids, list):\n        params['bug_ids'] = [bug_ids]\n    \n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if not already present\n    if 'ids' not in params:\n        params['ids'] = bug_ids\n    elif isinstance(params['ids'], list):\n        params['ids'].extend(bug_ids)\n    else:\n        params['ids'] = [params['ids']] + bug_ids\n    \n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if not already present\n    if 'ids' not in params:\n        params['ids'] = bug_ids\n    elif isinstance(params['ids'], list):\n        params['ids'].extend(bug_ids)\n    else:\n        params['ids'] = [params['ids']] + bug_ids\n    \n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if not already present\n    if 'ids' not in params:\n        params['ids'] = bug_ids\n    elif isinstance(params['ids'], list):\n        params['ids'].extend(bug_ids)\n    else:\n        params['ids'] = [params['ids']] + bug_ids\n    \n    # Ensure the params are properly formatted for Bugzilla queries\n    return params"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 887, "num_retrieved": 1, "inference_time": 0.877564087510109, "generate_results": ["def deep_merge_nodes(nodes):\n    from collections import defaultdict\n    \n    # Group nodes by key (ScalarNode value)\n    key_to_nodes = defaultdict(list)\n    for key_node, value_node in nodes:\n        key_value = key_node.value\n        key_to_nodes[key_value].append((key_node, value_node))\n    \n    result = []\n    for key_value, value_nodes in key_to_nodes.items():\n        # Get the first value node as base\n        base_value_node = value_nodes[0][1]\n        \n        # If the value node is not a MappingNode, just use it (last value wins)\n        if not hasattr(base_value_node, 'value') or base_value_node.tag != 'tag:yaml.org,2002:map':\n            # For non-MappingNodes, just use the last value\n            result.append((value_nodes[-1][0], value_nodes[-1][1]))\n            continue\n        \n        # Deep merge the mapping nodes\n        merged_mapping = base_value_node.copy()\n        \n        # Process each value node in the list\n        for _, value_node in value_nodes[1:]:\n            if hasattr(value_node, 'value') and value_node.tag == 'tag:yaml.org,2002:map':\n                # Merge the mapping nodes\n                for sub_key_node, sub_value_node in value_node.value:\n                    # If sub_key already exists in merged_mapping, we need to merge\n                    # Otherwise, just add it\n                    found = False\n                    for i, (sub_key, sub_value) in enumerate(merged_mapping.value):\n                        if sub_key.value == sub_key_node.value:\n                            # Replace with the new value (last wins)\n                            merged_mapping.value[i] = (sub_key_node, sub_value_node)\n                            found = True\n                            break\n                    if not found:\n                        # Add new key-value pair\n                        merged_mapping.value.append((sub_key_node, sub_value_node))\n            else:\n                # If value is not a mapping, just use the last value\n                # This will be handled by the fact that we're using the last value in the list\n                pass\n        \n        # Append the merged result\n        result.append((value_nodes[0][0], merged_mapping))\n    \n    return result", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    from collections import defaultdict\n    \n    # Group nodes by key (ScalarNode value)\n    key_to_nodes = defaultdict(list)\n    for key_node, value_node in nodes:\n        key_value = key_node.value\n        key_to_nodes[key_value].append((key_node, value_node))\n    \n    result = []\n    for key_value, value_nodes in key_to_nodes.items():\n        # Get the first value node as base\n        base_value_node = value_nodes[0][1]\n        \n        # If the value node is not a MappingNode, just use it (last value wins)\n        if not hasattr(base_value_node, 'value') or base_value_node.tag != 'tag:yaml.org,2002:map':\n            # For non-MappingNodes, just use the last value\n            result.append((value_nodes[-1][0], value_nodes[-1][1]))\n            continue\n        \n        # For MappingNodes, deep merge the values\n        merged_mapping = base_value_node.value.copy()\n        \n        for _, value_node in value_nodes[1:]:\n            if hasattr(value_node, 'value') and value_node.tag == 'tag:yaml.org,2002:map':\n                # Deep merge the mapping\n                for child_key_node, child_value_node in value_node.value:\n                    # Check if child key already exists\n                    child_key_value = child_key_node.value\n                    if child_key_value in merged_mapping:\n                        # If it exists, replace with the latest value\n                        merged_mapping[child_key_value] = child_value_node\n                    else:\n                        merged_mapping[child_key_value] = child_value_node\n            else:\n                # If it's not a mapping, just use the last value\n                merged_mapping = value_node.value\n        \n        # Create a new MappingNode with the merged content\n        merged_value_node = base_value_node.copy()\n        merged_value_node.value = merged_mapping\n        \n        result.append((value_nodes[0][0], merged_value_node))\n    \n    return result", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1050, "num_retrieved": 7, "inference_time": 0.877564087510109, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--repository', help='Path to the repository')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--exclude', nargs='*', help='List of paths to exclude')\n    parser.add_argument('--include', nargs='*', help='List of paths to include')\n    \n    # Parse the arguments\n    parsed_args = parser.parse_args(arguments)\n    \n    return parsed_args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Perform a dry run without making changes')\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--repository', help='Path to the Borg repository')\n    parser.add_argument('--archive', help='Name of the archive')\n    parser.add_argument('--exclude', nargs='+', help='List of paths to exclude')\n    parser.add_argument('--include', nargs='+', help='List of paths to include')\n    parser.add_argument('--compression', help='Compression algorithm to use')\n    parser.add_argument('--stats', action='store_true', help='Show statistics')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Perform a dry run without making changes')\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--repository', help='Path to the Borg repository')\n    parser.add_argument('--archive', help='Name of the archive')\n    parser.add_argument('--exclude', nargs='+', help='List of paths to exclude')\n    parser.add_argument('--include', nargs='+', help='List of paths to include')\n    parser.add_argument('--compression', help='Compression algorithm to use')\n    parser.add_argument('--stats', action='store_true', help='Show statistics')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--config', default='borgmatic.yaml', help='Path to configuration file')\n    parser.add_argument('--repository', help='Path to the Borg repository')\n    parser.add_argument('--archive', help='Name of the archive')\n    parser.add_argument('--exclude', action='append', help='Files or patterns to exclude')\n    parser.add_argument('--include', action='append', help='Files or patterns to include')\n    parser.add_argument('--compression', default='zstd', help='Compression algorithm to use')\n    parser.add_argument('--stats', action='store_true', help='Show statistics')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Perform a dry run without making changes')\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--repository', help='Path to the Borg repository')\n    parser.add_argument('--archive', help='Name of the archive')\n    parser.add_argument('--exclude', action='append', help='Exclude files matching the given pattern')\n    parser.add_argument('--include', action='append', help='Include files matching the given pattern')\n    parser.add_argument('--compression', help='Compression algorithm to use')\n    parser.add_argument('--stats', action='store_true', help='Show statistics')\n    \n    return parser.parse_args(arguments)"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1027, "num_retrieved": 6, "inference_time": 0.877564087510109, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 964, "num_retrieved": 4, "inference_time": 0.877564087510109, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--debug', action='store_true', help='enable debug mode')\n    global_parser.add_argument('--info', action='store_true', help='enable info mode')\n    global_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    # Add subparsers (example subcommands)\n    backup_parser = subparsers.add_parser('backup', help='perform a backup')\n    backup_parser.add_argument('--archive', help='archive name')\n    backup_parser.add_argument('--repository', help='repository path')\n    \n    restore_parser = subparsers.add_parser('restore', help='restore data')\n    restore_parser.add_argument('--archive', help='archive name')\n    restore_parser.add_argument('--path', help='path to restore')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Build result dict\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--debug', action='store_true', help='enable debug mode')\n    global_parser.add_argument('--info', action='store_true', help='show info')\n    global_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    # Add subparser examples (simplified)\n    backup_parser = subparsers.add_parser('backup', help='perform a backup')\n    backup_parser.add_argument('--archive', help='archive name')\n    backup_parser.add_argument('--repo', help='repository path')\n    \n    list_parser = subparsers.add_parser('list', help='list archives or paths')\n    list_parser.add_argument('--remote-path', help='remote path')\n    list_parser.add_argument('--lock-wait', type=int, help='lock wait time')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create result dict\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments if present\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='subcommand help')\n    \n    # Add common arguments\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    parser.add_argument('--remote-path', help='Remote path')\n    \n    # Add subcommands (example)\n    backup_parser = subparsers.add_parser('backup', help='Backup command')\n    backup_parser.add_argument('archive', help='Archive name or pattern')\n    \n    restore_parser = subparsers.add_parser('restore', help='Restore command')\n    restore_parser.add_argument('archive', help='Archive name or pattern')\n    restore_parser.add_argument('path', help='Path to restore')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create result dict\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--debug', action='store_true', help='enable debug mode')\n    global_parser.add_argument('--info', action='store_true', help='show info')\n    global_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    # Add subparsers (example subcommands)\n    backup_parser = subparsers.add_parser('backup', help='perform a backup')\n    backup_parser.add_argument('--archive', help='archive name')\n    backup_parser.add_argument('--repo', help='repository path')\n    \n    list_parser = subparsers.add_parser('list', help='list archives or paths')\n    list_parser.add_argument('--remote-path', help='remote path')\n    list_parser.add_argument('--lock-wait', type=int, help='lock wait time')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to Namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments if present\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    global_parser.add_argument('--info', action='store_true', help='Show info messages')\n    global_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add subparser for list command\n    list_parser = subparsers.add_parser('list', help='List archives or paths')\n    list_parser.add_argument('--remote-path', help='Remote path to use')\n    list_parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    \n    # Add subparser for archive command\n    archive_parser = subparsers.add_parser('archive', help='Archive data')\n    archive_parser.add_argument('--remote-path', help='Remote path to use')\n    archive_parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to Namespace\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 941, "num_retrieved": 3, "inference_time": 0.78706955909729, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            parsed_namespace = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = parsed_namespace\n            # Remove parsed arguments from remaining list\n            remaining_args = remaining_args[len(list(parser._get_args(remaining_args))):]\n        except SystemExit:\n            # If parsing fails, skip this subparser\n            pass\n    \n    return (parsed_args, remaining_args)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            parsed_namespace = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = parsed_namespace\n            # Remove parsed arguments from remaining_args\n            remaining_args = remaining_args[len(list(parser._get_args(remaining_args))):]\n        except SystemExit:\n            # If parsing fails, skip this subparser\n            continue\n    \n    return (parsed_args, remaining_args)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            parsed_namespace = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = parsed_namespace\n            # Remove parsed arguments from remaining_args\n            remaining_args = remaining_args[len(list(parser._get_args())):]\n        except SystemExit:\n            # If parsing fails, skip this subparser\n            continue\n    \n    return (parsed_args, remaining_args)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            parsed_namespace = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = parsed_namespace\n            # Remove the parsed arguments from remaining_args\n            remaining_args = remaining_args[len(list(parser._get_args(remaining_args))):]\n        except SystemExit:\n            # If parsing fails, skip this subparser\n            continue\n    \n    return (parsed_args, remaining_args)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_args = {}\n    remaining_args = list(unparsed_arguments)\n    \n    for subparser_name, parser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            parsed_namespace = parser.parse_args(remaining_args)\n            parsed_args[subparser_name] = parsed_namespace\n            # Remove parsed arguments from remaining list\n            remaining_args = remaining_args[len(list(parser._get_args(remaining_args))):]\n        except SystemExit:\n            # If parsing fails, skip this subparser\n            continue\n    \n    return (parsed_args, remaining_args)"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 969, "num_retrieved": 4, "inference_time": 0.78706955909729, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - backup and restore with Borg')\n    \n    # Add common global flags\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--log-level', help='Log level')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Perform a dry run')\n    \n    # Create subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add subparser for backup\n    backup_parser = subparsers.add_parser('backup', help='Perform a backup')\n    backup_parser.add_argument('repository', help='Repository path')\n    backup_parser.add_argument('--archive', help='Archive name')\n    backup_parser.add_argument('--exclude', action='append', help='Files or patterns to exclude')\n    \n    # Add subparser for restore\n    restore_parser = subparsers.add_parser('restore', help='Restore from a backup')\n    restore_parser.add_argument('repository', help='Repository path')\n    restore_parser.add_argument('archive', help='Archive name')\n    restore_parser.add_argument('--target', help='Target path')\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - a tool for managing Borg repositories')\n    subparsers = parser.add_subparsers(dest='command', help='sub-commands')\n    \n    # Add common global flags\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--log-level', help='Log level')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--no-verify', action='store_true', help='Skip verification of archives')\n    parser.add_argument('--exclude', help='Exclude files matching pattern')\n    \n    # Add subcommands (example)\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    archive_parser = subparsers.add_parser('archive', help='Archive files')\n    restore_parser = subparsers.add_parser('restore', help='Restore files')\n    \n    return (parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - a tool for managing Borg repositories')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add common flags to top-level parser\n    parser.add_argument('--config', help='Path to configuration file')\n    parser.add_argument('--log-level', help='Log level')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    \n    # Add subparsers for different actions\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    restore_parser = subparsers.add_parser('restore', help='Restore a backup')\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    \n    # Add common arguments to subparsers\n    backup_parser.add_argument('repository', help='Repository path')\n    backup_parser.add_argument('--target', help='Target path')\n    backup_parser.add_argument('--exclude', action='append', help='Exclude files or directories')\n    \n    restore_parser.add_argument('repository', help='Repository path')\n    restore_parser.add_argument('archive', help='Archive name')\n    restore_parser.add_argument('--target', help='Target path')\n    \n    prune_parser.add_argument('repository', help='Repository path')\n    prune_parser.add_argument('--keep-daily', type=int, help='Number of daily backups to keep')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Number of weekly backups to keep')\n    \n    return (parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore with Borg')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add common flags to top-level parser\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--log-level', help='Log level')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--dry-run', action='store_true', help='Perform a dry run')\n    \n    # Add subparsers for different actions\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    restore_parser = subparsers.add_parser('restore', help='Restore from a backup')\n    \n    # Add arguments to backup parser\n    backup_parser.add_argument('repository', help='Repository path')\n    backup_parser.add_argument('archive', help='Archive name')\n    \n    # Add arguments to restore parser\n    restore_parser.add_argument('archive', help='Archive name')\n    restore_parser.add_argument('path', help='Path to restore to')\n    \n    return (parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool with configuration and automation')\n    \n    # Add common global flags\n    parser.add_argument('--config', default='~/.borgmatic/config', help='Path to configuration file')\n    parser.add_argument('--log-level', default='info', help='Log level (debug, info, warning, error)')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--no-verify', action='store_true', help='Skip verification of backups')\n    parser.add_argument('--exclude', action='append', help='Exclude files or directories')\n    parser.add_argument('--include', action='append', help='Include files or directories')\n    \n    # Create subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add subparser for backup\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    backup_parser.add_argument('repository', help='Repository path')\n    backup_parser.add_argument('archive', help='Archive name')\n    backup_parser.add_argument('--exclude', action='append', help='Exclude files or directories')\n    backup_parser.add_argument('--include', action='append', help='Include files or directories')\n    \n    # Add subparser for restore\n    restore_parser = subparsers.add_parser('restore', help='Restore a backup')\n    restore_parser.add_argument('repository', help='Repository path')\n    restore_parser.add_argument('archive', help='Archive name')\n    restore_parser.add_argument('--target', help='Target directory')\n    \n    # Add subparser for list\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('repository', help='Repository path')\n    \n    return parser, subparsers"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.78706955909729, "generate_results": ["def deep_merge_nodes(nodes):\n    from collections import defaultdict\n\n    # Group nodes by their key (ScalarNode value)\n    key_to_nodes = defaultdict(list)\n    for key_node, value_node in nodes:\n        key_value = key_node.value\n        key_to_nodes[key_value].append((key_node, value_node))\n\n    result = []\n    for key_value, value_nodes in key_to_nodes.items():\n        # If there's only one node, just add it\n        if len(value_nodes) == 1:\n            result.append(value_nodes[0])\n            continue\n\n        # Otherwise, merge the values (deep merge the MappingNodes)\n        # Start with the first value node\n        merged_value = value_nodes[0][1]\n        \n        # For each subsequent value node, merge it into the current merged_value\n        for _, value_node in value_nodes[1:]:\n            if isinstance(merged_value, dict) and isinstance(value_node, dict):\n                # Deep merge the dictionaries\n                for k, v in value_node.items():\n                    if k in merged_value:\n                        # If the key already exists, check if it's a mapping or a scalar\n                        if isinstance(merged_value[k], dict) and isinstance(v, dict):\n                            # Deep merge nested mappings\n                            deep_merge_nodes([(k, merged_value[k]), (k, v)])\n                        else:\n                            # Non-mapping value: last one wins\n                            merged_value[k] = v\n                    else:\n                        merged_value[k] = v\n            else:\n                # If the value is not a mapping, just use the last one\n                merged_value = value_node\n\n        # Add the merged result with the key\n        result.append((value_nodes[0][0], merged_value))\n\n    return result", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    from collections import defaultdict\n    \n    # Group nodes by their key (ScalarNode value)\n    key_to_nodes = defaultdict(list)\n    for key_node, value_node in nodes:\n        key_value = key_node.value\n        key_to_nodes[key_value].append((key_node, value_node))\n    \n    result = []\n    for key_value, value_nodes in key_to_nodes.items():\n        # Get the first value node as base\n        base_value_node = value_nodes[0][1]\n        \n        # If the value node is not a MappingNode, just use it (last value wins)\n        if not hasattr(base_value_node, 'value') or base_value_node.tag != 'tag:yaml.org,2002:map':\n            # For non-MappingNodes, just use the last one\n            result.append((value_nodes[-1][0], value_nodes[-1][1]))\n            continue\n        \n        # Deep merge the mapping nodes\n        merged_mapping = base_value_node.value.copy()\n        \n        for _, value_node in value_nodes[1:]:\n            if hasattr(value_node, 'value') and value_node.tag == 'tag:yaml.org,2002:map':\n                # Merge the mapping values\n                for key, val in value_node.value:\n                    # If key already exists, update it (last value wins)\n                    if key.value in [item[0].value for item in merged_mapping]:\n                        # Replace existing entry with new one\n                        for i, (k, v) in enumerate(merged_mapping):\n                            if k.value == key.value:\n                                merged_mapping[i] = (key, val)\n                                break\n                    else:\n                        merged_mapping.append((key, val))\n        \n        # Create a new tuple with the key and merged mapping\n        result.append((value_nodes[0][0], merged_mapping))\n    \n    return result", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support,"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 998, "num_retrieved": 5, "inference_time": 0.78706955909729, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    # Since the function is expected to parse command-line arguments and return an ArgumentParser instance,\n    # we need to parse the provided arguments and return the parser with the parsed values.\n    # However, the standard argparse.ArgumentParser does not directly parse arguments when passed as *arguments.\n    # Instead, we would typically use parser.parse_args(arguments) to parse them.\n    # But the docstring says to return an ArgumentParser instance, not the parsed results.\n    # Therefore, we return a parser instance with no arguments parsed.\n    return parser", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1073, "num_retrieved": 8, "inference_time": 0.78706955909729, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1000, "num_retrieved": 6, "inference_time": 0.78706955909729, "generate_results": ["def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    import borgmatic.commands.completion.actions\n    import sys\n    from itertools import chain\n    \n    # Get the argument parser from borgmatic\n    parser = borgmatic.commands.arguments.get_parser()\n    \n    # Generate the bash completion script\n    script = []\n    \n    # Start with the header\n    script.append('#!/bin/bash')\n    script.append('''\n__borgmatic_complete() {\n    local cur prev words cword\n    _init_completion || return\n    \n    cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    prev=\"${COMP_WORDS[COMP_CWORD-1]}\"\n    \n    # Get the current command and arguments\n    local command=\"${COMP_WORDS[0]}\"\n    \n    # Check if we're in a valid context\n    if [[ \"$command\" != \"borgmatic\" ]]; then\n        return 0\n    fi\n    \n    # Get the arguments from the current command line\n    local args=()\n    for i in \"${COMP_WORDS[@]}\"; do\n        args+=(\"$i\")\n    done\n    \n    # Get the current argument index\n    local cword_index=${COMP_CWORD}\n    \n    # Get the current argument\n    local current_arg=\"${args[cword_index-1]}\"\n    \n    # Generate completions based on the parser\n    local completions=()\n    \n    # Use the parser to generate completions\n    local parsed_args=$(python3 -c \"\nimport argparse\nimport sys\nfrom borgmatic.commands.arguments import get_parser\n\nparser = get_parser()\nargs = parser.parse_args(['--help'])\nprint(' '.join([str(arg) for arg in args]))\n\" 2>/dev/null)\n    \n    # Add the completions to the script\n    completions+=(\"backup\")\n    completions+=(\"prune\")\n    completions+=(\"check\")\n    completions+=(\"create\")\n    completions+=(\"list\")\n    \n    # Output the completions\n    COMPREPLY=($(compgen -W \\\"${completions[*]}\\\" -- ${cur}))\n}\n    \n    # Register the completion function\n    complete -F __borgmatic_complete borgmatic\n    ''')\n    \n    return '\\n'.join(script)", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    import borgmatic.commands.completion.actions\n    import sys\n    from itertools import chain\n    \n    # Get the argument parser from borgmatic\n    parser = borgmatic.commands.arguments.get_parser()\n    \n    # Generate the completion script\n    script = []\n    \n    # Start with the basic completion header\n    script.append('#!/bin/bash')\n    script.append('''\n__borgmatic_complete() {\n    local cur prev words cword\n    _init_completion || return\n    \n    cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    prev=\"${COMP_WORDS[COMP_CWORD-1]}\"\n    \n    # Get the current command and arguments\n    local command=\"${COMP_WORDS[0]}\"\n    \n    # Check if we're in a valid context\n    if [[ \"$command\" != \"borgmatic\" ]]; then\n        return 0\n    fi\n    \n    # Get the arguments from the current command line\n    local args=(\"${COMP_WORDS[@]}\")\n    local arg_count=${#args[@]}\n    \n    # Generate the list of available commands\n    local commands=()\n    for cmd in $(python3 -c \"import borgmatic.commands.arguments; print('\\\\n'.join(borgmatic.commands.arguments.get_parser().get_command_names()))\"); do\n        commands+=(\"$cmd\")\n    done\n    \n    # If no arguments, complete commands\n    if [[ $arg_count -eq 1 ]]; then\n        COMPREPLY=($(compgen -W \"${commands[*]}\" -- \"$cur\"))\n        return 0\n    fi\n    \n    # For each argument, generate completions based on the parser\n    local current_arg=\"${args[1]}\"\n    local current_index=1\n    \n    # Get the parser for the current command\n    local parser=$(python3 -c \"import borgmatic.commands.arguments; print(borgmatic.commands.arguments.get_parser().get_command_names())\")\n    \n    # Generate completions for the current argument\n    local completions=()\n    for arg in $(python3 -c \"import borgmatic.commands.arguments; import argparse; parser = borgmatic.commands.arguments.get_parser(); args = ['borgmatic']; args += ['--help']; print('\\\\n'.join([arg for arg in parser._actions if hasattr(arg, '", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    import borgmatic.commands.completion.actions\n    import sys\n    from itertools import chain\n    \n    # Get the argument parser from borgmatic\n    parser = borgmatic.commands.arguments.get_parser()\n    \n    # Generate the completion script\n    script = []\n    \n    # Add header\n    script.append('#!/bin/bash')\n    script.append('''\n__borgmatic_completion() {\n    local cur prev words cword\n    _init_completion || return\n    \n    # Get the current command line\n    cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    \n    # Get the command name\n    local command=\"${COMP_WORDS[0]}\"\n    \n    # If the command is not borgmatic, return\n    if [[ \"$command\" != \"borgmatic\" ]]; then\n        return\n    fi\n    \n    # Get the arguments from the parser\n    local args=()\n    for i in \"${COMP_WORDS[@]}\"; do\n        args+=(\"$i\")\n    done\n    \n    # Get the current argument index\n    local cword=${#args[@]}\n    \n    # Get the current argument\n    local cur=\"${args[cword]}\"\n    \n    # Get the parser for the current command\n    local parser\n    parser=$(get_parser_for_command \"$command\")\n    \n    # If no parser found, return\n    if [[ -z \"$parser\" ]]; then\n        return\n    fi\n    \n    # Generate the completion list\n    local completions=()\n    local completion_args=()\n    \n    # Get the arguments from the parser\n    for arg in $(get_arguments_from_parser \"$parser\"); do\n        # Get the argument name and type\n        local arg_name=$(get_argument_name \"$arg\")\n        local arg_type=$(get_argument_type \"$arg\")\n        \n        # If the argument is a flag, add it to the completion list\n        if [[ \"$arg_type\" == \"flag\" ]]; then\n            completions+=(\"$arg_name\")\n        # If the argument is a value, add it to the completion list\n        elif [[ \"$arg_type\" == \"value\" ]]; then\n            # Add the value completion based on the argument type\n            if [[ \"$arg_name\" == \"repository\" ]]; then\n                completions+=(\"path/to/repository", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    import borgmatic.commands.completion.actions\n    import sys\n    from itertools import chain\n    \n    # Get the argument parser from borgmatic\n    parser = borgmatic.commands.arguments.get_parser()\n    \n    # Generate the bash completion script\n    script = []\n    \n    # Start the script with the header\n    script.append('#!/bin/bash')\n    script.append('''\n__borgmatic_complete() {\n    local cur prev words cword\n    _init_completion || return\n    \n    cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    prev=\"${COMP_WORDS[COMP_CWORD-1]}\"\n    \n    # Get the current command and arguments\n    local command=\"${COMP_WORDS[0]}\"\n    \n    # Check if we're in a valid context\n    if [[ \"$command\" != \"borgmatic\" ]]; then\n        return 0\n    fi\n    \n    # Get the arguments from the current command line\n    local args=(\"${COMP_WORDS[@]}\")\n    local arg_count=${#args[@]}\n    \n    # Get the current argument index\n    local current_index=$((arg_count - 1))\n    \n    # Get the current argument value\n    local current_arg=\"${args[current_index]}\"\n    \n    # Get the command-line arguments from the parser\n    local all_args=()\n    for arg in \"${!parser}\"; do\n        all_args+=(\"$arg\")\n    done\n    \n    # Generate completion suggestions based on the current argument\n    local completions=()\n    \n    # For each argument in the parser, generate completions\n    for arg in \"${all_args[@]}\"; do\n        # Skip internal arguments\n        if [[ \"$arg\" == \"_\"* ]]; then\n            continue\n        fi\n        \n        # Get the argument type\n        local arg_type=$(python3 -c \"import argparse; print(argparse.ArgumentParser().get_action('$arg').type)\" 2>/dev/null || echo \"\")\n        \n        # Generate completions based on the argument type\n        if [[ \"$arg\" == \"repository\" ]]; then\n            # Repository path completion\n            completions+=(\"path/to/repository\")\n        elif [[ \"$arg\" == \"archive\" ]]; then\n            # Archive name completion\n            completions+=(\"archive1\" \"archive2\" \"", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    import borgmatic.commands.completion.actions\n    import itertools\n    from borgmatic.commands.arguments import get_parser\n    \n    parser = get_parser()\n    args = parser.parse_args([])\n    \n    # Generate the completion script content\n    script = '''\n#!/bin/bash\n# Generated bash completion script for borgmatic\n\n# Function to get the current command\n_borgmatic() {\n    local cur prev words cword\n    _init_completion || return\n    \n    # Get the current command and arguments\n    cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    prev=\"${COMP_WORDS[COMP_CWORD-1]}\"\n    \n    # Determine the current subcommand\n    if [[ \"${cur}\" == -* ]]; then\n        # Handle flags\n        COMPREPLY=()\n        for arg in $(get_borgmatic_flags \"$cur\"); do\n            COMPREPLY+=(\"$arg\")\n        done\n    else\n        # Handle non-flag arguments\n        COMPREPLY=()\n        for arg in $(get_borgmatic_arguments \"$cur\"); do\n            COMPREPLY+=(\"$arg\")\n        done\n    fi\n}\n\n# Function to get borgmatic flags\nget_borgmatic_flags() {\n    local flag=\"$1\"\n    local flags=()\n    \n    # Get all arguments from the parser\n    local all_args=$(python3 -c \"\nimport argparse\nfrom borgmatic.commands.arguments import get_parser\nparser = get_parser()\nargs = parser.parse_args([])\nflags = []\nfor name, action in parser._actions:\n    if action.option_strings and name.startswith('-'):\n        flags.append(name)\nprint(' '.join(flags))\n\" | tr ' ' '\\n')\n    \n    # Filter flags based on the current argument\n    for f in $flags; do\n        if [[ \"$f\" == \"$flag\" ]]; then\n            echo \"$f\"\n        fi\n    done\n}\n\n# Function to get borgmatic arguments\nget_borgmatic_arguments() {\n    local arg=\"$1\"\n    local args=()\n    \n    # Get all arguments from the parser\n    local all_args=$(python3 -c \"\nimport argparse\nfrom borgmatic.commands.arguments import get_parser\nparser = get_parser()\nargs = parser.parse_args([])\narguments = []\nfor name"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 911, "num_retrieved": 2, "inference_time": 0.78706955909729, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='subcommand help')\n    \n    # Add common global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--lock-wait', type=int, help='lock wait time in seconds')\n    \n    # Add example subparsers\n    backup_parser = subparsers.add_parser('backup', help='perform a backup')\n    backup_parser.add_argument('--prefix', help='prefix for archive names')\n    backup_parser.add_argument('--match-archives', help='match archives with a pattern')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create result dict\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments if available\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='subcommand help')\n    \n    # Add common global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output log in JSON format')\n    global_parser.add_argument('--lock-wait', type=int, help='lock wait time in seconds')\n    \n    # Add subparsers for different commands\n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--prefix', help='filter archives by prefix')\n    list_parser.add_argument('--match-archives', help='match archives with a pattern')\n    list_parser.add_argument('--glob-archives', help='glob archives with a pattern')\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to argparse.Namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments if present\n    if hasattr(args, 'subparser_name') and args.subparser_name:\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='subcommand help')\n    \n    # Add common global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--log-json', action='store_true', help='output log in JSON format')\n    global_parser.add_argument('--lock-wait', type=int, help='lock wait time in seconds')\n    \n    # Add subparsers for different commands\n    # Example subparsers (can be extended as needed)\n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--prefix', help='filter archives by prefix')\n    list_parser.add_argument('--match-archives', help='match archives using shell pattern')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create result dict\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments if present\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='log in JSON format')\n    global_parser.add_argument('--lock-wait', type=int, help='lock wait time in seconds')\n    \n    # Add example subparsers\n    backup_parser = subparsers.add_parser('backup', help='backup command')\n    backup_parser.add_argument('--prefix', help='prefix for archives')\n    backup_parser.add_argument('--match-archives', help='match archives with pattern')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to Namespace\n    result = {}\n    \n    # Add global arguments\n    result['global'] = args\n    \n    # Add subparser arguments if they exist\n    if hasattr(args, 'subparser_name') and args.subparser_name:\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--lock-wait', type=int, help='lock wait time in seconds')\n    \n    # Add example subparsers (these would be defined in actual implementation)\n    backup_parser = subparsers.add_parser('backup', help='perform a backup')\n    backup_parser.add_argument('--prefix', help='prefix for archive names')\n    backup_parser.add_argument('--match-archives', help='match archives using a pattern')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to Namespace\n    result = {}\n    \n    # Handle global arguments\n    result['global'] = args\n    \n    # Handle subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 944, "num_retrieved": 3, "inference_time": 0.78706955909729, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order of their names\n    for subparser_name in subparsers.keys():\n        subparser = subparsers[subparser_name]\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = []\n            # Reconstruct remaining_arguments from the original list\n            # We need to find the indices of the parsed arguments and remove them\n            # Instead, we'll use a different approach: parse the arguments in a way that respects the order\n            break\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # Since the above approach is flawed, we need to properly parse arguments in sequence\n    # We'll use a more robust method: try each subparser in order and parse as many arguments as possible\n    \n    parsed_arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    for subparser_name in subparsers.keys():\n        subparser = subparsers[subparser_name]\n        # Create a namespace to hold the parsed arguments\n        args_namespace = subparser.parse_args(remaining_arguments)\n        \n        # If we successfully parsed arguments, store them\n        if args_namespace:\n            parsed_arguments[subparser_name] = args_namespace\n            # Remove the parsed arguments from remaining_arguments\n            # We need to find the indices of the parsed arguments and remove them\n            # Instead, we'll use a different approach: parse the arguments in a way that respects the order\n            break\n    \n    # The correct implementation should try each subparser in order and parse arguments until none remain\n    parsed_arguments = {}\n    remaining_arguments = list(unparsed", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order of their keys\n    for subparser_name in subparsers.keys():\n        subparser = subparsers[subparser_name]\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Use the subparser to parse the arguments\n            args = parser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = args\n            # Remove the parsed arguments from the remaining list\n            remaining_arguments = []\n            # Break out of the loop since we've processed all arguments\n            break\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # If no subparser was able to parse the arguments, return empty parsed dict and original arguments\n    if not parsed_arguments:\n        parsed_arguments = {}\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order of their keys\n    for subparser_name in subparsers.keys():\n        subparser = subparsers[subparser_name]\n        # Create a namespace for the current subparser\n        parser = subparser\n        # Try to parse the arguments using the subparser\n        try:\n            # Use the subparser to parse arguments, but only if there are arguments left\n            if remaining_arguments:\n                # Parse arguments using the subparser\n                args = parser.parse_args(remaining_arguments)\n                parsed_arguments[subparser_name] = args\n                # Remove the parsed arguments from the remaining list\n                remaining_arguments = []\n                # Break out of the loop since we've parsed all arguments\n                break\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # If no subparser was able to parse arguments, return empty dict and original arguments\n    if not parsed_arguments:\n        parsed_arguments = {}\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order of their names\n    for subparser_name in subparsers.keys():\n        subparser = subparsers[subparser_name]\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse arguments using the subparser\n        try:\n            # Use the subparser to parse the remaining arguments\n            args = parser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = []\n            # Rebuild remaining_arguments from the original list\n            # This is a simplified approach; in practice, we'd need to track which arguments were consumed\n            break\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # In a real implementation, we would need to properly handle argument consumption\n    # This is a simplified version that assumes a single subparser can consume all arguments\n    # A more robust implementation would iterate through subparsers and consume arguments\n    # until none remain\n    \n    # For now, return a basic structure\n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order of their names\n    for subparser_name in subparsers.keys():\n        subparser = subparsers[subparser_name]\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse arguments using the current subparser\n        try:\n            # Use the subparser to parse arguments, but only if there are arguments left\n            if remaining_arguments:\n                args = parser.parse_args(remaining_arguments)\n                parsed_arguments[subparser_name] = args\n                # Remove the parsed arguments from the remaining list\n                remaining_arguments = []\n                # Reconstruct the remaining arguments list\n                for arg in remaining_arguments:\n                    # This is a placeholder - actual parsing logic would be more complex\n                    pass\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # Reconstruct the remaining arguments list properly\n    # We need to try each subparser in turn and parse as many arguments as possible\n    parsed_arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    for subparser_name in subparsers.keys():\n        subparser = subparsers[subparser_name]\n        # Create a parser instance for the current subparser\n        parser = subparser\n        \n        # Try to parse arguments using the current subparser\n        try:\n            # Parse the arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            # Store the parsed arguments\n            parsed_arguments[subparser_name] = args\n            # Update the remaining arguments list\n            remaining_arguments = []\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    # The above logic is incomplete. A correct implementation should:\n    # 1. Iterate through each subparser\n    # 2. Try to parse the remaining arguments with that subparser\n    # 3. If"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1082, "num_retrieved": 8, "inference_time": 0.78706955909729, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore with Borg')\n    \n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add subparsers for different commands (example)\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    restore_parser = subparsers.add_parser('restore', help='Restore data from a backup')\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    \n    return (parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore with Borg')\n    \n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add subparsers for different commands (example)\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    restore_parser = subparsers.add_parser('restore', help='Restore data')\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    \n    return (parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore with Borg')\n    \n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add subparsers for different commands (example)\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    restore_parser = subparsers.add_parser('restore', help='Restore data')\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    \n    return (parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - Backup and restore with Borg')\n    \n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add subparsers for different commands (example)\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    restore_parser = subparsers.add_parser('restore', help='Restore a backup')\n    \n    return (parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool with configuration and automation')\n    \n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Add subparsers for different commands (example placeholders)\n    backup_parser = subparsers.add_parser('backup', help='Create a backup')\n    prune_parser = subparsers.add_parser('prune', help='Prune old backups')\n    check_parser = subparsers.add_parser('check', help='Check repository integrity')\n    \n    return (parser, subparsers)"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 921, "num_retrieved": 2, "inference_time": 0.78706955909729, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1006, "num_retrieved": 5, "inference_time": 0.78706955909729, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Extract the number part\n    number_part = ''\n    unit_part = ''\n    \n    for char in size:\n        if char.isdigit() or char == '.':\n            number_part += char\n        else:\n            unit_part = char.upper()\n            break\n    \n    if not number_part:\n        return 0\n    \n    number = float(number_part)\n    \n    # Convert based on unit\n    if unit_part == 'K':\n        return int(number * 1000)\n    elif unit_part == 'M':\n        return int(number * 1000000)\n    elif unit_part == 'G':\n        return int(number * 1000000000)\n    elif unit_part == 'T':\n        return int(number * 1000000000000)\n    else:\n        return int(number)", "def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Extract the number part\n    number_part = ''\n    unit_part = ''\n    \n    for char in size:\n        if char.isdigit() or char == '.':\n            number_part += char\n        else:\n            unit_part = char.upper()\n            break\n    \n    if not number_part:\n        return 0\n    \n    number = float(number_part)\n    \n    # Convert based on unit\n    if unit_part == 'K':\n        return int(number * 1000)\n    elif unit_part == 'M':\n        return int(number * 1000000)\n    elif unit_part == 'G':\n        return int(number * 1000000000)\n    elif unit_part == 'T':\n        return int(number * 1000000000000)\n    else:\n        return int(number)", "def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Extract the number part\n    number_part = ''\n    unit_part = ''\n    \n    for char in size:\n        if char.isdigit() or char == '.':\n            number_part += char\n        else:\n            unit_part = char.upper()\n            break\n    \n    if not number_part:\n        return 0\n    \n    number = float(number_part)\n    \n    # Convert based on unit\n    if unit_part == 'K':\n        return int(number * 1000)\n    elif unit_part == 'M':\n        return int(number * 1000000)\n    elif unit_part == 'G':\n        return int(number * 1000000000)\n    elif unit_part == 'T':\n        return int(number * 1000000000000)\n    else:\n        return int(number)", "def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Extract the number part\n    number_part = ''\n    unit_part = ''\n    \n    for char in size:\n        if char.isdigit() or char == '.':\n            number_part += char\n        else:\n            unit_part = char.upper()\n            break\n    \n    if not number_part:\n        return 0\n    \n    number = float(number_part)\n    \n    # Convert based on unit\n    if unit_part == 'K':\n        return int(number * 1000)\n    elif unit_part == 'M':\n        return int(number * 1000000)\n    elif unit_part == 'G':\n        return int(number * 1000000000)\n    elif unit_part == 'T':\n        return int(number * 1000000000000)\n    else:\n        return int(number)", "def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Extract the number part\n    number_part = ''\n    unit_part = ''\n    \n    for char in size:\n        if char.isdigit() or char == '.':\n            number_part += char\n        else:\n            unit_part = char.upper()\n            break\n    \n    if not number_part:\n        return 0\n    \n    number = float(number_part)\n    \n    # Convert based on unit\n    if unit_part == 'K':\n        return int(number * 1000)\n    elif unit_part == 'M':\n        return int(number * 1000000)\n    elif unit_part == 'G':\n        return int(number * 1000000000)\n    elif unit_part == 'T':\n        return int(number * 1000000000000)\n    else:\n        return int(number)"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 967, "num_retrieved": 4, "inference_time": 0.78706955909729, "generate_results": ["def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 971, "num_retrieved": 4, "inference_time": 0.78706955909729, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    for key, value in url_args.items():\n        placeholder = '{{{}}}'.format(key)\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    for key, value in url_args.items():\n        placeholder = '{{{}}}'.format(key)\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    for key, value in url_args.items():\n        placeholder = '{{{}}}'.format(key)\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    for key, value in url_args.items():\n        placeholder = '{{{}}}'.format(key)\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    for key, value in url_args.items():\n        placeholder = '{{{}}}'.format(key)\n        url = url.replace(placeholder, str(value))\n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1300, "num_retrieved": 13, "inference_time": 0.78706955909729, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        return val.lower() in ['none', 'null', 'nil', 'n/a', '']\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        return val.lower() in ['none', 'null', 'nil', 'n/a', '']\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        return val.lower() in ['none', 'null', 'nil', 'n/a', '']\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        return val.lower() in ['none', 'null', 'nil', 'n/a', '']\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        return val.lower() in ['none', 'null', 'nil', 'n/a', '']\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1073, "num_retrieved": 8, "inference_time": 0.78706955909729, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 916, "num_retrieved": 2, "inference_time": 0.78706955909729, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    and check if it's already in the set. If it is, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    import os\n    absolute_path = os.path.abspath(path_name)\n    if absolute_path in processed:\n        if verbose:\n            print(f\"Warning: Recursion detected for {path_name}\")\n        return True\n    processed.add(absolute_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    and check if it's already in the set. If it is, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    import os\n    absolute_path = os.path.abspath(path_name)\n    if absolute_path in processed:\n        if verbose:\n            print(f\"Warning: Recursion detected for {path_name}\")\n        return True\n    processed.add(absolute_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    and check if it's already in the set. If it is, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    import os\n    absolute_path = os.path.abspath(path_name)\n    if absolute_path in processed:\n        if verbose:\n            print(f\"Warning: Recursion detected for {path_name}\")\n        return True\n    processed.add(absolute_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    and check if it's already in the set. If it is, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    import os\n    absolute_path = os.path.abspath(path_name)\n    if absolute_path in processed:\n        if verbose:\n            print(f\"Warning: Recursion detected for {path_name}\")\n        return True\n    processed.add(absolute_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    and check if it's already in the set. If it is, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    import os\n    absolute_path = os.path.abspath(path_name)\n    if absolute_path in processed:\n        if verbose:\n            print(f\"Warning: Recursion detected for {path_name}\")\n        return True\n    processed.add(absolute_path)\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 972, "num_retrieved": 4, "inference_time": 0.7412484437227249, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    n = len(vertices)\n    if n < 3:\n        return []\n    \n    result = []\n    for i in range(n):\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    n = len(vertices)\n    if n < 3:\n        return []\n    \n    result = []\n    for i in range(n):\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    n = len(vertices)\n    if n < 3:\n        return []\n    \n    result = []\n    for i in range(n):\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    n = len(vertices)\n    if n < 3:\n        return []\n    \n    result = []\n    for i in range(n):\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    n = len(vertices)\n    if n < 3:\n        return []\n    \n    result = []\n    for i in range(n):\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 942, "num_retrieved": 3, "inference_time": 0.7412484437227249, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 934, "num_retrieved": 4, "inference_time": 0.7412484437227249, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number\n    return flow_params", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number\n    return flow_params", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number\n    return flow_params", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number\n    return flow_params", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number\n    return flow_params"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1109, "num_retrieved": 10, "inference_time": 0.7412484437227249, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1172, "num_retrieved": 14, "inference_time": 0.7412484437227249, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 942, "num_retrieved": 3, "inference_time": 0.7412484437227249, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 951, "num_retrieved": 3, "inference_time": 0.7412484437227249, "generate_results": ["import requests\nimport tarfile\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Create destination directory if it doesn't exist\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the .tar.gz archive\n    with tarfile.open(fileobj=io.BytesIO(response.content)) as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path", "import requests\nimport tarfile\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Create destination directory if it doesn't exist\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the .tar.gz archive\n    with tarfile.open(fileobj=io.BytesIO(response.content)) as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path", "import requests\nimport tarfile\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Create destination directory if it doesn't exist\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the .tar.gz archive\n    with tarfile.open(fileobj=io.BytesIO(response.content)) as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path", "import requests\nimport tarfile\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Create destination directory if it doesn't exist\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the .tar.gz archive\n    with tarfile.open(fileobj=io.BytesIO(response.content)) as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path", "import requests\nimport tarfile\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Create destination directory if it doesn't exist\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the .tar.gz archive\n    with tarfile.open(fileobj=io.BytesIO(response.content)) as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1162, "num_retrieved": 10, "inference_time": 0.7412484437227249, "generate_results": ["import os\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.environ.get('OS', '') == 'Darwin'", "import os\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.environ.get('OS', '') == 'Darwin'", "import os\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.environ.get('OS', '') == 'Darwin'", "import os\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.environ.get('OSTYPE', '').startswith('darwin')", "import os\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.environ.get('OSTYPE', '').startswith('darwin')"]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 918, "num_retrieved": 2, "inference_time": 0.7412484437227249, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Escape special regex characters in the pattern\n        regex_key = re.escape(regex_key)\n        # Replace the original key with the regex pattern\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Escape special regex characters in the pattern\n        regex_key = re.escape(regex_key)\n        # Replace the original key with the regex pattern\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Escape any special regex characters in the key\n        regex_key = re.escape(regex_key)\n        # Remove the leading and trailing parentheses if they were added by re.sub\n        # Since we used re.sub with a replacement, we need to ensure the pattern is correct\n        # Actually, we want to keep the pattern as is but with proper escaping\n        # The correct approach is to escape the key and then replace * with .* properly\n        # Let's do it step by step\n        # First, replace * with .* and then escape the entire string\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.escape(regex_key)\n        # Remove any extra parentheses that might have been added\n        # Actually, we don't want to remove them, they are part of the regex\n        # The correct pattern is to have .* in place of *\n        # So we need to properly escape the key after substitution\n        # Let's fix this by doing the substitution first and then escaping\n        # But we already did that above\n        # The issue is that re.escape will escape the * and turn it into \\*, which is wrong\n        # So we need to do the substitution before escaping\n        # Let's restart this part\n        # Convert * to .* and then escape the rest\n        # But we must not escape the * because we want it to be .* in the regex\n        # So we do the substitution first, then escape the entire string\n        # But we need to escape only the non-special characters\n        # Actually, we should do the substitution first, then escape the result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Escape special regex characters in the pattern\n        regex_key = re.escape(regex_key)\n        # Replace the original key with the regex version\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Escape special regex characters in the pattern\n        regex_key = re.escape(regex_key)\n        # Replace the original key with the regex pattern\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1257, "num_retrieved": 14, "inference_time": 0.7412484437227249, "generate_results": ["import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    else:\n        return name", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    else:\n        return name", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 949, "num_retrieved": 3, "inference_time": 0.7412484437227249, "generate_results": ["import re\nimport sys\nimport os\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_is_linux() or os_is_mac() else 0\n    \n    # Regex pattern to match quoted strings and escaped characters\n    # This handles spaces, quotes, and escaping similar to shlex\n    if platform == 1:  # POSIX (Linux, macOS)\n        # POSIX shell splitting: handles quotes and escaped spaces\n        pattern = r'([^\"\\\\]|\\\\.)*\"([^\"]*)\"'  # quoted strings\n        pattern += r'|([^\"\\\\]|\\\\.)*'  # unquoted parts\n        parts = []\n        current = \"\"\n        in_quote = False\n        quote_char = None\n        \n        for char in s:\n            if char == '\"' and not in_quote:\n                in_quote = True\n                quote_char = '\"'\n                current += char\n            elif char == '\"' and in_quote:\n                in_quote = False\n                quote_char = None\n                parts.append(current.strip('\"'))\n                current = \"\"\n            elif char == '\\\\' and in_quote:\n                # Handle escaped characters\n                current += char\n            else:\n                current += char\n        \n        if current:\n            parts.append(current.strip())\n        \n        # Split on whitespace, but preserve quoted parts\n        result = []\n        for part in parts:\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            else:\n                result.append(part)\n        \n        # Remove empty strings\n        return [part for part in result if part]\n    \n    else:  # Windows (CMD)\n        # Windows CMD splitting: handles spaces and quotes\n        # Use a simpler approach: split on spaces, but preserve quoted strings\n        # This is a simplified version for Windows CMD\n        result = []\n        current = \"\"\n        in_quote = False\n        \n        for char in s:\n            if char == '\"' and not in_quote:\n                in_quote = True\n                current += char\n            elif char == '\"' and in_quote:\n                in_quote = False\n                current += char\n                result.append(current.strip('\"'))", "import re\nimport os\nimport sys\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_is_linux() or os_is_mac() else 0\n    \n    # Regex pattern to match quoted strings and unquoted tokens\n    # Handles spaces, quotes, and escaped characters\n    pattern = r'([^\"\\'\\\\]|\\\\.)*\"([^\"\\\\]|\\\\.)*\"|\\'([^\\'\\\\]|\\\\.)*\\'|([^\"\\'\\\\]|\\\\.)*'\n    \n    # Split the input string using the pattern\n    tokens = re.split(r'(\\s+)', s)\n    \n    # Filter out empty tokens and clean up\n    result = []\n    i = 0\n    while i < len(tokens):\n        token = tokens[i]\n        if token.strip():\n            if token.startswith('\"') and token.endswith('\"'):\n                # Handle quoted strings\n                result.append(token[1:-1])\n            elif token.startswith(\"'\") and token.endswith(\"'\"):\n                # Handle single quoted strings\n                result.append(token[1:-1])\n            else:\n                # Split unquoted tokens by spaces, but preserve quoted parts\n                # This is a simplified version for basic splitting\n                result.append(token.strip())\n        i += 1\n    \n    # Handle cases where we have multiple spaces or complex quoting\n    # Use a more robust approach for unquoted tokens\n    result = []\n    for token in re.split(r'(\\s+)', s):\n        token = token.strip()\n        if token:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            elif token.startswith(\"'\") and token.endswith(\"'\"):\n                result.append(token[1:-1])\n            else:\n                # For unquoted tokens, split on spaces but preserve sequences\n                # This is a simplified version\n                result.append(token)\n    \n    # Final cleanup: remove empty strings\n    return [t for t in result if t]", "import re\nimport sys\nimport os\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        if os.name == 'nt':\n            platform = 0\n        else:\n            platform = 1\n\n    # Regex pattern to match quoted strings and unquoted tokens\n    # Handles spaces, quotes, and escaped characters\n    if platform == 0:  # Windows/CMD\n        # In Windows CMD, spaces are split by default, and quotes are used to group\n        # We use a pattern that matches quoted strings and unquoted tokens\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        tokens = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):  # quoted string\n                tokens.append(match.group(1))\n            else:  # unquoted token\n                tokens.append(match.group(2))\n        return tokens\n    else:  # POSIX (Linux, macOS, etc.)\n        # POSIX shell splitting: split on whitespace, but preserve quoted strings\n        # Use a pattern that matches quoted strings and unquoted tokens\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        tokens = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):  # quoted string\n                tokens.append(match.group(1))\n            else:  # unquoted token\n                tokens.append(match.group(2))\n        return tokens", "import re\nimport sys\nimport os\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        if os.name == 'nt':\n            platform = 0\n        else:\n            platform = 1\n\n    if platform == 0:  # Windows/CMD\n        # In Windows, split on spaces, but preserve quoted strings\n        # Use regex to match quoted strings and unquoted tokens\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        tokens = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):  # quoted string\n                tokens.append(match.group(1))\n            else:  # unquoted token\n                tokens.append(match.group(2))\n        return tokens\n\n    else:  # POSIX (Linux, macOS, etc.)\n        # POSIX split using shlex-like behavior\n        # Split on whitespace, but preserve quoted strings\n        pattern = r'(?:\"([^\"]*)\")|(\\S+)'\n        tokens = []\n        for match in re.finditer(pattern, s):\n            if match.group(1):  # quoted string\n                tokens.append(match.group(1))\n            else:  # unquoted token\n                tokens.append(match.group(2))\n        return tokens", "import re\nimport sys\nimport os\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_is_linux() or os_is_mac() else 0\n    \n    # Regex pattern to match quoted strings and unquoted tokens\n    # Handles spaces, quotes, and escaped characters\n    pattern = r'((?:[^\"\\\\]|\\\\.)*|\"(?:[^\"\\\\]|\\\\.)*\")'\n    \n    # Split the string using the pattern\n    tokens = re.findall(pattern, s)\n    \n    # Clean up tokens: remove quotes and handle escaped characters\n    result = []\n    for token in tokens:\n        if token.startswith('\"') and token.endswith('\"'):\n            # Remove quotes\n            result.append(token[1:-1])\n        else:\n            # Handle escaped characters (e.g., \\n, \\t)\n            result.append(token)\n    \n    # Remove empty tokens\n    result = [t for t in result if t]\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 984, "num_retrieved": 4, "inference_time": 0.7412484437227249, "generate_results": ["import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = os.path.abspath(tmp_path)\n    if not os.path.exists(tmp_dir):\n        os.makedirs(tmp_dir, exist_ok=True)\n    \n    # Determine the output filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).replace(\".tar.gz\", \"\").replace(\".tar\", \"\")\n    \n    output_path = os.path.join(tmp_dir, filename)\n    \n    # Extract the archive using tar\n    try:\n        subprocess.run(['tar', '-xzf', archive_path, '-C', tmp_dir], check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the origin URL (assuming the extracted directory is the repo)\n    # The origin URL is typically the path to the extracted directory\n    return os.path.join(tmp_dir, filename)", "import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = os.path.abspath(tmp_path)\n    if not os.path.exists(tmp_dir):\n        os.makedirs(tmp_dir, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = os.path.join(tmp_dir, os.path.splitext(os.path.basename(archive_path))[0])\n    os.makedirs(extract_dir, exist_ok=True)\n    \n    # Use tar to extract the archive\n    try:\n        subprocess.run(['tar', '-xzf', archive_path, '-C', extract_dir], check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repo URL based on the extracted files\n    # Assuming the extracted directory contains a file named 'desc' or similar\n    # that identifies the repository origin\n    if filename:\n        desc_path = os.path.join(extract_dir, filename)\n        if os.path.exists(desc_path):\n            # Extract the origin URL from the desc file (example logic)\n            # This would depend on the actual format of the desc file\n            with open(desc_path, 'r') as f:\n                content = f.read()\n                # Example: look for a line containing \"origin\" or similar\n                # This is a placeholder - actual parsing depends on the file format\n                # For now, return a dummy URL based on the directory path\n                return f\"file://{extract_dir}\"\n    \n    # Default fallback: return file URL of the extracted directory\n    return f\"file://{extract_dir}\"", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(exist_ok=True, parents=True)\n    \n    # Determine the output filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).replace(\".tar.gz\", \"\")\n    \n    output_path = tmp_dir / filename\n    \n    # Extract the archive using tar\n    try:\n        subprocess.run(\n            [\"tar\", \"-xzf\", archive_path, \"-C\", str(tmp_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as a file repo url\n    # Assuming the extracted directory is the origin URL\n    return str(output_path)", "import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = os.path.abspath(tmp_path)\n    if not os.path.exists(tmp_dir):\n        os.makedirs(tmp_dir, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = os.path.join(tmp_dir, os.path.splitext(os.path.basename(archive_path))[0])\n    os.makedirs(extract_dir, exist_ok=True)\n    \n    # Use tar to extract the archive\n    try:\n        subprocess.run(\n            ['tar', '-xzf', archive_path, '-C', extract_dir],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repo URL based on the extracted files\n    # Assuming the archive contains a single file or directory that represents the repo\n    # The filename parameter can be used to specify the target file/directory\n    if filename:\n        file_path = os.path.join(extract_dir, filename)\n        if os.path.exists(file_path):\n            # For a file, return the file path as the origin URL\n            return file_path\n        else:\n            # If file not found, return the directory path\n            return extract_dir\n    else:\n        # Return the directory path as the origin URL\n        return extract_dir", "import subprocess\nimport os\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = os.path.abspath(tmp_path)\n    if not os.path.exists(tmp_dir):\n        os.makedirs(tmp_dir, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = os.path.join(tmp_dir, os.path.splitext(os.path.basename(archive_path))[0])\n    os.makedirs(extract_dir, exist_ok=True)\n    \n    # Use tar to extract the archive\n    try:\n        subprocess.run(['tar', '-xzf', archive_path, '-C', extract_dir], check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repo URL based on the extracted files\n    # Assuming the extracted directory contains a file named 'desc' or similar\n    # that can be used to derive the origin URL\n    if filename:\n        desc_path = os.path.join(extract_dir, filename)\n        if os.path.exists(desc_path):\n            # Extract the repo URL from the desc file (example logic)\n            # This is a simplified implementation - actual parsing depends on file format\n            with open(desc_path, 'r') as f:\n                content = f.read()\n                # Example: if the desc file contains a URL, extract it\n                # This is a placeholder - actual implementation depends on the format\n                # For now, return a dummy URL based on the directory path\n                return f\"file://{extract_dir}\"\n    \n    # Default fallback: return file URL of the extracted directory\n    return f\"file://{extract_dir}\""]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 989, "num_retrieved": 5, "inference_time": 0.7412484437227249, "generate_results": ["import subprocess\nimport sortedcontainers\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter to only include files that are in the ignored list\n        filtered_files = [file.strip() for file in ignored_files if file.strip() in ignored]\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Join with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\nimport sortedcontainers\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                                capture_output=True, text=True, check=False)\n        \n        # Parse the output to get the list of ignored files\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter to only include files that are in the provided ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the filtered files\n        sorted_files = sorted(filtered_files)\n        \n        # Join the files with commas\n        return ','.join(sorted_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git command fails, return empty string\n        return \"\"", "import subprocess\nimport sortedcontainers\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                                capture_output=True, text=True, check=False)\n        \n        if result.returncode != 0:\n            return \"\"\n            \n        # Split the output into lines and filter only the ignored files\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter to keep only files that are in the ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Join with commas\n        return ','.join(filtered_files)\n        \n    except Exception as e:\n        return \"\"", "import subprocess\nimport sortedcontainers\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                                capture_output=True, text=True, check=False)\n        \n        # Parse the output to get the list of ignored files\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter only the files that are in the provided ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the filtered files\n        sorted_files = sorted(filtered_files)\n        \n        # Join the files with commas\n        return ','.join(sorted_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git command fails, return empty string\n        return \"\"", "import subprocess\nimport sortedcontainers\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    \n    # Filter files to only include those in the ignored list\n    filtered_files = [file for file in ignored_files if file in ignored]\n    \n    # Sort the files\n    filtered_files.sort()\n    \n    # Join with comma separator\n    return ','.join(filtered_files)"]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 986, "num_retrieved": 4, "inference_time": 0.7412484437227249, "generate_results": ["import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.ini', '.cfg', '.yaml', '.yml'\n    }\n    \n    # Convert filename to lowercase and extract extension\n    filename_lower = filename.lower()\n    file_extension = os.path.splitext(filename_lower)[1]\n    \n    # Check if the extension is supported\n    return file_extension in supported_extensions", "import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.ini', '.cfg', '.yaml', '.yml'\n    }\n    \n    # Get the file extension (lowercase)\n    file_extension = os.path.splitext(filename)[1].lower()\n    \n    # Check if the extension is supported\n    return file_extension in supported_extensions", "import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.ini', '.cfg', '.yaml', '.yml'\n    }\n    \n    # Convert filename to lowercase and get the extension\n    filename_lower = filename.lower()\n    file_extension = os.path.splitext(filename_lower)[1]\n    \n    # Check if the extension is supported\n    return file_extension in supported_extensions", "import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Supported file types (example list - can be expanded based on module requirements)\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.ini', '.cfg', '.yaml', '.yml'\n    }\n    \n    # Convert filename to lowercase and extract extension\n    filename_lower = filename.lower()\n    extension = os.path.splitext(filename_lower)[1]\n    \n    # Check if extension is in supported list\n    return extension in supported_extensions", "import os\n\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Define supported file extensions\n    supported_extensions = {\n        '.py', '.txt', '.md', '.json', '.ini', '.cfg', '.yaml', '.yml'\n    }\n    \n    # Convert filename to lowercase and get the extension\n    filename_lower = filename.lower()\n    file_extension = os.path.splitext(filename_lower)[1]\n    \n    # Check if the extension is supported\n    return file_extension in supported_extensions"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 940, "num_retrieved": 3, "inference_time": 0.7412484437227249, "generate_results": ["import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)(\\s+(week|weeks|day|days|hour|hours|minute|minutes|second|seconds))$', frequency.strip())\n    if not match:\n        raise ValueError(f'Could not parse frequency string: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Map units to timedelta components\n    unit_map = {\n        'week': datetime.timedelta(weeks=number),\n        'weeks': datetime.timedelta(weeks=number),\n        'day': datetime.timedelta(days=number),\n        'days': datetime.timedelta(days=number),\n        'hour': datetime.timedelta(hours=number),\n        'hours': datetime.timedelta(hours=number),\n        'minute': datetime.timedelta(minutes=number),\n        'minutes': datetime.timedelta(minutes=number),\n        'second': datetime.timedelta(seconds=number),\n        'seconds': datetime.timedelta(seconds=number),\n    }\n\n    if unit not in unit_map:\n        raise ValueError(f'Unsupported time unit: {unit}')\n\n    return unit_map[unit]", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    if not match:\n        raise ValueError(f'Could not parse frequency string: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return datetime.timedelta(weeks=number)\n    elif unit == 'days':\n        return datetime.timedelta(days=number)\n    elif unit == 'hours':\n        return datetime.timedelta(hours=number)\n    elif unit == 'minutes':\n        return datetime.timedelta(minutes=number)\n    elif unit == 'seconds':\n        return datetime.timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unsupported time unit: {unit}')", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s+([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f'Could not parse frequency string: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Map units to timedelta components\n    unit_map = {\n        'second': datetime.timedelta(seconds=number),\n        'seconds': datetime.timedelta(seconds=number),\n        'minute': datetime.timedelta(minutes=number),\n        'minutes': datetime.timedelta(minutes=number),\n        'hour': datetime.timedelta(hours=number),\n        'hours': datetime.timedelta(hours=number),\n        'day': datetime.timedelta(days=number),\n        'days': datetime.timedelta(days=number),\n        'week': datetime.timedelta(weeks=number),\n        'weeks': datetime.timedelta(weeks=number),\n        'month': None,  # Not supported - would require month length\n        'months': None,  # Not supported - would require month length\n        'year': None,  # Not supported - would require year length\n        'years': None,  # Not supported - would require year length\n    }\n\n    if unit not in unit_map:\n        raise ValueError(f'Unknown time unit: {unit}')\n\n    # For unsupported units (month, year), we don't return anything\n    # Since the docstring specifies only number and unit of time, and examples are weeks, days, etc.\n    # We assume only supported units are those with direct timedelta mapping\n    if unit_map[unit] is None:\n        raise ValueError(f'Unsupported time unit: {unit}')\n\n    return unit_map[unit]", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)(\\s+(?:week|weeks|day|days|hour|hours|minute|minutes|second|seconds))$', frequency.strip())\n    if not match:\n        raise ValueError(f'Could not parse frequency string: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Map units to timedelta components\n    unit_map = {\n        'week': 'weeks',\n        'weeks': 'weeks',\n        'day': 'days',\n        'days': 'days',\n        'hour': 'hours',\n        'hours': 'hours',\n        'minute': 'minutes',\n        'minutes': 'minutes',\n        'second': 'seconds',\n        'seconds': 'seconds'\n    }\n\n    if unit not in unit_map:\n        raise ValueError(f'Unknown time unit: {unit}')\n\n    # Create timedelta with the appropriate component\n    try:\n        td = datetime.timedelta(**{unit_map[unit]: number})\n    except (ValueError, TypeError):\n        raise ValueError(f'Could not create timedelta for frequency: {frequency}')\n\n    return td", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency.strip(), re.IGNORECASE)\n    if not match:\n        raise ValueError(f'Could not parse frequency string: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return datetime.timedelta(weeks=number)\n    elif unit == 'days':\n        return datetime.timedelta(days=number)\n    elif unit == 'hours':\n        return datetime.timedelta(hours=number)\n    elif unit == 'minutes':\n        return datetime.timedelta(minutes=number)\n    elif unit == 'seconds':\n        return datetime.timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unsupported time unit: {unit}')"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1150, "num_retrieved": 10, "inference_time": 0.7412484437227249, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Convert host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Convert host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Convert host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Convert host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Convert host to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 964, "num_retrieved": 4, "inference_time": 0.7164140492677689, "generate_results": ["import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        # Check if the path starts with a scheme (like 'pp:', 'sh:', etc.)\n        if path.startswith(('sh:', 'pp:', 'rs:', 'gs:')):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use shell glob pattern with ** for recursive matching\n            # Replace any existing slashes with the glob pattern\n            # Add wildcards around the path\n            pattern = f\"sh:**/*{path}*/**\"\n            result.append(pattern)\n    return result", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use ** to match any directory path, and * to match any filename\n            # Replace any path separator with /* to match any directory\n            # Add wildcards at the beginning and end to match any path\n            pattern = f\"sh:**/*{path}*/**\"\n            result.append(pattern)\n    return result", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        # Check if the path already starts with a scheme (like 'pp:', 'sh:', etc.)\n        if path.startswith(('sh:', 'pp:', 'rs:', 'gs:')):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Replace any existing slashes with glob wildcards\n            # Use ** to match any directory path\n            # Add wildcards around the filename\n            pattern = f\"sh:**/*{path}*/**\"\n            result.append(pattern)\n    return result", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        # Check if the path already starts with a protocol (like 'pp:', 'sh:', etc.)\n        if path.startswith(('sh:', 'pp:', 'rs:', 'gs:')):\n            result.append(path)\n        else:\n            # Transform path fragment into a glob pattern\n            # Use shell glob pattern with ** for recursive matching\n            # Replace any existing slashes with the glob pattern\n            # Add wildcards around the path\n            pattern = f\"sh:**/*{path}*/**\"\n            result.append(pattern)\n    return result", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use ** to match any directory level, and * to match any filename\n            # Replace any existing slashes with / and add wildcards around the path\n            # Use a simple pattern: sh:**/*path*/**\n            # Strip leading/trailing slashes and add wildcards\n            path = path.strip('/')\n            # Add wildcards around the path\n            glob_pattern = f\"sh:**/*{path}*/**\"\n            result.append(glob_pattern)\n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1105, "num_retrieved": 8, "inference_time": 0.7164140492677689, "generate_results": ["import os\n\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n    \n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and \"GIT_BASH\" in os.environ", "import os\n\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n    \n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and \"GIT_BASH\" in os.environ", "import os\n\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n    \n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and \"GIT_BASH\" in os.environ", "import os\n\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n    \n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and \"GIT_BASH\" in os.environ", "import os\n\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n    \n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and \"GIT_BASH\" in os.environ"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 938, "num_retrieved": 3, "inference_time": 0.7164140492677689, "generate_results": ["import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n\n    # Write the configuration to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n\n    # Write the configuration to the file\n    with open(config_filename, 'w', encoding='utf-8') as f:\n        f.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n\n    # Write the configuration to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n\n    # Write the configuration to the file\n    with open(config_filename, 'w', encoding='utf-8') as f:\n        f.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n\n    # Write the configuration to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1019, "num_retrieved": 5, "inference_time": 0.7164140492677689, "generate_results": ["import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = [line.strip() for line in lines if line.strip()]\n    return seperator.join(commands)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = [line.strip() for line in lines if line.strip()]\n    return seperator.join(commands)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = [line.strip() for line in lines if line.strip()]\n    return seperator.join(commands)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = [line.strip() for line in lines if line.strip()]\n    return seperator.join(commands)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    lines = script.strip().splitlines()\n    commands = [line.strip() for line in lines if line.strip()]\n    return seperator.join(commands)"]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 948, "num_retrieved": 3, "inference_time": 0.7164140492677689, "generate_results": ["import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the command to run the function\n    # We need to import the module and get the function\n    # Since func is a function, we need to get its module\n    module_name = func.__module__\n    \n    # Import the module\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        raise ImportError(f\"Cannot import module {module_name}: {e}\")\n    \n    # Create the command to execute the function\n    # We use python -c to run a command in subprocess\n    command = [\"python\", \"-c\"]\n    \n    # Build the Python code to execute\n    code = f\"\"\"\nimport sys\nimport importlib\nimport os\nfrom {module_name} import {func.__name__}\n\n# Set environment variables\nenv = {{}}\n{', '.join(f'env[\"{k}\"] = \"{v}\"' for k, v in (extra_env or {}).items()) if extra_env else ''}\n\n# Set the environment for the subprocess\nos.environ.update(env)\n\n# Execute the function\nresult = {func.__name__}({', '.join([f'\"{arg}\"' if isinstance(arg, str) else str(arg) for arg in args])})\nprint(result)\n\"\"\"\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            command,\n            input=code,\n            text=True,\n            timeout=timeout,\n            env=dict(os.environ, **(extra_env or {}))\n        )\n        return result.stdout.strip()\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function execution timed out after {timeout} seconds\")\n    except Exception as e:\n        raise RuntimeError(f\"Error running function: {e}\")", "import subprocess\nimport importlib.util\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the command to run the function\n    # We need to create a temporary script that imports and calls the function\n    script_content = f\"\"\"\nimport sys\nimport importlib\nimport {func.__module__} as module\nfunc = module.{func.__name__}\nsys.argv = {repr(list(args))} if args else []\nsys.exit(func(*sys.argv))\n\"\"\"\n    \n    # Create a temporary script file\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write(script_content)\n        temp_script = f.name\n    \n    try:\n        # Set up environment\n        env = dict(os.environ)\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script in subprocess\n        result = subprocess.run(\n            [sys.executable, temp_script],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        # Return the result\n        return result.returncode, result.stdout, result.stderr\n        \n    finally:\n        # Clean up the temporary file\n        import os\n        if os.path.exists(temp_script):\n            os.unlink(temp_script)", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the command to run the function\n    # We need to create a Python script that imports and runs the function\n    script = f\"\"\"\nimport sys\nimport importlib\nimport subprocess\n\n# Import the module containing the function\nmodule_name = '{func.__module__}'\nmodule = importlib.import_module(module_name)\n\n# Get the function\ntarget_func = module.{func.__name__}\n\n# Execute the function with provided arguments\nresult = target_func({', '.join([repr(arg) for arg in args])})\n\n# Return the result\nprint(result)\n\"\"\"\n    \n    # Prepare environment\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the script in a subprocess\n    result = subprocess.run(\n        ['python', '-c', script],\n        capture_output=True,\n        text=True,\n        timeout=timeout,\n        env=env\n    )\n    \n    # Return the output (or raise exception if failed)\n    if result.returncode != 0:\n        raise RuntimeError(f\"Command failed with return code {result.returncode}: {result.stderr}\")\n    \n    return result.stdout.strip()", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the module path and function name from the function object\n    module_name = func.__module__\n    function_name = func.__name__\n    \n    # Create a temporary script that imports the module and calls the function\n    script_content = f\"\"\"\nimport sys\nimport {module_name}\nsys.path.insert(0, '{module_name}')\n{function_name}({', '.join([f'\"{arg}\"' if isinstance(arg, str) else str(arg) for arg in args])})\n\"\"\"\n    \n    # Execute the script in a subprocess\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    try:\n        result = subprocess.run(\n            ['python', '-c', script_content],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        return result.returncode, result.stdout, result.stderr\n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Timeout expired\"\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the module path and function name from the function object\n    module_name = func.__module__\n    function_name = func.__name__\n    \n    # Create a temporary script that imports the module and calls the function\n    script_content = f\"\"\"\nimport sys\nimport {module_name}\nsys.path.insert(0, '{module_name}')\n{function_name}({', '.join([f'\"{arg}\"' if isinstance(arg, str) else str(arg) for arg in args])})\n\"\"\"\n    \n    # Create a temporary file for the script\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write(script_content)\n        temp_script = f.name\n    \n    try:\n        # Set up environment variables\n        env = dict(os.environ)\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script in a subprocess\n        result = subprocess.run(\n            ['python', temp_script],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        # Return the result\n        return result.returncode, result.stdout, result.stderr\n        \n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Subprocess timed out after {timeout} seconds\")\n    except Exception as e:\n        raise RuntimeError(f\"Subprocess execution failed: {str(e)}\")\n    finally:\n        # Clean up the temporary script file\n        import os\n        if os.path.exists(temp_script):\n            os.unlink(temp_script)"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 896, "num_retrieved": 3, "inference_time": 0.7164140492677689, "generate_results": ["import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(?:-([a-zA-Z0-9_]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(2)\n    \n    if default_value:\n        env_value = os.environ.get(var_name, default_value)\n        return env_value\n    else:\n        env_value = os.environ.get(var_name)\n        if env_value is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n        return env_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(?:-([a-zA-Z0-9_]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(2)\n    \n    if default_value:\n        env_value = os.environ.get(var_name, default_value)\n        return env_value\n    else:\n        env_value = os.environ.get(var_name)\n        if env_value is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n        return env_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(?:-([a-zA-Z0-9_]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(2)\n    \n    if default_value:\n        env_value = os.environ.get(var_name, default_value)\n        return env_value\n    else:\n        env_value = os.environ.get(var_name)\n        if env_value is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n        return env_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(?:-([a-zA-Z0-9_]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(2)\n    \n    if default_value:\n        env_value = os.environ.get(var_name, default_value)\n        return env_value\n    else:\n        env_value = os.environ.get(var_name)\n        if env_value is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n        return env_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)(?:-([a-zA-Z0-9_]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(2)\n    \n    if default_value:\n        env_value = os.environ.get(var_name, default_value)\n        return env_value\n    else:\n        env_value = os.environ.get(var_name)\n        if env_value is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n        return env_value"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1012, "num_retrieved": 6, "inference_time": 0.7164140492677689, "generate_results": ["import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    parsed = urllib.parse.urlparse(image_href)\n    \n    if not parsed.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parsed.scheme in ('https', 'http')\n    \n    # Extract image_id from the path after the base URL\n    path = parsed.path.lstrip('/')\n    if not path:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    # Split path to get the last segment as image_id\n    image_id = path.split('/')[-1]\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    parsed = urllib.parse.urlparse(image_href)\n    \n    if not parsed.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parsed.scheme in ('https', 'http')\n    \n    # Extract image_id from path after the last slash\n    path_parts = parsed.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    image_id = path_parts[-1]\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    parsed = urllib.parse.urlparse(image_href)\n    \n    if not parsed.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parsed.scheme in ('https', 'http')\n    \n    # Extract image_id from the path after the base URL\n    path = parsed.path.lstrip('/')\n    if not path:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    # Split path to get the last segment as image_id\n    image_id = path.split('/')[-1]\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    parsed = urllib.parse.urlparse(image_href)\n    \n    if not parsed.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parsed.scheme in ('https', 'http')\n    \n    # Extract image_id from the path after the base URL\n    path = parsed.path\n    if not path:\n        raise ValueError(\"Invalid image href: missing path\")\n    \n    # Split path by '/' and take the last part as image_id\n    path_parts = path.split('/')\n    image_id = path_parts[-1] if path_parts else \"\"\n    \n    if not image_id:\n        raise ValueError(\"Invalid image href: missing image ID\")\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    parsed = urllib.parse.urlparse(image_href)\n    \n    if not parsed.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parsed.scheme in ('https', 'http')\n    \n    # Extract image_id from the path after the base URL\n    path = parsed.path.lstrip('/')\n    if not path:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    # Split path to get the last segment as image_id\n    image_id = path.split('/')[-1]\n    \n    return (image_id, parsed.netloc, use_ssl)"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 918, "num_retrieved": 2, "inference_time": 0.7164140492677689, "generate_results": ["import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if not isinstance(item, str):\n            raise TypeError(\"All elements in input_list must be strings\")\n        \n        if len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1066, "num_retrieved": 8, "inference_time": 0.7164140492677689, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern\n    pattern = pattern.replace(' ', r'\\s+')\n    # Handle special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern\n    pattern = pattern.replace(' ', r'\\s+')\n    # Handle special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern\n    pattern = pattern.replace(' ', r'\\s+')\n    # Handle special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern\n    pattern = pattern.replace(' ', r'\\s+')\n    # Handle special regex characters by escaping them\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern\n    pattern = pattern.replace(' ', r'\\s+')\n    # Handle special regex characters\n    pattern = re.escape(pattern)\n    return pattern"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1034, "num_retrieved": 7, "inference_time": 0.7164140492677689, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env\n        )\n        \n        # Print output if verbose or if there's an error\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr)\n        \n        # Return the return code\n        return result.returncode\n        \n    except Exception as e:\n        print(f\"Error executing command: {e}\", file=sys.stderr)\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands + args\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env\n        )\n        \n        # Check if command was successful\n        if result.returncode != 0:\n            if not hide_stderr:\n                print(\"Error executing command:\", result.stderr)\n            return result.returncode\n            \n        # Print output if verbose mode is enabled\n        if verbose and result.stdout:\n            print(\"Command output:\", result.stdout)\n            \n        return 0\n        \n    except subprocess.SubprocessError as e:\n        print(\"Subprocess error:\", str(e))\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands + args\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env\n        )\n        \n        # Check if the command was successful\n        if result.returncode != 0:\n            if not hide_stderr:\n                print(\"Error executing command:\", result.stderr)\n            return result.returncode\n            \n        # Print output if verbose mode is enabled\n        if verbose and result.stdout:\n            print(\"Command output:\", result.stdout)\n            \n        return 0\n        \n    except subprocess.SubprocessError as e:\n        print(\"Subprocess error:\", str(e))\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env\n        )\n        \n        # Print output if verbose or if there's an error\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr)\n        \n        # Return the return code\n        return result.returncode\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(f\"Command failed with return code {e.returncode}\")\n            if e.stdout:\n                print(f\"Stdout: {e.stdout}\")\n            if e.stderr:\n                print(f\"Stderr: {e.stderr}\")\n        return e.returncode\n    except Exception as e:\n        if verbose:\n            print(f\"Error executing command: {str(e)}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env\n        )\n        \n        # Print output if verbose or if there's an error\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n        \n        # Return the return code\n        return result.returncode\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        return e.returncode\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        return 1"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 0.7164140492677689, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 983, "num_retrieved": 7, "inference_time": 0.7164140492677689, "generate_results": ["import rdflib\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    # A root is a node that has no parent in the hierarchy\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., an object for the property)\n        # If it doesn't, it's a root\n        if not list(graph.objects(subject, prop)):\n            roots.add(subject)\n    \n    return roots", "import rdflib\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has no parent (i.e., it's a root)\n        # A root has no triple (subject, prop, parent)\n        # So we need to check if there's any parent for this subject\n        has_parent = False\n        for obj in graph.objects(subject, prop):\n            has_parent = True\n            break\n        \n        if not has_parent:\n            roots.add(subject)\n    \n    return roots", "import rdflib\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    # A root is a node that has no parent in the hierarchy\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., an object for the property)\n        # If it doesn't, it's a root\n        has_parent = False\n        for obj in graph.objects(subject, prop):\n            has_parent = True\n            break\n        \n        if not has_parent:\n            roots.add(subject)\n    \n    return roots", "import rdflib\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    # A root is a node that has no parent in the hierarchy\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., if it has a triple (subject, prop, parent))\n        # If it doesn't have a parent, it's a root\n        has_parent = False\n        for obj in graph.objects(subject, prop):\n            has_parent = True\n            break\n        \n        if not has_parent:\n            roots.add(subject)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set, Union\n\ndef find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property as their object\n    # These are the potential roots (nodes with no parent)\n    for s, p, o in graph.triples((None, prop, None)):\n        # If this node is not already in roots, add it\n        if o not in roots:\n            roots.add(o)\n    \n    # Now, recursively find roots by checking if any of the current roots\n    # have a parent (i.e., if they are pointed to by another node)\n    # We need to find nodes that are not pointed to by any other node\n    # So we check all nodes that have the property pointing to the current roots\n    \n    # Create a set of all nodes that are pointed to by the property\n    # These are the \"children\" of the property\n    children = set()\n    for s, p, o in graph.triples((None, prop, None)):\n        children.add(o)\n    \n    # Find nodes that are not in children (i.e., roots)\n    # But we already have the roots from above, so we need to refine\n    # Actually, we need to find nodes that are not pointed to by any other node\n    \n    # Instead, we can do: find all nodes that are not the object of any triple (s, prop, o)\n    # So we need to find all nodes that are not in the set of objects of (s, prop, o)\n    \n    # Get all nodes in the graph\n    all_nodes = set()\n    for s, p, o in graph.triples((None, None, None)):\n        all_nodes.add(s)\n        all_nodes.add(o)\n    \n    # Find nodes that are not the object of any triple with property\n    # These are the roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1056, "num_retrieved": 5, "inference_time": 0.7164140492677689, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1306, "num_retrieved": 17, "inference_time": 0.7164140492677689, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1156, "num_retrieved": 9, "inference_time": 0.7164140492677689, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n    \n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n    \n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n    \n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n    \n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n    \n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1109, "num_retrieved": 9, "inference_time": 0.7164140492677689, "generate_results": ["import numpy as np\n\ndef gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "import numpy as np\n\ndef gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "import numpy as np\n\ndef gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "import numpy as np\n\ndef gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "import numpy as np\n\ndef gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 931, "num_retrieved": 3, "inference_time": 0.9512907117605209, "generate_results": ["import logging\nimport os\nimport yaml\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    errors = []\n\n    for filename in config_filenames:\n        if not os.path.exists(filename):\n            error_msg = f\"Configuration file not found: {filename}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n            continue\n\n        try:\n            with open(filename, 'r', encoding='utf-8') as f:\n                config_content = f.read()\n                config_data = yaml.safe_load(config_content)\n                if config_data is None:\n                    config_data = {}\n\n                # Apply overrides if provided\n                if overrides:\n                    apply_overrides(config_data, overrides)\n\n                # Resolve environment variables if requested\n                if resolve_env:\n                    # Placeholder for environment variable resolution\n                    # This would typically involve replacing ${VAR} with os.getenv('VAR')\n                    pass\n\n                config_dict[filename] = config_data\n\n        except Exception as e:\n            error_msg = f\"Error parsing configuration file {filename}: {str(e)}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n\n    return (config_dict, errors)", "import logging\nimport os\nimport yaml\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    errors = []\n\n    for filename in config_filenames:\n        if not os.path.isfile(filename):\n            error_msg = f\"Configuration file not found: {filename}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n            continue\n\n        try:\n            with open(filename, 'r') as f:\n                config_content = f.read()\n                config_data = yaml.safe_load(config_content)\n                if config_data is None:\n                    config_data = {}\n\n                # Apply overrides if provided\n                if overrides:\n                    apply_overrides(config_data, overrides)\n\n                # Resolve environment variables if requested\n                if resolve_env:\n                    # Placeholder for environment variable resolution\n                    # This would typically involve replacing ${VAR} with os.environ.get('VAR')\n                    pass\n\n                config_dict[filename] = config_data\n\n        except Exception as e:\n            error_msg = f\"Error parsing configuration file {filename}: {str(e)}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n\n    return (config_dict, errors)", "import logging\nimport os\nimport yaml\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    errors = []\n\n    for filename in config_filenames:\n        if not os.path.exists(filename):\n            error_msg = f\"Configuration file not found: {filename}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n            continue\n\n        try:\n            with open(filename, 'r', encoding='utf-8') as f:\n                config_content = f.read()\n                config_data = yaml.safe_load(config_content) or {}\n                \n                # Apply overrides if provided\n                if overrides:\n                    apply_overrides(config_data, overrides)\n                \n                # Resolve environment variables if requested\n                if resolve_env:\n                    # Placeholder for environment variable resolution\n                    # This would typically involve replacing ${VAR} with os.environ.get('VAR')\n                    pass\n                \n                config_dict[filename] = config_data\n                \n        except Exception as e:\n            error_msg = f\"Error parsing configuration file {filename}: {str(e)}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n    \n    return (config_dict, errors)", "import logging\nimport os\nimport yaml\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    errors = []\n\n    for filename in config_filenames:\n        if not os.path.isfile(filename):\n            error_msg = f\"Configuration file not found: {filename}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n            continue\n\n        try:\n            with open(filename, 'r', encoding='utf-8') as f:\n                config_content = f.read()\n                config_data = yaml.safe_load(config_content)\n                if config_data is None:\n                    config_data = {}\n                \n                # Apply overrides if provided\n                if overrides:\n                    apply_overrides(config_data, overrides)\n                \n                # Resolve environment variables if requested\n                if resolve_env:\n                    # Placeholder for environment variable resolution\n                    # This would typically involve replacing ${VAR} with os.environ.get('VAR')\n                    pass\n                \n                config_dict[filename] = config_data\n                \n        except Exception as e:\n            error_msg = f\"Error parsing configuration file {filename}: {str(e)}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n    \n    return (config_dict, errors)", "import logging\nimport os\nimport yaml\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    config_dict = {}\n    errors = []\n    \n    for filename in config_filenames:\n        if not os.path.exists(filename):\n            error_msg = f\"Configuration file not found: {filename}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n            continue\n            \n        try:\n            with open(filename, 'r', encoding='utf-8') as f:\n                config_content = f.read()\n                config_data = yaml.safe_load(config_content)\n                if config_data is None:\n                    config_data = {}\n                    \n                # Apply overrides if provided\n                if overrides:\n                    apply_overrides(config_data, overrides)\n                    \n                # Resolve environment variables if requested\n                if resolve_env:\n                    # Placeholder for environment variable resolution\n                    # This would typically involve replacing ${VAR} with os.environ.get('VAR')\n                    pass\n                    \n                config_dict[filename] = config_data\n                \n        except Exception as e:\n            error_msg = f\"Error parsing configuration file {filename}: {str(e)}\"\n            errors.append(logging.LogRecord(\n                name=\"borgmatic\",\n                level=logging.ERROR,\n                pathname=\"\", \n                lineno=0,\n                msg=error_msg,\n                args=(),\n                exc_info=None\n            ))\n    \n    return (config_dict, errors)"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 967, "num_retrieved": 5, "inference_time": 0.9512907117605209, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 918, "num_retrieved": 3, "inference_time": 0.9512907117605209, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in bounds (0 <= ticks < 86400000000000)\")\n    \n    # Convert nanoseconds to hours, minutes, seconds\n    seconds = ticks // 1000000000\n    nanoseconds = ticks % 1000000000\n    \n    # Convert seconds to hours, minutes\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    \n    # Create time object\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        # For timezone handling, we would typically adjust the time\n        # However, since Time is a time without timezone, we just return the time\n        # with the timezone info stored separately if needed\n        pass\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in bounds (0 <= ticks < 86400000000000)\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds = ticks // 1_000_000_000\n    nanoseconds = ticks % 1_000_000_000\n    \n    # Extract hours, minutes, seconds from total seconds\n    hours, remainder = divmod(seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    \n    # Create time object with the extracted components\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # If timezone is provided, apply it\n    if tz is not None:\n        # For time objects, timezone is typically not stored in the time itself\n        # but can be used to adjust the time if needed\n        # However, Time objects in Neo4j do not store timezone information\n        # so we just return the time without timezone\n        pass\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds = ticks // 1_000_000_000\n    nanoseconds = ticks % 1_000_000_000\n    \n    # Extract hours, minutes, seconds from total seconds\n    hours, remainder = divmod(seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    \n    # Create time object with the extracted components\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # If timezone is provided, apply it\n    if tz is not None:\n        # For time objects, timezone is typically not stored in the time itself\n        # but can be used to create a timezone-aware datetime if needed\n        # However, the Time class itself does not store timezone info\n        # So we just return the time without timezone info\n        pass\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in bounds (0 <= ticks < 86400000000000)\")\n    \n    # Convert nanoseconds to hours, minutes, seconds\n    seconds = ticks // 1000000000\n    nanoseconds = ticks % 1000000000\n    \n    # Convert seconds to hours, minutes\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    \n    # Create time object\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        # For timezone handling, we would typically adjust the time\n        # However, since Time is a time without timezone, we just return the time\n        # with the timezone information stored separately if needed\n        pass\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in bounds (0 <= ticks < 86400000000000)\")\n    \n    # Convert nanoseconds to hours, minutes, seconds\n    seconds = ticks // 1000000000\n    nanoseconds = ticks % 1000000000\n    \n    # Convert seconds to hours, minutes\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    \n    # Create time object\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        # For timezone handling, we would typically adjust the time\n        # However, since Time is a time without timezone, we just return the time\n        # with the timezone info stored separately if needed\n        pass\n    \n    return time_obj"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 913, "num_retrieved": 2, "inference_time": 0.9512907117605209, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Filter to only include if version is supported\n        return {k: v for k, v in supported_versions.items() if v is not None}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) for version in cls._get_supported_versions() if cls._get_handler_for_version(version) is not None}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Filter to only include if version is supported\n        return {k: v for k, v in supported_versions.items() if v is not None}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) for version in cls._get_supported_versions() if cls._get_handler_for_version(version) is not None}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Filter to only include if version is supported\n        return {k: v for k, v in supported_versions.items() if v is not None}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) for version in cls._get_supported_versions() if cls._get_handler_for_version(version) is not None}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Filter to only include supported versions\n        return {k: v for k, v in supported_versions.items() if v is not None}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) for version in cls._get_supported_versions() if cls._get_handler_for_version(version) is not None}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple or None\")\n        supported_versions = {protocol_version: cls._get_handler_for_version(protocol_version)}\n        # Only include if version is supported\n        if supported_versions[protocol_version] is not None:\n            return supported_versions\n        else:\n            return {}\n    else:\n        # Return all supported versions\n        return {version: cls._get_handler_for_version(version) \n                for version in cls._get_supported_versions() \n                if cls._get_handler_for_version(version) is not None}"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 988, "num_retrieved": 4, "inference_time": 0.9512907117605209, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(set(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(set(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(set(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(set(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1096, "num_retrieved": 9, "inference_time": 0.9512907117605209, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq* (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence):\n        seq_obj = seq\n        seq_type = type(seq)\n    else:\n        # Try to convert seq to a sequence type\n        seq_obj = lena_sequence(seq, bufsize=bufsize)\n        seq_type = type(seq_obj)\n    \n    return seq_obj, seq_type", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq* (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence):\n        seq_obj = seq\n        seq_type = type(seq)\n    else:\n        # Attempt to convert seq to a sequence type\n        seq_obj = lena_sequence(seq, bufsize=bufsize)\n        seq_type = type(seq_obj)\n    \n    return seq_obj, seq_type", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq* (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence):\n        seq_obj = seq\n        seq_type = type(seq)\n    else:\n        # Try to convert seq to a sequence type\n        seq_obj = lena_sequence(seq, bufsize=bufsize)\n        seq_type = type(seq_obj)\n    \n    return seq_obj, seq_type", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence):\n        seq_obj = seq\n        seq_type = type(seq)\n    else:\n        # Try to convert seq to a sequence type\n        seq_obj = lena_sequence(seq, bufsize=bufsize)\n        seq_type = type(seq_obj)\n    \n    return seq_obj, seq_type", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq* (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence):\n        seq_obj = seq\n        seq_type = type(seq)\n    else:\n        # Try to convert seq to a sequence type\n        seq_obj = lena_sequence(seq, bufsize=bufsize)\n        seq_type = type(seq_obj)\n    \n    return seq_obj, seq_type"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 912, "num_retrieved": 2, "inference_time": 0.9512907117605209, "generate_results": ["def scale(self, other=None, recompute=False):\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale self to other (float)\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram to the given value\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram to other value\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other (float)\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram to other value\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other (float)\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram to the given value\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    if other is None:\n        # Return scale of this histogram\n        if not hasattr(self, '_scale') or self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed before\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        self._scale = other\n        return self._scale"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 910, "num_retrieved": 2, "inference_time": 0.9512907117605209, "generate_results": ["def scale(self, other=None):\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    else:\n        # Rescale to the provided value\n        if isinstance(other, (int, float)):\n            # Use the last coordinate to rescale\n            # Assuming self._coordinates contains the coordinates of the graph\n            if not self._coordinates:\n                raise lena.core.LenaValueError(\"graph has no coordinates\")\n            last_coord = self._coordinates[-1]\n            # Rescale the last coordinate to the provided value\n            # This would typically involve scaling the coordinate values\n            # For now, we just set the scale\n            self._scale = other\n            return self._scale\n        else:\n            # Handle non-numeric other (e.g., selector)\n            # This would involve scaling based on a selector\n            # For now, just return the scale\n            raise lena.core.LenaValueError(\"scale must be a number\")", "def scale(self, other=None):\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    else:\n        # Rescale to the provided value\n        if isinstance(other, (int, float)):\n            # Use the last coordinate for rescaling\n            last_coord = self._get_last_coordinate()\n            # Apply scaling to the last coordinate\n            scale_to(other, self._data, self._allow_zero_scale, self._allow_unknown_scale)\n        else:\n            # Handle non-numeric other (e.g., selector)\n            scale_to(other, self._data, self._allow_zero_scale, self._allow_unknown_scale)\n        return self._scale", "def scale(self, other=None):\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    else:\n        # Rescale to the provided value\n        if isinstance(other, (int, float)):\n            # Use the last coordinate for rescaling\n            last_coord = self._coordinates[-1] if self._coordinates else None\n            if last_coord is None:\n                raise lena.core.LenaValueError(\"No coordinates available to rescale\")\n            # Apply scaling to the last coordinate\n            self._scale = other\n            return self._scale\n        else:\n            raise lena.core.LenaValueError(\"Scale value must be a number\")", "def scale(self, other=None):\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    else:\n        # Rescale to the provided value\n        if isinstance(other, (int, float)):\n            # Use the last coordinate for rescaling\n            last_coord = self._coordinates[-1] if self._coordinates else None\n            if last_coord is None:\n                raise lena.core.LenaValueError(\"No coordinates available to rescale\")\n            # Rescale the last coordinate to the provided value\n            self._scale = other\n            return self._scale\n        else:\n            raise lena.core.LenaValueError(\"Scale value must be a number\")", "def scale(self, other=None):\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    else:\n        # Rescale to the provided value\n        if isinstance(other, (int, float)):\n            # Get the last coordinate (dimension) of the graph\n            dimensions = len(self._coordinates)\n            if dimensions == 0:\n                raise lena.core.LenaValueError(\"graph has no coordinates\")\n            last_dim = dimensions - 1\n            \n            # Rescale only the last coordinate\n            scale_to = other\n            # Apply scaling to the last coordinate and its associated errors\n            self._scale = scale_to\n            return self._scale\n        else:\n            raise lena.core.LenaValueError(\"scale must be a number\")"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.9512907117605209, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create coordinate mapping based on get_coordinate\n    if get_coordinate == \"left\":\n        x_coords = [bin_.left for bin_ in hist.bins]\n    elif get_coordinate == \"right\":\n        x_coords = [bin_.right for bin_ in hist.bins]\n    else:  # \"middle\"\n        x_coords = [bin_.middle for bin_ in hist.bins]\n    \n    # Apply make_value to get y values\n    y_values = [make_value(bin_) for bin_ in hist.bins]\n    \n    # Create graph with field names and scale\n    graph = Graph(\n        x_coords=x_coords,\n        y_values=y_values,\n        field_names=field_names,", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create coordinate mapping based on get_coordinate\n    if get_coordinate == \"left\":\n        x_coords = [bin_.left for bin_ in hist.bins]\n    elif get_coordinate == \"right\":\n        x_coords = [bin_.right for bin_ in hist.bins]\n    else:  # \"middle\"\n        x_coords = [bin_.middle for bin_ in hist.bins]\n    \n    # Apply make_value to get y values\n    y_values = [make_value(bin_) for bin_ in hist.bins]\n    \n    # Create graph with appropriate field names\n    graph = Graph(\n        x=x_coords,\n        y=y_values,\n        field_names=field_names,\n        scale=scale", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create coordinate mapping based on get_coordinate\n    if get_coordinate == \"left\":\n        x_coords = [bin_.left for bin_ in hist.bins]\n    elif get_coordinate == \"right\":\n        x_coords = [bin_.right for bin_ in hist.bins]\n    else:  # \"middle\"\n        x_coords = [bin_.middle for bin_ in hist.bins]\n    \n    # Apply make_value to get y values\n    y_values = [make_value(bin_) for bin_ in hist.bins]\n    \n    # Create graph with field names and scale\n    graph = Graph(\n        x_coords=x_coords,\n        y_values=y_values,\n        field_names=field_names,", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create coordinate mapping based on get_coordinate\n    if get_coordinate == \"left\":\n        x_coords = [bin_.left for bin_ in hist.bins]\n    elif get_coordinate == \"right\":\n        x_coords = [bin_.right for bin_ in hist.bins]\n    else:  # \"middle\"\n        x_coords = [bin_.middle for bin_ in hist.bins]\n    \n    # Apply make_value to get y values\n    y_values = [make_value(bin_) for bin_ in hist.bins]\n    \n    # Create graph with specified field names\n    graph = Graph(\n        x=x_coords,\n        y=y_values,\n        field_names=field_names,\n        scale=scale", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Validate get_coordinate\n    valid_coordinates = {\"left\", \"right\", \"middle\"}\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Create the graph points\n    points = []\n    for bin_idx, bin_data in enumerate(hist.bins):\n        # Get coordinate based on get_coordinate\n        if get_coordinate == \"left\":\n            x = bin_data[0]\n        elif get_coordinate == \"right\":\n            x = bin_data[-1]\n        elif get_coordinate == \"middle\":\n            x = (bin_data[0] + bin_data[-1]) / 2\n        \n        # Apply make_value to get y value(s)\n        y_value = make_value(bin_data)\n        \n        # Create point with field names\n        point = {\n            field_names[0]: x\n        }\n        if len(field_names) > 1:"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 832, "num_retrieved": 1, "inference_time": 0.9512907117605209, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name, method in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate does not define method %s\" % name)\n\n    # Step 3: Check method signatures\n    for name, method in iface:\n        if hasattr(candidate, name):\n            try:\n                # Compare method signatures (simplified - actual signature comparison would be more complex)\n                candidate_method = getattr(candidate, name)\n                if not hasattr(candidate_method, '__call__'):\n                    errors.append(\"Method %s is not callable\" % name)\n            except Exception as e:\n                errors.append(\"Error accessing method %s: %s\" % (name, e))\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name, attr in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate does not define attribute %s\" % name)", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name, method_spec in iface._methods.items():\n        method_name = name\n        if not hasattr(candidate, method_name):\n            errors.append(\"Candidate does not define method %s\" % method_name)\n        else:\n            method = getattr(candidate, method_name)\n            # Check method signature\n            try:\n                # Compare method signature with interface specification\n                if not _check_method_signature(method, method_spec):\n                    errors.append(\"Method %s has incorrect signature\" % method_name)\n            except Exception as e:\n                errors.append(\"Method %s has invalid signature: %s\" % (method_name, str(e)))\n\n    # Step 3: Check that candidate defines all necessary attributes\n    for name, attr_spec in iface._attributes.items():\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate does not define attribute %s\" % name)\n\n    # Report", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that all necessary methods are defined\n    for name, method in iface._methods.items():\n        try:\n            candidate_method = getattr(candidate, name)\n            if not callable(candidate_method):\n                errors.append(\"Missing method %s\" % name)\n        except AttributeError:\n            errors.append(\"Missing method %s\" % name)\n\n    # Step 3: Check method signatures\n    for name, method in iface._methods.items():\n        try:\n            candidate_method = getattr(candidate, name)\n            if not callable(candidate_method):\n                errors.append(\"Method %s is not callable\" % name)\n            # Check signature (simplified - actual signature checking would be more complex)\n            # This is a placeholder for signature verification\n        except AttributeError:\n            errors.append(\"Method %s not found\" % name)\n\n    # Step 4: Check that all necessary attributes are defined\n    for name, attr in iface._attributes.items():", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name, method in iface._methods.items():\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate missing required method: %s\" % name)\n\n    # Step 3: Check method signatures\n    for name, method in iface._methods.items():\n        if hasattr(candidate, name):\n            try:\n                # Compare method signatures\n                candidate_method = getattr(candidate, name)\n                if not _check_method_signature(method, candidate_method):\n                    errors.append(\"Method %s has incorrect signature\" % name)\n            except Exception as e:\n                errors.append(\"Method %s has invalid signature: %s\" % (name, str(e)))\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name, attr in iface._attributes.items():\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate missing required attribute: %s\" % name", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name, method in iface._methods.items():\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate missing required method: %s\" % name)\n\n    # Step 3: Check method signatures\n    for name, method in iface._methods.items():\n        if hasattr(candidate, name):\n            try:\n                # Compare method signatures\n                candidate_method = getattr(candidate, name)\n                if not _check_method_signature(method, candidate_method):\n                    errors.append(\"Method %s has incorrect signature\" % name)\n            except Exception as e:\n                errors.append(\"Method %s has invalid signature: %s\" % (name, str(e)))\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name, attr in iface._attributes.items():\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate missing required attribute: %s\" % name"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 916, "num_retrieved": 2, "inference_time": 0.9512907117605209, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that all necessary methods are defined\n    for name, method in iface._methods.items():\n        if not hasattr(candidate, name):\n            errors.append(\"Missing method %s\" % name)\n\n    # Step 3: Check method signatures\n    for name, method in iface._methods.items():\n        if hasattr(candidate, name):\n            try:\n                candidate_method = getattr(candidate, name)\n                # Compare method signatures (simplified - actual signature comparison would be more complex)\n                if not hasattr(candidate_method, '__call__'):\n                    errors.append(\"Method %s is not callable\" % name)\n            except Exception as e:\n                errors.append(\"Error accessing method %s: %s\" % (name, e))\n\n    # Step 4: Check that all necessary attributes are defined\n    for name, attr in iface._attributes.items():\n        if not hasattr(candidate, name):\n            errors.append(\"Missing attribute %s\" % name)", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that all necessary methods are defined\n    for name, method_spec in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Missing method %s\" % name)\n\n    # Step 3: Check that methods have correct signatures\n    for name, method_spec in iface:\n        method = getattr(candidate, name, None)\n        if method is not None:\n            # Compare method signature (simplified - actual signature checking would be more complex)\n            # This is a placeholder for actual signature validation\n            if not hasattr(method, '__name__') or method.__name__ != name:\n                errors.append(\"Method %s has incorrect name\" % name)\n\n    # Step 4: Check that all necessary attributes are defined\n    for name, attr_spec in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Missing attribute %s\" % name)\n\n    # Report errors\n    if errors:\n        if len(errors", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that all necessary methods are defined\n    for name, method_spec in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Missing method %s\" % name)\n\n    # Step 3: Check method signatures\n    for name, method_spec in iface:\n        method = getattr(candidate, name, None)\n        if method is None:\n            continue\n        # Compare method signature (simplified - actual signature comparison would be more complex)\n        try:\n            # This is a simplified check; in practice, you'd compare actual method signatures\n            # using inspect or similar tools\n            pass\n        except Exception as e:\n            errors.append(\"Method %s has incorrect signature: %s\" % (name, e))\n\n    # Step 4: Check that all necessary attributes are defined\n    for name, attr_spec in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Missing attribute %s\" % name", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that all necessary methods are defined\n    for name, method_spec in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Missing method %s\" % name)\n\n    # Step 3: Check method signatures\n    for name, method_spec in iface:\n        method = getattr(candidate, name, None)\n        if method is None:\n            continue\n        # Compare method signature (simplified - actual signature checking would be more complex)\n        try:\n            # This is a simplified check; in practice, you'd compare actual method signatures\n            if not hasattr(method, '__name__') or method.__name__ != name:\n                errors.append(\"Method %s has incorrect name\" % name)\n        except Exception:\n            errors.append(\"Method %s has invalid signature\" % name)\n\n    # Step 4: Check that all necessary attributes are defined\n    for name, attr_spec in iface:\n        if not hasattr(candidate,", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(\"Candidate does not claim to provide interface %s\" % iface)\n\n    # Step 2: Check that all necessary methods are defined\n    for name, method in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate does not define method %s\" % name)\n\n    # Step 3: Check that methods have correct signature\n    for name, method in iface:\n        if hasattr(candidate, name):\n            try:\n                candidate_method = getattr(candidate, name)\n                # Compare method signatures (simplified - actual signature comparison would be more complex)\n                if not hasattr(candidate_method, '__call__'):\n                    errors.append(\"Method %s is not callable\" % name)\n            except Exception as e:\n                errors.append(\"Error accessing method %s: %s\" % (name, e))\n\n    # Step 4: Check that all necessary attributes are defined\n    for name, attr in iface:\n        if not hasattr(candidate, name):\n            errors.append(\"Candidate does not define attribute %s\" % name)"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1370, "num_retrieved": 18, "inference_time": 0.9512907117605209, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1390, "num_retrieved": 18, "inference_time": 0.9512907117605209, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, try to determine from bases\n    # In Zope interface context, we typically use ABCInterface or similar\n    # Check if any base class provides a metaclass via __metaclass__\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # Default to the standard metaclass if none found\n    # In the absence of explicit metaclass or metaclass in bases, use the default\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, try to determine from bases\n    # In Zope interface, we typically use ABCInterface or similar\n    # For simplicity, if no explicit metaclass is given and bases are provided,\n    # we fall back to the default metaclass (typically ABCMeta or type)\n    \n    # Check if any base class has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no metaclass found in bases, return the default metaclass\n    # For Zope interface, this is typically ABCMeta or type\n    # Default to type (the standard metaclass)\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, use the metaclass of the first base\n    # or the default metaclass (type) if no bases are provided\n    if not bases:\n        return type\n    \n    # Get the metaclass from the first base class\n    base_metaclass = bases[0].__class__.__class__\n    \n    # If the first base class has a metaclass defined, use it\n    if hasattr(bases[0], '__metaclass__'):\n        return bases[0].__metaclass__\n    \n    # Otherwise, fall back to the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, use the metaclass of the first base\n    # or the default metaclass (type) if no bases are provided\n    if not bases:\n        return type\n    \n    # Get the metaclass from the first base class\n    base_metaclass = bases[0].__class__.__class__ if hasattr(bases[0], '__class__') and hasattr(bases[0].__class__, '__class__') else type\n    \n    # If the first base has a metaclass, use it\n    if hasattr(bases[0], '__metaclass__'):\n        return bases[0].__metaclass__\n    \n    # Otherwise, fall back to the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, use the metaclass of the first base\n    # or default to type if no bases are provided\n    if not bases:\n        return type\n    \n    # Get the metaclass from the first base class\n    base_metaclass = bases[0].__class__.__class__ if hasattr(bases[0], '__class__') and hasattr(bases[0].__class__, '__class__') else type\n    \n    # If the first base has a metaclass, use it\n    if hasattr(bases[0], '__metaclass__'):\n        return bases[0].__metaclass__\n    \n    # Otherwise, fall back to the default metaclass (type)\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 945, "num_retrieved": 3, "inference_time": 0.9512907117605209, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 969, "num_retrieved": 4, "inference_time": 0.9512907117605209, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the (key, value) pair least frequently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Find the key with the minimum frequency\n    min_freq = min(self._freq.values())\n    keys_with_min_freq = [k for k, v in self._freq.items() if v == min_freq]\n    \n    # Choose one of the keys with minimum frequency (arbitrary choice)\n    key = keys_with_min_freq[0]\n    \n    # Remove the key and its value\n    value = self.pop(key)\n    del self._freq[key]\n    \n    return (key, value)\n\nBut since the context does not define any frequency tracking or related attributes (like `_freq`), this implementation cannot be derived from the given context.\n\nThus, based on the retrieved context, the function cannot be implemented as requested.\n\nFinal answer:\ndef popitem(self):\n    raise NotImplementedError(\"LFU cache implementation not available in context.\")", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least frequently used.\n    \"\"\"\n    raise NotImplementedError(\"LFU cache implementation not available in context.\")", "The provided context does not contain any implementation of a cache that removes and returns the `(key, value)` pair least frequently used. The context includes implementations for LRU (Least Recently Used), RR (Random Replacement), and FIFO (First In First Out) caches, but none of them implement a \"Least Frequently Used\" (LFU) cache.\n\nSince the target function `popitem` is supposed to remove and return the pair least frequently used, and no such implementation exists in the provided context, we cannot complete this function based on the given information.\n\nHowever, if we were to implement a basic `popitem` method for an LFU cache (as per the docstring), it would require tracking access frequencies and removing the least frequently used item. But without any context or data structure details for an LFU cache, such an implementation cannot be reliably derived.\n\nTherefore, based on the retrieved context, the function cannot be implemented as requested.\n\n(Note: This is a limitation of the context provided  no LFU cache implementation exists in the retrieved content.)", "The provided context does not contain any implementation of a cache that removes and returns the least frequently used (LFU) item. The context includes implementations for LRU (Least Recently Used), RR (Random Replacement), and FIFO (First In First Out) caches, but none of them implement the LFU (Least Frequently Used) eviction policy.\n\nSince the requested function `popitem` is supposed to remove and return the least frequently used item, and no such implementation exists in the provided context, we cannot generate a valid implementation based solely on the given information.\n\nTherefore, the function cannot be completed as per the requirement.", "def popitem(self):\n    \"\"\"\n    Remove and return the (key, value) pair least frequently used.\n    \"\"\"\n    raise NotImplementedError(\"LFU cache implementation not available in context.\")"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1025, "num_retrieved": 6, "inference_time": 0.9512907117605209, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Find the key with the least recent usage (last in the order)\n    keys = list(self)\n    key = keys[-1]  # Last key in the list is the least recently used\n    value = self.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Find the key with the least recent usage (last in the order)\n    keys = list(self.keys())\n    key = keys[-1]  # Last key in the iteration order is the least recently used\n    value = self.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Find the key with the least recent usage (last in the order)\n    keys = list(self)\n    key = keys[-1]  # Last key in the order is the least recently used\n    value = self.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Find the key with the least recent usage (last in the order)\n    keys = list(self.keys())\n    key = keys[-1]  # Last key in the iteration order\n    value = self.pop(key)\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair least recently used.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Find the key with the least recent usage (last in the order)\n    keys = list(self.keys())\n    key = keys[-1]  # Last key in the iteration order\n    value = self.pop(key)\n    return (key, value)"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1024, "num_retrieved": 6, "inference_time": 0.83138707280159, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"\n    Remove and return the `(key, value)` pair most recently used.\n    \"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 970, "num_retrieved": 4, "inference_time": 0.83138707280159, "generate_results": ["import random\n\ndef popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    keys = list(self.keys())\n    key = random.choice(keys)\n    value = self.pop(key)\n    return (key, value)", "import random\n\ndef popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    keys = list(self.keys())\n    key = random.choice(keys)\n    value = self.pop(key)\n    return (key, value)", "import random\n\ndef popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    keys = list(self.keys())\n    key = random.choice(keys)\n    value = self.pop(key)\n    return (key, value)", "import random\n\ndef popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    keys = list(self.keys())\n    key = random.choice(keys)\n    value = self.pop(key)\n    return (key, value)", "import random\n\ndef popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    keys = list(self.keys())\n    key = random.choice(keys)\n    value = self.pop(key)\n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1057, "num_retrieved": 8, "inference_time": 0.83138707280159, "generate_results": ["def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    import re\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    import re\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    import re\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    import re\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    import re\n    return re.compile(self.param_regex)"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1156, "num_retrieved": 11, "inference_time": 0.83138707280159, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    if self._in_style == self._out_style:\n        return self._converter_class()\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NamedToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NamedToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NumericToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NumericToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    else:\n        raise ValueError(f\"Unsupported parameter style conversion from {self._in_style} to {self._out_style}\")", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    if self._in_style == self._out_style:\n        return self._converter_class()\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NamedToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NamedToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NumericToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NumericToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    else:\n        raise ValueError(f\"Unsupported parameter style conversion from {self._in_style} to {self._out_style}\")", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    if self._in_style == self._out_style:\n        return self._converter_class()\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NamedToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NamedToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NumericToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NumericToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    else:\n        raise ValueError(f\"Unsupported parameter style conversion from {self._in_style} to {self._out_style}\")", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    if self._in_style == self._out_style:\n        return self._converter_class()\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NamedToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NamedToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NumericToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NumericToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    else:\n        raise NotImplementedError(f\"Conversion from {self._in_style} to {self._out_style} is not supported\")", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    if self._in_style == self._out_style:\n        return self._converter_class()\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NamedToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.OrdinalStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NamedStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NamedToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NamedStyle):\n        return NumericToNamedConverter(_in_style=self._in_style, _out_style=self._out_style)\n    elif isinstance(self._in_style, _styles.NumericStyle) and isinstance(self._out_style, _styles.NumericStyle):\n        return NumericToNumericConverter(_in_style=self._in_style, _out_style=self._out_style)\n    else:\n        raise ValueError(f\"Unsupported parameter style conversion from {self._in_style} to {self._out_style}\")"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 888, "num_retrieved": 1, "inference_time": 0.83138707280159, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 824, "num_retrieved": 0, "inference_time": 0.83138707280159, "generate_results": ["def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    if not isinstance(timestr, str):\n        raise TypeError(\"Expected string input for timestr\")\n    \n    # Parse the string using the internal parser\n    result = self._parse(timestr, default=default, ignoretz=ignoretz, tzinfos=tzinfos, **kwargs)\n    \n    # If fuzzy_with_tokens is True, return a tuple (datetime, tokens)\n    if hasattr(result, 'fuzzy_tokens') and result.fuzzy_tokens:\n        return (result.datetime, result.fuzzy_tokens)\n    \n    return result.datetime", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    if not isinstance(timestr, str):\n        raise TypeError(\"Expected string input for timestr\")\n    \n    # Parse the string using the internal parser\n    result = self._parse(timestr, default=default, ignoretz=ignoretz, tzinfos=tzinfos, **kwargs)\n    \n    # If fuzzy_with_tokens is enabled, return a tuple\n    if hasattr(result, 'fuzzy_tokens') and result.fuzzy_tokens:\n        return (result, result.fuzzy_tokens)\n    \n    return result", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 913, "num_retrieved": 3, "inference_time": 0.83138707280159, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous at this point,\n    # we determine if it's in a fold state by checking if it's ambiguous\n    # and if it's in a fold (first occurrence of the ambiguous time)\n    if self.is_ambiguous(dt):\n        # Resolve the ambiguous time by finding the correct index\n        idx = self._find_last_transition(dt)\n        _fold = self._fold(dt)\n        if idx is None or idx == 0:\n            return dt.replace(tzinfo=self)\n        \n        # Adjust index based on fold state\n        idx_offset = int(not _fold and self.is_ambiguous(dt, idx))\n        adjusted_idx = idx - idx_offset\n        \n        # Create a new datetime with the resolved time\n        new_dt = dt.replace(tzinfo=None)\n        # Apply the transition logic to get the correct time\n        # This is a simplified version based on the context\n        # In practice, this would involve more complex timezone logic\n        return new_dt.replace(tzinfo=self)\n    else:\n        # If not ambiguous, simply convert\n        return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous at this point,\n    # we determine if it's in a fold state by checking if it's ambiguous\n    # and if it falls within a fold period.\n    \n    # First, check if the datetime is ambiguous in this timezone\n    if self.is_ambiguous(dt):\n        # If ambiguous, resolve to the correct index by checking fold state\n        idx = self._find_last_transition(dt)\n        _fold = self._fold(dt)\n        \n        if idx is None or idx == 0:\n            # No transition found, return as is\n            return dt.replace(tzinfo=self)\n        \n        # Adjust index based on fold state\n        idx_offset = int(not _fold and self.is_ambiguous(dt, idx))\n        resolved_idx = idx - idx_offset\n        \n        # Create a new datetime with the resolved index\n        # This effectively resolves the ambiguity by selecting the correct occurrence\n        new_dt = dt.replace(tzinfo=None)\n        new_dt = new_dt.replace(tzinfo=self)\n        \n        # Adjust the datetime to the correct transition\n        # This is a simplified representation - actual implementation would\n        # involve adjusting the time based on the transition rules\n        return new_dt.replace(tzinfo=self)\n    else:\n        # If not ambiguous, simply convert the datetime\n        return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous at this point,\n    # we determine if it's in a fold state\n    if self.is_ambiguous(dt):\n        # Resolve the ambiguous time by finding the correct index\n        idx = self._find_last_transition(dt)\n        _fold = self._fold(dt)\n        if idx is None or idx == 0:\n            return dt.replace(tzinfo=self)\n        \n        # Adjust index based on fold state\n        idx_offset = int(not _fold and self.is_ambiguous(dt, idx))\n        adjusted_idx = idx - idx_offset\n        \n        # Create new datetime with adjusted index\n        new_dt = dt.replace(tzinfo=None)\n        new_dt = new_dt.replace(tzinfo=self)\n        \n        # Adjust the time based on the transition index\n        # This is a simplified version - actual implementation would depend on internal transition logic\n        return new_dt\n    \n    # If not ambiguous, simply return the datetime with the new timezone\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous at this point,\n    # we determine if it's in a fold state\n    if self.is_ambiguous(dt):\n        # Resolve the ambiguous time by finding the correct index\n        idx = self._find_last_transition(dt)\n        _fold = self._fold(dt)\n        if idx is None or idx == 0:\n            return dt.replace(tzinfo=self)\n        \n        # Adjust index based on fold state\n        idx_offset = int(not _fold and self.is_ambiguous(dt, idx))\n        idx -= idx_offset\n        \n        # Create new datetime with resolved index\n        new_dt = dt.replace(tzinfo=None)\n        # Apply the transition logic to get the correct time\n        # This is a simplified version based on the context\n        # In practice, this would involve more complex time zone logic\n        return new_dt.replace(tzinfo=self)\n    \n    # If not ambiguous, simply convert\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Since we know the datetime is unambiguous at this point,\n    # we determine if it's in a fold state\n    if self.is_ambiguous(dt):\n        # Resolve the ambiguous time by finding the correct index\n        idx = self._find_last_transition(dt)\n        _fold = self._fold(dt)\n        if idx is None or idx == 0:\n            return dt.replace(tzinfo=self)\n        \n        # Adjust index based on fold state\n        idx_offset = int(not _fold and self.is_ambiguous(dt, idx))\n        adjusted_idx = idx - idx_offset\n        \n        # Create new datetime with adjusted index\n        new_dt = dt.replace(tzinfo=None)\n        new_dt = new_dt.replace(tzinfo=self)\n        \n        # Adjust the datetime to the correct fold\n        if _fold:\n            # In fold state, we need to adjust to the correct occurrence\n            new_dt = new_dt.replace(tzinfo=self)\n        \n        return new_dt\n    else:\n        # If not ambiguous, simply convert\n        return dt.replace(tzinfo=self)"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 918, "num_retrieved": 2, "inference_time": 0.83138707280159, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1038, "num_retrieved": 6, "inference_time": 0.83138707280159, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._before_cut = before\n    self._after_cut = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._before_cut = before\n    self._after_cut = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._before_cut = before\n    self._after_cut = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._before_cut = before\n    self._after_cut = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._before_cut = before\n    self._after_cut = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1038, "num_retrieved": 6, "inference_time": 0.83138707280159, "generate_results": ["def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First try to identify as a public message\n    if identify_protocol(\"is_public_message\", request):\n        return identify_protocol(\"is_public_message\", request)\n    \n    # Then try to identify as a private message\n    if identify_protocol(\"is_private_message\", request):\n        return identify_protocol(\"is_private_message\", request)\n    \n    # Check if it's a legacy payload\n    if identify_protocol(\"is_legacy_payload\", request):\n        return identify_protocol(\"is_legacy_payload\", request)\n    \n    # No suitable protocol found\n    raise NoSuitableProtocolFoundError()", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First try to identify as a public message\n    if identify_protocol(\"is_public_message\", request):\n        return identify_protocol(\"is_public_message\", request)\n    \n    # Then try to identify as a private message\n    if identify_protocol(\"is_private_message\", request):\n        return identify_protocol(\"is_private_message\", request)\n    \n    # Check if it's a legacy payload\n    if identify_protocol(\"is_legacy_payload\", request):\n        return identify_protocol(\"is_legacy_payload\", request)\n    \n    # No suitable protocol found\n    raise NoSuitableProtocolFoundError()", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First try to identify as a public message\n    if identify_protocol(\"is_public_message\", request):\n        return identify_protocol(\"is_public_message\", request)\n    \n    # Then try to identify as a private message\n    if identify_protocol(\"is_private_message\", request):\n        return identify_protocol(\"is_private_message\", request)\n    \n    # Check if it's a legacy payload\n    if identify_protocol(\"is_legacy_payload\", request):\n        return identify_protocol(\"is_legacy_payload\", request)\n    \n    # No suitable protocol found\n    raise NoSuitableProtocolFoundError()", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First try to identify as a public message\n    if identify_protocol(\"is_public_message\", request):\n        return identify_protocol(\"is_public_message\", request)\n    \n    # Then try to identify as a private message\n    if identify_protocol(\"is_private_message\", request):\n        return identify_protocol(\"is_private_message\", request)\n    \n    # Check if it's a legacy payload\n    if identify_protocol(\"is_legacy_payload\", request):\n        return identify_protocol(\"is_legacy_payload\", request)\n    \n    # No suitable protocol found\n    raise NoSuitableProtocolFoundError()", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message first\n    if isinstance(request, dict) and \"public\" in request and request[\"public\"]:\n        return \"public\"\n    \n    # Check if it's a private message\n    if isinstance(request, dict) and \"private\" in request and request[\"private\"]:\n        return \"private\"\n    \n    # Check if it's a legacy payload\n    if isinstance(request, dict) and \"legacy\" in request and request[\"legacy\"]:\n        return \"legacy\"\n    \n    # If none of the above, try to identify using the protocol system\n    try:\n        return identify_protocol(\"identify\", request)\n    except NoSuitableProtocolFoundError:\n        return None"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1108, "num_retrieved": 9, "inference_time": 0.83138707280159, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request)", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request)", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request)", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request)", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    return identify_protocol('request', request)"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1037, "num_retrieved": 6, "inference_time": 0.83138707280159, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from lxml import etree\n    \n    # Format datetime as ISO 8601 with timezone offset\n    if isinstance(dt, datetime):\n        return dt.strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n    else:\n        # Fallback to string if dt is not a datetime object\n        return dt", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from lxml import etree\n    \n    # Format datetime as ISO 8601 with timezone offset\n    if isinstance(dt, datetime):\n        return dt.strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n    else:\n        # Fallback to string if dt is not a datetime object\n        return dt", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from lxml import etree\n    \n    # Format datetime as ISO 8601 with timezone offset\n    if isinstance(dt, datetime):\n        return dt.strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n    else:\n        # Fallback to string if dt is not a datetime object\n        return dt", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    import time\n    \n    # Format datetime as ISO 8601 with timezone offset\n    if isinstance(dt, datetime):\n        return dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n    elif isinstance(dt, float):\n        # Handle timestamp as float (Unix timestamp)\n        return datetime.fromtimestamp(dt).strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n    else:\n        # Default fallback\n        return datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from lxml import etree\n    \n    # Format datetime as ISO 8601 with timezone offset\n    if isinstance(dt, datetime):\n        return dt.strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n    else:\n        # Fallback to string if dt is not a datetime object\n        return dt"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 989, "num_retrieved": 4, "inference_time": 0.83138707280159, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block - skip tag detection\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers and check for tags\n        # Tags are in the format #tag\n        import re\n        tag_pattern = r'#(\\w+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        # Process each match\n        line_without_tags = line\n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            if replacer:\n                # Replace the tag with the result of replacer function\n                line_without_tags = line_without_tags.replace(f'#{tag}', replacer(tag))\n        \n        result_lines.append(line_without_tags)\n    \n    result_text = '\\n'.join(result_lines)\n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with three backticks)\n        if line.strip().startswith(\"```\"):\n            # Inside code block - skip tag detection\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers if any\n        if line.strip().startswith(\"```\") or line.strip().endswith(\"```\"):\n            result_lines.append(line)\n            continue\n            \n        # Extract tags (words starting with #)\n        import re\n        tag_matches = re.finditer(r'#(\\w+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                replacement = replacer(tag)\n                line = line.replace(f'#{tag}', f'{replacement}')\n        \n        result_lines.append(line)\n    \n    result_text = '\\n'.join(result_lines)\n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with three backticks)\n        if line.strip().startswith(\"```\"):\n            # Inside code block - skip tag detection\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers and process the line\n        # Look for tags (e.g., #tag) that are not inside code blocks\n        # We'll use a simple approach: find #tag patterns\n        import re\n        tag_pattern = r'#(\\w+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        # Extract tags and update the result line\n        line_without_tags = line\n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            if replacer:\n                # Replace the tag with the result of the replacer function\n                line_without_tags = line_without_tags.replace(f'#{tag}', replacer(tag))\n        \n        result_lines.append(line_without_tags)\n    \n    # Join the processed lines back into a single string\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block - skip tag detection\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers\n        if line.strip().startswith('```'):\n            # Start of code block\n            in_code_block = True\n            # Skip the rest of the line\n            result_lines.append(line)\n            continue\n        elif line.strip().startswith('```'):\n            # End of code block\n            in_code_block = False\n            result_lines.append(line)\n            continue\n            \n        # Check for tags (e.g., #tag)\n        # We only consider tags that are not inside code blocks\n        if not in_code_block:\n            # Find all hashtags\n            import re\n            tag_matches = re.findall(r'#(\\w+)', line)\n            for tag in tag_matches:\n                tags.add(tag)\n                # If replacer is provided, replace the tag\n                if replacer:\n                    replacement = replacer(tag)\n                    line = line.replace(f'#{tag}', f'#{replacement}')\n        else:\n            # Inside code block - skip tag processing\n            result_lines.append(line)\n    \n    # Reconstruct the text\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with three backticks)\n        if line.strip().startswith(\"```\"):\n            # Inside code block - skip tag detection\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers if any\n        if line.strip().startswith(\"```\"):\n            result_lines.append(line)\n            continue\n            \n        # Find tags (words starting with #)\n        import re\n        tag_matches = re.finditer(r'#(\\w+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            # Replace the tag if replacer is provided\n            if replacer:\n                replacement = replacer(tag)\n                line = line.replace(f'#{tag}', replacement)\n        \n        result_lines.append(line)\n    \n    result_text = '\\n'.join(result_lines)\n    return tags, result_text"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1132, "num_retrieved": 9, "inference_time": 0.83138707280159, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    return process_text_links(text)", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    return process_text_links(text)", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Use the existing function from the context to process text links\n    return find_tags(text)", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    return process_text_links(text)", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    return process_text_links(text)"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1029, "num_retrieved": 6, "inference_time": 0.83138707280159, "generate_results": ["def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        import requests\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n        return None\n    except requests.exceptions.RequestException:\n        return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        import requests\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n        return None\n    except requests.exceptions.RequestException:\n        return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        import requests\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n        return None\n    except requests.exceptions.RequestException:\n        return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        import requests\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        import requests\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n        return None\n    except requests.exceptions.RequestException:\n        return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1169, "num_retrieved": 10, "inference_time": 0.83138707280159, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not contain spaces or special characters (except hyphens and underscores)\n    # They should be alphanumeric with allowed special characters\n    if not re.match(r'^[a-zA-Z0-9_-]+$', tag):\n        return False\n    \n    # Tags should not start or end with a hyphen or underscore\n    if tag.startswith('-') or tag.startswith('_') or tag.endswith('-') or tag.endswith('_'):\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(tag) > 255:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not contain spaces or special characters (except hyphens and underscores)\n    # and should start with a letter or underscore\n    if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', tag):\n        return False\n    \n    # Tags should not be too long (common limit is 255 characters)\n    if len(tag) > 255:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not contain spaces or special characters (except hyphens and underscores)\n    # and should start and end with a letter or digit\n    if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9_-]*[a-zA-Z0-9]$', tag):\n        return False\n    \n    # Tags should not be too long (common limit is 255 characters)\n    if len(tag) > 255:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not contain spaces or special characters (except hyphens and underscores)\n    # and should follow common XML tag naming rules\n    if re.search(r'[^\\w\\-]', tag):\n        return False\n    \n    # Tags should not start or end with a hyphen or underscore\n    if tag.startswith('-') or tag.startswith('_') or tag.endswith('-') or tag.endswith('_'):\n        return False\n    \n    # Tags should not be empty or consist of only whitespace\n    if tag.strip() != tag:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not contain spaces or special characters (except hyphens and underscores)\n    # and should follow common XML tag naming rules\n    if re.search(r'[^\\w\\-]', tag):\n        return False\n    \n    # Tags should not start or end with a hyphen or underscore\n    if tag.startswith('-') or tag.startswith('_') or tag.endswith('-') or tag.endswith('_'):\n        return False\n    \n    # Tags should not be empty or consist of only whitespace\n    if tag.strip() != tag:\n        return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1038, "num_retrieved": 6, "inference_time": 0.7561732530593872, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 985, "num_retrieved": 4, "inference_time": 0.7561732530593872, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\"\"\"\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\"\"\"\n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 964, "num_retrieved": 5, "inference_time": 0.7561732530593872, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    document, code, exception = fetch_document(host=url, path=document_path)\n    if exception:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        data = json.loads(document)\n        return data\n    except (json.JSONDecodeError, TypeError):\n        return None", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    document, code, exception = fetch_document(host=url, path=document_path)\n    if exception:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        data = json.loads(document)\n        return data\n    except (json.JSONDecodeError, TypeError):\n        return None", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    document, code, exception = fetch_document(host=url, path=document_path)\n    if exception:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        data = json.loads(document)\n        return data\n    except (json.JSONDecodeError, TypeError):\n        return None", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    document, code, exception = fetch_document(host=url, path=document_path)\n    if exception:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        data = json.loads(document)\n        return data\n    except (json.JSONDecodeError, TypeError):\n        return None", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict\n    \"\"\"\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    document, code, exception = fetch_document(host=url, path=document_path)\n    if exception:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        data = json.loads(document)\n        return data\n    except (json.JSONDecodeError, TypeError):\n        return None"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1023, "num_retrieved": 5, "inference_time": 0.7561732530593872, "generate_results": ["def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        \n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the XML content to be verified (typically the post or message body)\n        # Assuming the signature is over the XML content, we need to hash it\n        xml_content = etree.tostring(root, encoding='unicode')\n        \n        # Create a hash of the XML content using SHA-256\n        hash_obj = SHA256.new(xml_content.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pk = RSA.import_key(public_key)\n        verifier = pkcs1_15.new(pk)\n        verifier.verify(hash_obj, signature)\n        \n        return True\n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        \n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the XML content to be signed (excluding the envelope)\n        # Assuming the signature is over the post content\n        post_content = root.find(\".//post\")\n        if post_content is None:\n            raise SignatureVerificationError(\"Post element not found in document\")\n            \n        # Create a hash of the post content\n        content = etree.tostring(post_content, encoding='unicode')\n        hash_obj = SHA256.new(content.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pk = RSA.import_key(public_key)\n        verifier = pkcs1_15.new(pk)\n        verifier.verify(hash_obj, signature)\n        \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        \n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Create a hash of the document content\n        hash_obj = SHA256.new()\n        hash_obj.update(etree.tostring(root, encoding='unicode').encode('utf-8'))\n        \n        # Verify the signature using the public key\n        verifier = pkcs1_15.new(RSA.import_key(public_key))\n        verifier.verify(hash_obj, signature)\n        \n        return True\n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        \n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the XML content to be signed (excluding the envelope)\n        # Assuming the signature is over the post content\n        post_content = root.find(\".//post\")\n        if post_content is None:\n            raise SignatureVerificationError(\"Post element not found in document\")\n            \n        # Create a hash of the post content\n        content = etree.tostring(post_content, encoding='unicode')\n        hash_obj = SHA256.new(content.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pk = RSA.import_key(public_key)\n        verifier = pkcs1_15.new(pk)\n        verifier.verify(hash_obj, signature)\n        \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        \n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the XML content to be signed (excluding the envelope)\n        # Assuming the signature is on the post content\n        post_content = root.find(\".//post\")\n        if post_content is None:\n            raise SignatureVerificationError(\"Post element not found in document\")\n            \n        # Create a hash of the post content\n        content = etree.tostring(post_content, encoding='unicode')\n        hash_obj = SHA256.new(content.encode('utf-8'))\n        \n        # Verify the signature using the public key\n        pk = RSA.import_key(public_key)\n        verifier = pkcs1_15.new(pk)\n        verifier.verify(hash_obj, signature)\n        \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 979, "num_retrieved": 4, "inference_time": 0.7561732530593872, "generate_results": ["import json\nimport re\nfrom typing import Dict, Optional\nfrom xml.etree import ElementTree as etree\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if isinstance(data, dict) and 'subject' in data:\n            # Extract handle and host from subject\n            subject = data['subject']\n            if '@' in subject:\n                handle, host = subject.rsplit('@', 1)\n                return {\n                    'handle': handle,\n                    'host': host,\n                    'type': 'json'\n                }\n    except (json.JSONDecodeError, KeyError):\n        pass\n    \n    # Try to parse as XRD (old format)\n    try:\n        # XRD format is XML, typically with <Link> elements\n        root = etree.fromstring(document)\n        \n        # Look for <Link> elements with rel=\"http://webfinger.net/rel/alias\"\n        for link in root.findall('.//{http://webfinger.net/rel/alias}Link'):\n            # Extract href and rel\n            href = link.get('href')\n            if href and '@' in href:\n                handle, host = href.rsplit('@', 1)\n                return {\n                    'handle': handle,\n                    'host': host,\n                    'type': 'xrd'\n                }\n    except etree.ParseError:\n        pass\n    \n    # If no valid handle/host found, return empty dict\n    return {}", "import json\nimport re\nfrom typing import Dict, Optional\nfrom xml.etree import ElementTree as etree\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if isinstance(data, dict) and 'links' in data:\n            # Extract the first link with rel='http://diaspora.github.io/diaspora_federation/rel/alias'\n            alias_link = None\n            for link in data['links']:\n                if link.get('rel') == 'http://diaspora.github.io/diaspora_federation/rel/alias':\n                    alias_link = link\n                    break\n            if alias_link and 'href' in alias_link:\n                return {\n                    'handle': alias_link['href'],\n                    'type': 'json'\n                }\n        return {}\n    except json.JSONDecodeError:\n        pass\n    \n    # Try to parse as XRD (old format)\n    try:\n        # XRD is XML, so parse as XML\n        root = etree.fromstring(document)\n        \n        # Look for the 'link' element with rel='http://diaspora.github.io/diaspora_federation/rel/alias'\n        alias_link = None\n        for link in root.findall('.//{http://docs.oasis-open.org/ns/xri/xrd-1.0}link'):\n            rel = link.get('{http://docs.oasis-open.org/ns/xri/xrd-1.0}rel')\n            if rel == 'http://diaspora.github.io/diaspora_federation/rel/alias':\n                href = link.get('{http://docs.oasis-open.org/ns/xri/xrd-1.0}href')\n                if href:\n                    alias_link = href\n                    break\n        \n        if alias_link:\n            return {\n                'handle': alias_link,\n                'type': 'xrd'\n            }\n    except etree.ParseError:\n        pass\n    \n    return {}", "import json\nimport re\nfrom typing import Dict, Optional\nfrom xml.etree import ElementTree as etree\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if isinstance(data, dict) and 'links' in data:\n            # Extract the first link with rel='http://diaspora.github.io/diaspora_federation/rel/alias'\n            alias_link = None\n            for link in data['links']:\n                if link.get('rel') == 'http://diaspora.github.io/diaspora_federation/rel/alias':\n                    alias_link = link\n                    break\n            if alias_link and 'href' in alias_link:\n                return {\n                    'handle': alias_link['href'],\n                    'type': 'json'\n                }\n        return {}\n    except (json.JSONDecodeError, TypeError):\n        pass\n    \n    # Try to parse as XRD (old format)\n    try:\n        # XRD is XML, so parse as XML\n        root = etree.fromstring(document)\n        \n        # Look for <link> elements with rel=\"http://diaspora.github.io/diaspora_federation/rel/alias\"\n        alias_link = None\n        for link in root.findall('.//{http://docs.oasis-open.org/ns/xri/xrd-1.0}link'):\n            rel = link.get('{http://docs.oasis-open.org/ns/xri/xrd-1.0}rel')\n            if rel == 'http://diaspora.github.io/diaspora_federation/rel/alias':\n                href = link.get('{http://docs.oasis-open.org/ns/xri/xrd-1.0}href')\n                if href:\n                    alias_link = href\n                    break\n        \n        if alias_link:\n            return {\n                'handle': alias_link,\n                'type': 'xrd'\n            }\n    except etree.ParseError:\n        pass\n    \n    return {}", "import json\nimport re\nfrom typing import Dict, Optional\nfrom xml.etree import ElementTree as etree\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if isinstance(data, dict) and 'subject' in data:\n            # Extract subject and links\n            subject = data['subject']\n            links = data.get('links', [])\n            \n            # Validate subject format (diaspora handle)\n            if not validate_handle(subject):\n                raise ValueError(\"Invalid handle format\")\n                \n            # Extract profile link if available\n            profile_link = None\n            for link in links:\n                if link.get('rel') == 'http://webfingers.org/rel/profile':\n                    profile_link = link.get('href')\n                    break\n                    \n            return {\n                'subject': subject,\n                'profile': profile_link\n            }\n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        # XRD format is XML, so parse as XML\n        root = etree.fromstring(document)\n        \n        # Find the 'Link' elements\n        links = []\n        for link in root.findall('.//{http://webfingers.org/rel}link'):\n            href = link.get('href')\n            rel = link.get('rel')\n            if href and rel:\n                links.append({'href': href, 'rel': rel})\n        \n        # Extract subject from the first 'Subject' element\n        subject = None\n        for elem in root.findall('.//{http://webfingers.org/rel}subject'):\n            subject = elem.text\n            break\n            \n        if not subject:\n            # Fallback: look for subject in the root\n            subject = root.get('subject')\n            \n        if not subject:\n            raise ValueError(\"Subject not found in XRD document\")\n            \n        # Validate subject format\n        if not validate_handle(subject):\n            raise ValueError(\"Invalid handle format\")\n            \n        # Extract profile link\n        profile_link = None\n        for link in links:\n            if link['rel'] == 'http://webfingers.org/rel/profile':\n                profile", "import json\nimport re\nfrom typing import Dict, Optional\nfrom xml.etree import ElementTree as etree\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        if isinstance(data, dict) and 'subject' in data and 'links' in data:\n            # Extract the subject and links\n            subject = data['subject']\n            links = data['links']\n            \n            # Validate subject format (handle)\n            if not validate_handle(subject):\n                raise ValueError(\"Invalid handle format in webfinger response\")\n                \n            # Extract the first link with rel='http://diaspora.github.io/diaspora_federation/rel/actor'\n            actor_link = None\n            for link in links:\n                if link.get('rel') == 'http://diaspora.github.io/diaspora_federation/rel/actor':\n                    actor_link = link\n                    break\n                    \n            # Return structured result\n            return {\n                'handle': subject,\n                'actor': actor_link.get('href') if actor_link else None,\n                'links': links\n            }\n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        # XRD format is XML, parse it as XML\n        root = etree.fromstring(document)\n        \n        # Look for the 'Link' elements with rel='http://diaspora.github.io/diaspora_federation/rel/actor'\n        actor_link = None\n        for link in root.findall('.//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link'):\n            rel = link.get('{http://docs.oasis-open.org/ns/xri/xrd-1.0}rel')\n            href = link.get('{http://docs.oasis-open.org/ns/xri/xrd-1.0}href')\n            if rel == 'http://diaspora.github.io/diaspora_federation/rel/actor':\n                actor_link = href\n                break\n                \n        # Extract the subject (handle) from the XRD\n        subject = None\n        for link in root.findall('.//"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 985, "num_retrieved": 6, "inference_time": 0.7561732530593872, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Construct the webfinger URL\n        webfinger_url = f\"http://webfinger.net/{handle}\"\n        # Fetch the document\n        document = fetch_document(webfinger_url)\n        return document[0] if document else None\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Construct the webfinger URL\n        webfinger_url = f\"http://webfinger.net/{handle}\"\n        # Fetch the document\n        document = fetch_document(webfinger_url)\n        return document[0] if document else None\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Construct the webfinger URL\n        webfinger_url = f\"http://webfinger.net/{handle}\"\n        # Fetch the document\n        document = fetch_document(webfinger_url)\n        return document[0] if document else None\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Construct the webfinger URL\n        webfinger_url = f\"http://webfinger.net/{handle}\"\n        # Fetch the document\n        document = fetch_document(webfinger_url)\n        return document[0] if document else None\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Construct the webfinger URL\n        webfinger_url = f\"http://webfinger.net/{handle}\"\n        # Fetch the document\n        document = fetch_document(webfinger_url)\n        return document[0] if document else None\n    except Exception:\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 967, "num_retrieved": 5, "inference_time": 0.7561732530593872, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    try:\n        # Use the webfinger protocol to retrieve the Diaspora profile\n        webfinger_response = try_retrieve_webfinger_document(handle)\n        if not webfinger_response:\n            return None\n        \n        # Extract the profile information from the webfinger response\n        # Diaspora webfinger typically returns a JSON document with profile details\n        profile_info = webfinger_response.get('subject')\n        if not profile_info:\n            return None\n            \n        # Parse and return the profile as a dictionary\n        # The actual parsing logic depends on the structure of the Diaspora webfinger response\n        return {\n            'handle': handle,\n            'profile': profile_info\n        }\n    except Exception as e:\n        logger.warning(f\"Error retrieving Diaspora webfinger for {handle}: {e}\")\n        return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    try:\n        # Use the webfinger protocol to retrieve the Diaspora profile\n        webfinger_response = try_retrieve_webfinger_document(handle)\n        if not webfinger_response:\n            return None\n        \n        # Extract the profile information from the webfinger response\n        # Diaspora webfinger typically returns a link to the profile\n        profile_url = webfinger_response.get('links', [])\n        if not profile_url:\n            return None\n            \n        # Extract the first link that points to a profile\n        profile_link = next((link for link in profile_url if link.get('rel') == 'profile'), None)\n        if not profile_link:\n            return None\n            \n        profile_url = profile_link.get('href')\n        if not profile_url:\n            return None\n            \n        # Retrieve and parse the profile document\n        profile_document = fetch_document(profile_url)\n        if not profile_document:\n            return None\n            \n        # Parse the profile document (assuming it's in a format that can be parsed)\n        # For Diaspora, this is typically JSON or a structured format\n        try:\n            profile_data = json.loads(profile_document)\n        except (json.JSONDecodeError, TypeError):\n            return None\n            \n        # Validate the profile data (basic validation)\n        # In a real implementation, you might validate against Diaspora schema\n        return profile_data\n        \n    except Exception as e:\n        logger.warning(f\"retrieve_and_parse_diaspora_webfinger - error retrieving or parsing webfinger: {e}\")\n        return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    try:\n        # Use the webfinger protocol to retrieve the Diaspora profile\n        webfinger_response = try_retrieve_webfinger_document(handle)\n        if not webfinger_response:\n            return None\n        \n        # Parse the webfinger response to extract the profile information\n        # Diaspora webfinger typically returns a JSON document with profile details\n        profile_data = webfinger_response.get('subject', {})\n        \n        # Extract and structure the profile data as a dictionary\n        profile_dict = {\n            'handle': handle,\n            'url': profile_data.get('url'),\n            'display_name': profile_data.get('displayName'),\n            'email': profile_data.get('email'),\n            'avatar': profile_data.get('avatar'),\n            'bio': profile_data.get('bio'),\n            'location': profile_data.get('location'),\n            'website': profile_data.get('website')\n        }\n        \n        return profile_dict\n        \n    except Exception as e:\n        logger.warning(f\"retrieve_and_parse_diaspora_webfinger - error retrieving or parsing webfinger document for {handle}: {e}\")\n        return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    try:\n        # Use the webfinger protocol to retrieve the Diaspora profile\n        webfinger_response = try_retrieve_webfinger_document(handle)\n        if not webfinger_response:\n            return None\n        \n        # Extract the profile information from the webfinger response\n        # Diaspora webfinger typically returns a JSON response with a 'links' field\n        links = webfinger_response.get('links')\n        if not links:\n            return None\n        \n        # Look for the profile link (usually with rel=\"http://diaspora.com/rel/profile\")\n        profile_link = None\n        for link in links:\n            if link.get('rel') == 'http://diaspora.com/rel/profile':\n                profile_link = link.get('href')\n                break\n        \n        if not profile_link:\n            return None\n        \n        # Fetch the profile document from the retrieved link\n        profile_document = fetch_document(profile_link)\n        if not profile_document:\n            return None\n        \n        # Parse the profile document (assuming it's in a format that can be parsed)\n        # For Diaspora, the profile is typically in JSON format\n        try:\n            profile_data = json.loads(profile_document)\n        except json.JSONDecodeError:\n            return None\n        \n        # Return the parsed profile data\n        return profile_data\n        \n    except Exception as e:\n        logger.warning(f\"retrieve_and_parse_diaspora_webfinger - error retrieving or parsing Diaspora webfinger: {e}\")\n        return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    try:\n        # Use the webfinger protocol to retrieve the Diaspora profile\n        webfinger_response = try_retrieve_webfinger_document(handle)\n        if not webfinger_response:\n            return None\n        \n        # Extract the profile information from the webfinger response\n        # Diaspora webfinger typically returns a JSON document with a 'subject' and 'links' field\n        profile_info = webfinger_response.get('subject')\n        if not profile_info:\n            return None\n            \n        # Parse the profile document (Diaspora profile is typically a JSON document)\n        # The actual profile data is usually found in the 'links' field\n        links = webfinger_response.get('links', [])\n        profile_data = {}\n        \n        for link in links:\n            if link.get('rel') == 'http://diaspora.github.io/diaspora/rel/profile':\n                profile_data['profile_url'] = link.get('href')\n            elif link.get('rel') == 'http://diaspora.github.io/diaspora/rel/actor':\n                profile_data['actor_url'] = link.get('href')\n        \n        # If no profile data found, return None\n        if not profile_data:\n            return None\n            \n        return profile_data\n        \n    except Exception as e:\n        logger.warning(f\"retrieve_and_parse_diaspora_webfinger - error retrieving or parsing webfinger: {e}\")\n        return None"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1067, "num_retrieved": 7, "inference_time": 0.7561732530593872, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the URL for the host-meta document\n    host_meta_url = f\"https://{host}/.well-known/host-meta\"\n    \n    # Fetch the document using the network utility\n    document = fetch_document(host_meta_url)\n    \n    # Parse and return the XRD document\n    if document:\n        # Assuming the document is already in XRD format and can be directly returned\n        # In a real implementation, parsing might be needed based on the structure\n        return document\n    \n    # If no document is found, return None or raise an exception\n    return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Use the fetch_document utility to retrieve the host-meta document from the given host\n    # Assuming the host-meta document is available at a standard path like /host-meta\n    try:\n        # Fetch the host-meta document from the Diaspora host\n        document = fetch_document(f\"https://{host}/host-meta\")\n        # Parse the document and return as XRD instance\n        # Since the exact parsing logic is not specified, we assume it's handled by the parser\n        # In a real implementation, this would involve parsing the XRD format\n        return document\n    except Exception as e:\n        # Handle potential errors in fetching or parsing\n        logging.error(f\"Failed to retrieve host-meta from {host}: {e}\")\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the URL for the host-meta document\n    hostmeta_url = f\"https://{host}/.well-known/host-meta\"\n    \n    # Fetch the document using the network utility\n    document = fetch_document(hostmeta_url)\n    \n    # Parse the document as JSON and return as XRD instance\n    # Assuming the parsing logic is handled by the appropriate parser\n    # and the document is already in the correct format\n    if document:\n        # Return the parsed XRD instance\n        # This would typically involve parsing the JSON structure\n        # and creating an XRD object from it\n        return document\n    else:\n        # Return None if document not found or failed to retrieve\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Use the fetch_document utility to retrieve the host-meta document from the Diaspora host\n    # The host-meta document is typically available at a specific endpoint (e.g., /host-meta)\n    # We assume the base URL is the provided host, and we append the host-meta endpoint\n    host_meta_url = f\"{host}/host-meta\"\n    \n    # Fetch the document from the remote host\n    document = fetch_document(host_meta_url)\n    \n    # Parse and return the XRD instance (assuming the document is already in XRD format)\n    # In a real implementation, this would involve parsing the document and creating an XRD object\n    # For now, we return the document as an XRD instance\n    return document", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the URL for the host-meta document\n    host_meta_url = f\"https://{host}/.well-known/host-meta\"\n    \n    # Fetch the document using the network utility\n    document = fetch_document(host_meta_url)\n    \n    # Parse the document as JSON (assuming it's in JSON format)\n    try:\n        data = json.loads(document)\n    except json.JSONDecodeError:\n        # If JSON parsing fails, return None or handle as appropriate\n        return None\n    \n    # Return the parsed XRD document (assuming it's structured as a dictionary)\n    # In a real implementation, this would be wrapped in an XRD instance\n    return data"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 919, "num_retrieved": 2, "inference_time": 0.7561732530593872, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    import requests\n    \n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    import requests\n    \n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    import requests\n    \n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    import requests\n    \n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    import requests\n    \n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 888, "num_retrieved": 1, "inference_time": 0.7561732530593872, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = current.get(key, {})\n    for k in keys:\n        current = current[k]\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = current.get(key, {})\n    for k in keys:\n        current = current[key]\n        current[k] = current.get(k, {})\n    current[keys[-1]] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = {}\n    current = current[key]\n    for k in keys:\n        current[k] = {}\n        current = current[k]\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = current.get(key, {})\n    for k in keys:\n        current = current[k]\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = current.get(key, {})\n    for k in keys:\n        current = current[k]\n    current[val] = val"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1002, "num_retrieved": 5, "inference_time": 0.7561732530593872, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    def resolve_value(value):\n        if not value:\n            return []\n        if isinstance(value, list):\n            return value\n        if isinstance(value, str):\n            return [value]\n        return []\n    \n    return type('IniType', (), {\n        'resolve': resolve_value\n    })", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    def resolve_value(value):\n        if not value:\n            return []\n        if isinstance(value, list):\n            return value\n        elif isinstance(value, str):\n            return [value]\n        else:\n            return []\n    \n    return type('IniType', (), {\n        'resolve': resolve_value\n    })", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    def resolve_value(value):\n        if not value:\n            return []\n        if isinstance(value, list):\n            return value\n        elif isinstance(value, str):\n            return [value]\n        else:\n            return [str(value)]\n    \n    return type('IniType', (), {\n        'resolve': resolve_value\n    })", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    def resolve_value(value):\n        if not value:\n            return []\n        if isinstance(value, list):\n            return value\n        if isinstance(value, str):\n            return [value]\n        return []\n    \n    return type('IniType', (), {\n        'resolve': resolve_value\n    })", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    def resolve_value(value):\n        if not value:\n            return []\n        if isinstance(value, list):\n            return value\n        if isinstance(value, str):\n            return [value]\n        return []\n    \n    return type('IniType', (), {\n        'resolve': resolve_value\n    })"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1149, "num_retrieved": 10, "inference_time": 0.7561732530593872, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # The context does not provide information about how to obtain the ansible config manager\n    # Based on the pattern in other classes, it's likely tied to CoreServices\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # The context does not provide information about how to obtain the ansible config manager\n    # Based on the pattern in the context, it's likely tied to CoreServices\n    # Since no direct reference to ansible config manager exists in the context,\n    # we assume it's part of CoreServices similar to other managers\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # The context does not provide information about how to obtain the ansible config manager\n    # Based on the pattern in other classes (e.g., WorkspaceManagerSpec, PluginManagerSpec)\n    # that use CoreServices, it's likely the ansible config manager is accessed via CoreServices\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # The context does not provide information about how to obtain the ansible config manager\n    # Based on the pattern in the context, it's likely tied to CoreServices\n    # Since no direct reference to ansible config manager exists in the context,\n    # we assume it's part of CoreServices similar to other managers\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # The context does not provide information about how to obtain the ansible config manager\n    # Based on the pattern in the context, it's likely tied to CoreServices\n    # Since no direct reference to ansible config manager exists in the context,\n    # we assume it's part of CoreServices similar to other managers\n    return CoreServices.ansible_config_manager()"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1281, "num_retrieved": 14, "inference_time": 0.7561732530593872, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # In a real scenario, this would return an instance of WorkspaceManager\n    # The actual implementation would depend on the specific class structure\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # In a real scenario, this would return an instance of WorkspaceManager\n    # The actual implementation would depend on the class's state and dependencies\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # In a real scenario, this would return an instance of WorkspaceManager\n    # The actual implementation would depend on the class's state and dependencies\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # In a real scenario, this would return an instance of WorkspaceManager\n    # The actual implementation would depend on the specific class structure\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # In a real implementation, this would return an instance of WorkspaceManager\n    # or similar service object, likely from CoreServices\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1176, "num_retrieved": 12, "inference_time": 0.7561732530593872, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and already initialized\n    # based on the context where CoreServices.plugins_manager() is used\n    # and the plugin manager is accessed through CoreServices\n    from infrared.core.services import CoreServices\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    from infrared.core.services import CoreServices\n    return CoreServices.plugins_manager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 893, "num_retrieved": 1, "inference_time": 0.7561732530593872, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields (example - adjust based on actual spec structure)\n    required_fields = ['commands', 'options', 'description']  # Example required fields\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added based on specific spec structure\n    # For example, validate commands, options, etc.\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields (example - adjust based on actual spec structure)\n    required_fields = ['commands', 'options', 'description']  # Example required fields\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation can be added here based on specific spec structure\n    # For example, validate commands, options, etc.\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields that should be present in the spec\n    required_fields = [\n        'commands',\n        'options',\n        'description',\n        'version'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate that 'commands' is a list\n    if not isinstance(spec_data['commands'], list):\n        raise IRValidatorException(\"'commands' must be a list\")\n\n    # Validate that each command has required fields\n    for command in spec_data['commands']:\n        if not isinstance(command, dict):\n            raise IRValidatorException(\"'commands' must contain dictionaries\")\n        \n        required_command_fields = ['name', 'description']\n        for field in required_command_fields:\n            if field not in command:\n                raise IRValidatorException(f\"Missing required field in command '{command['name']}': {field}\")\n\n        # Validate options for each command\n        if 'options' not in command:\n            raise IRValidatorException(f\"Missing 'options' field in command '{command['name']}'\")\n\n        options = command['options']\n        if not isinstance(options, list):\n            raise IRValidatorException(\"'options' must be a list in command\")\n\n        for option in options:\n            if not isinstance(option, dict):\n                raise IRValidatorException(\"'options' must contain dictionaries\")\n            \n            required_option_fields = ['name', 'type']\n            for field in required_option_fields:\n                if field not in option:\n                    raise IRValidatorException(f\"Missing required field in option '{option['name']}': {field}\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields that must be present in the spec\n    required_fields = ['commands', 'options', 'description']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n    \n    # Validate that 'commands' is a list\n    if not isinstance(spec_data.get('commands'), list):\n        raise IRValidatorException(\"'commands' must be a list\")\n    \n    # Validate that each command has required fields\n    for command in spec_data['commands']:\n        if not isinstance(command, dict):\n            raise IRValidatorException(\"'commands' must contain dictionaries\")\n        \n        required_command_fields = ['name', 'description', 'options']\n        for field in required_command_fields:\n            if field not in command:\n                raise IRValidatorException(f\"Missing required field in command '{command['name']}': {field}\")\n        \n        # Validate options is a list of dictionaries\n        if not isinstance(command.get('options'), list):\n            raise IRValidatorException(f\"Command '{command['name']}' must have 'options' as a list\")\n        \n        for option in command['options']:\n            if not isinstance(option, dict):\n                raise IRValidatorException(f\"Invalid option format in command '{command['name']}'\")\n            \n            required_option_fields = ['name', 'type']\n            for field in required_option_fields:\n                if field not in option:\n                    raise IRValidatorException(f\"Missing required field in option '{option['name']}': {field}\")\n    \n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        import yaml\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields based on typical spec structure\n    required_fields = [\n        'commands', \n        'options', \n        'description', \n        'version'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate commands section\n    if not spec_data.get('commands'):\n        raise IRValidatorException(\"Missing 'commands' section in spec\")\n\n    # Validate options section\n    if not spec_data.get('options'):\n        raise IRValidatorException(\"Missing 'options' section in spec\")\n\n    # Validate description\n    if not spec_data.get('description'):\n        raise IRValidatorException(\"Missing 'description' field\")\n\n    # Validate version\n    if not spec_data.get('version'):\n        raise IRValidatorException(\"Missing 'version' field\")\n\n    return spec_data"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 954, "num_retrieved": 3, "inference_time": 0.7561732530593872, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.load(file, Loader=yaml.FullLoader)\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file: {str(e)}\")\n\n    # Validate required fields (example - actual required fields should be defined based on context)\n    # This is a placeholder - specific validation rules should be implemented based on required fields\n    if data is None:\n        raise IRValidatorException(\"YAML file is empty or malformed\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise IRValidatorException(f\"File not found: {yaml_file}\")\n\n    with open(yaml_file, 'r') as file:\n        try:\n            data = yaml.load(file, Loader=yaml.FullLoader)\n        except yaml.YAMLError as e:\n            raise IRValidatorException(f\"Error parsing YAML file: {str(e)}\")\n\n    # Validate required fields (example - adjust based on actual requirements)\n    required_fields = ['required_field_1', 'required_field_2']  # Replace with actual required fields\n    for field in required_fields:\n        if field not in data:\n            raise IRValidatorException(f\"Mandatory field '{field}' is missing in YAML file\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.load(file, Loader=yaml.FullLoader)\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file: {str(e)}\")\n\n    # Validate required fields (example - adjust based on actual requirements)\n    required_fields = ['required_field1', 'required_field2']  # Replace with actual required fields\n    for field in required_fields:\n        if field not in data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.load(file, Loader=yaml.FullLoader)\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file: {str(e)}\")\n\n    # Validate required fields (example - actual required fields should be defined based on context)\n    # This is a placeholder - specific required fields should be defined based on the actual requirements\n    if data is None:\n        raise IRValidatorException(\"YAML file is empty or malformed\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.load(file, Loader=yaml.FullLoader)\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file: {str(e)}\")\n\n    # Validate required fields (example - actual required fields should be defined based on context)\n    # This is a placeholder - specific validation rules should be implemented based on required fields\n    if data is None:\n        raise IRValidatorException(\"YAML file is empty or invalid\")\n\n    return data"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 988, "num_retrieved": 4, "inference_time": 0.7323543578386307, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name in shared_groups:\n            continue\n        # Add the group to shared_groups if not already present\n        shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name in shared_groups:\n            continue\n        # Add the group to shared_groups if it's not already present\n        shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name in shared_groups:\n            continue\n        # Add the group to shared_groups if it's not already present\n        shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name in shared_groups:\n            continue\n        # Add the group to shared_groups if it's not already present\n        shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name in shared_groups:\n            continue\n        # Add the group to shared_groups if it's not already present\n        shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1143, "num_retrieved": 9, "inference_time": 0.7323543578386307, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_dict\n    \n    # Iterate over commands in the spec dictionary\n    for command_name, command_spec in spec_dict.get('commands', {}).items():\n        # Get default values from command specification\n        command_defaults = command_spec.get('defaults', {})\n        for option_name, option_value in command_defaults.items():\n            defaults[(command_name, option_name)] = option_value\n        \n        # Handle subcommands and their defaults\n        if 'subcommands' in command_spec:\n            for subcommand_name, subcommand_spec in command_spec['subcommands'].items():\n                subcommand_defaults = subcommand_spec.get('defaults', {})\n                for option_name, option_value in subcommand_defaults.items():\n                    defaults[(command_name, subcommand_name, option_name)] = option_value\n    \n    # Merge with any global defaults\n    global_defaults = spec_dict.get('global_defaults', {})\n    for option_name, option_value in global_defaults.items():\n        defaults[('global', option_name)] = option_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_dict\n    \n    # Iterate through the main parsers and subparsers to collect default values\n    for parser_name, parser_dict in spec_dict.get('parsers', {}).items():\n        # Get default values from the parser specification\n        parser_defaults = parser_dict.get('defaults', {})\n        defaults.update(parser_defaults)\n        \n        # Handle subparsers and their defaults\n        if 'subparsers' in parser_dict:\n            for subparser_name, subparser_dict in parser_dict['subparsers'].items():\n                subparser_defaults = subparser_dict.get('defaults', {})\n                defaults.update(subparser_defaults)\n    \n    # Add defaults from command-specific specifications\n    for command_name, command_dict in spec_dict.get('commands', {}).items():\n        command_defaults = command_dict.get('defaults', {})\n        defaults.update(command_defaults)\n        \n        # Handle arguments for each command\n        if 'args' in command_dict:\n            for arg_name, arg_spec in command_dict['args'].items():\n                if 'default' in arg_spec:\n                    defaults[f\"{command_name}.{arg_name}\"] = arg_spec['default']\n    \n    # Merge with any external defaults (e.g., environment variables or config files)\n    # This would typically come from external sources like config files or environment\n    # For now, we only resolve from spec\n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_dict\n    \n    # Get default values from spec dictionary\n    for command_name, command_spec in spec_dict.get('commands', {}).items():\n        command_defaults = {}\n        for option_name, option_spec in command_spec.get('options', {}).items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                command_defaults[option_name] = default_value\n        \n        # Add command-specific defaults\n        if command_defaults:\n            defaults[command_name] = command_defaults\n    \n    # Merge with any global defaults if present\n    global_defaults = spec_dict.get('global_defaults', {})\n    for option_name, default_value in global_defaults.items():\n        defaults.setdefault('global', {})[option_name] = default_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_dict\n    \n    # Get default values from spec dictionary\n    for command_name, command_spec in spec_dict.get('commands', {}).items():\n        command_defaults = {}\n        for option_name, option_spec in command_spec.get('options', {}).items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                command_defaults[option_name] = default_value\n        \n        # Add command-specific defaults\n        if command_defaults:\n            defaults[command_name] = command_defaults\n    \n    # Merge with any global defaults if present\n    global_defaults = spec_dict.get('global_defaults', {})\n    for option_name, default_value in global_defaults.items():\n        defaults.setdefault('global', {})[option_name] = default_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_dict\n    \n    # Get default values from spec dictionary\n    for command_name, command_spec in spec_dict.get('commands', {}).items():\n        defaults[command_name] = {}\n        for option_name, option_spec in command_spec.get('options', {}).items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[command_name][option_name] = default_value\n    \n    # Merge with any external defaults (if available)\n    # This is a placeholder - actual implementation would depend on external sources\n    # For now, we only resolve from the spec\n    \n    return defaults"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1051, "num_retrieved": 6, "inference_time": 0.7323543578386307, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 949, "num_retrieved": 4, "inference_time": 0.7323543578386307, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\"\"\"\n    # Combine both argument sources\n    all_args = {**cli_args, **answer_file_args}\n    \n    # Define deprecated arguments (example - this should be configured based on actual project needs)\n    deprecated_args = {\n        'old_option': 'This argument is deprecated and will be removed in a future version.',\n        'legacy_flag': 'The legacy_flag is no longer supported and has been replaced with new_option.'\n    }\n    \n    # Check for deprecated arguments\n    for arg, message in deprecated_args.items():\n        if arg in all_args:\n            print(f\"DEPRECATION WARNING: {message}\")\n            # Optionally, you can remove or warn about the argument being used\n            # For example, you might want to remove it from the final args dict\n            # del all_args[arg]\n    \n    # Return True if any deprecations were found, False otherwise\n    return len(deprecated_args) > 0", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\"\"\"\n    # Combine both argument sources\n    all_args = {**cli_args, **answer_file_args}\n    \n    # Define deprecated arguments (example - this should be configured based on actual context)\n    deprecated_args = {\n        'old_option': 'This option is deprecated and will be removed in a future version.',\n        'legacy_flag': 'The legacy_flag is no longer supported and has been replaced with new_option.'\n    }\n    \n    # Check for deprecated arguments\n    deprecated_found = False\n    for arg, message in deprecated_args.items():\n        if arg in all_args:\n            print(f\"DEPRECATION WARNING: {message}\")\n            deprecated_found = True\n    \n    # Return True if any deprecations were found, False otherwise\n    return deprecated_found", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\"\"\"\n    # Combine both argument sources\n    all_args = {**cli_args, **answer_file_args}\n    \n    # Define deprecated arguments (example - this should be configured based on actual context)\n    deprecated_args = {\n        'old_option': 'This option is deprecated and will be removed in a future version.',\n        'legacy_flag': 'The legacy_flag is no longer supported and has been replaced with new_option.'\n    }\n    \n    # Check for deprecated arguments\n    for arg, message in deprecated_args.items():\n        if arg in all_args:\n            print(f\"DEPRECATION WARNING: {message}\")\n            # Optionally, you could remove or warn about the argument here\n            # For now, just print the warning\n    \n    # If no deprecations found, print a message\n    if not deprecated_args:\n        print(\"No deprecated arguments found.\")", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\"\"\"\n    # Combine both argument sources\n    all_args = {**cli_args, **answer_file_args}\n    \n    # Define deprecated arguments (example - actual list should be defined elsewhere)\n    deprecated_args = {\n        'old_option': 'This option is deprecated and will be removed in a future version.',\n        'legacy_flag': 'The legacy_flag is no longer supported and has been replaced with new_option.'\n    }\n    \n    # Check for deprecated arguments\n    for arg, message in deprecated_args.items():\n        if arg in all_args:\n            print(f\"DEPRECATION WARNING: {message}\")\n            # Optionally, you could remove or warn about the argument here\n            # For now, just print the warning\n    \n    # If no deprecations found, print a message\n    if not deprecated_args:\n        print(\"No deprecated arguments found.\")", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\"\"\"\n    # Combine both argument sources\n    all_args = {**cli_args, **answer_file_args}\n    \n    # Define deprecated arguments (example - this should be configured based on actual context)\n    deprecated_args = {\n        'old_option': 'This argument is deprecated and will be removed in a future version.',\n        'legacy_flag': 'The legacy flag is no longer supported and has been replaced with new options.'\n    }\n    \n    # Check for deprecated arguments\n    deprecated_found = False\n    for arg, message in deprecated_args.items():\n        if arg in all_args:\n            print(f\"DEPRECATION WARNING: {message}\")\n            deprecated_found = True\n    \n    # Return True if any deprecations were found, False otherwise\n    return deprecated_found"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 960, "num_retrieved": 3, "inference_time": 0.7323543578386307, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1026, "num_retrieved": 5, "inference_time": 0.7323543578386307, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for spec_parser in self.spec_helper.iterate_parsers():\n        if spec_parser['name'] == command_name:\n            arg_spec = self.spec_helper.get_option_spec(spec_parser['name'], argument_name)\n            return arg_spec\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for spec_parser in self.spec_helper.iterate_parsers():\n        if spec_parser['name'] == command_name:\n            arg_spec = self.spec_helper.get_option_spec(spec_parser['name'], argument_name)\n            return arg_spec\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for spec_parser in self.spec_helper.iterate_parsers():\n        if spec_parser['name'] == command_name:\n            arg_spec = self.spec_helper.get_option_spec(spec_parser['name'], argument_name)\n            return arg_spec\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for spec_parser in self.spec_helper.iterate_parsers():\n        if spec_parser['name'] == command_name:\n            arg_spec = self.spec_helper.get_option_spec(spec_parser['name'], argument_name)\n            return arg_spec\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for spec_parser in self.spec_helper.iterate_parsers():\n        if spec_parser['name'] == command_name:\n            arg_spec = self.spec_helper.get_option_spec(spec_parser['name'], argument_name)\n            return arg_spec\n    return None"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 988, "num_retrieved": 4, "inference_time": 0.7323543578386307, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for command_name in args:\n        if isinstance(args[command_name], dict):\n            for option_name in args[command_name]:\n                option_spec = self.get_option_spec(command_name, option_name)\n                if option_spec and option_spec.get('action', '') == 'store_true':\n                    option_value = args[command_name][option_name]\n                    if option_value is False:\n                        silent_args.append(option_name)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for command_name in args:\n        if isinstance(args[command_name], dict):\n            for option_name in args[command_name]:\n                option_spec = self.get_option_spec(command_name, option_name)\n                if option_spec and option_spec.get('action', '') == 'store_true':\n                    option_value = args[command_name][option_name]\n                    if option_value is False:\n                        silent_args.append(option_name)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for command_name in args:\n        if isinstance(args[command_name], dict):\n            for option_name in args[command_name]:\n                option_spec = self.get_option_spec(command_name, option_name)\n                if option_spec and option_spec.get('action', '') in ['store_true'] and args[command_name][option_name] is False:\n                    silent_args.append(option_name)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for command_name in args:\n        if isinstance(args[command_name], dict):\n            for option_name in args[command_name]:\n                option_spec = self.get_option_spec(command_name, option_name)\n                if option_spec and option_spec.get('action', '') == 'store_true':\n                    option_value = args[command_name][option_name]\n                    if option_value is False:\n                        silent_args.append(option_name)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for command_name in args:\n        if isinstance(args[command_name], dict):\n            for option_name in args[command_name]:\n                option_spec = self.get_option_spec(command_name, option_name)\n                if option_spec and option_spec.get('action', '') == 'store_true':\n                    option_value = args[command_name][option_name]\n                    if option_value is False:\n                        silent_args.append(option_name)\n    return silent_args"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1057, "num_retrieved": 6, "inference_time": 0.7323543578386307, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 989, "num_retrieved": 4, "inference_time": 0.7323543578386307, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        condition_args = required_when.split(',')\n        condition_met = False\n        \n        # Check if the condition is met based on the provided args\n        for cond_arg in condition_args:\n            cond_arg = cond_arg.strip()\n            if cond_arg.startswith('not_'):\n                # Check if the argument is not set\n                cond_name = cond_arg[4:]\n                if cond_name not in args.get(command_name, {}):\n                    condition_met = True\n                    break\n            else:\n                # Check if the argument is set\n                cond_name = cond_arg\n                if cond_name not in args.get(command_name, {}):\n                    condition_met = False\n                    break\n                else:\n                    condition_met = True\n                    \n        # If condition is met, add the option to matched_args\n        if condition_met:\n            matched_args.append(option_name)\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition = required_when\n        \n        # Check if the condition is met\n        condition_met = False\n        condition_parts = condition.split(',')\n        \n        for part in condition_parts:\n            part = part.strip()\n            if '=' in part:\n                key, value = part.split('=', 1)\n                # Check if the key exists in args and has the specified value\n                if key in args.get(command_name, {}) and str(args[command_name][key]) == value:\n                    condition_met = True\n                    break\n            else:\n                # Check if the key exists in args (without value)\n                if key in args.get(command_name, {}):\n                    condition_met = True\n                    break\n        \n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # required_when is a dict with keys 'when' and 'value'\n        when_condition = required_when.get('when')\n        value = required_when.get('value')\n        \n        # Check if the condition is met\n        if when_condition and value:\n            # Evaluate the condition based on args\n            # For now, assume when_condition is a string that can be evaluated\n            # in the context of args\n            condition_met = False\n            if when_condition == 'is_set':\n                condition_met = args.get(command_name, {}).get(value, False)\n            elif when_condition == 'is_not_set':\n                condition_met = value not in args.get(command_name, {})\n            elif when_condition == 'is_true':\n                condition_met = args.get(command_name, {}).get(value, False)\n            elif when_condition == 'is_false':\n                condition_met = args.get(command_name, {}).get(value, True) is False\n            else:\n                # Handle other conditions if needed\n                condition_met = False\n                \n            if condition_met:\n                matched_args.append(option_name)\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        condition_args = required_when.split(',')\n        condition_met = False\n        \n        # Check if the condition is met based on the provided args\n        for condition_arg in condition_args:\n            condition_arg = condition_arg.strip()\n            if condition_arg.startswith('!'):\n                # Negated condition\n                negated_condition = condition_arg[1:]\n                condition_value = args.get(command_name, {}).get(negated_condition, False)\n                if condition_value:\n                    condition_met = False\n                    break\n            else:\n                # Positive condition\n                condition_value = args.get(command_name, {}).get(condition_arg, False)\n                if condition_value:\n                    condition_met = True\n                    break\n        \n        # If condition is met, add the option to matched_args\n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"List arguments with ``required_when`` condition matched.\"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n        \n        # Extract the condition from required_when\n        condition_args = required_when.split(',')\n        condition_met = False\n        \n        # Check if the condition is met based on the provided args\n        for cond_arg in condition_args:\n            cond_arg = cond_arg.strip()\n            if cond_arg.startswith('!'):\n                # Negated condition\n                negated_cond = cond_arg[1:].strip()\n                if negated_cond in args.get(command_name, {}):\n                    condition_met = False\n                    break\n            else:\n                # Positive condition\n                if cond_arg in args.get(command_name, {}):\n                    condition_met = True\n                    break\n        \n        # If condition is met, add the argument to matched list\n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 992, "num_retrieved": 4, "inference_time": 0.7323543578386307, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, str):\n            # Assuming length specification is in the args dictionary with key ending in '_max_length'\n            max_length_key = key + '_max_length'\n            if max_length_key in args:\n                max_length = args[max_length_key]\n                if len(value) > max_length:\n                    raise ValueError(f\"Value for '{key}' exceeds maximum length of {max_length}\")\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, str):\n            # Assuming length specification is in the args dictionary with key ending in '_max_len'\n            max_len_key = key + '_max_len'\n            if max_len_key in args:\n                max_len = args[max_len_key]\n                if len(value) > max_len:\n                    raise ValueError(f\"Value for '{key}' exceeds the maximum length of {max_len}\")\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, str):\n            # Assuming length specification is in the args dictionary with key ending in '_max_len'\n            max_len_key = key + '_max_len'\n            if max_len_key in args:\n                max_len = args[max_len_key]\n                if len(value) > max_len:\n                    raise ValueError(f\"Value for '{key}' exceeds maximum length of {max_len}\")\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, str):\n            # Assuming length specification is part of the args structure\n            # For example, if args contains a key like 'max_length' for each argument\n            max_length = args.get(f\"{key}_max_length\")\n            if max_length is not None:\n                if len(value) > max_length:\n                    raise ValueError(f\"Value for '{key}' exceeds maximum length of {max_length}\")\n    return True", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for key, value in args.items():\n        if isinstance(value, str):\n            # Assuming length specification is in the args dictionary with key ending in '_max_len'\n            max_len_key = key + '_max_len'\n            if max_len_key in args:\n                max_len = args[max_len_key]\n                if len(value) > max_len:\n                    raise ValueError(f\"Value for '{key}' exceeds maximum length of {max_len}\")\n    return True"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 994, "num_retrieved": 4, "inference_time": 0.7323543578386307, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, option_name, available_choices in self.get_choice_options():\n        if option_name not in args.get(command_name, {}):\n            continue\n            \n        option_value = args[command_name][option_name]\n        if option_value not in available_choices:\n            raise IRInvalidChoiceException([\n                (command_name, option_value, available_choices)\n            ])", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, option_name, available_choices in self.get_choice_options():\n        if option_name not in args.get(command_name, {}):\n            continue\n            \n        option_value = args[command_name][option_name]\n        if option_value not in available_choices:\n            raise IRInvalidChoiceException([\n                (command_name, option_value, available_choices)\n            ])", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, option_name, available_choices in self.get_choice_options():\n        if option_name not in args.get(command_name, {}):\n            continue\n            \n        option_value = args[command_name][option_name]\n        if option_value not in available_choices:\n            raise IRInvalidChoiceException([\n                (command_name, option_value, available_choices)\n            ])", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, option_name, available_choices in self.get_choice_options():\n        if option_name not in args.get(command_name, {}):\n            continue\n            \n        option_value = args[command_name][option_name]\n        if option_value not in available_choices:\n            raise IRInvalidChoiceException([\n                (command_name, option_value, available_choices)\n            ])", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, option_name in self.get_option_specs().items():\n        if option_name in args.get(command_name, {}):\n            option_value = args[command_name][option_name]\n            option_spec = self.get_option_spec(command_name, option_name)\n            if option_spec and 'choices' in option_spec:\n                available_choices = option_spec['choices']\n                if option_value not in available_choices:\n                    invalid_options = [(command_name, option_value, available_choices)]\n                    raise IRInvalidChoiceException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 991, "num_retrieved": 4, "inference_time": 0.7323543578386307, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Placeholder implementation - actual logic depends on expected argument structure\n    # This function should validate that specific argument values are within min/max bounds\n    # Since no specific command or argument names are provided in context, \n    # this is a generic implementation that could be extended based on specific use cases\n    \n    for command_name, option_dict in args.items():\n        for option_name, option_value in option_dict.items():\n            # Example: if we had min/max defined for an option, we would check here\n            # This is a stub - actual implementation would require knowing min/max values\n            pass\n    \n    # Return True if all values are within bounds, False otherwise\n    # This is a minimal implementation - real use would require specific min/max definitions\n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Placeholder implementation - actual logic depends on context of expected arguments\n    # This function should validate that specific argument values are within min/max bounds\n    # Since no specific arguments or bounds are defined in the context, \n    # this implementation is minimal and would need to be customized based on actual use case\n    \n    # Example: if there were specific min/max values defined for certain arguments\n    # For instance, if we were validating a 'count' argument with min=1, max=100\n    # we would check args.get('count') against those bounds\n    \n    # As no specific argument names or bounds are provided, we return True by default\n    # In a real implementation, this would check specific keys against defined min/max values\n    return True", "def validate_min_max_args(self, args):\n    \"\"\"Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Assuming the function is meant to validate a specific argument with min and max constraints\n    # Since no specific command or option names are provided, we'll assume a generic validation\n    # for a single argument with min and max values defined in the args dictionary\n    \n    for command_name, option_dict in args.items():\n        for option_name, option_value in option_dict.items():\n            # Check if min and max values are defined for this option\n            min_val = option_dict.get('min', None)\n            max_val = option_dict.get('max', None)\n            \n            if min_val is not None and max_val is not None:\n                if not (min_val <= option_value <= max_val):\n                    raise ValueError(f\"Value {option_value} for '{option_name}' is not between min {min_val} and max {max_val}\")\n    \n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Placeholder implementation - actual logic depends on specific argument validation requirements\n    # This function should validate that each argument's value falls within specified min and max bounds\n    # Since specific min/max values are not provided in the context, this implementation is generic\n    \n    for key, value in args.items():\n        # Assuming min and max values are stored in a configuration or passed as part of the context\n        # This is a stub - actual implementation would require min/max definitions\n        if not isinstance(value, (int, float)):\n            raise ValueError(f\"Argument '{key}' must be a number\")\n        \n        # Example: if min and max were defined in a class attribute or config\n        # min_val = self.min_values.get(key)\n        # max_val = self.max_values.get(key)\n        # if min_val is not None and value < min_val:\n        #     raise ValueError(f\"Argument '{key}' value {value} is below minimum {min_val}\")\n        # if max_val is not None and value > max_val:\n        #     raise ValueError(f\"Argument '{key}' value {value} is above maximum {max_val}\")\n    \n    return True", "def validate_min_max_args(self, args):\n    \"\"\"Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Assuming the function is meant to validate specific arguments with min/max constraints\n    # Since no specific command or argument names are provided, this is a generic implementation\n    # that checks for a common pattern where min and max values are expected in the args dict\n    \n    for key in args:\n        # If the key has a 'min' and 'max' attribute or is expected to have them\n        if key.endswith('_min') or key.endswith('_max'):\n            continue  # Skip min/max keys themselves\n            \n        value = args[key]\n        # Check if value is a number\n        if not isinstance(value, (int, float)):\n            continue  # Skip non-numeric values\n            \n        # Check if min and max values exist for this key\n        min_val = args.get(f\"{key}_min\")\n        max_val = args.get(f\"{key}_max\")\n        \n        if min_val is not None and max_val is not None:\n            if not isinstance(min_val, (int, float)) or not isinstance(max_val, (int, float)):\n                continue  # Skip if min or max is not numeric\n                \n            if value < min_val or value > max_val:\n                raise ValueError(f\"Value {value} for {key} is not between min {min_val} and max {max_val}\")\n    \n    return True"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1016, "num_retrieved": 5, "inference_time": 0.7323543578386307, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create a complex type instance based on the type name\n    complex_type = None\n    \n    # Check if the type name matches a known complex type\n    if type_name == 'Value':\n        complex_type = Value()\n    else:\n        # For other complex types, instantiate the base ComplexType\n        complex_type = ComplexType()\n        \n        # Set the argument name and option specifications\n        complex_type.arg_name = option_name\n        complex_type.spec_option = spec_option\n    \n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create the complex type instance based on the type name\n    if type_name == 'Value':\n        return Value(option_name)\n    else:\n        # For other complex types, instantiate with the option name and spec\n        complex_type = ComplexType()\n        complex_type.arg_name = option_name\n        complex_type.spec_option = spec_option\n        return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create a complex type instance based on the type_name\n    complex_type = None\n    \n    # Check if the type_name matches a known complex type\n    if type_name == 'Value':\n        complex_type = Value()\n    else:\n        # For other complex types, instantiate the base ComplexType\n        complex_type = ComplexType()\n        \n        # Set the argument name and option specifications\n        complex_type.arg_name = option_name\n        complex_type.spec_option = spec_option\n    \n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create a complex type instance based on the type_name\n    complex_type = None\n    \n    # Check if the type_name is 'Value' or another complex type\n    if type_name == 'Value':\n        complex_type = Value()\n    else:\n        # For other complex types, instantiate the appropriate class\n        # Assuming the complex type is defined in the ComplexType hierarchy\n        complex_type = ComplexType()\n    \n    # Set the argument name and option specifications\n    complex_type.arg_name = option_name\n    complex_type.spec_option = spec_option\n    \n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create a complex type instance based on the type name\n    complex_type = None\n    \n    # Check if the type name is 'Value' (simple type)\n    if type_name == 'Value':\n        complex_type = Value()\n    else:\n        # For other complex types, instantiate the appropriate class\n        # Assuming the complex type is defined in the ComplexType hierarchy\n        complex_type = ComplexType()\n    \n    # Set the argument name for the complex type\n    complex_type.arg_name = option_name\n    \n    # Set the option specifications if available\n    if spec_option:\n        complex_type.spec_option = spec_option\n    \n    # Set the subcommand if needed\n    complex_type.subcommand = subcommand\n    \n    return complex_type"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 951, "num_retrieved": 3, "inference_time": 0.7323543578386307, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    for arg in args:\n        if arg.startswith('--additional-args'):\n            # Parse additional-args format: option1=value1;option2=value2\n            additional_args = arg[len('--additional-args'):]\n            if ';' in additional_args:\n                pairs = additional_args.split(';')\n            else:\n                pairs = [additional_args]\n            \n            for pair in pairs:\n                pair = pair.strip()\n                if '=' in pair:\n                    name, value = pair.split('=', 1)\n                    custom_args[name] = value\n                else:\n                    custom_args[pair] = None\n        else:\n            # Assume standard arguments: control or nested\n            # Use a simple heuristic: if it starts with --control, it's control\n            # Otherwise, it's nested\n            if arg.startswith('--control'):\n                key = arg[len('--control'):].strip()\n                control_args[key] = True\n            else:\n                # Default to nested if not control\n                key = arg.strip()\n                if key.startswith('--'):\n                    key = key[2:]\n                nested_args[key] = True\n\n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    \n    for arg in args:\n        if arg.startswith('--additional-args'):\n            # Parse additional-args format: option1=value1;option2=value2\n            additional_args = arg.split('=', 1)[1] if '=' in arg else ''\n            if additional_args:\n                for item in additional_args.split(';'):\n                    item = item.strip()\n                    if '=' in item:\n                        name, value = item.split('=', 1)\n                        nested_args[name] = value\n                    else:\n                        nested_args[item] = True\n        elif arg.startswith('--'):\n            # Split the argument into name and value\n            name = arg[2:]  # Remove the leading --\n            if '=' in name:\n                name, value = name.split('=', 1)\n                # Determine if it's a control or nested argument\n                # If it starts with 'control', it's a control argument\n                if name.startswith('control'):\n                    control_args[name[len('control'):]] = value\n                else:\n                    nested_args[name] = value\n            else:\n                # If no value, treat as a boolean flag\n                if name.startswith('control'):\n                    control_args[name[len('control'):]] = True\n                else:\n                    nested_args[name] = True\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    \n    for arg in args:\n        if arg.startswith('--additional-args'):\n            # Parse additional-args format: option1=value1;option2=value2\n            additional_args = arg.split('=', 1)[1] if '=' in arg else ''\n            if additional_args:\n                for item in additional_args.split(';'):\n                    item = item.strip()\n                    if '=' in item:\n                        name, value = item.split('=', 1)\n                        nested_args[name] = value\n                    else:\n                        nested_args[item] = True\n        elif arg.startswith('--'):\n            # Split the argument into name and value\n            name = arg[2:]  # Remove the leading --\n            if '=' in name:\n                name, value = name.split('=', 1)\n                # Determine if it's a control or nested argument\n                # If it starts with 'control', it's a control argument\n                if name.startswith('control'):\n                    control_args[name[7:]] = value\n                else:\n                    nested_args[name] = value\n            else:\n                # If no value, treat as a boolean flag\n                if name.startswith('control'):\n                    control_args[name[7:]] = True\n                else:\n                    nested_args[name] = True\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    \n    for arg in args:\n        if arg.startswith('--additional-args'):\n            # Parse additional-args format: option1=value1;option2=value2\n            additional_args = arg[len('--additional-args'):]\n            if ';' in additional_args:\n                pairs = additional_args.split(';')\n            else:\n                pairs = [additional_args]\n            \n            for pair in pairs:\n                pair = pair.strip()\n                if '=' in pair:\n                    name, value = pair.split('=', 1)\n                    nested_args[name] = value\n                else:\n                    nested_args[pair] = True\n        else:\n            # Assume all other arguments are control arguments\n            # Split by '=' to handle key=value format\n            if '=' in arg:\n                key, value = arg.split('=', 1)\n                control_args[key] = value\n            else:\n                control_args[arg] = True\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    for arg in args:\n        if arg.startswith('--additional-args'):\n            # Parse additional-args format: option1=value1;option2=value2\n            additional_args = arg.split('=', 1)[1] if '=' in arg else ''\n            if additional_args:\n                for item in additional_args.split(';'):\n                    item = item.strip()\n                    if '=' in item:\n                        name, value = item.split('=', 1)\n                        custom_args[name] = value\n                    else:\n                        custom_args[item] = None\n        elif arg.startswith('--'):\n            # Split the argument into name and value\n            name = arg[2:].split('=', 1)[0]\n            value = arg[2:].split('=', 1)[1] if '=' in arg else None\n            \n            # Determine if it's a control or nested argument\n            # Control arguments are those that don't follow the nested pattern\n            # Nested arguments are typically under a specific key structure\n            # For simplicity, assume all non-additional-args are control args\n            # unless they match a known nested pattern\n            \n            # Since no specific pattern is defined, we'll treat all non-additional-args\n            # as control arguments\n            control_args[name] = value\n    \n    return control_args, custom_args"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 992, "num_retrieved": 4, "inference_time": 0.7323543578386307, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n\n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            dict_insert(vars_dict, extra_var, *extra_var.keys())\n        else:\n            # Handle case where extra_var is not a dict\n            # This is a fallback; ideally, extra_vars should be dicts\n            pass\n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            # Use dict_insert to handle nested keys\n            dict_insert(vars_dict, extra_var, *extra_var.keys())\n        else:\n            # Handle case where extra_var is not a dict\n            # This is a fallback; ideally, extra_vars should be dicts\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            # Use dict_insert to handle nested keys\n            dict_insert(vars_dict, extra_var, *extra_var.keys())\n        else:\n            # Handle case where extra_var is not a dict\n            # This is a fallback; ideally extra_vars should be dicts\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            # Use dict_insert to handle nested keys\n            dict_insert(vars_dict, extra_var, *extra_var.keys())\n        else:\n            # Handle case where extra_var is not a dict\n            # This is a fallback; ideally, extra_vars should be dicts\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            # Use dict_insert to handle nested keys\n            dict_insert(vars_dict, extra_var, *extra_var.keys())\n        else:\n            # Handle case where extra_var is not a dict\n            # This is a fallback; ideally extra_vars should be dicts\n            pass\n    \n    return vars_dict"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 886, "num_retrieved": 1, "inference_time": 0.7323543578386307, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Ensure playbook path exists\n    if not os.path.exists(playbook_path):\n        raise FileNotFoundError(f\"Playbook file not found: {playbook_path}\")\n    \n    # Prepare ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join([f\"--extra-vars='{k}={v}'\" for k, v in extra_vars.items()])\n        cmd.extend(vars_str.split())\n    \n    # Add additional arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend([f\"--{key}\", str(value)])\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            check=False,\n            cwd=ir_workspace.get_root_dir()\n        )\n        \n        if result.returncode != 0:\n            print(f\"Ansible playbook failed with error: {result.stderr}\")\n            return False\n        \n        print(f\"Ansible playbook executed successfully: {result.stdout}\")\n        return True\n        \n    except Exception as e:\n        print(f\"Error executing ansible-playbook: {str(e)}\")\n        return False", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n\n    # Prepare the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n\n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n\n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join([f'--extra-vars=\"{k}={v}\"' for k, v in extra_vars.items()])\n        cmd.extend(vars_str.split())\n\n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend([f'--{key}', str(value)])\n\n    # Execute the command in the context of the workspace\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.get_path(),\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        if result.returncode == 0:\n            print(\"Ansible playbook executed successfully\")\n            return result.stdout\n        else:\n            print(f\"Ansible playbook failed with error: {result.stderr}\")\n            return result.stderr\n    except Exception as e:\n        print(f\"Error executing ansible-playbook: {str(e)}\")\n        return str(e)", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Ensure playbook path exists\n    if not os.path.exists(playbook_path):\n        raise FileNotFoundError(f\"Playbook file not found: {playbook_path}\")\n    \n    # Prepare ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join([f\"--extra-vars='{k}={v}'\" for k, v in extra_vars.items()])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend([f\"--{key}\", str(value)])\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            check=False,\n            cwd=ir_workspace.get_root_dir()\n        )\n        \n        if result.returncode != 0:\n            print(f\"Ansible playbook failed with error: {result.stderr}\")\n            return False\n        \n        print(f\"Ansible playbook executed successfully: {result.stdout}\")\n        return True\n        \n    except Exception as e:\n        print(f\"Error executing ansible-playbook: {str(e)}\")\n        return False", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Ensure playbook path exists\n    if not os.path.exists(playbook_path):\n        raise FileNotFoundError(f\"Playbook file not found: {playbook_path}\")\n    \n    # Prepare ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join([f\"--extra-vars='{k}={v}'\" for k, v in extra_vars.items()])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend([f\"--{key}\", str(value)])\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            check=False,\n            cwd=ir_workspace.get_root_dir()\n        )\n        \n        if result.returncode != 0:\n            print(f\"Ansible playbook failed with error: {result.stderr}\")\n            return False\n        \n        print(f\"Ansible playbook executed successfully: {result.stdout}\")\n        return True\n        \n    except Exception as e:\n        print(f\"Error executing ansible-playbook: {str(e)}\")\n        return False", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Ensure playbook path exists\n    if not os.path.exists(playbook_path):\n        raise FileNotFoundError(f\"Playbook file not found: {playbook_path}\")\n    \n    # Prepare ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join([f\"--extra-vars='{k}={v}'\" for k, v in extra_vars.items()])\n        cmd.extend(vars_str.split())\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            cmd.extend([f\"--{key}\", str(value)])\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            check=False,\n            cwd=ir_workspace.get_root_path()\n        )\n        \n        if result.returncode != 0:\n            print(f\"Ansible playbook failed with error: {result.stderr}\")\n            return False\n        \n        print(f\"Ansible playbook executed successfully: {result.stdout}\")\n        return True\n        \n    except Exception as e:\n        print(f\"Error executing ansible-playbook: {str(e)}\")\n        return False"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 940, "num_retrieved": 3, "inference_time": 0.8927492300669352, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Construct ansible command with provided arguments and extra vars\n    ansible_command = [\"ansible\", \"--extra-vars\", str(vars_dict)]\n    \n    # Add additional CLI arguments from cli_args\n    ansible_command.extend(cli_args)\n    \n    # Execute ansible command using the plugin's execution capabilities\n    return ir_plugin.run_command(ansible_command, workspace=ir_workspace)", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Construct ansible command with provided arguments and extra vars\n    ansible_command = [\"ansible\", \"--extra-vars\", str(vars_dict)]\n    \n    # Add additional CLI arguments from cli_args\n    ansible_command.extend(cli_args)\n    \n    # Execute ansible command using the plugin's execution capabilities\n    # Assuming the plugin has a method to run ansible commands\n    return ir_plugin.run_ansible_command(ansible_command, workspace=ir_workspace)", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Construct ansible command with provided arguments and extra vars\n    ansible_command = [\"ansible\", \"--extra-vars\", str(vars_dict)]\n    \n    # Append additional CLI arguments from cli_args\n    ansible_command.extend(cli_args)\n    \n    # Execute ansible command using the plugin's environment or workspace context\n    # This is a placeholder - actual implementation would depend on how ansible is invoked\n    # in the Infrared environment (e.g., via subprocess or ansible module)\n    \n    import subprocess\n    try:\n        result = subprocess.run(\n            ansible_command,\n            capture_output=True,\n            text=True,\n            check=False,\n            env=ir_workspace.get_env()\n        )\n        return {\n            \"stdout\": result.stdout,\n            \"stderr\": result.stderr,\n            \"returncode\": result.returncode\n        }\n    except Exception as e:\n        return {\n            \"stdout\": \"\",\n            \"stderr\": str(e),\n            \"returncode\": -1\n        }", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Construct ansible command with provided arguments and extra vars\n    ansible_command = [\"ansible\", \"--extra-vars\", str(vars_dict)]\n    \n    # Add additional CLI arguments from cli_args\n    ansible_command.extend(cli_args)\n    \n    # Execute ansible command using the plugin's execution capabilities\n    return ir_plugin.run_ansible_command(ansible_command, workspace=ir_workspace)", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Construct ansible command with provided arguments and extra vars\n    ansible_command = [\"ansible\", \"--extra-vars\", str(vars_dict)]\n    \n    # Add additional CLI arguments from cli_args\n    ansible_command.extend(cli_args)\n    \n    # Execute ansible command using the plugin's execution capabilities\n    # Assuming the plugin has a method to run ansible commands\n    return ir_plugin.run_ansible_command(ansible_command, workspace=ir_workspace)"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 950, "num_retrieved": 3, "inference_time": 0.8927492300669352, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.name in values_dict:\n            value = values_dict[option.name]\n            # Convert based on option type if specified\n            if option.type == 'int':\n                values_dict[option.name] = int(value)\n            elif option.type == 'bool':\n                values_dict[option.name] = value.lower() in ['true', '1', 'yes', 'on']\n            elif option.type == 'float':\n                values_dict[option.name] = float(value)\n            elif option.type == 'str':\n                values_dict[option.name] = str(value)\n            # Add more type conversions as needed\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.name in values_dict:\n            value = values_dict[option.name]\n            # Convert based on option type if specified\n            if option.type == 'int':\n                values_dict[option.name] = int(value)\n            elif option.type == 'bool':\n                values_dict[option.name] = value.lower() in ['true', '1', 'yes', 'on']\n            elif option.type == 'float':\n                values_dict[option.name] = float(value)\n            elif option.type == 'str':\n                values_dict[option.name] = str(value)\n            # Add more type conversions as needed\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.name in values_dict:\n            value = values_dict[option.name]\n            # Convert based on option type (e.g., int, bool, float, etc.)\n            if option.type == 'int':\n                values_dict[option.name] = int(value)\n            elif option.type == 'bool':\n                values_dict[option.name] = value.lower() in ['true', '1', 'yes', 'on']\n            elif option.type == 'float':\n                values_dict[option.name] = float(value)\n            elif option.type == 'choice':\n                values_dict[option.name] = value\n            # Default to string if no specific type\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.get('type'):\n            # Assuming type is a string representation of a Python type\n            # e.g., 'int', 'bool', 'float', 'str'\n            value = values_dict.get(option['name'])\n            if value is not None:\n                try:\n                    if option['type'] == 'int':\n                        values_dict[option['name']] = int(value)\n                    elif option['type'] == 'float':\n                        values_dict[option['name']] = float(value)\n                    elif option['type'] == 'bool':\n                        values_dict[option['name']] = value.lower() in ('true', '1', 'yes', 'on')\n                    elif option['type'] == 'str':\n                        values_dict[option['name']] = str(value)\n                    # Add more types as needed\n                except (ValueError, TypeError):\n                    # Handle conversion errors gracefully\n                    pass\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    options = self.get_parser_option_specs(parser_name)\n    for option in options:\n        if option.name in values_dict:\n            value = values_dict[option.name]\n            # Convert based on option type (e.g., int, bool, float, etc.)\n            if option.type == 'int':\n                values_dict[option.name] = int(value)\n            elif option.type == 'bool':\n                values_dict[option.name] = value.lower() in ['true', '1', 'yes', 'on']\n            elif option.type == 'float':\n                values_dict[option.name] = float(value)\n            elif option.type == 'choice':\n                values_dict[option.name] = value\n            # Default to string if no specific type\n    return values_dict"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 990, "num_retrieved": 4, "inference_time": 0.8927492300669352, "generate_results": ["import os\nimport configparser\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    config_path = os.path.join(plugin_dir, 'plugin.spec')\n    spec_dict = {}\n    \n    if not os.path.exists(config_path):\n        return spec_dict\n    \n    config = configparser.ConfigParser()\n    config.read(config_path)\n    \n    for section in config.sections():\n        for key, value in config.items(section):\n            spec_dict[f\"{section}.{key}\"] = value\n    \n    return spec_dict", "import os\nimport configparser\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    config_path = os.path.join(plugin_dir, 'plugin.spec')\n    spec_dict = {}\n    \n    if not os.path.exists(config_path):\n        return spec_dict\n    \n    config = configparser.ConfigParser()\n    config.read(config_path)\n    \n    for section in config.sections():\n        for key, value in config.items(section):\n            spec_dict[f\"{section}.{key}\"] = value\n    \n    return spec_dict", "import os\nimport configparser\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    config_path = os.path.join(plugin_dir, 'plugin.spec')\n    spec_dict = {}\n    \n    if not os.path.exists(config_path):\n        return spec_dict\n    \n    config = configparser.ConfigParser()\n    config.read(config_path)\n    \n    for section in config.sections():\n        for key, value in config.items(section):\n            spec_dict[key] = value\n    \n    return spec_dict", "import os\nimport configparser\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    config_path = os.path.join(plugin_dir, 'plugin.spec')\n    spec_dict = {}\n    \n    if not os.path.exists(config_path):\n        return spec_dict\n    \n    config = configparser.ConfigParser()\n    config.read(config_path)\n    \n    for section in config.sections():\n        for key, value in config.items(section):\n            spec_dict[f\"{section}.{key}\"] = value\n    \n    return spec_dict", "import os\nimport configparser\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    config_path = os.path.join(plugin_dir, 'plugin.spec')\n    spec_dict = {}\n    \n    if not os.path.exists(config_path):\n        return spec_dict\n    \n    config = configparser.ConfigParser()\n    config.read(config_path)\n    \n    for section in config.sections():\n        for key, value in config.items(section):\n            spec_dict[key] = value\n    \n    return spec_dict"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 963, "num_retrieved": 4, "inference_time": 0.8927492300669352, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INIR_CONFIG_PATH'):\n        # Assuming the config path is derived from the common directory\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        os.environ['INIR_CONFIG_PATH'] = os.path.join(common_path, 'config')", "def inject_config(self):\n    \"\"\"Set the environment variable for config path, if it is undefined.\"\"\"\n    common_path = pkg.resource_filename(version.VersionInfo('infrared'), 'common')\n    config_path = os.environ.get('INIR_CONFIG_PATH', '')\n    if not config_path:\n        os.environ['INIR_CONFIG_PATH'] = os.path.join(common_path, 'config')", "def inject_config(self):\n    \"\"\"Set the environment variable for config path, if it is undefined.\"\"\"\n    if not os.environ.get('INIR_CONFIG_PATH'):\n        # Assuming the config path is derived from the common directory\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        os.environ['INIR_CONFIG_PATH'] = os.path.join(common_path, 'config')", "def inject_config(self):\n    \"\"\"Set the environment variable for config path, if it is undefined.\"\"\"\n    common_path = pkg.resource_filename(version.VersionInfo('infrared'), 'common')\n    config_path = os.environ.get('INIR_CONFIG_PATH', '')\n    if not config_path:\n        os.environ['INIR_CONFIG_PATH'] = os.path.join(common_path, 'config')", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INIR_CONFIG_PATH'):\n        # Assuming the config path is derived from the common directory\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        os.environ['INIR_CONFIG_PATH'] = os.path.join(common_path, 'config')"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1073, "num_retrieved": 7, "inference_time": 0.8927492300669352, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\"\"\"\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific spec details\n    # and how the cli options are defined in the spec dictionary\n    \n    # Assuming the spec has a 'subparsers' section or other CLI definitions\n    # that need to be extended to the root_subparsers\n    \n    # Example: if there are subparsers defined in the spec, add them\n    # This is a simplified version based on the context provided\n    \n    spec_dict = self.spec_helper.spec_dict\n    \n    # Add subparsers if defined\n    if 'subparsers' in spec_dict:\n        subparsers_dict = spec_dict['subparsers']\n        for subparser_name, subparser_config in subparsers_dict.items():\n            # Create a subparser with the given name and configuration\n            subparser = root_subparsers.add_parser(\n                subparser_name,\n                help=subparser_config.get('help', '')\n            )\n            \n            # Extend the subparser with any additional options or arguments\n            # This would typically be handled by the spec's specific implementation\n            if 'args' in subparser_config:\n                for arg in subparser_config['args']:\n                    subparser.add_argument(arg['name'], **arg.get('kwargs', {}))\n    \n    # Add shared groups if defined\n    if 'shared_groups' in spec_dict:\n        shared_groups = spec_dict['shared_groups']\n        for group in shared_groups:\n            # Add shared groups to the root parser\n            # This would typically involve creating argparse groups\n            # and adding them to the subparser\n            pass\n    \n    # Add any other CLI options defined in the spec\n    # This would be handled by the specific spec implementation\n    # For now, we just return as the base implementation\n    return", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific spec details\n    # and how the cli options are defined in the spec dictionary\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\"\"\"\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\"\"\"\n    pass"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 1024, "actual_tokens": 1230, "num_retrieved": 16, "inference_time": 0.8927492300669352, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if not path.startswith(root):\n        raise ValueError(f\"Path {path} does not start with root {root}\")\n    return path[len(root):]"]}
