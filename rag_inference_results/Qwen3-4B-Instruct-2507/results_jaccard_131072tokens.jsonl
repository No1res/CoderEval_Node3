{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 116757, "num_retrieved": 500, "inference_time": 28.122745290398598, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Nanoseconds must be int\")\n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, 1000000000)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Nanoseconds must be int\")\n    \n    if nanoseconds < 0 or nanoseconds >= 86400000000000:\n        raise ValueError(\"Nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, 1000000000)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time(hour, minute, second, nanosecond, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 110903, "num_retrieved": 500, "inference_time": 28.122745290398598, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta data.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta data.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 110063, "num_retrieved": 500, "inference_time": 28.122745290398598, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Time with %d dimensions\" % dim)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 106039, "num_retrieved": 500, "inference_time": 28.122745290398598, "generate_results": ["def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' or 'Y' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' and coordinates\n    \"\"\"\n    srid = value.srid\n    coordinates = value.coordinates\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns: Structure with tag 'X' and fields (srid, *coordinates)\n    \"\"\"\n    srid = value.srid\n    coordinates = value\n    return Structure(b\"X\", srid, *coordinates)"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 101213, "num_retrieved": 500, "inference_time": 28.122745290398598, "generate_results": ["def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140429, "num_retrieved": 305, "inference_time": 28.122745290398598, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if isinstance(protocol_version, tuple) and len(protocol_version) == 2:\n            # Check for specific versions in the code context\n            # For example, (3, 5), (4, 1), (4, 2), (4, 3), (4, 4), (5, 1), (5, 3)\n            if protocol_version == (3, 5):\n                handlers[protocol_version] = Bolt3\n            elif protocol_version == (4, 1):\n                handlers[protocol_version] = Bolt4x1\n            elif protocol_version == (4, 2):\n                handlers[protocol_version] = Bolt4x2\n            elif protocol_version == (4, 3):\n                handlers[protocol_version] = Bolt4x3\n            elif protocol_version == (4, 4):\n                handlers[protocol_version] = Bolt4x4\n            elif protocol_version == (5, 1):\n                handlers[protocol_version] = Bolt5x1\n            elif protocol_version == (5, 3):\n                handlers[protocol_version] = Bolt5x3\n            else:\n                handlers = {}\n        return handlers\n    \n    # If no specific version is provided, return all available handlers\n    handlers = {\n        (3, 5): Bolt3,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4,", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if isinstance(protocol_version, tuple) and len(protocol_version) == 2:\n            # Check for specific versions in the code context\n            # For example, (3, 5), (4, 1), (4, 2), (4, 3), (4, 4), (5, 1), (5, 3)\n            if protocol_version == (3, 5):\n                handlers[protocol_version] = Bolt3\n            elif protocol_version == (4, 1):\n                handlers[protocol_version] = Bolt4x1\n            elif protocol_version == (4, 2):\n                handlers[protocol_version] = Bolt4x2\n            elif protocol_version == (4, 3):\n                handlers[protocol_version] = Bolt4x3\n            elif protocol_version == (4, 4):\n                handlers[protocol_version] = Bolt4x4\n            elif protocol_version == (5, 1):\n                handlers[protocol_version] = Bolt5x1\n            elif protocol_version == (5, 3):\n                handlers[protocol_version] = Bolt5x3\n            else:\n                handlers = {}\n        return handlers\n    \n    # If no specific version is provided, return all available versions\n    handlers = {\n        (3, 5): Bolt3,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4,", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if isinstance(protocol_version, tuple) and len(protocol_version) == 2:\n            # Check for specific versions\n            if protocol_version == (3, 5):\n                handlers[protocol_version] = Bolt3\n            elif protocol_version == (4, 0):\n                handlers[protocol_version] = Bolt4x0\n            elif protocol_version == (4, 1):\n                handlers[protocol_version] = Bolt4x1\n            elif protocol_version == (4, 2):\n                handlers[protocol_version] = Bolt4x2\n            elif protocol_version == (4, 3):\n                handlers[protocol_version] = Bolt4x3\n            elif protocol_version == (4, 4):\n                handlers[protocol_version] = Bolt4x4\n            elif protocol_version == (5, 1):\n                handlers[protocol_version] = Bolt5x1\n            elif protocol_version == (5, 3):\n                handlers[protocol_version] = Bolt5x3\n        return handlers\n    \n    # Return all available handlers if no specific version is provided\n    handlers = {\n        (3, 5): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5,", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if isinstance(protocol_version, tuple) and len(protocol_version) == 2:\n            # Check for specific protocol versions\n            if protocol_version == (3, 5):\n                handlers[protocol_version] = Bolt3\n            elif protocol_version == (4, 0):\n                handlers[protocol_version] = Bolt4x0\n            elif protocol_version == (4, 1):\n                handlers[protocol_version] = Bolt4x1\n            elif protocol_version == (4, 2):\n                handlers[protocol_version] = Bolt4x2\n            elif protocol_version == (4, 3):\n                handlers[protocol_version] = Bolt4x3\n            elif protocol_version == (4, 4):\n                handlers[protocol_version] = Bolt4x4\n            elif protocol_version == (5, 1):\n                handlers[protocol_version] = Bolt5x1\n            elif protocol_version == (5, 3):\n                handlers[protocol_version] = Bolt5x3\n        return handlers\n    \n    # Return all available versions if no specific version is provided\n    handlers = {\n        (3, 5): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if isinstance(protocol_version, tuple) and len(protocol_version) == 2:\n            # Check for specific protocol versions\n            if protocol_version == (3, 5):\n                handlers[protocol_version] = Bolt3\n            elif protocol_version == (4, 0):\n                handlers[protocol_version] = Bolt4x0\n            elif protocol_version == (4, 1):\n                handlers[protocol_version] = Bolt4x1\n            elif protocol_version == (4, 2):\n                handlers[protocol_version] = Bolt4x2\n            elif protocol_version == (4, 3):\n                handlers[protocol_version] = Bolt4x3\n            elif protocol_version == (4, 4):\n                handlers[protocol_version] = Bolt4x4\n            elif protocol_version == (5, 1):\n                handlers[protocol_version] = Bolt5x1\n            elif protocol_version == (5, 3):\n                handlers[protocol_version] = Bolt5x3\n        return handlers\n    \n    # Return all available handlers if no specific version is provided\n    handlers = {\n        (3, 5): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 139003, "num_retrieved": 260, "inference_time": 28.122745290398598, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a new function with the metadata and timeout attributes\n            new_func = func\n            if metadata is not None:\n                new_func.metadata = metadata\n            if timeout is not None:\n                new_func.timeout = timeout\n            return new_func(*args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        # Attach metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a new function with the metadata and timeout attributes\n            func.metadata = metadata\n            func.timeout = timeout\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        # Attach metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # Create a new function with the metadata and timeout attributes\n            new_func = func\n            # Set the metadata and timeout attributes on the function\n            if metadata is not None:\n                new_func.metadata = metadata\n            if timeout is not None:\n                new_func.timeout = timeout\n            return new_func(*args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 124085, "num_retrieved": 500, "inference_time": 28.122745290398598, "generate_results": ["def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._list.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._list.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._values.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._list.index(key)\n    except ValueError:\n        return -1"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 143424, "num_retrieved": 400, "inference_time": 28.122745290398598, "generate_results": ["def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return list(self._values.values())\n    \n    values = []\n    for key in keys:\n        if isinstance(key, str):\n            values.append(self._values[key])\n        else:\n            values.append(self._values[key])\n    return values", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return list(self._values.values())\n    \n    values = []\n    for key in keys:\n        if isinstance(key, str):\n            values.append(self._values[key])\n        else:\n            values.append(self._values[key])\n    return values", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return list(self._values.values())\n    \n    values = []\n    for key in keys:\n        if isinstance(key, str):\n            values.append(self._values[key])\n        else:\n            values.append(self._values[key])\n    return values", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return list(self._values.values())\n    \n    values = []\n    for key in keys:\n        if isinstance(key, str):\n            values.append(self._values[key])\n        else:\n            values.append(self._values[key])\n    return values", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return list(self._values.values())\n    \n    values = []\n    for key in keys:\n        if isinstance(key, str):\n            values.append(self._values[key])\n        else:\n            values.append(self._values[key])\n    return values"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140453, "num_retrieved": 305, "inference_time": 28.122745290398598, "generate_results": ["def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if keys:\n        for key in keys:\n            if isinstance(key, int):\n                if key < 0 or key >= len(self._values):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self._keys[key]] = self._values[key]\n            else:\n                record_dict[key] = self._values[self._keys.index(key)]\n    else:\n        record_dict = dict(zip(self._keys, self._values))\n    \n    # If keys provided in the items that are not in the record, insert with None\n    # This is handled by the dict constructor which will insert missing keys with None\n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if keys:\n        # If keys are provided, use them to get values\n        for key in keys:\n            if isinstance(key, int):\n                # Handle index\n                if key < 0:\n                    # Negative index - convert to positive\n                    key = len(self._values) + key\n                if key >= len(self._values):\n                    raise IndexError(f\"Index {key} is out of bounds for record with {len(self._values)} values\")\n                record_dict[self._keys[key]] = self._values[key]\n            else:\n                # Handle key name\n                if key not in self._keys:\n                    record_dict[key] = None\n                else:\n                    record_dict[key] = self._values[self._keys.index(key)]\n    else:\n        # If no keys are provided, return all values\n        record_dict = dict(zip(self._keys, self._values))\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if not keys:\n        # If no keys are provided, return all values\n        for key, value in self.items():\n            record_dict[key] = value\n    else:\n        # If keys are provided, include only those values\n        for key in keys:\n            if isinstance(key, int):\n                # Handle index-based access\n                if key < 0:\n                    # Negative index - convert to positive\n                    key = len(self._keys) + key\n                if key >= len(self._keys):\n                    raise IndexError(f\"Index {key} is out of bounds for record with {len(self._keys)} keys\")\n                record_dict[self._keys[key]] = self._values[key]\n            else:\n                # Handle key-based access\n                if key not in self._keys:\n                    record_dict[key] = None\n                else:\n                    record_dict[key] = self._values[self._keys.index(key)]\n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if keys:\n        # If keys are provided, use them to get values\n        for key in keys:\n            if isinstance(key, int):\n                # Index access\n                if key < 0:\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                if key >= len(self._values):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self._keys[key]] = self._values[key]\n            else:\n                # Key access\n                if key not in self._keys:\n                    record_dict[key] = None\n                else:\n                    record_dict[key] = self._values[self._keys.index(key)]\n    else:\n        # If no keys are provided, return all values\n        for i, key in enumerate(self._keys):\n            record_dict[key] = self._values[i]\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if keys:\n        # If keys are provided, use them to get values\n        for key in keys:\n            if isinstance(key, int):\n                # Index access\n                if key < 0:\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                if key >= len(self._values):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self._keys[key]] = self._values[key]\n            else:\n                # Key access\n                if key not in self._keys:\n                    record_dict[key] = None\n                else:\n                    record_dict[key] = self._values[self._keys.index(key)]\n    else:\n        # If no keys are provided, return all values\n        record_dict = dict(zip(self._keys, self._values))\n    \n    return record_dict"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 145881, "num_retrieved": 482, "inference_time": 28.122745290398598, "generate_results": ["def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read a 16-bit unsigned integer\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 16-bit unsigned integer\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read a 16-bit unsigned integer\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read a 16-bit unsigned integer\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 16-bit unsigned integer\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 141119, "num_retrieved": 326, "inference_time": 28.122745290398598, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 139949, "num_retrieved": 289, "inference_time": 28.122745290398598, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port,\n              \" \".join(map(repr, fields)))\n    self._append(b\"\\x11\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port,\n              \" \".join(map(repr, fields)))\n    self._append(b\"\\x11\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x03\", fields,\n                 response=Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x11\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not mode:\n        mode = \"w\"\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\x11\", fields,\n                 Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 119984, "num_retrieved": 500, "inference_time": 28.122745290398598, "generate_results": ["def round_half_to_even(n):\n    \"\"\"\n    Rounds a number to the nearest integer, with ties rounded to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even number\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even number\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even number\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 144129, "num_retrieved": 423, "inference_time": 28.122745290398598, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            super().__init__(*coordinates)\n            # Store the class name and fields for potential use in hydration\n            self._name = name\n            self._fields = fields\n            \n        def __repr__(self):\n            return f\"{self._name}({self._fields})\"\n            \n        def __eq__(self, other):\n            if not isinstance(other, PointSubclass):\n                return False\n            return self._fields == other._fields\n            \n        def __hash__(self):\n            return hash(self._fields)\n            \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            \"\"\"Create an instance of this point type with the given SRID.\"\"\"\n            if srid not in srid_map:\n                raise ValueError(f\"SRID {srid} not found in srid_map\")\n            return cls(*coordinates)\n            \n    # Register the class with the srid_map for hydration\n    srid_map[name] = (PointSubclass, len(fields))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            super().__init__(*coordinates)\n            # Store the class name and fields for potential use in hydration\n            self._name = name\n            self._fields = fields\n            \n        def __repr__(self):\n            return f\"{self._name}({self._fields})\"\n            \n        def __eq__(self, other):\n            if not isinstance(other, PointSubclass):\n                return False\n            return self._fields == other._fields\n            \n        def __hash__(self):\n            return hash(self._fields)\n            \n    # Register this class with the srid table\n    srid_map[srid_map.get(name, 0)] = (PointSubclass, len(self._fields))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointType(Point):\n        def __init__(self, *coordinates):\n            super().__init__(*coordinates)\n            # Store the fields and srid_map for potential use in hydration\n            self._fields = fields\n            self._srid_map = srid_map\n            \n        def __repr__(self):\n            return f\"{name}({self._fields})\" if self._fields else f\"{name}()\"\n            \n        def __eq__(self, other):\n            if not isinstance(other, PointType):\n                return False\n            return self._fields == other._fields\n            \n        def __hash__(self):\n            return hash(self._fields)\n            \n    # Register the new Point subclass with the srid_map\n    srid_map[name] = (PointType, len(self._fields))\n    \n    return PointType", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            super().__init__(*coordinates)\n            # Store the class name and fields for potential use in hydration\n            self._name = name\n            self._fields = fields\n            \n        def __repr__(self):\n            return f\"{self._name}({self._fields})\"\n            \n        def __eq__(self, other):\n            if not isinstance(other, PointSubclass):\n                return False\n            return self._fields == other._fields\n            \n        def __hash__(self):\n            return hash(self._fields)\n            \n    # Register the new class with the srid_map for hydration purposes\n    srid_map[PointSubclass.__name__] = (PointSubclass, len(self._fields))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            super().__init__(*coordinates)\n            # Store the name and fields for this point type\n            self._name = name\n            self._fields = fields\n            \n        def __repr__(self):\n            return f\"{self._name}({self._fields})\" if self._fields else f\"{self._name}()\"\n            \n        def __eq__(self, other):\n            if not isinstance(other, PointSubclass):\n                return False\n            return self._fields == other._fields and super().__eq__(other)\n            \n        def __hash__(self):\n            return hash((self._name, self._fields))\n            \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            # Use the srid_map to determine the appropriate point type\n            if srid in srid_map:\n                point_class, dim = srid_map[srid]\n                if len(coordinates) != dim:\n                    raise ValueError(f\"SRID {srid} requires {dim} coordinates ({len(coordinates)} provided)\")\n                return point_class(coordinates)\n            else:\n                # Default to Point if no specific SRID is found\n                return Point(coordinates)\n    \n    # Register the new point type with the srid_map if needed\n    if srid_map is not None:\n        srid_map[name] = (PointSubclass, len(fields))\n    \n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 132203, "num_retrieved": 500, "inference_time": 28.122745290398598, "generate_results": ["def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated: {message}\",\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated: {message}\",\n                DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        func._deprecated = True\n        func._deprecated_message = message\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 52035, "num_retrieved": 224, "inference_time": 3.697717174887657, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # R options are set using the R option system, which is typically\n    # configured via R's command line or within R scripts using options()\n    # This function would typically inject R options into the R code\n    # to configure behavior that cannot be set through environment variables\n    \n    # Common R options that might be needed for pre-commit hooks\n    # These are set as R options that will be processed when R starts\n    r_options = [\n        # Set R to use a specific working directory\n        \"options(warn = 1)\",\n        # Set R to not show messages about missing packages\n        \"options(show.error.messages = FALSE)\",\n        # Set R to not show warnings about deprecated functions\n        \"options(deprecated = FALSE)\",\n        # Set R to not show messages about missing packages\n        \"options(pkg.load = FALSE)\"\n    ]\n    \n    # Inject the R options into the code\n    # This would typically be done by modifying the R script to include\n    # these options at the beginning\n    return f\"{code}\\n{''.join(r_options)}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # R options are set using the R option system, which is typically\n    # configured via R's command line or within R scripts using options()\n    # Since this function is meant to set R options inline, we would typically\n    # use R's options() function to set these options directly in the R code.\n    # This function takes R code as input and returns the modified code with\n    # inline R options added.\n    \n    # Common R options that might be needed for various behaviors\n    # These are examples and should be adjusted based on specific requirements\n    options_code = \"\"\"\noptions(\n    # Set R options that cannot be configured via environment variables\n    # Example: Set the default number of decimal places for printing\n    digits = 4,\n    \n    # Example: Set the width of the output for printing\n    width = 80,\n    \n    # Example: Set the default behavior for missing values\n    na.action = \"na.omit\",\n    \n    # Example: Set the default behavior for printing large matrices\n    show.signif.stars = TRUE,\n    \n    # Example: Set the default behavior for printing factor levels\n    stringsAsFactors = FALSE\n)\n\"\"\"\n    \n    # Insert the options code at the beginning of the input code\n    # This ensures that the R options are set before any other R code is executed\n    return options_code + \"\\n\" + code", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be configured via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # R options that need to be set inline\n    r_options = [\n        '--vanilla',  # Disable R startup scripts and options\n        '--no-save',  # Don't save the R session\n        '--no-restore',  # Don't restore the R session\n        '--no-site-file',  # Don't load site-specific configuration\n        '--no-user-site',  # Don't load user-specific configuration\n        '--no-restore',  # Don't restore the R session\n    ]\n    \n    # Add R options to the code\n    if code.strip():\n        # If code already has R commands, prepend the options\n        return f'{r_options[0]} {r_options[1]} {r_options[2]} {r_options[3]} {r_options[4]} {r_options[5]} {code}'\n    else:\n        # If no code, just return the options\n        return ' '.join(r_options)"]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 58886, "num_retrieved": 257, "inference_time": 3.697717174887657, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're not using concurrency, run sequentially\n    if jobs == 1:\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout\n    \n    # For concurrent execution, use a simple parallel approach\n    # This is a simplified version - in practice, this would use a proper\n    # parallelization library or subprocess with proper job management\n    import multiprocessing as mp\n    from concurrent.futures import ProcessPoolExecutor\n    \n    def run_single(file_arg):\n        try:\n            result = subprocess.run(\n                cmd, \n                input=file_arg.encode(), \n                capture_output=True, \n                text=True, \n                timeout=300\n            )\n            return result.returncode, result.stdout.encode()\n        except subprocess.TimeoutExpired:\n            return 1, b'Command timed out'\n    \n    # Use a process pool to run the commands in parallel\n    with ProcessPoolExecutor(max_workers=jobs) as executor:\n        futures = [executor.submit(run_single, arg) for arg in varargs]\n        results = [future.result() for future in futures]\n    \n    # Aggregate results\n    overall_returncode = 0\n    overall_output = b''\n    \n    for retcode, output in results:\n        if retcode != 0:\n            overall_returncode = retcode\n        overall_output += output\n    \n    return overall_returncode, overall_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're not using concurrency, run sequentially\n    if jobs == 1:\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout\n    \n    # For concurrency, we need to split the files into partitions\n    # This is a simplified version - in practice, this would use a proper xargs implementation\n    # or a multiprocessing approach\n    \n    # For now, we'll just run the command with the files as arguments\n    # This is a placeholder implementation that doesn't actually parallelize\n    \n    # Use the actual xargs implementation from the library\n    # This would typically be a more complex implementation with proper job control\n    return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency to use\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're not using concurrency, run sequentially\n    if jobs == 1:\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).returncode, subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True).stdout\n    \n    # For concurrency, use a simple parallel execution approach\n    # This is a simplified version that doesn't implement full xargs behavior\n    import multiprocessing as mp\n    from concurrent.futures import ProcessPoolExecutor\n    \n    def run_single(file_arg):\n        try:\n            result = subprocess.run(cmd, input=file_arg.encode(), capture_output=True)\n            return result.returncode, result.stdout\n        except Exception as e:\n            return 1, e.__str__.encode()\n    \n    with ProcessPoolExecutor(max_workers=jobs) as executor:\n        futures = [executor.submit(run_single, arg) for arg in varargs]\n        results = [future.result() for future in futures]\n    \n    # Aggregate results\n    retv = 0\n    out = b''\n    for code, output in results:\n        retv |= code\n        out += output\n    \n    return retv, out", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're running in a serial mode, just run one at a time\n    if jobs == 1:\n        result = subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True)\n        return result.returncode, result.stdout\n    \n    # For parallel execution, we need to handle the file arguments\n    # This is a simplified version that just runs the command with the arguments\n    # In a real implementation, this would spawn multiple processes\n    # and collect results\n    \n    # For now, we'll just run the command with all arguments\n    result = subprocess.run(cmd, input='\\n'.join(varargs).encode(), capture_output=True)\n    return result.returncode, result.stdout", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if not varargs:\n        return 0, b''\n\n    # Determine the actual concurrency based on environment and platform\n    jobs = target_concurrency\n    if jobs <= 0:\n        jobs = 1\n    \n    # If we're running in serial mode, just run the command once\n    if jobs == 1:\n        return subprocess.run(cmd, input='\\n'.join(varargs).encode(), **kwargs).returncode, b'\\n'.join(varargs).encode()\n    \n    # For parallel execution, use a simple approach with subprocess\n    # This is a simplified version that doesn't fully implement xargs\n    # behavior like input handling or error propagation\n    \n    # Create a temporary file to hold the input\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n        f.write('\\n'.join(varargs))\n        temp_file = f.name\n    \n    try:\n        # Run the command with the temporary file\n        result = subprocess.run(\n            cmd,\n            input=None,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=False,\n            **kwargs\n        )\n        return result.returncode, result.stdout\n    finally:\n        import os\n        if os.path.exists(temp_file):\n            os.unlink(temp_file)"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 28187, "num_retrieved": 116, "inference_time": 3.697717174887657, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle every time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 45680, "num_retrieved": 198, "inference_time": 3.697717174887657, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 62520, "num_retrieved": 288, "inference_time": 3.697717174887657, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a proper executable path\n    # This handles the case where the first element might be a shebang path that needs to be resolved\n    # and ensures that on Windows, the command is properly formatted for execution\n    \n    # If the command starts with a shebang, we need to ensure it's properly resolved\n    # The shebang is typically a path to an interpreter, and we need to make sure it's valid\n    if cmd and cmd[0].startswith('#!'):\n        # Extract the interpreter path from the shebang\n        interpreter_path = cmd[0][2:]  # Remove the '#!' prefix\n        # Normalize the path to ensure it's in a valid format\n        # This handles deep-path shebangs by ensuring the path is properly formatted\n        # and resolves to a valid executable\n        # On Windows, we need to ensure the path is properly formatted with the correct extension\n        if sys.platform == 'win32':\n            # On Windows, we need to ensure the path ends with .exe or is properly formatted\n            if not interpreter_path.endswith(('.exe', '.py')):\n                # If it doesn't end with .exe or .py, we need to append .exe\n                interpreter_path += '.exe'\n        # Replace the first element with the normalized interpreter path\n        cmd = (interpreter_path,) + cmd[1:]\n    \n    # Ensure the command is properly formatted for execution on Windows\n    # This includes ensuring that paths are properly formatted and that the command is valid\n    # We also need to ensure that the command is properly formatted for execution\n    # This includes ensuring that the command is properly formatted for execution on Windows\n    # and that the command is properly formatted for execution on other platforms\n    \n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a valid executable path\n    # This handles the case where the first element might be a shebang path that needs to be resolved\n    # and ensures that on Windows, the command is properly formatted for execution\n    \n    # If the first element is a shebang path, we need to ensure it's properly formatted\n    # For Windows, we need to handle the case where the shebang might be in a deep path\n    # and ensure that the path is properly resolved\n    \n    # The implementation follows the pattern of the pre-commit library's normalize_cmd\n    # which handles both shebang resolution and path normalization\n    \n    # If the first element is a shebang path, we need to resolve it to the actual executable\n    # This is done by using the parse_shebang module to find the executable\n    # If the executable is found, we replace the first element with the actual executable path\n    \n    # For Windows, we need to ensure that the path is properly formatted\n    # This includes handling the case where the path might have a different separator\n    \n    # The function returns the normalized command tuple\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a proper executable path\n    # This handles the case where the first element might be a shebang that needs to be resolved\n    if len(cmd) == 0:\n        return cmd\n    \n    # If the first element is a shebang, we need to resolve it to the actual executable\n    # This is particularly important on Windows where shebangs are not parsed by default\n    if cmd[0].startswith('#!'):\n        # Extract the path from the shebang\n        shebang_path = cmd[0][2:].strip()\n        # If the path is relative, resolve it to an absolute path\n        if not os.path.isabs(shebang_path):\n            shebang_path = os.path.abspath(shebang_path)\n        # Replace the first element with the resolved path\n        return (shebang_path,) + cmd[1:]\n    \n    # For non-shebang cases, ensure the first element is a valid path\n    # This handles the case where the first element might be a relative path\n    if not os.path.isabs(cmd[0]):\n        cmd = (os.path.abspath(cmd[0]),) + cmd[1:]\n    \n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a proper executable path\n    # This handles the case where the first element might be a shebang or a path\n    # that needs to be resolved properly on Windows\n    if len(cmd) == 0:\n        return cmd\n    \n    # If the first element is a shebang, we need to ensure it's properly handled\n    # On Windows, we need to ensure that the path is properly resolved\n    first_element = cmd[0]\n    \n    # Check if the first element starts with a shebang\n    if first_element.startswith('#!'):\n        # For shebangs, we need to ensure the path is properly resolved\n        # On Windows, we need to ensure that the path is properly resolved\n        # by using the system's path resolution\n        # We'll replace the shebang with the actual executable path\n        # This is a simplified version that just replaces the shebang\n        # with a placeholder that will be resolved by the system\n        # In practice, this would involve resolving the path using the system's path resolution\n        # For now, we just return the original command\n        pass\n    \n    # On Windows, we need to ensure that the path is properly resolved\n    # This is done by ensuring that the path is properly resolved\n    # using the system's path resolution\n    # We'll ensure that the path is properly resolved by using the system's path resolution\n    # This is a simplified version that just returns the original command\n    # In practice, this would involve resolving the path using the system's path resolution\n    \n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Normalize the command by ensuring the first element is a proper executable path\n    # This handles the case where a shebang might be present in a deep path\n    # and ensures that the command is properly formatted for execution\n    if len(cmd) == 0:\n        return cmd\n    \n    # If the first element is a shebang, we need to ensure it's properly handled\n    # For Windows, we need to ensure that the path is properly resolved\n    # and that the shebang is not being misinterpreted\n    if cmd[0].startswith('#!'):\n        # For shebangs, we need to ensure the path is properly resolved\n        # and that the shebang is not being misinterpreted\n        # This is a workaround for Windows not parsing shebangs properly\n        # We replace the shebang with a proper executable path\n        # This is a simplified version that just replaces the shebang\n        # with a proper path that will work on Windows\n        return (cmd[0].replace('#!', ''),) + cmd[1:]\n    \n    # For non-shebang commands, we just return the original command\n    return cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 10175, "num_retrieved": 54, "inference_time": 3.697717174887657, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[cache_key]\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n            \n            return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args, **key_kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[cache_key]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n                return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            import functools\n            wrapper.__wrapped__ = func\n            @functools.wraps(func)\n            def locked_wrapper(*args, **kwargs):\n                with lock:\n                    return wrapper(*args, **kwargs)\n            return locked_wrapper\n        \n        # Otherwise return the wrapper directly\n        wrapper.__wrapped__ = func\n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[cache_key]\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n            \n            return result\n        \n        # Add a lock if provided for thread safety\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_value = key(*args, **kwargs)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[key_value]\n                return result\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_value] = result\n                return result\n        \n        # Set up thread safety if needed\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 10517, "num_retrieved": 52, "inference_time": 3.697717174887657, "generate_results": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    from cachetools.keys import hashkey, typedkey\n    \n    key_func = typedkey if typed else hashkey\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    from cachetools.keys import hashkey, typedkey\n    \n    key_func = typedkey if typed else hashkey\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        update_wrapper(wrapper, func)\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 3.697717174887657, "generate_results": ["def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    def cache_key(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        else:\n            return hashkey(*args, **kwargs)\n    \n    def decorator(func):\n        cache = MRUCache(maxsize=maxsize)\n        \n        def wrapper(*args, **kwargs):\n            key = cache_key(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return functools.update_wrapper(wrapper, func)\n    \n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 3.697717174887657, "generate_results": ["def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        # Use a typed key to distinguish between different types of arguments\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = lambda *args, **kwargs: tuple((arg, type(arg)) for arg in args) + tuple((k, type(v)) for k, v in sorted(kwargs.items()))\n    else:\n        # Use a simple hash key\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = lambda *args, **kwargs: tuple(args) + tuple(sorted(kwargs.items()))\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                result = func(*args, **kwargs)\n                cache[key] = result\n                return result\n        return wrapper\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize=maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 10041, "num_retrieved": 49, "inference_time": 3.697717174887657, "generate_results": ["def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the cache based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            cache[key] = result\n            return result\n        \n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            # Call the original function\n            result = func(*args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        # Update wrapper with function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        update_wrapper(wrapper, func)\n        \n        return wrapper\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\"\"\"\n    from functools import wraps\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            key = args + tuple(sorted(kwargs.items())) if not typed else args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = key + tuple(type(arg) for arg in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n            \n            # Try to get from cache\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            \n            # Call the function and store result\n            result = func(*args, **kwargs)\n            cache[key] = result\n            \n            return result\n        \n        return wrapper\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            # Call the original function\n            result = func(*args, **kwargs)\n            \n            # Store the result in cache\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        \n        def wrapper(*args, **kwargs):\n            # Create a key based on arguments\n            if typed:\n                key = (args, tuple(type(arg) for arg in args), tuple(type(v) for v in kwargs.values()))\n            else:\n                key = args + tuple(sorted(kwargs.items()))\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            # Call the original function\n            result = func(*args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        # Update wrapper with original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 8936, "num_retrieved": 45, "inference_time": 3.697717174887657, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 11144, "num_retrieved": 57, "inference_time": 3.697717174887657, "generate_results": ["def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 11342, "num_retrieved": 59, "inference_time": 3.697717174887657, "generate_results": ["def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 11327, "num_retrieved": 60, "inference_time": 3.697717174887657, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the key for this call\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss - compute the value and store it\n                    v = func(self, *args, **kwargs)\n                    cache[k] = v\n                    return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would need to be implemented based on the cache's info\n                # For now, we just return a placeholder\n                return _CacheInfo(0, 0, cache.maxsize, cache.currsize)\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        # Add cache info and clear methods\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss, compute the value and store it\n                    v = func(self, *args, **kwargs)\n                    # In case of a race condition, prefer the item already in the cache\n                    try:\n                        with lock:\n                            cache[k] = v\n                    except ValueError:\n                        return v  # value too large\n            return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache.hits, cache.misses\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    cache.hits = 0\n                    cache.misses = 0\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle the caching logic\n        def wrapper(self, *args, **kwargs):\n            # Generate the key for this call\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss - compute the value and store it\n                    v = func(self, *args, **kwargs)\n                    cache[k] = v\n                    return v\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would need to be implemented based on cache internals\n                # For now, we just return a placeholder\n                return _CacheInfo(0, 0, cache.maxsize, cache.currsize)\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        # Create a wrapper function that will handle caching\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the cached value\n                    return cache[k]\n                except KeyError:\n                    # Cache miss - compute the value and store it\n                    result = method(self, *args, **kwargs)\n                    cache[k] = result\n                    return result\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would need to be implemented based on the cache's info method\n                # For now, we just return a placeholder\n                return {'hits': 0, 'misses': 0, 'maxsize': cache.maxsize, 'currsize': cache.currsize}\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        # Add cache_info and cache_clear to the wrapper\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        # Update the wrapper with the original method's metadata\n        functools.update_wrapper(wrapper, method)\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        # Create a wrapper function that will handle the caching\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the value from the cache\n                    return cache[k]\n                except KeyError:\n                    # If not found, call the original method and store the result\n                    result = method(self, *args, **kwargs)\n                    cache[k] = result\n                    return result\n        \n        # Add cache_info and cache_clear methods to the wrapper\n        def cache_info():\n            with lock:\n                # This would need to be implemented based on the cache's info method\n                # For now, we just return a placeholder\n                return {'hits': 0, 'misses': 0, 'maxsize': cache.maxsize, 'currsize': cache.currsize}\n        \n        def cache_clear():\n            with lock:\n                cache.clear()\n        \n        # Add cache_info and cache_clear to the wrapper\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        \n        # Update the wrapper with the original method's metadata\n        functools.update_wrapper(wrapper, method)\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 7109, "num_retrieved": 23, "inference_time": 3.697717174887657, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback information\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_lines = []\n        for frame in tb:\n            tb_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = str(e)\n        \n        # Format the traceback\n        tb_str = \"\\n\".join(tb_lines)\n        \n        # Combine everything\n        result = f\"{exc_type}: {exc_msg}\\nTraceback (most recent call last):\\n{tb_str}\"\n        \n        return result\n        \n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            line = frame.line\n            \n            # Limit path level\n            if filename.startswith('/'):\n                path_parts = filename.split('/')\n                if len(path_parts) > max_path_level:\n                    filename = '/'.join(path_parts[:max_path_level]) + '...'\n            \n            trace_lines.append(f\"{filename}:{lineno} in {name}\")\n            if line:\n                trace_lines.append(f\"    {line}\")\n        \n        # Format the result\n        result = f\"{exc_type}: {exc_msg}\"\n        if trace_lines:\n            result += \"\\n\" + \"\\n\".join(trace_lines)\n        \n        return result\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            trace_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback\n    tb = traceback.extract_tb(e.__traceback__, max_level)\n    \n    # Format the exception\n    exc_type = e.__class__.__name__\n    exc_msg = e.args[0] if e.args else \"No message\"\n    \n    # Build the trace string\n    trace_lines = []\n    for frame in tb:\n        filename = frame.filename\n        line_no = frame.lineno\n        name = frame.name\n        trace_lines.append(f\"{filename}:{line_no}:{name}\")\n    \n    # Limit path level if needed\n    path_lines = []\n    for frame in tb:\n        filename = frame.filename\n        if filename.startswith('/'):\n            path_lines.append(filename)\n    \n    # Format the result\n    result = f\"{exc_type}: {exc_msg}\"\n    if trace_lines:\n        result += \"\\nTraceback (most recent call last):\\n\"\n        for line in trace_lines:\n            result += f\"  {line}\\n\"\n    \n    return result", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            trace_lines.append(f\"{filename}:{lineno}:{name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 11406, "num_retrieved": 43, "inference_time": 3.697717174887657, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Error writing to file %s: %s\", file_name, SolBase.extostr(e))\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 11524, "num_retrieved": 44, "inference_time": 12.552903220057487, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 11387, "num_retrieved": 44, "inference_time": 12.552903220057487, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 5115, "num_retrieved": 16, "inference_time": 12.552903220057487, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters or local data if needed\n    # Note: ContextFilter uses gevent.local, so we don't need to explicitly reset it\n    # as it's per-thread and managed by the local instance\n    \n    # Optionally, reset any platform-specific state if needed\n    # This is a placeholder - actual implementation depends on what needs to be reset\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state found in context, so no additional reset)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state is referenced in context, so minimal reset)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters or local data if needed\n    # Note: ContextFilter uses gevent.local, so we don't need to explicitly reset it\n    # as it's managed by the gevent context\n    \n    # Reinitialize any platform-specific logging if needed\n    # This is a placeholder - actual implementation depends on application needs\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state is referenced in the context, so minimal reset)"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 98613, "num_retrieved": 431, "inference_time": 12.552903220057487, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the class being tested\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the naming convention of the classes being tested\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the specific test case\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # of the test being run and the available implementations\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the test environment\n    class_name = self.__class__.__name__\n    if class_name.endswith('Py'):\n        return class_name[:-2]\n    elif class_name.endswith('Fallback'):\n        return class_name[:-8]\n    return class_name", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the naming convention of the classes being tested\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the specific test case\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the class being tested\n    return self._getTargetClass()"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 93884, "num_retrieved": 418, "inference_time": 12.552903220057487, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and the final merged ordering\n    positions = {}\n    merged = []\n    \n    # Process each ordering\n    for ordering in orderings:\n        # For each element in the ordering, record its position\n        for i, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = []\n            positions[element].append(i)\n    \n    # Sort the orderings by the first element that appears in them\n    # This ensures that the first element of each ordering is processed in order\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else None)\n    \n    # Merge the orderings by processing each ordering in order\n    # and ensuring that any common elements maintain their relative order\n    result = []\n    \n    # Keep track of the current position in each ordering\n    current_pos = [0] * len(orderings)\n    \n    # Continue until all elements have been processed\n    while any(current_pos[i] < len(orderings[i]) for i in range(len(orderings))):\n        # Find the next element to add to the result\n        next_element = None\n        next_pos = float('inf')\n        \n        for i, ordering in enumerate(orderings):\n            if current_pos[i] < len(ordering):\n                # Check if this element has already been added\n                if ordering[current_pos[i]] not in result:\n                    # Check if this element appears in any other ordering\n                    # and if it does, ensure that the suffix starting with it\n                    # is in all orderings\n                    found_in_all = True\n                    for j, other_ordering in enumerate(orderings):\n                        if j != i and", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Sort the orderings by the first element\n    sorted_orderings = sorted(orderings, key=lambda x: x[0])\n    \n    # Build the result list\n    result = []\n    seen = set()\n    \n    # Process each ordering\n    for ordering in sorted_orderings:\n        # For each element in the ordering, if it's not already in the result,\n        # add it to the result\n        for element in ordering:\n            if element not in seen:\n                # Find the position of this element in the current ordering\n                # and ensure that the suffix starting with this element is preserved\n                # in all orderings that contain it\n                result.append(element)\n                seen.add(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Find the first ordering as a base\n    base = orderings[0]\n    \n    # For each subsequent ordering, merge it into the base\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the base\n        # We need to find the longest suffix of the base that matches a prefix of the ordering\n        merged = []\n        base_idx = 0\n        ordering_idx = 0\n        \n        # Process elements in order\n        while base_idx < len(base) and ordering_idx < len(ordering):\n            if base[base_idx] == ordering[ordering_idx]:\n                merged.append(base[base_idx])\n                base_idx += 1\n                ordering_idx += 1\n            else:\n                # If they don't match, we need to insert the ordering element\n                # But we can only do this if it's not already in the merged list\n                # and if it's not a suffix of the base\n                if ordering_idx < len(ordering):\n                    # Insert the ordering element\n                    merged.append(ordering[ordering_idx])\n                    ordering_idx += 1\n                else:\n                    # We've exhausted the ordering, so just append the rest of the base\n                    merged.extend(base[base_idx:])\n                    break\n        else:\n            # If we've processed all elements in the ordering, append the rest of the base\n            if ordering_idx < len(ordering):\n                merged.extend(ordering[ordering_idx:])\n            else:\n                merged.extend(base[base_idx:])\n        \n        # Update the base to be the merged result\n        base = merged\n    \n    return base", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Find the minimum position for each element across all orderings\n    min_positions = {}\n    for element, orderings_positions in positions.items():\n        min_positions[element] = min(orderings_positions.values())\n    \n    # Create a list to store the result\n    result = []\n    \n    # Process each ordering to build the result\n    # We need to ensure that if an element appears in multiple orderings,\n    # the suffix that begins with that element must be in both orderings\n    # This means we need to merge the orderings in a way that respects\n    # the constraints\n    \n    # Sort orderings by the first element that appears in them\n    # This ensures we process orderings in a way that respects the constraints\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else None)\n    \n    # Process each ordering in the sorted order\n    for ordering in sorted_orderings:\n        # Add elements from the ordering that haven't been added yet\n        for element in ordering:\n            if element not in result:\n                result.append(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Sort the orderings by their first element\n    sorted_orderings = sorted(orderings, key=lambda x: x[0])\n    \n    # Build the result ordering\n    result = []\n    seen = set()\n    \n    # Process each ordering in the sorted order\n    for ordering in sorted_orderings:\n        # For each element in the ordering, if it's not already in the result,\n        # add it to the result\n        for element in ordering:\n            if element not in seen:\n                # Check if this element appears in any other ordering\n                # and if so, ensure that the suffix starting with this element\n                # is preserved in all orderings\n                found = False\n                for other_ordering in orderings:\n                    if element in other_ordering:\n                        # Find the position of this element in the other ordering\n                        pos_in_other = other_ordering.index(element)\n                        # Check if the suffix starting with this element is preserved\n                        suffix = other_ordering[pos_in_other:]\n                        # If the suffix is not already in the result, add it\n                        if not any(suffix[i] in result for i in range(len(suffix))):\n                            result.extend(suffix)\n                            found = True\n                            break\n                if not found:\n                    result.append(element)\n                seen.add(element)\n    \n    return result"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 84001, "num_retrieved": 384, "inference_time": 12.552903220057487, "generate_results": ["def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.declarations import Declaration\n        return Declaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 87068, "num_retrieved": 363, "inference_time": 12.552903220057487, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a simple version that removes duplicates and maintains order\n    unique_classes = []\n    seen = set()\n    \n    for cls in classes:\n        if cls not in seen:\n            unique_classes.append(cls)\n            seen.add(cls)\n    \n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a basic version that removes duplicates and maintains order\n    unique_classes = []\n    seen = set()\n    \n    for cls in classes:\n        if cls not in seen:\n            unique_classes.append(cls)\n            seen.add(cls)\n    \n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a basic version that removes duplicates and maintains order\n    unique_classes = []\n    seen = set()\n    \n    for cls in classes:\n        if cls not in seen:\n            unique_classes.append(cls)\n            seen.add(cls)\n    \n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a basic version that would typically be used in interface\n    # resolution to avoid redundant base classes.\n    \n    if not classes:\n        return ()\n        \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n            \n    # Sort by name to ensure consistent ordering\n    # This is a simplification - actual implementation may use MRO or other\n    # criteria to determine minimal bases\n    return tuple(sorted(unique_classes, key=lambda x: x.__name__))", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide the exact algorithm for minimal bases,\n    # we implement a basic version that removes duplicates and maintains order\n    unique_bases = []\n    seen = set()\n    \n    for cls in classes:\n        if cls not in seen:\n            unique_bases.append(cls)\n            seen.add(cls)\n    \n    return unique_bases"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 81609, "num_retrieved": 348, "inference_time": 12.552903220057487, "generate_results": ["def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base classes\n            names_and_descriptions = []\n            # Get all attributes from the interface and its base interfaces\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names_and_descriptions.append((name, desc))\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base classes\n            names_and_descriptions = []\n            # Get all attributes from the interface and its base interfaces\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names_and_descriptions.append((name, desc))\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, (Attribute, IMethod)):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes defined by the interface and its base interfaces\n            names_and_descriptions = []\n            # Get all base interfaces\n            for base in self.__bases__:\n                if hasattr(base, 'namesAndDescriptions'):\n                    # Recursively get names and descriptions from base interfaces\n                    base_names = base.namesAndDescriptions(all=True)\n                    names_and_descriptions.extend(base_names)\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, (Attribute, IMethod)):\n                    names_and_descriptions.append((name, desc))\n            # Remove duplicates while preserving order\n            seen = set()\n            result = []\n            for name, desc in names_and_descriptions:\n                if name not in seen:\n                    seen.add(name)\n                    result.append((name, desc))\n            return result", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base classes\n            names_and_descriptions = []\n            # Get all attributes from the interface and its base interfaces\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names_and_descriptions.append((name, desc))\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base classes\n            names_and_descriptions = []\n            # Get all attributes from the interface and its base interfaces\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names_and_descriptions.append((name, desc))\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if isinstance(desc, Attribute):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 82798, "num_retrieved": 356, "inference_time": 12.552903220057487, "generate_results": ["def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names_and_descriptions__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names_and_descriptions__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names__', []) + self.__dict__.get('__all_names__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names__', []) + self.__dict__.get('__all_names__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names_and_descriptions__', []))"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 66665, "num_retrieved": 273, "inference_time": 12.552903220057487, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence.flattened())\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 97698, "num_retrieved": 422, "inference_time": 12.552903220057487, "generate_results": ["def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 98931, "num_retrieved": 437, "inference_time": 12.552903220057487, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    return _use_c_impl()"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 89434, "num_retrieved": 361, "inference_time": 12.552903220057487, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # TODO: Implement this function\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # TODO: Implement this function\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # TODO: Implement this function\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # This function is the opposite of _c_optimizations_required\n    # It returns True if C optimizations are not available or ignored\n    # based on the context of the system\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    # This function is the opposite of _c_optimizations_required\n    # It returns True if C optimizations are not available or ignored\n    # based on the context of the system\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 98905, "num_retrieved": 437, "inference_time": 12.552903220057487, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 115047, "num_retrieved": 337, "inference_time": 12.552903220057487, "generate_results": ["def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = init_bins(self.edges, self.initial_value, deepcopy=True)\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 115760, "num_retrieved": 378, "inference_time": 12.552903220057487, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV instead for converting graphs to CSV\n    raise NotImplementedError(\"to_csv is deprecated and not used in Lena 0.5+\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use ToCSV class instead for converting graphs to CSV\n    # The actual implementation would be in ToCSV class\n    # which handles the conversion of histograms and iterables to CSV\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV for converting graphs to CSV\n    # Instead of implementing this function, we raise a deprecation warning\n    import warnings\n    warnings.warn(\n        \"to_csv is deprecated in Lena 0.5. Use lena.output.ToCSV instead.\",\n        DeprecationWarning\n    )\n    # Since this is a deprecated function, we return an empty string\n    # or raise an error to indicate it's not functional\n    raise DeprecationWarning(\"to_csv is deprecated and not functional in Lena 0.5+\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV instead for converting graphs to CSV\n    # The actual implementation would convert graph points to CSV\n    # by iterating through coordinates and values, but this is\n    # deprecated in favor of the ToCSV class\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # Use lena.output.ToCSV for converting graphs to CSV\n    # The actual implementation is handled by ToCSV class\n    # which converts histograms and iterables to CSV format\n    pass"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 107439, "num_retrieved": 298, "inference_time": 28.69390033185482, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    # This is based on the coordinate name in the graph's field names\n    for i, field_name in enumerate(self._field_names):\n        if field_name == coord_name:\n            # Return the error index for this coordinate\n            # The error index is stored in the graph's error indices\n            # which are stored in the _error_x_ind and _error_y_ind attributes\n            if field_name == self._field_names[0]:\n                return self._error_x_ind\n            elif field_name == self._field_names[1]:\n                return self._error_y_ind\n    # If coordinate name not found, return None\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    # This is based on the coordinate name in the graph's field names\n    for i, field_name in enumerate(self._field_names):\n        if field_name == coord_name:\n            # Return the error index for this coordinate\n            # The error index is stored in the graph's error fields\n            # which are stored in self._error_x_ind and self._error_y_ind\n            if field_name == self._field_names[0]:\n                return self._error_x_ind\n            elif field_name == self._field_names[1]:\n                return self._error_y_ind\n    # If coordinate name not found, return None\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    # This is typically done by searching through the error suffixes\n    # and matching them to the coordinate name\n    for i, (error_name, coord, error_ind) in enumerate(self._parsed_error_names):\n        if coord == coord_name:\n            return error_ind\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    for i, (coord, error) in enumerate(zip(self._coords, self._errors)):\n        if coord == coord_name:\n            return i\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    # This is based on the coordinate name in the graph's field names\n    # and the error indices stored in the graph's error fields\n    for i, field_name in enumerate(self._field_names):\n        if field_name == coord_name:\n            # Return the error index for this coordinate\n            # This is typically stored in the graph's error fields\n            # and accessed via the error indices stored in the graph\n            return self._error_x_ind if field_name == self._field_names[0] else self._error_y_ind\n    # If coordinate name not found, return None\n    return None"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 115255, "num_retrieved": 385, "inference_time": 28.69390033185482, "generate_results": ["def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self.field_names:\n        if field.endswith('_low') or field.endswith('_high'):\n            # Extract the coordinate name (x, y, z) from the field name\n            coord_name = field[:-4]  # Remove '_low' or '_high'\n            if coord_name in ['x', 'y', 'z']:\n                error_fields.append((coord_name, field))\n    \n    # If there are error fields, update the context\n    if error_fields:\n        error_context = {\"error\": {}}\n        for coord_name, field_name in error_fields:\n            # Find the index of the error field\n            # This is a simplified implementation - in practice,\n            # the actual index would be determined by the graph's data\n            error_context[\"error\"][coord_name + \"_low\"] = {\"index\": 0}\n        \n        # Update the context with the error information\n        # Existing values are not removed from context.value and its subcontexts\n        lena.context.update_recursively(context, error_context)", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self.field_names:\n        if field.endswith(\"_low\") or field.endswith(\"_high\"):\n            # Extract the coordinate name (x, y, z) from the field name\n            coord_name = field[:-4]  # Remove \"_low\" or \"_high\"\n            if coord_name in [\"x\", \"y\", \"z\"]:\n                error_fields.append((coord_name, field))\n    \n    # Build the error context\n    if error_fields:\n        error_context = {\"error\": {}}\n        for coord_name, field_name in error_fields:\n            # Find the index of the error field in the graph's coordinates\n            # This is a simplified approach - in practice, the index would be determined\n            # by the field's position in the graph's coordinate system\n            error_context[\"error\"][coord_name + \"_low\"] = {\"index\": error_fields.index((coord_name, field_name))}\n        \n        # Update the context with the error information\n        # Existing values are not removed from context.value and its subcontexts\n        lena.context.update_recursively(context, error_context)", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self.field_names:\n        if field.endswith('_low') or field.endswith('_high'):\n            # Extract the coordinate name (x, y, z) from the field name\n            coord_name = field[:-4]  # Remove '_low' or '_high'\n            if coord_name in ['x', 'y', 'z']:\n                error_fields.append((coord_name, field))\n    \n    # Build the error context\n    if error_fields:\n        error_context = {\"error\": {}}\n        for coord_name, field_name in error_fields:\n            # Find the index of the error field in the graph's coordinates\n            # This is a simplified approach - in practice, the index would be determined\n            # by the field's position in the graph's coordinate system\n            error_context[\"error\"][coord_name + \"_low\"] = {\"index\": error_fields.index((coord_name, field_name))}\n        \n        # Update the context with the error information\n        # Existing values are not removed from context.value and its subcontexts\n        lena.context.update_recursively(context, error_context)", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self.field_names:\n        if field.endswith('_low') or field.endswith('_high'):\n            # Extract the base name (without _low or _high)\n            base_name = field[:-4] if field.endswith('_low') or field.endswith('_high') else field\n            # Map to x, y, z coordinates\n            if base_name in ['x', 'y', 'z']:\n                error_fields.append(base_name)\n    \n    # Create error context\n    if error_fields:\n        error_context = {\"error\": {}}\n        for field in error_fields:\n            # Find the index of the error field\n            # This is a simplified implementation - in practice,\n            # the actual index would be determined by the graph's structure\n            error_context[\"error\"][field] = {\"index\": 0}  # Placeholder index\n        \n        # Update context with error information\n        # Existing values are not removed from context.value and its subcontexts\n        if \"error\" not in context:\n            context[\"error\"] = error_context[\"error\"]\n        else:\n            # Append to existing error context\n            for key, val in error_context[\"error\"].items():\n                if key not in context[\"error\"]:\n                    context[\"error\"][key] = val\n    \n    # Ensure value context is preserved\n    # This is a placeholder - in practice, the value context would be preserved\n    # as specified in the docstring", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Extract error fields from the graph\n    error_fields = []\n    for field in self._fields:\n        if field.startswith(\"error_\"):\n            error_fields.append(field)\n    \n    # Create error context\n    error_context = {\"error\": {}}\n    \n    # Map error field names to coordinate names\n    for field in error_fields:\n        # Extract coordinate name from error field (e.g., \"error_x_low\" -> \"x\")\n        coord_name = field[7:]  # Remove \"error_\" prefix\n        if coord_name.startswith(\"x\"):\n            coord_name = \"x\"\n        elif coord_name.startswith(\"y\"):\n            coord_name = \"y\"\n        elif coord_name.startswith(\"z\"):\n            coord_name = \"z\"\n        else:\n            continue\n            \n        # Extract the specific error component (e.g., \"x_low\" -> \"low\")\n        error_component = coord_name + \"_low\"\n        if error_component in field:\n            error_component = \"low\"\n        elif \"high\" in field:\n            error_component = \"high\"\n        else:\n            error_component = \"value\"\n            \n        # Store the index of the error field\n        error_context[\"error\"][coord_name] = {\"index\": field}\n    \n    # Update the context with error information\n    if error_context:\n        lena.context.update_recursively(context, error_context)\n    \n    # Ensure value context is preserved\n    # Existing values in context.value and its subcontexts are not removed"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 115695, "num_retrieved": 372, "inference_time": 28.69390033185482, "generate_results": ["def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify 1D and multidimensional edges for consistent processing\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin in the histogram\n    for i in range(len(edges)):\n        for j in range(len(edges[i]) - 1):\n            # Calculate bin width (difference between edges)\n            bin_width = edges[i][j + 1] - edges[i][j]\n            \n            # Add the bin value multiplied by its width to the total integral\n            bin_value = bins[i][j]\n            total += bin_value * bin_width\n    \n    return total", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify 1D and multidimensional edges for integration\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin in the multidimensional mesh\n    for i in range(len(edges)):\n        for j in range(len(edges[i])):\n            # Get the bin value\n            bin_val = bins[i][j]\n            \n            # Calculate the bin volume (area in 2D, length in 1D)\n            # For each dimension, calculate the width of the bin\n            bin_volume = 1.0\n            for dim in range(len(edges)):\n                if dim == 0:\n                    bin_volume *= (edges[dim][j+1] - edges[dim][j])\n                else:\n                    bin_volume *= (edges[dim][j+1] - edges[dim][j])\n            \n            # Add the contribution of this bin to the total integral\n            total += bin_val * bin_volume\n    \n    return total", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify 1D and multidimensional edges\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # Get bin content\n            bin_val = bins[i][j]\n            \n            # Calculate bin volume (area in 2D, length in 1D)\n            if len(edges) == 1:\n                # 1D case\n                bin_volume = edges[0][j+1] - edges[0][j]\n            else:\n                # 2D case\n                bin_volume = (edges[0][i+1] - edges[0][i]) * (edges[1][j+1] - edges[1][j])\n            \n            # Add contribution to total integral\n            total += bin_val * bin_volume\n    \n    return total", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify 1D and multidimensional edges for integration\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin in the histogram\n    for i in range(len(edges)):\n        for j in range(len(edges[i]) - 1):\n            # Calculate the bin width (edge difference)\n            bin_width = edges[i][j+1] - edges[i][j]\n            \n            # Add the bin content multiplied by width to the total integral\n            bin_value = bins[i][j]\n            total += bin_value * bin_width\n    \n    return total", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify the 1D and multidimensional edges format\n    edges = lena.structures.unify_1_md(edges)\n    \n    # Initialize total integral\n    total = 0.0\n    \n    # Iterate over each bin in the multidimensional mesh\n    for i in range(len(edges)):\n        for j in range(len(edges[i])):\n            # Get the bin value\n            bin_value = bins[i][j]\n            \n            # Calculate the bin width in each dimension\n            width = edges[i+1][j] - edges[i][j]\n            \n            # Add the contribution of this bin to the total integral\n            total += bin_value * width\n    \n    return total"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 119542, "num_retrieved": 357, "inference_time": 28.69390033185482, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq is a sequence that contains a FillRequest element\n    if isinstance(seq, (lena.core.Sequence, tuple)):\n        # If seq is a tuple, expand it\n        if isinstance(seq, tuple):\n            seq = seq[0]\n        \n        # Check if seq has a FillRequest element\n        if hasattr(seq, '_after') or hasattr(seq, '_fill_request'):\n            return True\n    \n    # Check if seq is a FillRequest element\n    if hasattr(seq, 'fill') and hasattr(seq, 'request'):\n        return True\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n        \n    if isinstance(seq, tuple):\n        for element in seq:\n            if is_fill_request_seq(element):\n                return True\n        return False\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq is a sequence that contains a FillRequest element\n    if hasattr(seq, '_seq') and isinstance(seq, lena_sequence.LenaSequence):\n        for el in seq._seq:\n            if isinstance(el, fill_request_seq.FillRequestSeq):\n                return True\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, fill_request_seq.FillRequest):\n        return True\n    if isinstance(seq, source.Source):\n        return False\n    if isinstance(seq, tuple):\n        for element in seq:\n            if is_fill_request_seq(element):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq is a sequence that contains a FillRequest element\n    if hasattr(seq, '_seq') and isinstance(seq, lena_sequence.LenaSequence):\n        for el in seq._seq:\n            if isinstance(el, fill_request_seq.FillRequestSeq):\n                return True\n    \n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 115801, "num_retrieved": 326, "inference_time": 28.69390033185482, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 114340, "num_retrieved": 313, "inference_time": 28.69390033185482, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 115801, "num_retrieved": 326, "inference_time": 28.69390033185482, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 114499, "num_retrieved": 385, "inference_time": 28.69390033185482, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion: check all keys and values\n        result = {}\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n        return result\n    \n    # Level 1: only check direct keys and values\n    result = {}\n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif d1[key] != d2[key]:\n            result[key] = d1[key]\n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Level > 1 - recursive check\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys and values\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # For level > 0, recursively process only up to the specified depth\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=-1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys and values\n        result = {}\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=0)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n        return result\n    else:\n        # Recursive case - check up to the specified level\n        result = {}\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=level-1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys and values\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Recursive case with finite level\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 110010, "num_retrieved": 303, "inference_time": 28.69390033185482, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and coordinates\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    if len(coord) != len(edges):\n        return\n    \n    # Determine the bin indices for each coordinate\n    bin_indices = []\n    for i, edge in enumerate(edges):\n        if len(edge) <= 1:\n            # One-dimensional case\n            bin_indices.append(0)\n        else:\n            # Multi-dimensional case\n            bin_idx = get_bin_on_value(coord[i], edge)\n            bin_indices.append(bin_idx)\n    \n    # Check if the bin indices are valid (within bounds)\n    for idx in bin_indices:\n        if idx < 0 or idx >= len(edges[i]) - 1:\n            return\n    \n    # Get the bin content at the calculated indices\n    bin_content = self.bins\n    for i, idx in enumerate(bin_indices):\n        if i == 0:\n            bin_content = bin_content[idx]\n        else:\n            bin_content = bin_content[idx]\n    \n    # Update the bin content with the weight\n    if isinstance(bin_content, (list, tuple)):\n        bin_content = bin_content + weight\n    else:\n        bin_content = bin_content + weight\n    \n    # Update the histogram bins with the new content\n    self.bins[bin_indices] = bin_content", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and check if the coordinate is within bounds\n    edges = self.edges\n    \n    # If the histogram is 1D, check against single edges\n    if len(edges) == 1:\n        low, high = edges[0][0], edges[0][1]\n        if low <= coord <= high:\n            # Get the bin index for the coordinate\n            bin_index = get_bin_on_value(coord, edges)\n            # Update the bin content with the weight\n            if bin_index is not None:\n                self.bins[bin_index] += weight\n    # If the histogram is multi-dimensional, check each coordinate\n    else:\n        # For multi-dimensional histograms, we need to check each coordinate\n        # against its corresponding edge\n        try:\n            # Get the bin indices for each coordinate\n            bin_indices = get_bin_on_value(coord, edges)\n            if bin_indices is not None:\n                # Update the bin content with the weight\n                self.bins[bin_indices] += weight\n        except Exception:\n            # If any coordinate is out of bounds, ignore the fill\n            pass", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and coordinates\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    if len(coord) != len(edges):\n        return\n    \n    # Determine the bin indices for each coordinate\n    bin_indices = []\n    for i, edge in enumerate(edges):\n        if len(edge) <= 1:\n            # One-dimensional case\n            bin_indices.append(0)\n        else:\n            # Multi-dimensional case\n            bin_index = get_bin_on_value(coord[i], edge)\n            bin_indices.append(bin_index)\n    \n    # Check if bin indices are valid (within bounds)\n    for i, bin_index in enumerate(bin_indices):\n        if bin_index < 0 or bin_index >= len(edges[i]) - 1:\n            return\n    \n    # Update the bin content with the weight\n    bin_content = self.bins\n    for i, bin_index in enumerate(bin_indices):\n        if i == 0:\n            bin_content = bin_content[bin_index]\n        else:\n            bin_content = bin_content[bin_index]\n    \n    # Add the weight to the bin content\n    if isinstance(bin_content, (list, tuple)):\n        bin_content = bin_content + [weight]\n    else:\n        bin_content = bin_content + weight\n    \n    # Update the histogram bins\n    self.bins = bin_content", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and coordinates\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    if len(coord) != len(edges):\n        return\n    \n    # Determine the bin indices for each coordinate\n    bin_indices = []\n    for i, edge in enumerate(edges):\n        if len(edge) <= 1:\n            # Handle one-dimensional case\n            if coord[i] < edge[0]:\n                bin_indices.append(0)\n            elif coord[i] >= edge[-1]:\n                bin_indices.append(len(edge) - 2)\n            else:\n                # Find the bin index using binary search\n                bin_idx = get_bin_on_value_1d(coord[i], edge)\n                bin_indices.append(bin_idx)\n        else:\n            # Handle multidimensional case\n            bin_idx = get_bin_on_value_1d(coord[i], edge)\n            bin_indices.append(bin_idx)\n    \n    # Get the bin content at the determined indices\n    bin_content = self.bins\n    for i, idx in enumerate(bin_indices):\n        if i == 0:\n            bin_content = bin_content[idx]\n        else:\n            bin_content = bin_content[idx][i]\n    \n    # Update the bin content with the weight\n    if bin_content is None:\n        bin_content = 0\n    \n    bin_content += weight\n    \n    # Update the histogram bins\n    self.bins = [bin_content]", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and coordinates\n    edges = self.edges\n    dim = len(edges)\n    \n    # Check if coordinates are within the valid range for each dimension\n    bin_indices = []\n    for i in range(dim):\n        # Get the current dimension's edges\n        edge_arr = edges[i]\n        # Check if coordinate is within bounds\n        if coord[i] < edge_arr[0] or coord[i] >= edge_arr[-1]:\n            # Coordinate is outside valid range, ignore this fill\n            return\n        # Find the bin index using binary search\n        bin_indices.append(get_bin_on_value_1d(coord[i], edge_arr))\n    \n    # Get the bin content at the calculated indices\n    bin_content = self.bins\n    # Navigate through the nested structure to get the bin content\n    current_bin = bin_content\n    for idx in bin_indices:\n        current_bin = current_bin[idx]\n    \n    # Add the weight to the bin content\n    current_bin += weight"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 139020, "num_retrieved": 273, "inference_time": 28.69390033185482, "generate_results": ["def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the regex pattern (example pattern - adjust as needed)\n        # This is a placeholder - actual regex should be defined in the context\n        if not re.match(r\"^[a-zA-Z][a-zA-Z0-9_]*$\", key):\n            errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string or bytes-like object\n        if not isinstance(value, (str, bytes)):\n            errors.append({str(value): \"Expected string or bytes-like object\"})\n            continue\n            \n        # Check if value matches the regex pattern (example pattern - adjust as needed)\n        # This is a placeholder - actual regex should be defined in the context\n        if not re.match(r\"^[a-zA-Z0-9][a-zA-Z0-9_\\-]*$\", value):\n            errors.append({value: f\"Label value '{value}' does", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the regex pattern for label keys\n        # (assuming a regex pattern is defined somewhere in the codebase)\n        # This is a placeholder - actual regex would be defined elsewhere\n        if not re.match(r\"^[a-z][a-z0-9_]*$\", key):\n            errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): \"Expected string or bytes-like object\"})\n            else:\n                errors.append({str(value): \"Expected string or bytes-like object\"})\n    \n    # If there are errors, raise ValidationError with the list of errors\n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the regex pattern for label keys\n        # (assuming a regex pattern is defined somewhere in the codebase)\n        # This is a placeholder - in a real implementation, the regex would be defined\n        import re\n        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', key):\n            errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, (str, bytes)):\n            errors.append({str(value): \"Label value must be a string or bytes-like object\"})\n            continue\n            \n        # Check if value matches the regex pattern for label values\n        # (assuming a regex pattern is defined somewhere in the codebase)\n        # This is a placeholder - in a real implementation, the regex would be defined\n        import re\n        if not re.match(r", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the regex pattern for label keys\n        # (assuming a regex pattern is defined somewhere in the codebase)\n        # For example, keys should not contain special characters like '$', 'True', etc.\n        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', key):\n            errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, (str, list)):\n            errors.append({str(value): \"Label value must be a string or list\"})\n            continue\n            \n        # If value is a list, validate each element\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"Label value must be a string\"})\n        # If value is a string, validate it\n        else", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the regex pattern for label keys\n        # (assuming a regex pattern is defined somewhere in the codebase)\n        # This is a placeholder - in a real implementation, the regex would be defined\n        import re\n        label_key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n        if not re.match(label_key_pattern, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, (str, list)):\n            errors.append({str(value): \"Label value must be a string or list\"})\n            continue\n            \n        # If value is a list, validate each element\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"Label value must be a string\"})"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 143965, "num_retrieved": 435, "inference_time": 28.69390033185482, "generate_results": ["def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$\")\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 142878, "num_retrieved": 402, "inference_time": 28.69390033185482, "generate_results": ["def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic would depend on the specific regular expression\n    # that is defined in the context. Since the context does not specify the exact\n    # regular expression, this implementation assumes a basic validation pattern.\n    # In a real implementation, this would be replaced with the specific regex pattern.\n    \n    # Example: Validate that value is a non-empty string containing only alphanumeric characters\n    if not value or not value.strip():\n        raise ValidationError(\"Value cannot be empty or whitespace\")\n    \n    # Example: Validate that value contains only alphanumeric characters\n    if not value.isalnum():\n        raise ValidationError(\"Value must contain only alphanumeric characters\")\n    \n    # If more specific validation rules exist in the context, they should be implemented here\n    return True", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic would depend on the specific regular expression\n    # that is defined in the context. Since the context does not specify the\n    # exact regular expression, this implementation assumes a basic validation\n    # that checks if the value is not empty and contains only alphanumeric characters.\n    # In a real implementation, this would be replaced with the specific regex pattern.\n    \n    if not value:\n        raise ValidationError(\"Value cannot be empty\")\n    \n    # Basic validation: check if value contains only alphanumeric characters and underscores\n    # This is a placeholder - the actual regex would be defined in the context\n    if not re.match(r'^[a-zA-Z0-9_]+$', value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected pattern\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The regular expression pattern for validating values is not explicitly defined\n    # in the provided context, so we'll implement a basic validation that checks\n    # if the value is a non-empty string and contains only valid characters\n    # This is a placeholder implementation - in a real scenario, the specific\n    # regex pattern would be defined elsewhere\n    \n    if not value or not isinstance(value, str):\n        raise ValidationError(\"Value must be a non-empty string\")\n    \n    # Basic validation: check if value contains only printable ASCII characters\n    # and is not just whitespace\n    if not value.strip():\n        raise ValidationError(\"Value cannot be empty or contain only whitespace\")\n    \n    # Additional validation can be added based on specific requirements\n    # For example, validating email format, URL format, etc.\n    \n    return True", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic would depend on the specific regular expression\n    # that is defined in the context. Since the context does not provide the\n    # specific regex pattern, this implementation assumes a basic validation\n    # pattern that checks for non-empty strings and basic format.\n    if not value or not isinstance(value, str):\n        raise ValidationError(\"Value must be a non-empty string\")\n    \n    # Additional validation can be added here based on specific requirements\n    # For example, validating email format, phone number, etc.\n    \n    # If specific validation rules are defined elsewhere, they should be applied here\n    return True", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic would depend on the specific regular expression\n    # that is defined in the context. Since the context does not specify the\n    # exact pattern, this implementation would typically use a pattern defined\n    # elsewhere in the codebase.\n    \n    # Example implementation (this is a placeholder - actual pattern would be defined):\n    # Pattern for a valid email address\n    import re\n    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    \n    if not re.match(email_pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected pattern\")"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 142970, "num_retrieved": 406, "inference_time": 28.69390033185482, "generate_results": ["def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid keys (alphanumeric, hyphens, underscores, and dots)\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid keys (alphanumeric, hyphens, underscores, and dots)\n    import re\n    pattern = re.compile(r'^[a-zA-Z0-9._-]+$')\n    if not pattern.match(key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid keys (alphanumeric, hyphens, underscores, and dots)\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid keys (alphanumeric, hyphens, underscores, and dots)\n    import re\n    pattern = re.compile(r'^[a-zA-Z0-9._-]+$')\n    if not pattern.match(key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression for valid keys (alphanumeric, hyphens, underscores, and dots)\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140118, "num_retrieved": 311, "inference_time": 28.69390033185482, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    # Handle first_level case - copy identifying fields from manifest\n    if first_level:\n        # Copy apiVersion and kind from manifest\n        if \"apiVersion\" in manifest_dict:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        if \"kind\" in manifest_dict:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n            \n        # Handle metadata dictionary\n        if \"metadata\" in manifest_dict:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            # Copy metadata fields\n            if \"name\" in metadata:\n                result[\"metadata\"][\"name\"] = metadata[\"name\"]\n            # Copy other metadata fields if present\n            for key, value in metadata.items():\n                if key not in [\"name\"]:\n                    result[\"metadata\"][key] = value\n    \n    # Process all other fields\n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        # If value is a dict or list, recursively process it\n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n            \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # If first_level is True, we are dealing with the complete observer schema\n    # for a resource, so we need to copy the identifying fields from the manifest\n    if first_level:\n        # Copy the apiVersion and kind fields from the manifest\n        observer_schema[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        observer_schema[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        # Copy the metadata dictionary if it exists\n        metadata = manifest_dict.get(\"metadata\")\n        if metadata:\n            observer_schema[\"metadata\"] = {}\n            # Copy metadata fields that are not None\n            for key, value in metadata.items():\n                if value is not None:\n                    observer_schema[\"metadata\"][key] = value\n                else:\n                    observer_schema[\"metadata\"][key] = None\n    \n    # Recursively process all values in the manifest dictionary\n    for key, value in manifest_dict.items():\n        # Skip if we already processed this key in the first_level case\n        if first_level and key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        # If the value is a dictionary, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        # If the value is a list, process it with the list function\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value)\n        # For all other values, set to", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # If first_level is True, we're dealing with the complete observer schema\n    # for a resource, so we need to copy the identifying fields from the manifest\n    if first_level:\n        # Copy the apiVersion and kind fields from the manifest\n        observer_schema[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n        observer_schema[\"kind\"] = manifest_dict.get(\"kind\")\n        \n        # Copy the metadata dictionary if it exists\n        metadata = manifest_dict.get(\"metadata\", {})\n        if metadata:\n            observer_schema[\"metadata\"] = {}\n            # Copy metadata fields that are not None\n            for key, value in metadata.items():\n                if value is not None:\n                    observer_schema[\"metadata\"][key] = value\n        else:\n            observer_schema[\"metadata\"] = {}\n            \n    # Process all other fields in the manifest dictionary\n    for key, value in manifest_dict.items():\n        # Skip already processed fields (apiVersion, kind, metadata)\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        # If the value is a dictionary or list, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n            \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    # Handle first level dictionary - complete observer schema for a resource\n    if first_level:\n        # Copy identifying fields from manifest\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            # Copy metadata fields\n            try:\n                result[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n            # Other metadata fields can be copied if needed\n        except KeyError:\n            pass\n            \n        # Replace all non-list and non-dict values with None\n        for key, value in manifest_dict.items():\n            if key not in [\"apiVersion\", \"kind\", \"metadata\"]:\n                if not isinstance(value, dict) and not isinstance(value, list):\n                    result[key] = None\n    else:\n        # For non-first level, just replace non-list and non-dict values with None\n        for key, value in manifest_dict.items():\n            if not isinstance(value, dict) and not isinstance(value, list):\n                result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    # Handle first level dictionary - complete observer schema for a resource\n    if first_level:\n        # Copy identifying fields from manifest\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            # Copy metadata fields\n            try:\n                result[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n            # Copy other metadata fields if present\n            for key, value in metadata.items():\n                if key not in [\"name\"]:\n                    result[\"metadata\"][key] = value\n        except KeyError:\n            pass\n    \n    # Recursively process all fields in the manifest\n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            # Replace non-list and non-dict values with None\n            result[key] = None\n            \n    return result"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140208, "num_retrieved": 313, "inference_time": 28.69390033185482, "generate_results": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        if idx >= len(last_applied_manifest):\n            # Add new resource to last_applied_manifest\n            last_applied_manifest.append(resource)\n            continue\n            \n        # Check if the resource already exists in last_applied_manifest\n        if last_applied_manifest[idx] == resource:\n            continue\n            \n        # If the resource is not present, add it\n        last_applied_manifest[idx] = resource\n        \n        # Update the resource in the observer schema if needed\n        if idx < len(observer_schema):\n            observer_schema[idx] = resource", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        # Check if the resource is already in last_applied_manifest\n        if idx >= len(last_applied_manifest):\n            # Add the new resource to last_applied_manifest\n            last_applied_manifest.append(resource)\n            continue\n            \n        # Check if the resource at the current index is different from the one in last_applied_manifest\n        if last_applied_manifest[idx] != resource:\n            # Update the resource in last_applied_manifest\n            last_applied_manifest[idx] = resource\n            continue\n            \n        # If the resource is the same, continue to the next one\n        continue\n        \n    # Update the last_applied_manifest with the observer_schema if needed\n    for idx, resource in enumerate(observer_schema):\n        if idx >= len(last_applied_manifest):\n            # Add the new resource to last_applied_manifest\n            last_applied_manifest.append(resource)\n            continue\n            \n        # Check if the resource at the current index is different from the one in last_applied_manifest\n        if last_applied_manifest[idx] != resource:\n            # Update the resource in last_applied_manifest\n            last_applied_manifest[idx] = resource\n            continue\n            \n        # If the resource is the same, continue to the next one\n        continue", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        # Find the corresponding observer schema entry\n        observed_idx = get_kubernetes_resource_idx(\n            observer_schema, resource\n        )\n        observed_resource = observer_schema[observed_idx]\n        \n        # Check if the resource already exists in last_applied_manifest\n        existing_idx = -1\n        for i, existing_resource in enumerate(last_applied_manifest):\n            if (existing_resource[\"apiVersion\"] == resource[\"apiVersion\"] and\n                existing_resource[\"kind\"] == resource[\"kind\"] and\n                existing_resource[\"metadata\"][\"name\"] == resource[\"metadata\"][\"name\"]):\n                existing_idx = i\n                break\n        \n        # If the resource is not present, add it to last_applied_manifest\n        if existing_idx == -1:\n            last_applied_manifest.append(resource)\n        else:\n            # Update the existing resource with the response values\n            # Only update fields that are present in the observer schema\n            for key, value in observed_resource.items():\n                if key in resource:\n                    # Skip namespace field for non-namespaced resources\n                    if key == \"namespace\" and resource[\"metadata\"].get(\"namespace\") is None:\n                        continue\n                    # Update the value in the existing resource\n                    resource[key] = value\n            # Update the resource at the existing index\n            last_applied_manifest[existing_idx] = resource", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        # Check if the resource is already in last_applied_manifest\n        existing_idx = -1\n        for i, existing_resource in enumerate(last_applied_manifest):\n            if (existing_resource[\"apiVersion\"] == resource[\"apiVersion\"] and\n                existing_resource[\"kind\"] == resource[\"kind\"] and\n                existing_resource[\"metadata\"][\"name\"] == resource[\"metadata\"][\"name\"]):\n                existing_idx = i\n                break\n        \n        # If resource is not present, add it to last_applied_manifest\n        if existing_idx == -1:\n            last_applied_manifest.append(resource)\n        else:\n            # Update the existing resource with the response values\n            # This is a simplified update - actual implementation would depend on\n            # the specific fields to update\n            last_applied_manifest[existing_idx] = resource", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, resource in enumerate(response):\n        # Check if the resource is already in last_applied_manifest\n        existing_idx = -1\n        for i, existing_resource in enumerate(last_applied_manifest):\n            if (existing_resource[\"apiVersion\"] == resource[\"apiVersion\"] and\n                existing_resource[\"kind\"] == resource[\"kind\"] and\n                existing_resource[\"metadata\"][\"name\"] == resource[\"metadata\"][\"name\"]):\n                existing_idx = i\n                break\n        \n        # If the resource is not present, add it to last_applied_manifest\n        if existing_idx == -1:\n            # Find the corresponding observer schema entry\n            observer_idx = -1\n            for i, obs in enumerate(observer_schema):\n                if (obs[\"apiVersion\"] == resource[\"apiVersion\"] and\n                    obs[\"kind\"] == resource[\"kind\"] and\n                    obs[\"metadata\"][\"name\"] == resource[\"metadata\"][\"name\"]):\n                    observer_idx = i\n                    break\n            \n            # Add the resource to last_applied_manifest\n            if observer_idx != -1:\n                last_applied_manifest.append(resource)\n            else:\n                # If no matching observer schema entry found, skip\n                continue\n        \n        # Update the existing resource with the response data\n        # This is a simplified update - actual implementation would depend on\n        # the specific fields to be updated\n        last_applied_manifest[existing_idx] = resource"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140013, "num_retrieved": 306, "inference_time": 28.69390033185482, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if key not in last_applied_manifest:\n            last_applied_manifest[key] = value\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 143170, "num_retrieved": 410, "inference_time": 12.027863338589668, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Create a default observer schema for each resource in the manifest\n    default_schema = []\n    \n    # Iterate through each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Create a default observer schema entry for each resource\n        # The schema includes the resource's API version, kind, and name\n        schema_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the resource to the default schema list\n        default_schema.append(schema_entry)\n    \n    # Set the default observer schema on the application\n    app.status.mangled_observer_schema = default_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Create a default observer schema for each resource in the manifest\n    default_schema = []\n    \n    # Iterate through each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Create a default observer schema entry for each resource\n        # The schema includes the resource's API version, kind, and name\n        schema_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the resource to the default schema list\n        default_schema.append(schema_entry)\n    \n    # Set the default observer schema on the application\n    app.status.mangled_observer_schema = default_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Create a copy of the manifest to avoid modifying the original\n    manifest = app.spec.manifest or []\n    \n    # Initialize the observer schema list\n    observer_schema = []\n    \n    # For each resource in the manifest, create a default observer schema entry\n    for resource in manifest:\n        # Extract the resource metadata\n        api_version = resource.get(\"apiVersion\")\n        kind = resource.get(\"kind\")\n        name = resource.get(\"metadata\", {}).get(\"name\")\n        namespace = resource.get(\"metadata\", {}).get(\"namespace\")\n        \n        # Skip if any required fields are missing\n        if not api_version or not kind or not name:\n            continue\n            \n        # Create a default observer schema entry\n        observer_entry = {\n            \"apiVersion\": api_version,\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name,\n                \"namespace\": namespace\n            }\n        }\n        \n        # Add the observer entry to the schema\n        observer_schema.append(observer_entry)\n    \n    # Set the observer schema on the application\n    app.spec.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Create a default observer schema for each resource in the manifest\n    default_schema = []\n    \n    # Iterate through each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Create a default observer schema entry for each resource\n        # The schema includes the resource's API version, kind, and name\n        schema_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the resource to the default schema list\n        default_schema.append(schema_entry)\n    \n    # Set the default observer schema on the application\n    app.status.mangled_observer_schema = default_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Create a default observer schema for each resource in the manifest\n    default_schema = []\n    \n    # Iterate through each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Create a default observer schema entry for each resource\n        # The schema includes the resource's API version, kind, and name\n        observer_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the resource to the default schema\n        default_schema.append(observer_entry)\n    \n    # Set the default observer schema on the application\n    app.status.mangled_observer_schema = default_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 28364, "num_retrieved": 74, "inference_time": 12.027863338589668, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query and parameters\n    param_conversions = []\n    out_format = self._out_style.out_format\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, out_format), sql)\n    \n    # Convert parameters\n    out_params = self.__convert_params(params, param_conversions)\n    \n    return out_sql, out_params"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 28438, "num_retrieved": 74, "inference_time": 12.027863338589668, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 41738, "num_retrieved": 112, "inference_time": 12.027863338589668, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.validate(path)\n            return True\n        except Exception:\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            (inv_dir, inv_file) = fs.path.split(path)\n            validator.obj_fs = open_fs(inv_dir, create=False)\n            validator.validate_inventory(inv_file, where='standalone')\n            return validator.log.num_errors == 0\n        except fs.errors.ResourceNotFound:\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            objdir = path\n            if not self.obj_fs.exists(objdir):\n                raise fs.errors.ResourceNotFound(\"Object directory not found\")\n            validator.obj_fs = self.obj_fs\n            if validator.validate(objdir):\n                return True\n            else:\n                return False\n        except Exception as e:\n            logging.error(\"Validation failed for path %s: %s\", path, str(e))\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            validator.validate(path)\n            return True\n        except Exception:\n            return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    if path is None:\n        # Validate the entire storage root\n        return self.validate_hierarchy(validate_objects=True, check_digests=True, show_warnings=False)[1] > 0\n    else:\n        # Validate a specific object at path\n        validator = Validator(show_warnings=True, check_digests=True)\n        try:\n            objdir = path\n            if not self.root_fs.exists(objdir):\n                return False\n            validator.obj_fs = open_fs(objdir, create=False)\n            if validator.validate(objdir):\n                return True\n            else:\n                return False\n        except Exception as e:\n            return False"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 50103, "num_retrieved": 122, "inference_time": 12.027863338589668, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    return self.__str__()"]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 43124, "num_retrieved": 108, "inference_time": 12.027863338589668, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 49205, "num_retrieved": 120, "inference_time": 12.027863338589668, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 41818, "num_retrieved": 110, "inference_time": 12.027863338589668, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='standalone', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the inventory type value\n            type_value = inventory.get('type')\n            if type_value:\n                # Extract version from type value (e.g., \"https://ocfl.io/1.0/spec/#inventory\")\n                match = re.search(r'/(\\d+\\.\\d+)', type_value)\n                if match:\n                    spec_version = match.group(1)\n                    inv_validator.spec_version = spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='standalone', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 40674, "num_retrieved": 99, "inference_time": 12.027863338589668, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in manifest_files[filepath]:\n            if digest not in digests_used:\n                self.log.error('E092b', where='root', content_path=filepath)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 41982, "num_retrieved": 113, "inference_time": 12.027863338589668, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version's spec version is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=self.spec_version, \n                       prev_version_dir='root', prev_spec_version=prior.spec_version)\n    # Ensure the prior version's digest algorithm is compatible\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.error('E064', root_inv_file='inventory.json', inv_file='inventory.json')", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.error('E104', where='root', prior_digest_algorithm=prior.digest_algorithm,\n                       current_digest_algorithm=self.digest_algorithm)\n    # Check that all files in prior manifest are present in current manifest\n    if 'manifest' in prior.inventory and 'manifest' in self.inventory:\n        for digest in prior.inventory['manifest']:\n            if digest not in self.inventory['manifest']:\n                self.log.error('E023b', where='root', missing_filepaths=digest)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        self.log.error('E064', root_inv_file='inventory.json', inv_file='inventory.json')", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        # If the digest algorithm changed, ensure that the root inventory digest matches\n        if self.check_digests:\n            root_inv_file = 'inventory.json'\n            prior_inv_file = 'inventory.json.' + prior.digest_algorithm\n            if not ocfl_files_identical(self.obj_fs, root_inv_file, prior_inv_file):\n                self.log.error('E064', root_inv_file=root_inv_file, inv_file=prior_inv_file)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='root', this_spec_version=prior.spec_version,\n                       prev_version_dir='root', prev_spec_version=self.spec_version)\n    # Check that the digest algorithm of prior is compatible with current digest algorithm\n    if prior.digest_algorithm != self.digest_algorithm:\n        # If the digest algorithm changed, ensure it's a valid transition\n        # For now, just log a warning as per spec requirements\n        self.log.warning('W011', prior_digest_algorithm=prior.digest_algorithm,\n                         current_digest_algorithm=self.digest_algorithm)"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 42172, "num_retrieved": 118, "inference_time": 12.027863338589668, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical paths in the state\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add the content files that correspond to this logical path\n            # The content files are stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical paths in the state\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            # Add the content files from the manifest for this digest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_path].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical files in the state\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add the content files that correspond to this logical file\n            # The content files are stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical file paths\n        for logical_file in logical_files:\n            # The logical file path is the key in the map\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add the content file paths to the set for this logical path\n            # The content file paths are stored in the manifest under the digest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # For each digest, we have a list of logical files in the state\n        # We need to map each logical file to the actual content files on disk\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # The content files are stored in the manifest under the same digest\n            # We need to find the actual source files from the manifest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 41181, "num_retrieved": 104, "inference_time": 12.027863338589668, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that all files in fixity are present in manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate structure of fixity block\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that all files in fixity are present in manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that all files in fixity are present in manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all files in fixity are present in manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 120397, "num_retrieved": 500, "inference_time": 12.027863338589668, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isfile(item_path):\n                files.append(item_path)\n        return files\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isfile(item_path):\n                files.append(item_path)\n        return files\n    return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for file in os.listdir(path):\n            file_path = os.path.join(path, file)\n            if os.path.isfile(file_path):\n                files.append(file_path)\n        return files\n    return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isfile(item_path):\n                files.append(item_path)\n        return files\n    return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if os.path.isfile(path):\n        return [path]\n    elif os.path.isdir(path):\n        files = []\n        for item in os.listdir(path):\n            item_path = os.path.join(path, item)\n            if os.path.isfile(item_path):\n                files.append(item_path)\n        return files\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 142482, "num_retrieved": 500, "inference_time": 12.027863338589668, "generate_results": ["def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.basename(xml_file)\n        xml_name = os.path.splitext(xml_basename)[0]\n        \n        # Initialize the group for this XML file\n        grouped_files[xml_name] = []\n        \n        # Add all files that match the XML basename pattern\n        for file in files:\n            file_basename = os.path.basename(file)\n            file_name = os.path.splitext(file_basename)[0]\n            \n            # Check if the file name matches the XML basename\n            if file_name == xml_name:\n                grouped_files[xml_name].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n    \n    for file in files:\n        file_basename = os.path.basename(file)\n        for xml_basename in grouped_files:\n            if file_basename.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n        \n        for file in files:\n            if file.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n        \n        for file in files:\n            if file.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_file in xmls:\n        xml_basename = os.path.splitext(xml_file)[0]\n        grouped_files[xml_basename] = []\n    \n    for file in files:\n        file_basename = os.path.basename(file)\n        for xml_basename in grouped_files:\n            if file_basename.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 132790, "num_retrieved": 500, "inference_time": 12.027863338589668, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns True for files belonging to a package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns True for files belonging to a package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns True for files belonging to a package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Returns True for files belonging to a package.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 129373, "num_retrieved": 500, "inference_time": 12.027863338589668, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if file.startswith(prefix)]"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 137836, "num_retrieved": 500, "inference_time": 21.383709847927094, "generate_results": ["def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get the file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract the basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # If this XML file is not already in the data, initialize it\n        if basename not in package_data:\n            package_data[basename] = []\n            \n        # Add the file path to the list for this XML basename\n        package_data[basename].append(file_path)\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # If this XML file is not in the package data yet, initialize it\n        if basename not in package_data:\n            package_data[basename] = {}\n            \n        # Add the file path to the package data\n        package_data[basename]['xml'] = file_path\n        \n        # Look for associated files (renditions and assets) with the same basename\n        # Renditions and assets typically have the same basename as the XML file\n        # with different extensions (e.g., .pdf, .jpg)\n        \n        # Check for renditions (e.g., .pdf)\n        rendition_files = [f for f in files if f.startswith(basename) and f.endswith('.pdf')]\n        for rendition_file in rendition_files:\n            rendition_path = os.path.join(folder, rendition_file)\n            package_data[basename]['renditions'] = package_data[basename].get('renditions', []) + [rendition_path]\n            \n        # Check for assets (e.g., .jpg, .png)\n        asset_files = [f for f in files if f.startswith(basename) and f.endswith(('.jpg', '.jpeg', '.png', '.tiff', '.gif'))]\n        for asset_file in asset_files:\n            asset_path = os.path.join(folder, asset_file)\n            package_data[basename]['assets'] = package_data[basename].get('assets', []) + [asset_path]\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # If this XML file is not in our data yet, initialize it\n        if basename not in package_data:\n            package_data[basename] = {}\n            \n        # Add the file path to the package data\n        package_data[basename]['xml'] = file_path\n        \n        # Check if there are any related files (renditions or assets) with the same basename\n        # Look for files that match the basename pattern\n        for related_file in files:\n            related_path = os.path.join(folder, related_file)\n            if not os.path.isfile(related_path):\n                continue\n                \n            related_basename, related_ext = os.path.splitext(related_file)\n            if related_basename == basename and related_ext.lower() in ['.pdf', '.jpg', '.jpeg', '.png', '.tiff']:\n                # Add the related file to the package data\n                if 'files' not in package_data[basename]:\n                    package_data[basename]['files'] = []\n                package_data[basename]['files'].append(related_path)\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename without extension\n        basename = os.path.splitext(filename)[0]\n        \n        # Initialize the package data if not exists\n        if basename not in package_data:\n            package_data[basename] = []\n            \n        # Add the file path to the package data\n        package_data[basename].append(file_path)\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Group files by their XML basename (without extension)\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # Initialize the package data if not exists\n        if basename not in package_data:\n            package_data[basename] = []\n            \n        # Add the file path to the package data\n        package_data[basename].append(file_path)\n    \n    return package_data"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 136943, "num_retrieved": 500, "inference_time": 21.383709847927094, "generate_results": ["def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if it's a rendition by comparing with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in ['pt', 'en', 'es', 'fr', 'de']:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if it's a rendition by comparing with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in get_pdf_langs():  # Assuming get_pdf_langs() is defined elsewhere\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if it's a rendition by comparing with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in ['en', 'pt', 'es', 'fr', 'de', 'it', 'ja', 'ko', 'zh']:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if the file is a rendition by comparing the filename with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in ['pt', 'en', 'es']:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    # Check if the file is a rendition by comparing with the prefix\n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in ['en', 'pt', 'es', 'fr', 'de', 'it', 'ja', 'ko', 'zh']:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    \n    # If not a rendition, it's an asset\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 56190, "num_retrieved": 181, "inference_time": 21.383709847927094, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == 'en':\n        rendition_dict['en'] = file_path\n    else:\n        rendition_dict['original'] = file_path\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == 'en':\n        rendition_dict['en'] = file_path\n    else:\n        rendition_dict['original'] = file_path\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == 'en':\n        rendition_dict['en'] = file_path\n    else:\n        rendition_dict['original'] = file_path\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == \"original\":\n        rendition_dict[\"original\"] = file_path\n    else:\n        rendition_dict[lang] = file_path\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    {\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n    }\n    \"\"\"\n    rendition_dict = {}\n    if lang == 'en':\n        rendition_dict['en'] = file_path\n    else:\n        rendition_dict['original'] = file_path\n    return rendition_dict"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 54083, "num_retrieved": 224, "inference_time": 21.383709847927094, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    # Create a dictionary to store the asset mapping\n    asset_map = {}\n    \n    # Extract the base name without extension\n    base_name = basename\n    \n    # Get the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create a key for the asset with the base name and extension\n    asset_key = f\"{base_name}{ext}\"\n    \n    # Add the asset to the map\n    asset_map[asset_key] = file_path\n    \n    return asset_map", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    self[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    # Create a dictionary to store the asset mapping\n    asset_map = {}\n    \n    # Extract the base name without extension\n    base_name = basename\n    \n    # Get the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create the canonical asset name by combining base name with extension\n    canonical_name = f\"{base_name}{ext}\"\n    \n    # Add the asset to the map with the canonical name as key and file path as value\n    asset_map[canonical_name] = file_path\n    \n    return asset_map", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    # Create a dictionary to store the asset mapping\n    asset_map = {}\n    \n    # Extract the base name without extension\n    base_name = basename\n    \n    # Get the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create a key for the asset with the base name and extension\n    asset_key = f\"{base_name}{ext}\"\n    \n    # Add the asset to the map\n    asset_map[asset_key] = file_path\n    \n    return asset_map", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    # Create a dictionary to store the asset mapping\n    asset_map = {}\n    \n    # Extract the base name without extension\n    base_name = basename\n    \n    # Get the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create a key for the asset with the base name and extension\n    asset_key = f\"{base_name}{ext}\"\n    \n    # Add the asset to the map\n    asset_map[asset_key] = file_path\n    \n    return asset_map"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 134426, "num_retrieved": 500, "inference_time": 21.383709847927094, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    package_data = {}\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        \n        for file_name in file_list:\n            # Extract the basename (without extension) of the XML file\n            file_base_name, file_ext = os.path.splitext(file_name)\n            \n            # Only process XML files\n            if file_ext.lower() == '.xml':\n                # Use the basename as the key in the dictionary\n                basename = file_base_name\n                \n                # Initialize the list if it doesn't exist\n                if basename not in package_data:\n                    package_data[basename] = []\n                \n                # Add the file path to the list\n                package_data[basename].append(file_name)\n    \n    return package_data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        for file_name in file_list:\n            if file_name.endswith(\".xml\"):\n                basename, ext = os.path.splitext(file_name)\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(file_name)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        for file_name in file_list:\n            if file_name.endswith(\".xml\"):\n                basename, ext = os.path.splitext(file_name)\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(file_name)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    package_data = {}\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        \n        for file_name in file_list:\n            # Extract the basename of the XML file (without extension)\n            file_base_name, file_ext = os.path.splitext(file_name)\n            \n            # Only process XML files\n            if file_ext.lower() == '.xml':\n                # Use the basename as the key in the dictionary\n                if file_base_name not in package_data:\n                    package_data[file_base_name] = []\n                \n                # Add the file path to the list for this basename\n                package_data[file_base_name].append(file_name)\n    \n    return package_data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    package_data = {}\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        \n        for file_name in file_list:\n            # Extract the basename of the XML file (without extension)\n            file_base_name, file_ext = os.path.splitext(file_name)\n            \n            # Only process XML files\n            if file_ext.lower() == '.xml':\n                # Use the basename as the key in the dictionary\n                package_data[file_base_name] = {\n                    'filename': file_name,\n                    'content': zip_file.read(file_name)\n                }\n    \n    return package_data"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 116756, "num_retrieved": 500, "inference_time": 21.383709847927094, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zf:\n        return zf.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zf:\n        return zf.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zf:\n        return zf.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zf:\n        return zf.namelist()"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 134895, "num_retrieved": 484, "inference_time": 21.383709847927094, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    replacement = r'w-st=\"\\1\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    return re.sub(pattern, r'w-st=\"\\1\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    return re.sub(pattern, r'w-st=\"\\1\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    replacement = r'w-st=\"'\\1\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    replacement = r'w-st=\"'\\1\"'\n    return re.sub(pattern, replacement, content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140827, "num_retrieved": 500, "inference_time": 21.383709847927094, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 130764, "num_retrieved": 461, "inference_time": 21.383709847927094, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, 0\n    \n    # Split the issue string by spaces and filter out empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract the number (first numeric part)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if the part is a number (including numbers with 's' suffix)\n        if part.isdigit():\n            number = int(part)\n            break\n        # Check if the part is a number with 's' suffix\n        elif part.endswith('s') and part[:-1].isdigit():\n            number = int(part[:-1])\n            break\n    \n    # Extract supplement if present (e.g., \"2s1\", \"2s\", \"s1\")\n    if number is not None:\n        # Look for supplement after the number\n        for part in parts:\n            if part.startswith('s') and part[1:].isdigit():\n                supplement = int(part[1:])\n                break\n            elif part.startswith('s') and part[1:].isdigit():\n                supplement = int(part[1:])\n                break\n    \n    # If no number found, return 0 for number and 0 for supplement\n    if number is None:\n        number = 0\n    \n    # If no supplement found, return 0 for supplement\n    if supplement is None:\n        supplement = 0\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and filter out empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract the number (first numeric part)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if part is a number (including numbers with 's' or 'spe')\n        if part.isdigit():\n            number = int(part)\n            break\n        elif part.endswith('s') and part[:-1].isdigit():\n            number = int(part[:-1])\n            break\n        elif part.endswith('spe') and part[:-3].isdigit():\n            number = int(part[:-3])\n            break\n    \n    # Extract supplement if present (after a 's' or 'spe')\n    for part in parts:\n        if part.startswith('s') or part.startswith('spe'):\n            supplement = part[1:] if part.startswith('s') else part[1:]\n            break\n    \n    # If no number was found, default to 0\n    if number is None:\n        number = 0\n    \n    # If no supplement was found, default to empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and filter out empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract the number (first numeric part)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if the part is a number (including numbers with 's' or 'spe')\n        if part.isdigit():\n            number = int(part)\n            break\n        elif part.endswith('s') or part.endswith('spe'):\n            # Check if the part before 's' or 'spe' is a number\n            if part[:-1].isdigit():\n                number = int(part[:-1])\n                break\n    \n    # Extract supplement if present (e.g., \"s2\", \"spe2\")\n    for part in parts:\n        if part.startswith('s') or part.startswith('spe'):\n            supplement = part\n            break\n    \n    # If no number was found, default to 0\n    if number is None:\n        number = 0\n    \n    # If supplement is not found, default to empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Try to parse the issue string and extract number and supplement\n    # The issue string might contain a number and a supplement (e.g., \"2(3)\" or \"2-3\")\n    issue_str = issue.strip()\n    \n    # Extract the number part (digits)\n    number_match = re.search(r'(\\d+)', issue_str)\n    number = 0 if number_match is None else int(number_match.group(1))\n    \n    # Extract the supplement part (everything after the number, if any)\n    supplement_match = re.search(r'\\((\\d+)\\)|-(\\d+)|(\\d+)$', issue_str)\n    supplement = \"\"\n    \n    if supplement_match:\n        # Check for (number) format\n        if supplement_match.group(1):\n            supplement = supplement_match.group(1)\n        # Check for -number format\n        elif supplement_match.group(2):\n            supplement = supplement_match.group(2)\n        # Check for number at the end\n        elif supplement_match.group(3):\n            supplement = supplement_match.group(3)\n    \n    # If no supplement found, try to extract from the remaining part\n    if not supplement:\n        # Remove the number part from the original string\n        remaining = re.sub(r'\\d+', '', issue_str)\n        # Extract any digits from the remaining string\n        if remaining.strip():\n            supplement = re.search(r'\\d+', remaining.strip())\n            supplement = supplement.group(0) if supplement else \"\"\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and remove any empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract the number (first part that looks like a number)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if the part is a number (digits only)\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # If no number was found, try to extract the supplement (everything after a number)\n    if number is None:\n        # Look for a supplement (e.g., \"s1\", \"s2\", etc.)\n        for part in parts:\n            if part.startswith('s') and part[1:].isdigit():\n                supplement = part\n                break\n    \n    # If no number was found, default to 0\n    if number is None:\n        number = 0\n    \n    # If no supplement was found, default to empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 21.383709847927094, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 21.383709847927094, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it matches absorption or negative absorption patterns\n        # This is a simplified implementation assuming expressions are in a form that can be evaluated\n        \n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using Boolean algebra rules\n        # We'll assume the expressions are already in a form that can be processed\n        # and that the absorption rules can be applied directly\n        \n        # In practice, this would require parsing and evaluating the expression tree\n        # For now, we return the original expression as a placeholder\n        # A full implementation would require traversing the expression tree\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # This is a simplified implementation - actual logic would depend on expression structure\n        # In practice, this would require parsing the expression tree\n        \n        # For now, we return the original expression as a placeholder\n        # A full implementation would need to traverse the expression tree\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        # Absorption: A & (A | B) = A, A | (A & B) = A\n        # Negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(expr, Expression):\n            # Check for absorption cases\n            # Case 1: A & (A | B) = A\n            # Case 2: A | (A & B) = A\n            # Case 3: A & (~A | B) = A & B\n            # Case 4: A | (~A & B) = A | B\n            \n            # We need to evaluate the structure of the expression\n            # This is a simplified implementation assuming we can detect patterns\n            # In a full implementation, we would parse the expression tree\n            \n            # For now, return the expression as is (placeholder)\n            result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we just return the original expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            # This is a simplified implementation that assumes the expression\n            # is already in a form that can be checked for these laws\n            # In a full implementation, we would need to parse the expression tree\n            # and apply the rules recursively\n            result.append(expr)\n        else:\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 144001, "num_retrieved": 447, "inference_time": 21.383709847927094, "generate_results": ["def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, '_hooks'):\n            self._hooks = {}\n        if hook not in self._hooks:\n            self._hooks[hook] = []\n        self._hooks[hook].append(fn)\n        return fn\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, '_handlers'):\n            self._handlers = {}\n        if hook not in self._handlers:\n            self._handlers[hook] = []\n        self._handlers[hook].append(fn)\n        return fn\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, '_handlers'):\n            self._handlers = {}\n        if hook not in self._handlers:\n            self._handlers[hook] = []\n        self._handlers[hook].append(fn)\n        return fn\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, '_handlers'):\n            self._handlers = {}\n        if hook not in self._handlers:\n            self._handlers[hook] = []\n        self._handlers[hook].append(fn)\n        return fn\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, '_hooks'):\n            self._hooks = {}\n        if hook not in self._hooks:\n            self._hooks[hook] = []\n        self._hooks[hook].append(fn)\n        return fn\n    \n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 143005, "num_retrieved": 418, "inference_time": 21.383709847927094, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"static\": {\n                \"enabled\": True,\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }\n    return config"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 53581, "num_retrieved": 195, "inference_time": 21.383709847927094, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # These isinstance checks are in datetime.tzinfo, so we'll preserve\n    # them, even if we don't care about duck typing.\n    if not isinstance(dt, datetime.datetime):\n        raise TypeError(\"fromutc() requires a datetime argument\")\n\n    if dt.tzinfo is not self:\n        raise ValueError(\"dt.tzinfo is not self\")\n\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n\n    fold = self.is_ambiguous(dt_out, idx=idx)\n\n    return enfold(dt_out, fold=int(fold))"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 53378, "num_retrieved": 203, "inference_time": 21.383709847927094, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the relative attributes and convert fractional values\n    # to integer equivalents\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if attr == 'days':\n                    days = int(value)\n                    # Adjust days if fractional part exists\n                    if value - days != 0:\n                        # Add the fractional part as hours\n                        hours = int((value - days) * 24)\n                        # Add the hours to the result\n                        result.hours += hours\n                elif attr == 'hours':\n                    hours = int(value)\n                    # Adjust hours if fractional part exists\n                    if value - hours != 0:\n                        # Add the fractional part as minutes\n                        minutes = int((value - hours) * 60)\n                        # Add the minutes to the result\n                        result.minutes += minutes\n                elif attr == 'minutes':\n                    minutes = int(value)\n                    # Adjust minutes if fractional part exists\n                    if value - minutes != 0:\n                        # Add the fractional part as seconds\n                        seconds = int((value - minutes) * 60)\n                        # Add the seconds to the result\n                        result.seconds += seconds\n                else:\n                    # For other attributes, just convert to integer\n                    setattr(result, attr, int(value))\n            else:\n                # For non-fractional values, just convert to integer\n                setattr(result, attr, int(value))\n    \n    # Handle the case where fractional values were converted to hours/minutes/seconds\n    # and need to be adjusted to ensure they are within valid ranges\n    if result.hours >= 24:\n        result.days += result.hours // 24\n        result.hours = result.hours % 24\n    \n    if result.minutes >= 60:\n        result.hours += result.minutes // 60", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if attr == 'days':\n                    days = int(value)\n                    # Adjust days if there's a fractional part\n                    if value - days != 0:\n                        # Add the fractional part as hours\n                        hours = int((value - days) * 24)\n                        # Add the hours to the result\n                        result.hours += hours\n                elif attr == 'hours':\n                    hours = int(value)\n                    # Adjust hours if there's a fractional part\n                    if value - hours != 0:\n                        # Add the fractional part as minutes\n                        minutes = int((value - hours) * 60)\n                        # Add the minutes to the result\n                        result.minutes += minutes\n                elif attr == 'minutes':\n                    minutes = int(value)\n                    # Adjust minutes if there's a fractional part\n                    if value - minutes != 0:\n                        # Add the fractional part as seconds\n                        seconds = int((value - minutes) * 60)\n                        # Add the seconds to the result\n                        result.seconds += seconds\n                else:\n                    # For other attributes, just convert to integer\n                    setattr(result, attr, int(value))\n            else:\n                # For non-fractional values, just set the attribute\n                setattr(result, attr, value)\n    \n    # Handle the case where we have fractional values that need to be converted\n    # to integers in a more complex way\n    if hasattr(self, 'days') and self.days is not None:\n        days = self.days\n        if isinstance(days, float):\n            days_int = int(days)\n            fractional_days = days - days_int\n            if fractional_days > 0:\n                # Convert fractional days", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the relative attributes and convert fractional values\n    # to integers by rounding up or down appropriately\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Round to nearest integer\n                rounded_value = round(value)\n                # If the value is exactly halfway, round to the nearest even integer\n                if abs(value - rounded_value) == 0.5:\n                    rounded_value = int(round(value))\n                # Ensure we don't lose precision\n                if rounded_value != value:\n                    # Adjust the value to maintain the same relative offset\n                    # This is a simplified approach - in practice, this would need\n                    # to consider the full context of the delta\n                    pass\n            else:\n                rounded_value = value\n                \n            # Set the attribute with the rounded value\n            setattr(result, attr, rounded_value)\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if attr == 'days':\n                    days = int(value)\n                    # Adjust days if there's a fractional part\n                    if value - days != 0:\n                        # Add the fractional part as hours\n                        hours = int((value - days) * 24)\n                        # Add the hours to the result\n                        result.hours += hours\n                elif attr == 'hours':\n                    hours = int(value)\n                    # Adjust hours if there's a fractional part\n                    if value - hours != 0:\n                        # Add the fractional part as minutes\n                        minutes = int((value - hours) * 60)\n                        # Add the minutes to the result\n                        result.minutes += minutes\n                elif attr == 'minutes':\n                    minutes = int(value)\n                    # Adjust minutes if there's a fractional part\n                    if value - minutes != 0:\n                        # Add the fractional part as seconds\n                        seconds = int((value - minutes) * 60)\n                        # Add the seconds to the result\n                        result.seconds += seconds\n                else:\n                    # For other attributes, just convert to integer\n                    setattr(result, attr, int(value))\n            else:\n                # For non-fractional values, just set the attribute\n                setattr(result, attr, value)\n    \n    # Handle the case where we have fractional days\n    if hasattr(self, 'days') and self.days is not None:\n        days = self.days\n        if isinstance(days, float):\n            # Convert fractional days to hours\n            fractional_days = days - int(days)\n            hours = int(fractional_days * 24)\n            result.hours += hours\n    \n    # Handle the case where we have", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the relative attributes and convert fractional values\n    # to integer equivalents\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if attr == 'days':\n                    days = int(value)\n                    # If fractional part exists, add the equivalent hours\n                    if value - days != 0:\n                        hours = int((value - days) * 24)\n                        # Add the hours to the result\n                        result.hours += hours\n                elif attr == 'hours':\n                    hours = int(value)\n                    # If fractional part exists, add the equivalent minutes\n                    if value - hours != 0:\n                        minutes = int((value - hours) * 60)\n                        # Add the minutes to the result\n                        result.minutes += minutes\n                elif attr == 'minutes':\n                    minutes = int(value)\n                    # If fractional part exists, add the equivalent seconds\n                    if value - minutes != 0:\n                        seconds = int((value - minutes) * 60)\n                        # Add the seconds to the result\n                        result.seconds += seconds\n                elif attr == 'seconds':\n                    seconds = int(value)\n                    # If fractional part exists, add the equivalent microseconds\n                    if value - seconds != 0:\n                        microseconds = int((value - seconds) * 1000000)\n                        # Add the microseconds to the result\n                        result.microseconds += microseconds\n            else:\n                # For integer values, just assign directly\n                setattr(result, attr, value)\n    \n    # Ensure all values are positive\n    for attr in self.__slots__:\n        value = getattr(result, attr)\n        if value is not None:\n            if value < 0:\n                setattr(result, attr, abs(value))\n    \n    return result"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 51772, "num_retrieved": 185, "inference_time": 21.383709847927094, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return the original function\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 16408, "num_retrieved": 60, "inference_time": 6.712018474936485, "generate_results": ["def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 13626, "num_retrieved": 43, "inference_time": 6.712018474936485, "generate_results": ["def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 16253, "num_retrieved": 58, "inference_time": 6.712018474936485, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    return \"+\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 10252, "num_retrieved": 29, "inference_time": 6.712018474936485, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        if verbose:\n            print(f\"Command executed: {' '.join(cmd)}\")\n            \n        if result.returncode == 0:\n            return result.stdout.strip()\n        else:\n            if not hide_stderr:\n                stderr_output = result.stderr.strip()\n                if stderr_output:\n                    print(f\"Error: {stderr_output}\", file=sys.stderr)\n            return None\n            \n    except FileNotFoundError as e:\n        print(f\"Error: Command not found - {e}\", file=sys.stderr)\n        return None\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        return None", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Return result\n        if result.returncode == 0:\n            return {\n                'returncode': 0,\n                'stdout': result.stdout,\n                'stderr': result.stderr\n            }\n        else:\n            return {\n                'returncode': result.returncode,\n                'stdout': result.stdout,\n                'stderr': result.stderr\n            }\n    except subprocess.SubprocessError as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT if hide_stderr else subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose or if there's output\n        if verbose and result.stdout:\n            print(result.stdout)\n            \n        # Return the result\n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr if not hide_stderr else None\n        }\n        \n    except subprocess.SubprocessError as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments to the command\n    cmd.extend(args)\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        if result.returncode == 0:\n            if verbose:\n                print(\"Command succeeded\")\n            return result.stdout.strip() if result.stdout else None\n        else:\n            if not hide_stderr:\n                stderr_output = result.stderr.strip() if result.stderr else \"\"\n                if stderr_output:\n                    print(f\"Command failed with error: {stderr_output}\", file=sys.stderr)\n            else:\n                print(f\"Command failed with return code {result.returncode}\", file=sys.stderr)\n            return None\n            \n    except FileNotFoundError as e:\n        print(f\"Command not found: {e}\", file=sys.stderr)\n        return None\n    except Exception as e:\n        print(f\"Error executing command: {e}\", file=sys.stderr)\n        return None"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 16221, "num_retrieved": 59, "inference_time": 6.712018474936485, "generate_results": ["def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    cfg = VersioneerConfig()\n    try:\n        root = get_root()\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n    return cfg"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 12606, "num_retrieved": 37, "inference_time": 6.712018474936485, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 46559, "num_retrieved": 104, "inference_time": 6.712018474936485, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    entries = self.obj_fs.listdir('')\n    \n    for entry in entries:\n        # Skip directories that are not valid OCFL object roots\n        if entry == 'extensions':\n            # Validate extensions directory\n            self.validate_extensions_dir()\n            continue\n            \n        # Check if this is an OCFL object root\n        if re.match(r'''v\\d+$''', entry):\n            # This is a version directory, skip it\n            continue\n            \n        # Check if this is a valid OCFL object root\n        if entry in ('inventory.json', 'inventory.json.' + self.digest_algorithm):\n            # This is part of an inventory, skip it\n            continue\n            \n        # Check if this is a valid object declaration\n        if entry.startswith('0='):\n            # This is a valid object declaration\n            num_objects += 1\n            \n            # Validate the object\n            if validate_objects:\n                validator = Validator(show_warnings=show_warnings, \n                                    show_errors=True, \n                                    check_digests=check_digests,\n                                    lax_digests=self.lax_digests)\n                passed = validator.validate(entry)\n                if passed:\n                    good_objects += 1\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    entries = self.obj_fs.listdir('')\n    \n    for entry in entries:\n        # Skip directories that are not valid OCFL object directories\n        if entry == 'extensions':\n            # Validate extensions directory as per OCFL spec\n            self.validate_extensions_dir()\n            continue\n            \n        # Check if this is an OCFL object directory\n        if re.match(r'''v\\d+$''', entry):\n            # This is a version directory, skip validation\n            continue\n            \n        # Check if this is an object declaration file\n        if entry.startswith('0='):\n            # This is a potential OCFL object, validate it\n            try:\n                # Validate the object at this path\n                validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests)\n                if validate_objects:\n                    passed = validator.validate(entry)\n                    if passed:\n                        good_objects += 1\n                num_objects += 1\n            except Exception as e:\n                self.log.error('E002', entry=entry, error=str(e))\n                num_objects += 1\n        elif entry == 'inventory.json':\n            # This is an inventory file, skip validation\n            continue\n        elif entry == 'inventory.json.' + self.digest_algorithm:\n            # This is an inventory sidecar, skip validation\n            continue\n        else:\n            # This is an unexpected file or directory\n            self.log.error('E001a', entry=entry)\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    for entry in self.obj_fs.scandir(''):\n        if entry.is_dir:\n            # Check if this is an OCFL object directory\n            if entry.name.startswith('v') or entry.name == 'extensions':\n                continue\n                \n            # Check if this is a valid OCFL object root\n            if entry.name == 'inventory.json':\n                # This is a valid object root\n                num_objects += 1\n                # Validate the object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, \n                                         show_errors=True, \n                                         check_digests=check_digests)\n                    try:\n                        validator.validate(entry.name)\n                        if validator.log.num_errors == 0:\n                            good_objects += 1\n                    except Exception as e:\n                        self.log.error('E002', error=str(e))\n            elif entry.name == '0=ocfl_object_1.1' or entry.name == '0=ocfl_object_1.0':\n                # This is a valid object declaration\n                num_objects += 1\n                # Validate the object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, \n                                         show_errors=True, \n                                         check_digests=check_digests)\n                    try:\n                        validator.validate(entry.name)\n                        if validator.log.num_errors == 0:\n                            good_objects += 1\n                    except Exception as e:\n                        self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    entries = self.obj_fs.listdir('')\n    \n    for entry in entries:\n        # Check if entry is a directory\n        if self.obj_fs.isdir(entry):\n            # Determine the type of directory based on its name\n            if entry.startswith('v'):\n                # Version directory - validate as part of object hierarchy\n                num_objects += 1\n                # Validate the version directory if requested\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests)\n                    try:\n                        # Validate the object at this version directory\n                        if validator.validate(entry):\n                            good_objects += 1\n                    except Exception as e:\n                        self.log.error('E046a', version_dir=entry)\n            elif entry == 'extensions':\n                # Extensions directory - validate its contents\n                self.validate_extensions_dir()\n            elif entry == 'inventory.json':\n                # Inventory file - validate it as part of the hierarchy\n                num_objects += 1\n                # Validate the inventory if requested\n                if validate_objects:\n                    try:\n                        validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests)\n                        validator.validate_inventory('inventory.json', where='root')\n                        good_objects += 1\n                    except Exception as e:\n                        self.log.error('E063')\n            elif entry == 'inventory.json.' + self.digest_algorithm:\n                # Inventory sidecar - validate it as part of the hierarchy\n                num_objects += 1\n            else:\n                # Unexpected directory - log error\n                self.log.error('E001b', dir=entry)\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Get all entries in the root directory\n    entries = self.obj_fs.listdir('')\n    \n    for entry in entries:\n        # Skip directories that are not valid OCFL object directories\n        if entry == 'extensions':\n            # Validate extensions directory\n            self.validate_extensions_dir()\n            continue\n            \n        # Check if this is an OCFL object directory\n        if re.match(r'''v\\d+$''', entry):\n            # This is a version directory, skip it\n            continue\n            \n        # Check if this is a valid OCFL object root\n        if entry in ('0=ocfl_object_1.1', '0=ocfl_object_1.0'):\n            # This is an object declaration file, check if it's valid\n            try:\n                # Validate the object\n                if validate_objects:\n                    validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests)\n                    passed = validator.validate(entry)\n                    if passed:\n                        good_objects += 1\n                num_objects += 1\n            except Exception as e:\n                self.log.error('E001', entry=entry, error=str(e))\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 45289, "num_retrieved": 92, "inference_time": 6.712018474936485, "generate_results": ["def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.content_directory is None:\n        self.content_directory = \"content\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Ensure the object filesystem is properly initialized\n    if self.obj_fs is None:\n        self.obj_fs = self.open_fs(self.path, create=True)\n    \n    # Initialize the root inventory with default structure\n    inventory = self.start_inventory()\n    \n    # Set up the object declaration\n    self.write_object_declaration()\n    \n    # Write the initial inventory and sidecar\n    self.write_inventory_and_sidecar(inventory)\n    \n    # Log initialization completion\n    self.log.info(\"Initialized OCFL storage root with identifier %s\", self.id)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.content_directory is None:\n        self.content_directory = \"content\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Ensure the object filesystem is properly initialized\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the root inventory structure\n    inventory = self.start_inventory()\n    \n    # Set up the initial state for the object\n    self.id = inventory['id']\n    self.spec_version = inventory['type'].split('/')[-1]\n    self.digest_algorithm = inventory['digestAlgorithm']\n    self.content_directory = inventory.get('contentDirectory', 'content')\n    \n    # Initialize the root inventory with default values\n    self.inventory = inventory\n    \n    # Set up the initial state for the object\n    self.root_inv_validator = InventoryValidator()\n    self.root_inv_validator.validate(inventory=inventory, extract_spec_version=True)\n    \n    # Log initialization completion\n    self.log.info(\"Initialized OCFL storage root with identifier %s\", self.id)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.content_directory is None:\n        self.content_directory = \"content\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Ensure the object filesystem is properly initialized\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the inventory with default structure\n    self.inventory = self.start_inventory()\n    \n    # Set up the object declaration\n    self.object_declaration = self.object_declaration_object()\n    \n    # Initialize the root inventory with default values\n    self.write_inventory_and_sidecar(self.inventory)\n    \n    # Initialize the object declaration file\n    self.write_object_declaration()\n    \n    # Log initialization success\n    self.log.info(\"Initialized OCFL storage root with ID %s\", self.id)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the storage root by creating the necessary directory structure\n    # and setting up the initial state for the OCFL object\n    pass", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = \"unknown\"\n    \n    if self.spec_version is None:\n        self.spec_version = \"1.0\"\n    \n    if self.digest_algorithm is None:\n        self.digest_algorithm = \"sha512\"\n    \n    if self.content_directory is None:\n        self.content_directory = \"content\"\n    \n    if self.filepath_normalization is None:\n        self.filepath_normalization = \"uri\"\n    \n    if self.forward_delta is None:\n        self.forward_delta = True\n    \n    if self.dedupe is None:\n        self.dedupe = True\n    \n    if self.lax_digests is None:\n        self.lax_digests = False\n    \n    if self.fixity is None:\n        self.fixity = []\n    \n    # Ensure the object filesystem is properly initialized\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the root inventory with default values\n    inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            inventory['fixity'][fixity_type] = {}\n    \n    # Write the initial inventory to the root\n    self.write_inventory_and_sidecar(inventory)\n    \n    # Write the object declaration\n    self.write_object_declaration()\n    \n    self.log.info(\"Initialized OCFL storage root with ID %s\", self.id)"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 49588, "num_retrieved": 114, "inference_time": 6.712018474936485, "generate_results": ["def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d{1,5})$''', version)\n    if not m:\n        raise ValueError(\"Invalid version format: %s\" % version)\n    \n    v = int(m.group(1))\n    \n    # Increment the version number\n    next_v = v + 1\n    \n    # Format with zero-padding to maintain consistent pattern\n    # The maximum version number is 99999 (5 digits)\n    return f\"v{next_v:05d}\"", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d+)$''', version)\n    if not m:\n        raise ValueError(\"Invalid version format: %s\" % version)\n    \n    # Get the version number\n    v = int(m.group(1))\n    \n    # Return the next version number with the same prefix\n    return 'v' + str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d+)$''', version)\n    if not m:\n        raise ValueError(\"Invalid version format: %s\" % version)\n    \n    # Get the version number\n    v = int(m.group(1))\n    \n    # Return the next version number\n    return 'v' + str(v + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d{1,5})$''', version)\n    if not m:\n        raise ValueError(\"Bad version directory name: %s\" % (version))\n    \n    v = int(m.group(1))\n    \n    # Increment the version number\n    next_v = v + 1\n    \n    # Format with zero padding to maintain consistent pattern\n    # The pattern is v followed by up to 5 digits\n    return f\"v{next_v:05d}\"", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Parse the version number\n    m = re.match(r'''v(\\d{1,5})$''', version)\n    if not m:\n        raise ValueError(\"Bad version directory name: %s\" % (version))\n    \n    v = int(m.group(1))\n    if v == 0:\n        raise ValueError(\"Bad version directory name: %s, v0 not allowed\" % (version))\n    \n    # Return the next version number\n    return 'v' + str(v + 1)"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 44686, "num_retrieved": 124, "inference_time": 6.712018474936485, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root inventory.\"\"\"\n    # Initialize a set to track digests that are different from root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if the version directory exists\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E074\", version=vdir)\n            return\n            \n        # Read the inventory for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E075\", version=vdir)\n            return\n            \n        # Parse the inventory\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E076\", version=vdir, error=str(e))\n            return\n            \n        # Validate the inventory structure (basic checks)\n        if 'id' not in version_inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        if 'type' not in version_inventory:\n            self.error(\"E036b\", version=vdir)\n            return\n            \n        if 'digestAlgorithm' not in version_inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # Check if this version's manifest has digests not present in root inventory\n        # This requires the root inventory to be available\n        if hasattr(self, 'root_inventory') and self.root_inventory:\n            root_manifest = self.root_inventory.get('manifest', {})\n            version_manifest = version_inventory.get('manifest', {})\n            \n            # Compare digests between root and version\n            root_digests = set(root_manifest.keys())\n            version_digests = set(version_manifest.keys())\n            \n            # Find digests that are in version but not in root\n            new_digests = version_digests - root_digests\n            if new_digests:\n                different_digests.update(new_digests)\n                \n        # Validate version metadata\n        if 'versions' not in version_inventory:\n            self.error(\"E041b\", version=vdir)\n            return\n            \n        # Validate version-specific fields\n        if 'created' not in version_inventory:\n            self.error(\"E048\",", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root inventory.\"\"\"\n    # Validate that each version directory exists and has an inventory\n    for vdir in version_dirs:\n        # Check if version directory exists\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E074\", version=vdir)\n            return\n        \n        # Check if inventory file exists in version directory\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E075\", version=vdir)\n            return\n        \n        # Parse the inventory to validate its structure\n        try:\n            inventory = self.parse_inventory(inventory_path)\n            # Validate the inventory structure\n            self.validate(inventory, extract_spec_version=False)\n        except Exception as e:\n            self.error(\"E076\", version=vdir, error=str(e))\n            return\n    \n    # Keep track of content digests different from root inventory\n    # This would typically involve comparing the manifest digests across versions\n    # and recording any differences\n    root_inventory = self.parse_inventory()\n    root_manifest = root_inventory.get('manifest', {})\n    \n    # For each version, compare its manifest to the root manifest\n    for vdir in version_dirs:\n        version_inventory = self.parse_inventory(fs.path.join(vdir, 'inventory.json'))\n        version_manifest = version_inventory.get('manifest', {})\n        \n        # Compare digests between root and version\n        for digest in version_manifest:\n            if digest not in root_manifest:\n                # This digest is new in this version\n                self.different_digests.add(digest)\n    \n    return self.different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep record of content digests different from root inventory.\"\"\"\n    # Initialize a set to track digests that are different from root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        inventory_path = os.path.join(vdir, 'inventory.json')\n        if not os.path.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            return\n            \n        # Parse the inventory for this version\n        try:\n            with open(inventory_path, 'r') as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            return\n            \n        # Check if the inventory has a valid structure\n        if 'id' not in version_inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        # Check if the inventory has a valid digestAlgorithm\n        if 'digestAlgorithm' not in version_inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # Check if the inventory has a valid head version\n        if 'head' not in version_inventory:\n            self.error(\"E036d\", version=vdir)\n            return\n            \n        # Check if the head version is valid (should be the current version)\n        if version_inventory['head'] != vdir:\n            self.warning(\"W002\", version=vdir, expected=vdir, got=version_inventory['head'])\n            \n        # Check if the inventory has a valid manifest\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            return\n            \n        # Check if the manifest has valid digests\n        manifest_digests = set()\n        for digest in version_inventory['manifest']:\n            if not isinstance(version_inventory['manifest'][digest], list):\n                self.error(\"E092\", version=vdir, digest=digest)\n                return\n            manifest_digests.add(digest)\n            \n        # Check if any of these digests are different from root inventory\n        # (This would require access to root inventory, which is not provided in this function)\n        # For now, we just validate the structure and format", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep record of content digests different from root inventory.\"\"\"\n    # Initialize a set to track digests that are different from the root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if the version directory exists and has an inventory\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E074\", version=vdir)\n            continue\n            \n        # Read the inventory for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E075\", version=vdir)\n            continue\n            \n        # Parse the inventory\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E076\", version=vdir, error=str(e))\n            continue\n            \n        # Check if the inventory has a manifest block\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            continue\n            \n        # Extract the manifest digests from this version\n        version_manifest_digests = set(version_inventory['manifest'].keys())\n        \n        # Compare with root inventory manifest (assuming root inventory is available)\n        # For now, we'll assume the root inventory is the first version\n        if hasattr(self, 'root_inventory') and self.root_inventory is not None:\n            root_manifest_digests = set(self.root_inventory['manifest'].keys())\n            # Find digests that are in this version but not in root\n            different_digests.update(version_manifest_digests - root_manifest_digests)\n            \n    # Return the set of digests that are different from the root inventory\n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root inventory.\"\"\"\n    # Initialize a set to track digests that are different from root inventory\n    different_digests = set()\n    \n    # Validate each version directory\n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        inventory_path = os.path.join(vdir, 'inventory.json')\n        if not os.path.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            return\n            \n        # Load the inventory for this version\n        with open(inventory_path, 'r') as fh:\n            version_inventory = json.load(fh)\n            \n        # Check if the inventory has a valid structure\n        if 'id' not in version_inventory:\n            self.error(\"E036a\", version=vdir)\n            return\n            \n        # Check if the inventory has a valid digest algorithm\n        if 'digestAlgorithm' not in version_inventory:\n            self.error(\"E036c\", version=vdir)\n            return\n            \n        # Check if the digest algorithm is supported\n        if version_inventory['digestAlgorithm'] not in ['sha512', 'sha256', 'md5', 'sha1', 'blake2b-512']:\n            self.error(\"E039\", version=vdir, digest_algorithm=version_inventory['digestAlgorithm'])\n            return\n            \n        # Check if the inventory has a valid manifest\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            return\n            \n        # Extract the manifest digests from this version\n        version_manifest_digests = set(version_inventory['manifest'].keys())\n        \n        # Compare with root inventory manifest to find different digests\n        # This is a simplified approach - in a real implementation, we would need\n        # to compare the actual content of the manifest files\n        # For now, we just track that digests are present in the version\n        # and different from root (this would require root inventory to be available)\n        \n        # Note: The actual implementation would need to compare the content\n        # of the manifest files with the root inventory to determine if digests\n        # are different\n        \n    # Return the set of different digests (this would be more fully implemented\n    # with actual content comparison)"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 50776, "num_retrieved": 127, "inference_time": 6.712018474936485, "generate_results": ["def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for filename in os.listdir(path):\n            if filename.startswith('0='):\n                namaste_files.append(filename)\n        \n        # If no 0=* files found, it's likely a file\n        if len(namaste_files) == 0:\n            return 'file'\n        \n        # If exactly one 0=* file, it's likely an object\n        if len(namaste_files) == 1:\n            # Check if it's a root declaration (0=ocfl_object_1.0 or similar)\n            if namaste_files[0] == '0=ocfl_object_1.0':\n                return 'root'\n            else:\n                return 'object'\n        \n        # If multiple 0=* files, it's likely a root with multiple declarations\n        if len(namaste_files) > 1:\n            return 'root'\n            \n        return 'file'\n        \n    except Exception as e:\n        return f\"error determining path type: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for item in os.listdir(path):\n            if item.startswith('0='):\n                namaste_files.append(item)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if there's exactly one 0=ocfl_object_1.0 file (root)\n        # or multiple 0=ocfl_object_x.x files (object)\n        namaste_patterns = []\n        for file in namaste_files:\n            if file == '0=ocfl_object_1.0':\n                namaste_patterns.append('root')\n            elif file.startswith('0=ocfl_object_'):\n                namaste_patterns.append('object')\n            else:\n                namaste_patterns.append('unknown')\n        \n        # If we have a 0=ocfl_object_1.0 file, it's a root\n        if 'root' in namaste_patterns:\n            return 'root'\n        \n        # If we have any 0=ocfl_object_x.x file (not 1.0), it's an object\n        if 'object' in namaste_patterns:\n            return 'object'\n        \n        # If we have any other pattern, it's unknown\n        return 'unknown'\n        \n    except Exception as e:\n        return \"error: \" + str(e)", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for item in os.listdir(path):\n            if item.startswith('0='):\n                namaste_files.append(item)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if any file matches the pattern for a storage root (0=ocfl_object_1.0)\n        root_pattern = '0=ocfl_object_1.0'\n        object_pattern = '0=ocfl_object_'\n        \n        # Check if there's a 0=ocfl_object_1.0 file (storage root)\n        if root_pattern in namaste_files:\n            return 'root'\n        \n        # Check if there's a 0=ocfl_object_x.x file (object)\n        for file in namaste_files:\n            if file.startswith(object_pattern):\n                return 'object'\n        \n        # If no clear pattern found, check if it's a file (inventory)\n        if len(namaste_files) == 1 and len(os.listdir(path)) == 1:\n            return 'file'\n        \n        # Default case - unclear type\n        return 'unknown type'\n        \n    except Exception as e:\n        return f\"error determining path type: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for filename in os.listdir(path):\n            if filename.startswith('0='):\n                namaste_files.append(filename)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if there's exactly one 0=ocfl_object_1.0 file (root)\n        # or if there are multiple 0= files (object)\n        namaste_patterns = []\n        for filename in namaste_files:\n            if filename.startswith('0=ocfl_object_'):\n                namaste_patterns.append(filename)\n        \n        if len(namaste_patterns) == 1:\n            # Check if it's the root declaration\n            if namaste_patterns[0] == '0=ocfl_object_1.0':\n                return 'root'\n            else:\n                return 'object'\n        elif len(namaste_patterns) > 1:\n            return 'object'\n        else:\n            # Check if there are any other 0=* files\n            for filename in namaste_files:\n                if filename.startswith('0=') and not filename.startswith('0=ocfl_object_'):\n                    return 'file'\n            return 'root'\n            \n    except Exception as e:\n        return \"error: \" + str(e)", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Check if path exists\n        if not os.path.exists(path):\n            return \"path does not exist\"\n        \n        # Check if it's a directory\n        if not os.path.isdir(path):\n            return \"path is not a directory\"\n        \n        # Look for 0=* Namaste files\n        namaste_files = []\n        for filename in os.listdir(path):\n            if filename.startswith('0='):\n                namaste_files.append(filename)\n        \n        # Determine type based on number and pattern of Namaste files\n        if len(namaste_files) == 0:\n            return \"no 0=* Namaste files found\"\n        \n        # Check if there's exactly one 0=ocfl_object_1.0 file (root)\n        # or if there are multiple 0= files (object)\n        namaste_patterns = []\n        for filename in namaste_files:\n            if filename.startswith('0=ocfl_object_'):\n                namaste_patterns.append(filename)\n        \n        if len(namaste_patterns) == 1:\n            # Check if it's the root declaration (1.0)\n            if namaste_patterns[0] == '0=ocfl_object_1.0':\n                return 'root'\n            else:\n                return 'object'\n        elif len(namaste_patterns) > 1:\n            return 'object'\n        else:\n            # Check if there are any 0= files at all\n            return 'file'\n            \n    except Exception as e:\n        return f\"error determining path type: {str(e)}\""]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 79408, "num_retrieved": 500, "inference_time": 6.712018474936485, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if provided\n    if bug_ids:\n        params[\"bug_id\"] = \",\".join(bug_ids)"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 63340, "num_retrieved": 192, "inference_time": 6.712018474936485, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 62185, "num_retrieved": 198, "inference_time": 6.712018474936485, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation as the full argument parsing logic is not provided in the context\n    # In a real implementation, this would include all the subparsers, actions, and options defined in the codebase\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation as the full argument parsing logic is not provided in the context\n    # In a real implementation, this would include all the subparsers, actions, and argument definitions\n    # from the borgmatic commands module\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would include all the subparsers and arguments defined in the borgmatic codebase\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation - the actual implementation would depend on the specific\n    # arguments and options that need to be parsed\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    # This is a placeholder implementation as the full argument parsing logic is not provided in the context\n    # In a real implementation, this would include all the subparsers, actions, and argument definitions\n    # from the borgmatic commands module\n    return parser.parse_args(arguments)"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 62481, "num_retrieved": 189, "inference_time": 6.712018474936485, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join(f'--{name}' for name in parser._actions if name.startswith('_') or not name.startswith('_'))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join(f'--{name}' for name in parser._actions if name.startswith('_') or name.replace('_', '-').replace('-', '_') in parser._actions)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join([f'--{name}' for name in parser._actions if name.startswith('_') == False])", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join([f'--{name}' for name in parser._actions if name.startswith('_') == False])", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    return ' '.join([f'--{name}' for name in parser._actions if name.startswith('_') == False])"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 61648, "num_retrieved": 204, "inference_time": 6.712018474936485, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subcommand', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--config', '-c', help='Configuration file path')\n    global_parser.add_argument('--version', action='store_true', help='Show version')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Show bash completion script')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides in format section.option=value')\n    \n    # Add subparsers for different commands\n    # (The actual subparsers would be defined with their own arguments)\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create a namespace for global arguments\n    global_namespace = argparse.Namespace(\n        config=None,\n        version=False,\n        bash_completion=False,\n        no_color=False,\n        verbosity='some',\n        syslog_verbosity='some',\n        log_file_verbosity='some',\n        monitoring_verbosity='some',\n        log_file=None,\n        dry_run=False,\n        resolve_env=True,\n        overrides=[]\n    )", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subcommand', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config', nargs='+', help='configuration files to load')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='set verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true', help='resolve environment variables in configuration')\n    \n    # Subcommands\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--repository', help='repository path')\n    create_parser.add_argument('--archive', help='archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='exclude patterns from file')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='exclude nodump')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='archive name or pattern')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    mount_parser = subparsers.add_parser('mount', help='mount an archive')\n    mount_parser.add_argument('--archive', help='archive name or pattern')\n    mount_parser.add_argument('--mount-point', help='mount point')\n    mount_parser.add_argument('--paths', nargs", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config', '-c', help='configuration file path')\n    global_parser.add_argument('--config-paths', '-C', nargs='+', help='configuration file paths')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='verbosity level')\n    global_parser.add_argument('--bash-completion', action='store_true', help='output bash completion script')\n    \n    # Create arguments\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--repository', help='repository path')\n    create_parser.add_argument('--archive', help='archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='source directories')\n    create_parser.add_argument('--exclude-from', nargs='+', help='exclude patterns from files')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='exclude nodump')\n    create_parser.add_argument('--patterns-from', nargs='+', help='patterns from files')\n    \n    # Extract arguments\n    extract_parser = subparsers.add_parser('extract', help='extract a backup')\n    extract_parser.add_argument('--repository', help='repository path')\n    extract_parser.add_argument('--archive', help='archive name or pattern')\n    extract_parser.add_argument('--paths', nargs='+', help='paths to extract')\n    extract_parser.add", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subcommand', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config', nargs='+', help='configuration files to load')\n    global_parser.add_argument('--dry-run', action='store_true', help='run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='set verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true', help='resolve environment variables in configuration')\n    \n    # Subcommands\n    create_parser = subparsers.add_parser('create', help='create a backup')\n    create_parser.add_argument('--repository', help='repository path')\n    create_parser.add_argument('--archive', help='archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='exclude patterns from file')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='exclude caches')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='exclude nodump')\n    \n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--archive', help='archive name or pattern')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    info_parser = subparsers.add_parser('info', help='display archive information')\n    info_parser.add_argument('--archive', help='archive name or pattern')\n    info_parser.add_argument('--prefix', help='prefix for archive names')\n    info_parser.add_argument('--json',", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config', nargs='?', default=None, help='Configuration file')\n    global_parser.add_argument('--config-paths', nargs='*', default=[], help='Configuration file paths')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    global_parser.add_argument('--log-file', default=None, help='Log file path')\n    global_parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Syslog verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Monitoring verbosity level')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Output bash completion script')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables')\n    global_parser.add_argument('--overrides', nargs='*', help='Configuration overrides in format section.option=value')\n    \n    # Add subparsers for each command\n    # (Note: The actual subparsers would be defined in the borgmatic.commands module)\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to a dictionary mapping from subparser name to an argparse.Namespace instance\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Add subparser results if they exist\n    # (Note: This would be populated by the actual subparsers in the full implementation)\n    \n    return result"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 61825, "num_retrieved": 205, "inference_time": 15.932739451527596, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, continue to next subparser\n            continue\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, continue to next subparser\n            continue\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, break out of the loop\n            break\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, break out of the loop\n            break\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = []\n    \n    # Process arguments in order\n    for arg in unparsed_arguments:\n        # Try to match the argument with each subparser\n        for subparser_name, subparser in subparsers.items():\n            # Check if the argument can be parsed by this subparser\n            try:\n                # Parse the argument with the subparser\n                parsed_args = subparser.parse_args([arg])\n                # If parsing succeeds, add the parsed arguments to the subparser's arguments\n                arguments[subparser_name] = parsed_args\n                # Remove the argument from the remaining arguments\n                remaining_arguments.remove(arg)\n                break\n            except SystemExit:\n                # If parsing fails, continue to the next subparser\n                continue\n        else:\n            # If no subparser could parse the argument, add it to the remaining arguments\n            remaining_arguments.append(arg)\n    \n    return (arguments, remaining_arguments)"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 60165, "num_retrieved": 180, "inference_time": 15.932739451527596, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # It would typically build a top-level argument parser with subparsers\n    # for different borgmatic commands\n    # Since no implementation is provided in the context, we return a placeholder\n    # that would need to be filled with actual parser construction code\n    top_level_parser = None\n    subparsers = None\n    return (top_level_parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # The context contains functions related to flags, arguments, and configuration\n    # but does not contain the implementation of make_parsers\n    # Therefore, we return a placeholder implementation\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(description='Borgmatic command-line tool')\n    \n    # Add subparsers for different actions\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be defined in the actual implementation)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    list_parser = subparsers.add_parser('list', help='List archives')\n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    \n    return (top_level_parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # It would typically build and return a top-level parser and its subparsers\n    # However, no implementation details are provided in the context\n    # Therefore, we return a placeholder tuple with empty parsers\n    return (None, None)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # The context contains functions related to flags, commands, and configuration\n    # but does not contain the implementation of make_parsers\n    # Therefore, we return a placeholder implementation\n    top_level_parser = argparse.ArgumentParser()\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add common subparsers (example implementation)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    list_parser = subparsers.add_parser('list', help='List archives')\n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    \n    return (top_level_parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # It would typically build a top-level argument parser with subparsers\n    # for different borgmatic commands\n    # Since no implementation is provided in the context, we return a placeholder\n    # that would need to be filled with actual parser construction logic\n    top_level_parser = None\n    subparsers = None\n    return (top_level_parser, subparsers)"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 90309, "num_retrieved": 286, "inference_time": 15.932739451527596, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 92602, "num_retrieved": 291, "inference_time": 15.932739451527596, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace or parser instance\n    \n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add common arguments here based on the context\n    parser.add_argument('--version', action='store_true', help='Show version information')\n    parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    \n    # Add other arguments as needed based on the borgmatic command structure\n    parser.add_argument('--config', nargs='+', help='Configuration files to load')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--verbosity', choices=['error', 'answer', 'some', 'lots'], default='answer', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace or ArgumentParser instance\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add common arguments here based on the context\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (actual implementation would be based on the full context)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--source', help='Source directories to backup')\n    create_parser.add_argument('--archive', help='Archive name')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Specific archive to list')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the full implementation would require\n    # defining the full argument parser structure which is not provided in the context\n    # The actual implementation would involve creating an ArgumentParser and adding\n    # appropriate arguments based on the borgmatic command structure\n    parser = argparse.ArgumentParser()\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace or ArgumentParser instance\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add common arguments here based on the context\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (actual implementation would be more detailed)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source', help='Source directories')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Archive name')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace or parser instance\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add common arguments here based on the context\n    parser.add_argument('--version', action='store_true', help='Show version information')\n    parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    \n    # Add other arguments as needed based on the borgmatic command structure\n    parser.add_argument('--config', nargs='+', help='Configuration files to load')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--verbosity', choices=['error', 'answer', 'some', 'lots'], default='answer', help='Set verbosity level')\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    return args"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 91951, "num_retrieved": 284, "inference_time": 15.932739451527596, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 91946, "num_retrieved": 290, "inference_time": 15.932739451527596, "generate_results": ["def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                bash -c '\n                    if test -f \"$this_filename\"\n                        then\n                            local this_script=$(cat $this_filename 2> /dev/null)\n                            local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                            if [ \"$this_script\" != \"$installed_script\" ] && [ \"$installed_script\" != \"\" ]\n                            then\n                                echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                                    'bash',\n                                    'borgmatic --bash-completion | sudo tee $this_filename',\n                                    '$this_filename',\n                                )}\"\n                            fi\n                        fi\n                '\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [ (count $all_args) -lt 2 ]\n                    then\n                        return 1\n                fi\n                for arg in $argv\n                    do\n                        if [ \"$arg\" = \"$all_args[-1]\" ]\n                            then\n                                return 0\n                        fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ $1 == \"complete\" ]] && return 0\n                [[ $1 == \"complete\" ]] &&", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                bash -c '\n                    if test -f \"$this_filename\"\n                        then\n                            local this_script=$(cat $this_filename 2> /dev/null)\n                            local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                            if [ \"$this_script\" != \"$installed_script\" ] && [ \"$installed_script\" != \"\" ]\n                                then\n                                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                                    'bash',\n                                    'borgmatic --bash-completion | sudo tee $this_filename',\n                                    '$this_filename',\n                                )}\"\n                            fi\n                        fi\n                '\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [ (count $all_args) -lt 2 ]\n                    then\n                        return 1\n                fi\n                for arg in $argv\n                    do\n                        if [ \"$arg\" = \"$all_args[-1]\" ]\n                            then\n                                return 0\n                        fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not _borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ $1 == _borgmatic_action_parser ]] && return 0\n                [[ $1 == _borgmatic", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee $this_filename',\n                        '$this_filename',\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ $1 == \"action\" ]] && echo \"$action_parser_condition\" && echo \"$exact_option_condition\"\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee $this_filename',\n                        '$this_filename',\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ $1 == \"complete\" ]] && return 0\n                [[ $1 == \"complete\" ]] && return 0\n                [[ $1 == \"complete\" ]] && return 0\n                [[ $1 == \"complete\" ]] && return 0\n                [[", "def bash_completion():\n    \"\"\"\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    \"\"\"\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" && \"$installed_script\" != \"\" && \"$installed_script\" != \"$this_filename\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee $this_filename',\n                        '$this_filename',\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(commandline -poc)\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _borgmatic_action_parser_condition() {{\n                local action_parser_condition=\"not __borgmatic_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not __borgmatic_current_arg {' '.join(exact_option_args)}\"\n                [[ -n \"$action_parser_condition\" ]] && [[ -n \"$exact_option_condition\" ]] && eval \"$action_parser_condition\" && eval \"$exact_option_condition\"\n            }}\n\n            _borgmatic_global_option_condition() {{\n                local exact_option_condition=\"not __borgmatic_current_arg {'"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 92784, "num_retrieved": 294, "inference_time": 15.932739451527596, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=['~/.config/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration file overrides')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='answer', help='Verbosity level')\n    global_parser.add_argument('--log-file', help='Path to log file')\n    global_parser.add_argument('--log-file-format', help='Format for log file')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', help='Repository path')\n    bootstrap_parser.add_argument('--archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source-filename', help='Source configuration file')\n    generate_parser.add_argument('--destination-filename", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Configuration file paths')\n    global_parser.add_argument('--overrides', nargs='+', default=[], help='Configuration file overrides')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='answer', help='Verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    global_parser.add_argument('--log-file', default=None, help='Log file path')\n    global_parser.add_argument('--log-file-format', default=None, help='Log file format')\n    global_parser.add_argument('--syslog-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='answer', help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='answer', help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='answer', help='Monitoring verbosity level')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', required=True, help='Repository path')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=['~/.config/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration file overrides')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='answer', help='Verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--log-file-format', help='Log file format')\n    global_parser.add_argument('--syslog-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='answer', help='Syslog verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='answer', help='Monitoring verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', help='Repository path')\n    bootstrap_parser.add_argument('--archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=[],\n                               help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', default=[],\n                               help='Configuration file overrides in the form option.suboption=value')\n    global_parser.add_argument('--dry-run', action='store_true',\n                               help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true',\n                               help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Verbosity level')\n    global_parser.add_argument('--log-file', default=None,\n                               help='Path to log file')\n    global_parser.add_argument('--log-file-format', default=None,\n                               help='Format for log file')\n    global_parser.add_argument('--syslog-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Syslog verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Monitoring verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true',\n                               help='Resolve environment variables in configuration')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', required=True,\n                                  help='Repository path')\n    bootstrap_parser.add_argument('--archive', required=True,\n                                  help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', default=os.path.expanduser('~/.config/borgmatic'),\n                                  help='Path to borgmatic source directory')\n    bootstrap_parser.add_argument", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=[],\n                               help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', default=[],\n                               help='Configuration file overrides')\n    global_parser.add_argument('--dry-run', action='store_true',\n                               help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true',\n                               help='Disable color output')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Verbosity level')\n    global_parser.add_argument('--log-file', default=None,\n                               help='Path to log file')\n    global_parser.add_argument('--log-file-format', default=None,\n                               help='Format for log file')\n    global_parser.add_argument('--syslog-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Syslog verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n                               default='answer',\n                               help='Monitoring verbosity level')\n    global_parser.add_argument('--resolve-env', action='store_true',\n                               help='Resolve environment variables in configuration')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', required=True,\n                                  help='Repository path')\n    bootstrap_parser.add_argument('--archive', required=True,\n                                  help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', default=os.path.expanduser('~/.config/borgmatic'),\n                                  help='Path to borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', default=None,"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 92979, "num_retrieved": 295, "inference_time": 15.932739451527596, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with the current subparser\n        try:\n            # Parse the arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, add them to the parsed_arguments dict\n            if args:\n                parsed_arguments[subparser_name] = args\n                \n                # Remove the parsed arguments from the remaining arguments list\n                remaining_arguments = []\n                for arg in remaining_arguments:\n                    if not parser._get_action(arg):\n                        remaining_arguments.append(arg)\n                        \n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse the arguments with the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, add them to the parsed_arguments dict\n            if args:\n                parsed_arguments[subparser_name] = args\n                \n                # Remove the parsed arguments from the remaining_arguments list\n                # Find the indices of the parsed arguments and remove them\n                indices_to_remove = []\n                for i, arg in enumerate(remaining_arguments):\n                    if arg.startswith('--') or arg.startswith('-'):\n                        # Check if this argument is part of the parsed arguments\n                        # This is a simplified check - in practice, you'd need to track the actual argument names\n                        indices_to_remove.append(i)\n                \n                # Remove the arguments from the remaining_arguments list\n                for i in sorted(indices_to_remove, reverse=True):\n                    remaining_arguments.pop(i)\n                    \n        except SystemExit:\n            # If the parser raises a SystemExit, it means the arguments were not valid\n            # This is expected behavior when trying to parse arguments with a subparser\n            pass\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process arguments in order, giving each subparser a chance to parse them\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Use the subparser to parse the arguments\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, add them to the result\n            if args:\n                parsed_arguments[subparser_name] = args\n                # Remove the parsed arguments from the remaining arguments\n                remaining_arguments = []\n                break\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # If no subparser was able to parse the arguments, return an empty dict and the original arguments\n    if not parsed_arguments:\n        return (parsed_arguments, unparsed_arguments)\n    \n    # Return the parsed arguments and the remaining arguments\n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dict\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse the arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, add them to the parsed_arguments dict\n            if args:\n                parsed_arguments[subparser_name] = args\n                \n                # Remove the parsed arguments from the remaining arguments list\n                # Find the indices of the parsed arguments and remove them\n                indices_to_remove = []\n                for i, arg in enumerate(remaining_arguments):\n                    # Check if the argument is part of the parsed arguments\n                    # This is a simplified check - in practice, you'd need to track the actual argument names\n                    if arg in vars(args):\n                        indices_to_remove.append(i)\n                \n                # Remove the arguments from the list\n                for i in sorted(indices_to_remove, reverse=True):\n                    remaining_arguments.pop(i)\n                    \n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Parse the arguments using the subparser\n            args = parser.parse_args(remaining_arguments)\n            \n            # If arguments were parsed successfully, store them in the result\n            parsed_arguments[subparser_name] = args\n            \n            # Remove the parsed arguments from the remaining arguments list\n            remaining_arguments = []\n            \n            # Reconstruct the remaining arguments list\n            for arg in unparsed_arguments:\n                if arg not in [str(getattr(args, key)) for key in vars(args).keys()]:\n                    remaining_arguments.append(arg)\n                    \n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n        except Exception:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return (parsed_arguments, remaining_arguments)"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 92623, "num_retrieved": 291, "inference_time": 15.932739451527596, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--version', action='version', version='borgmatic 1.0'\n    )\n    global_parser.add_argument(\n        '--config-paths', nargs='+', default=['~/.borgmatic/config.yaml'],\n        help='Paths to configuration files to load (default: ~/.borgmatic/config.yaml)'\n    )\n    global_parser.add_argument(\n        '--dry-run', action='store_true',\n        help='Run in dry-run mode (do not perform any actual operations)'\n    )\n    global_parser.add_argument(\n        '--log-json', action='store_true',\n        help='Output log messages in JSON format'\n    )\n    global_parser.add_argument(\n        '--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set the verbosity level (default: some)'\n    )\n    global_parser.add_argument(\n        '--no-color', action='store_true',\n        help='Disable color output'\n    )\n    global_parser.add_argument(\n        '--bash-completion', action='store_true',\n        help='Generate bash completion script'\n    )\n    global_parser.add_argument(\n        '--fish-completion', action='store_true',\n        help='Generate fish completion script'\n    )\n    global_parser.add_argument(\n        '--resolve-env', action='store_true',\n        help='Resolve environment variables in configuration files'\n    )\n    global_parser.add_argument(\n        '--overrides', nargs='+',\n        help='Configuration file overrides in the form of \"option.suboption=value\"'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Create parser for 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument(\n        '--repository', help='Repository path to use'\n    )\n    create_parser.add_argument(\n        '--archive', help='Archive name or pattern'\n    )\n    create_parser.add_argument(\n        '--source-directories', nargs='+',\n        help='", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A configuration-driven backup tool for Borg',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--version', action='version', version='borgmatic %(version)s'\n    )\n    global_parser.add_argument(\n        '--config-paths', \n        action='append', \n        default=[],\n        help='Path to configuration file(s) to load (can be used multiple times)'\n    )\n    global_parser.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode (no actual operations performed)'\n    )\n    global_parser.add_argument(\n        '--log-json', \n        action='store_true', \n        help='Output log messages in JSON format'\n    )\n    global_parser.add_argument(\n        '--verbosity', \n        choices=['disabled', 'error', 'answer', 'some', 'lots'], \n        default='some',\n        help='Set the verbosity level (disabled, error, answer, some, lots)'\n    )\n    global_parser.add_argument(\n        '--no-color', \n        action='store_true', \n        help='Disable color output'\n    )\n    global_parser.add_argument(\n        '--bash-completion', \n        action='store_true', \n        help='Output bash completion script'\n    )\n    global_parser.add_argument(\n        '--fish-completion', \n        action='store_true', \n        help='Output fish completion script'\n    )\n    global_parser.add_argument(\n        '--resolve-env', \n        action='store_true', \n        help='Resolve environment variables in configuration files'\n    )\n    global_parser.add_argument(\n        '--overrides', \n        action='append', \n        help='Configuration file override strings in the form of \"option.suboption=value\"'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    # Note: This is a simplified version - actual implementation would include all actions\n    # with their respective arguments\n    \n    # Example subparser (would be expanded in full implementation)\n    create_parser = subparsers.add_parser('", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    # Create the top-level parser\n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--version', action='version', version='borgmatic 1.0'\n    )\n    global_parser.add_argument(\n        '--config-paths', nargs='+', default=['~/.config/borgmatic/config.yaml'],\n        help='Paths to configuration files to load (can be directories or files)'\n    )\n    global_parser.add_argument(\n        '--dry-run', action='store_true', default=False,\n        help='Run in dry-run mode (no actual operations)'\n    )\n    global_parser.add_argument(\n        '--log-json', action='store_true', default=False,\n        help='Output log messages in JSON format'\n    )\n    global_parser.add_argument(\n        '--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set the verbosity level'\n    )\n    global_parser.add_argument(\n        '--no-color', action='store_true', default=False,\n        help='Disable color output'\n    )\n    global_parser.add_argument(\n        '--resolve-env', action='store_true', default=True,\n        help='Resolve environment variables in configuration files'\n    )\n    global_parser.add_argument(\n        '--bash-completion', action='store_true', default=False,\n        help='Print bash completion script'\n    )\n    global_parser.add_argument(\n        '--fish-completion', action='store_true', default=False,\n        help='Print fish completion script'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    # Note: The actual implementation would include specific subparsers for each action\n    # such as create, list, info, etc.\n    \n    # Example subparser for create action\n    create_parser = subparsers.add_parser('create', help='Create a backup archive')\n    create_parser.add_argument(\n        '--repository', help='Repository to create backup for'\n    )\n    create_parser.add_argument(\n        '--archive', help", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add subparsers for each action\n    for action_name, action_parser in borgmatic.commands.arguments.ACTIONS.items():\n        action_subparser = subparsers.add_parser(action_name, help=action_parser.description)\n        \n        # Add arguments from the action parser\n        for action in action_parser._actions:\n            if action.option_strings:\n                action_subparser.add_argument(*action.option_strings, **action.__dict__)\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    \n    return (parser, subparsers, parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='some', help='Verbosity level')\n    global_parser.add_argument('--log-json', action='store_true', help='Output log in JSON format')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a backup archive')\n    create_parser.add_argument('--repository', help='Repository to use')\n    create_parser.add_argument('--archive', help='Archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='File containing exclude patterns')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache files')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files from nodump')\n    create_parser.add_argument('--patterns-from', nargs='+', help='File containing patterns')\n    create_parser.add_argument('--progress', action='store"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 138788, "num_retrieved": 251, "inference_time": 15.932739451527596, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Iterate through the response in chunks of max_results\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Iterate through response in chunks of max_results\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Yield the first page\n    page_size = min(max_results, len(response))\n    for i in range(0, len(response), page_size):\n        yield response[i:i + page_size]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Process the response in pages\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Process the response in pages\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 20441, "num_retrieved": 146, "inference_time": 15.932739451527596, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit\n    if 'K' in size:\n        # Extract the number part\n        num_part = size.replace('K', '').strip()\n        return int(num_part) * 1024\n    elif 'M' in size:\n        # Extract the number part\n        num_part = size.replace('M', '').strip()\n        return int(num_part) * 1024 * 1024\n    elif 'G' in size:\n        # Extract the number part\n        num_part = size.replace('G', '').strip()\n        return int(num_part) * 1024 * 1024 * 1024\n    else:\n        # No unit, assume it's in bytes\n        return int(size)"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 76049, "num_retrieved": 251, "inference_time": 15.932739451527596, "generate_results": ["def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 23160, "num_retrieved": 60, "inference_time": 15.932739451527596, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if not url_args:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 83092, "num_retrieved": 500, "inference_time": 15.932739451527596, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.strip().lower() in ('none', 'null', 'nil', 'n/a', 'na', 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'null', 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'null', 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'none')"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 91951, "num_retrieved": 284, "inference_time": 15.932739451527596, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 142484, "num_retrieved": 393, "inference_time": 15.932739451527596, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(\"Skipping already processed path: {}\".format(abs_path))\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 84794, "num_retrieved": 407, "inference_time": 13.78547516465187, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and the two adjacent vertices\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 1167, "num_retrieved": 5, "inference_time": 13.78547516465187, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        remainder = number % len(alphabet)\n        result = alphabet[remainder] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        result = alphabet[number % len(alphabet)] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 124218, "num_retrieved": 500, "inference_time": 13.78547516465187, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_number] = flow[register_value]\n            del flow[register_value]"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 37299, "num_retrieved": 244, "inference_time": 13.78547516465187, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 121509, "num_retrieved": 467, "inference_time": 13.78547516465187, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 1161, "num_retrieved": 5, "inference_time": 13.78547516465187, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 20613, "num_retrieved": 149, "inference_time": 13.78547516465187, "generate_results": ["import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path, format=\"tar\") as extracted_files:\n            pass\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path) as extracted_files:\n            pass  # Extracted files are automatically handled by shutil\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path, format=\"tar\") as extracted_files:\n            pass  # Extracting is handled by shutil\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode to save the downloaded content\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as gz_file:\n        with open(destination_path / \"archive.tar\", \"wb\") as tar_file:\n            shutil.copyfileobj(gz_file, tar_file)\n    \n    # Extract the tar file\n    import tarfile\n    with tarfile.open(destination_path / \"archive.tar\", \"r:gz\") as tar:\n        tar.extractall(path=destination_path)\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, stream=True)\n    response.raise_for_status()\n    \n    # Open a file in binary write mode\n    with open(destination_path / \"archive.tar.gz\", \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n    \n    # Extract the archive to the destination path\n    with gzip.open(destination_path / \"archive.tar.gz\", \"rb\") as tar_file:\n        with shutil.unpack_archive(tar_file, destination_path, format=\"tar\") as extracted:\n            pass  # Extracted to destination_path\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 121777, "num_retrieved": 486, "inference_time": 13.78547516465187, "generate_results": ["import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\""]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 142910, "num_retrieved": 406, "inference_time": 13.78547516465187, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'.', regex_key)\n        # Ensure the pattern starts and ends with a word boundary\n        regex_key = r'^' + regex_key + r'$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'.', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', r'(\\.*)', key)\n        regex_key = re.sub(r'\\?', r'(\\.)', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'(.|)', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'.', regex_key)\n        # Ensure the regex starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 77542, "num_retrieved": 259, "inference_time": 13.78547516465187, "generate_results": ["def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Convert to string if it's bytes\n    if isinstance(name, bytes):\n        name = name.decode('latin1')\n    # Remove quotes from the beginning and end\n    name = name.strip('\"')\n    name = name.strip(\"'\")\n    return name"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 115448, "num_retrieved": 457, "inference_time": 13.78547516465187, "generate_results": ["import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules (no quotes, no escaping)\n    if platform == 0:\n        # Split on spaces, but preserve quoted strings\n        # In Windows CMD, quotes are used to group arguments\n        # We need to handle quoted strings properly\n        result = []\n        current = \"\"\n        in_quotes = False\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                current += char\n            elif char == '\"' and in_quotes:\n                in_quotes = False\n                current += char\n                if current.strip():\n                    result.append(current.strip())\n                current = \"\"\n            else:\n                current += char\n                \n        if current.strip():\n            result.append(current.strip())\n            \n        # Handle unquoted spaces\n        # In Windows CMD, spaces are used to separate arguments\n        # We need to split on spaces but preserve quoted strings\n        # This is a simplified version that just splits on spaces\n        # and handles quoted strings properly\n        return result\n    \n    # POSIX (Linux, macOS) uses shlex.split\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules than POSIX\n    if platform == 0:\n        # Windows CMD: split on spaces, but preserve quotes and backslashes\n        # Use regex to handle quoted strings and escape sequences\n        # This is a simplified version that handles basic cases\n        result = []\n        current = \"\"\n        in_quotes = False\n        quote_char = None\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                quote_char = '\"'\n            elif char == '\"' and in_quotes:\n                in_quotes = False\n                quote_char = None\n            elif char == '\\\\' and in_quotes:\n                # Handle escape sequences in quoted strings\n                current += char\n            elif char == ' ' and not in_quotes:\n                if current:\n                    result.append(current)\n                    current = \"\"\n            else:\n                current += char\n        \n        if current:\n            result.append(current)\n        \n        # Handle the case where there's no space between tokens\n        # This is a fallback to shlex.split for non-quoted cases\n        if not result:\n            result = shlex.split(s)\n            \n        return result\n    \n    # POSIX (Linux, macOS) - use shlex.split as default\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Pattern to match shell-like syntax: quoted strings, escaped characters, and word boundaries\n    # This pattern handles quotes, backslashes, and spaces\n    pattern = r'(?:\"([^\"]*)\"|\\'([^\\']*)\\'|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']+)*)|([^\"\\']+(?:\\s+[^\"\\']", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules than POSIX\n    if platform == 0:\n        # Windows CMD: split on spaces, but preserve quotes and backslashes\n        # Use a more robust regex that handles quoted strings and escapes\n        # This is a simplified version that handles basic cases\n        result = []\n        current = \"\"\n        in_quotes = False\n        quote_char = None\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                quote_char = '\"'\n            elif char == quote_char and in_quotes:\n                in_quotes = False\n                if current:\n                    result.append(current.strip())\n                    current = \"\"\n            elif char == '\\\\' and in_quotes:\n                # Handle escape sequences in quoted strings\n                current += char\n            elif char == ' ' and not in_quotes:\n                if current:\n                    result.append(current.strip())\n                    current = \"\"\n            else:\n                current += char\n        \n        if current:\n            result.append(current.strip())\n        \n        # Handle empty strings or just spaces\n        return [item for item in result if item]\n    \n    # POSIX (Linux, macOS) - use shlex.split for standard behavior\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin', 'posix'] else 0\n    \n    # Pattern to match quoted strings and unquoted tokens\n    # This handles quotes, escaped characters, and spaces\n    if platform == 1:  # POSIX (Linux, macOS)\n        # POSIX shell pattern: matches quoted strings and unquoted tokens\n        # This handles quotes, escaped characters, and spaces\n        pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^ \\t\\n\"\\\\]|\\\\.)*)(?:\\s+|$)'\n        tokens = re.findall(pattern, s)\n        # Remove empty tokens and strip quotes\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            elif token.startswith(\"'\") and token.endswith(\"'\"):\n                result.append(token[1:-1])\n            else:\n                result.append(token.strip())\n        return [t for t in result if t]\n    else:  # Windows/CMD\n        # Windows CMD pattern - handles spaces and quotes\n        # This pattern handles quoted strings and unquoted tokens\n        pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^ \\t\\n\"\\\\]|\\\\.)*)(?:\\s+|$)'\n        tokens = re.findall(pattern, s)\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            elif token.startswith(\"'\") and token.endswith(\"'\"):\n                result.append(token[1:-1])\n            else:\n                result.append(token.strip())\n        return [t for t in result if t]"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 87070, "num_retrieved": 370, "inference_time": 13.78547516465187, "generate_results": ["import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path)\n    \n    # Create the destination path\n    dest_path = tmp_dir / filename\n    \n    # Extract the archive to the destination path\n    try:\n        # Use subprocess to run the tar command\n        subprocess.run(\n            [\"tar\", \"-xzf\", archive_path, \"-C\", str(tmp_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        # If tar fails, try with gzip\n        try:\n            subprocess.run(\n                [\"gzip\", \"-d\", archive_path],\n                check=True\n            )\n            subprocess.run(\n                [\"tar\", \"-xzf\", archive_path, \"-C\", str(tmp_dir)],\n                check=True\n            )\n        except subprocess.CalledProcessError:\n            raise e\n    \n    # Return the file repo url (assuming it's the path to the extracted file)\n    return str(dest_path)", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).split('.')[0]\n    \n    # Create the destination path\n    dest_path = tmp_dir / filename\n    \n    # Extract the archive using tar\n    try:\n        subprocess.run(\n            ['tar', '-xzf', archive_path, '-C', str(tmp_dir)],\n            check=True,\n            capture_output=True\n        )\n    except subprocess.CalledProcessError as e:\n        # Fallback to using gzip if tar is not available\n        try:\n            subprocess.run(\n                ['gunzip', '-c', archive_path],\n                check=True,\n                capture_output=True\n            )\n            # If gzip works, we assume it's a gzipped file\n            # and we can use the extracted content\n            pass\n        except subprocess.CalledProcessError:\n            raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the file repo url (this is a placeholder - actual implementation\n    # would depend on the specific archive format and repository structure)\n    return f\"file://{dest_path}\"", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).split('.')[0]\n    \n    # Create the destination path\n    dest_path = tmp_dir / filename\n    \n    # Extract the archive using tar\n    try:\n        subprocess.run(\n            ['tar', '-xzf', archive_path, '-C', str(tmp_dir)],\n            check=True,\n            capture_output=True\n        )\n    except subprocess.CalledProcessError as e:\n        # Fallback to using gzip if the archive is a .gz file\n        if archive_path.endswith('.gz'):\n            try:\n                subprocess.run(\n                    ['gunzip', '-c', archive_path],\n                    check=True,\n                    capture_output=True\n                )\n                # Extract the decompressed file\n                subprocess.run(\n                    ['tar', '-xzf', archive_path, '-C', str(tmp_dir)],\n                    check=True,\n                    capture_output=True\n                )\n            except subprocess.CalledProcessError:\n                raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the file repo url (this is a placeholder - actual implementation\n    # would depend on the specific archive format and repository structure)\n    return f\"file://{dest_path}\"", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path)\n    \n    # Extract the archive to a temporary directory\n    extract_path = tmp_dir / filename\n    \n    # Use subprocess to extract the archive\n    # This handles common archive formats like .tar.gz, .tar.xz, .tar.bz2, etc.\n    try:\n        # Extract the archive using tar\n        cmd = [\"tar\", \"-xzf\", archive_path, \"-C\", str(tmp_dir)]\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        # If tar fails, try other formats\n        # For .tar.gz, .tar.xz, .tar.bz2, etc.\n        pass\n    \n    # Return the path to the extracted directory as the origin URL\n    # This is a placeholder - in a real implementation, you would return\n    # a proper URL based on the extracted files\n    return str(extract_path)", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).split('.')[0]\n    \n    # Create the destination path\n    dest_path = tmp_dir / filename\n    \n    # Use subprocess to extract the archive\n    # This handles common archive formats like .tar.gz, .tar.xz, .tar.bz2, etc.\n    try:\n        # Determine the archive type and extract accordingly\n        if archive_path.endswith('.tar.gz') or archive_path.endswith('.tgz'):\n            cmd = ['tar', '-xzf', archive_path, '-C', str(tmp_dir)]\n        elif archive_path.endswith('.tar.xz'):\n            cmd = ['tar', '-xJf', archive_path, '-C', str(tmp_dir)]\n        elif archive_path.endswith('.tar.bz2'):\n            cmd = ['tar', '-xjf', archive_path, '-C', str(tmp_dir)]\n        elif archive_path.endswith('.tar'):\n            cmd = ['tar', '-xvf', archive_path, '-C', str(tmp_dir)]\n        else:\n            # For other formats, try to extract with generic tools\n            cmd = ['tar', '-xzf', archive_path, '-C', str(tmp_dir)]\n        \n        subprocess.run(cmd, check=True)\n        \n        # Return the path to the extracted directory as the origin URL\n        return str(dest_path)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")"]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 30634, "num_retrieved": 110, "inference_time": 13.78547516465187, "generate_results": ["import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=False)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and any lines that don't match the ignored list\n        filtered_files = [file.strip() for file in ignored_files if file.strip() and file.strip() in ignored]\n        \n        # Sort the files alphabetically\n        filtered_files.sort()\n        \n        # Join the files with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 3282, "num_retrieved": 30, "inference_time": 13.78547516465187, "generate_results": ["def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Based on the context, the supported file types are those related to docopt examples\n    # such as .docopt files (as seen in test cases and examples)\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt') or filename.endswith('.ini') or filename.endswith('.json')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide information about which file types are supported\n    # Based on the retrieved context, there is no explicit list of supported file types\n    # Therefore, we return False by default as no specific pattern is defined\n    \n    return False", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide specific file types that this module supports\n    # Based on the examples and context, the module appears to handle configuration files\n    # like INI and JSON files, as seen in the config_file_example.py\n    # Therefore, we check if the filename ends with .ini or .json\n    \n    return filename.endswith('.ini') or filename.endswith('.json')"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 88443, "num_retrieved": 281, "inference_time": 13.78547516465187, "generate_results": ["import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+(days?|hours?|minutes?|seconds?|weeks?|months?|years?)$', frequency)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency \"{frequency}\"')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    elif unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'months':\n        # Approximate months as 30 days\n        return timedelta(days=number * 30)\n    elif unit == 'years':\n        # Approximate years as 365 days\n        return timedelta(days=number * 365)\n    else:\n        raise ValueError(f'Unknown time unit \"{unit}\"')", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+(days?|hours?|minutes?|seconds?|weeks?|months?|years?)$', frequency)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency \"{frequency}\"')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    elif unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'months':\n        # Approximate months as 30 days\n        return timedelta(days=number * 30)\n    elif unit == 'years':\n        # Approximate years as 365 days\n        return timedelta(days=number * 365)\n    else:\n        raise ValueError(f'Unsupported time unit: {unit}')", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Map units to timedelta\n    unit_map = {\n        'second': timedelta(seconds=number),\n        'seconds': timedelta(seconds=number),\n        'minute': timedelta(minutes=number),\n        'minutes': timedelta(minutes=number),\n        'hour': timedelta(hours=number),\n        'hours': timedelta(hours=number),\n        'day': timedelta(days=number),\n        'days': timedelta(days=number),\n        'week': timedelta(weeks=number),\n        'weeks': timedelta(weeks=number),\n        'month': timedelta(days=number * 30),  # Approximate\n        'months': timedelta(days=number * 30),  # Approximate\n        'year': timedelta(days=number * 365),  # Approximate\n        'years': timedelta(days=number * 365),  # Approximate\n    }\n    \n    if unit not in unit_map:\n        raise ValueError(f\"Unknown unit '{unit}' in frequency '{frequency}'\")\n    \n    return unit_map[unit]", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+(days?|hours?|minutes?|seconds?|weeks?|months?|years?)$', frequency.strip())\n    \n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'days':\n        return datetime.timedelta(days=number)\n    elif unit == 'hours':\n        return datetime.timedelta(hours=number)\n    elif unit == 'minutes':\n        return datetime.timedelta(minutes=number)\n    elif unit == 'seconds':\n        return datetime.timedelta(seconds=number)\n    elif unit == 'weeks':\n        return datetime.timedelta(weeks=number)\n    elif unit == 'months':\n        # Approximate months as 30 days\n        return datetime.timedelta(days=number * 30)\n    elif unit == 'years':\n        # Approximate years as 365 days\n        return datetime.timedelta(days=number * 365)\n    else:\n        raise ValueError(f\"Unsupported time unit '{unit}' in frequency '{frequency}'\")", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (e.g., \"3 weeks\", \"1 day\", \"2 hours\")\n    match = re.match(r'^(\\d+)\\s+(days?|hours?|minutes?|seconds?|weeks?|months?|years?)$', frequency)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency \"{frequency}\"')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    elif unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'months':\n        # Approximate months as 30 days\n        return timedelta(days=number * 30)\n    elif unit == 'years':\n        # Approximate years as 365 days\n        return timedelta(days=number * 365)\n    else:\n        raise ValueError(f'Unknown time unit \"{unit}\"')"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 116384, "num_retrieved": 466, "inference_time": 13.78547516465187, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP address is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP address is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        # or if the host is 'localhost' or '127.0.0.1'\n        return ip_address == '127.0.0.1' or host.lower() in ['localhost', '127.0.0.1']\n    except socket.gaierror:\n        # If the hostname cannot be resolved, it's not local\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 90792, "num_retrieved": 286, "inference_time": 16.624148219823837, "generate_results": ["def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:', 'sh:')) else path\n        for path in find_paths\n    ]", "def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:')) else path\n        for path in find_paths\n    ]", "def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:')) else path\n        for path in find_paths\n    ]", "def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:')) else path\n        for path in find_paths\n    ]", "def make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('pp:', 'sh:', 'sh:')) else path\n        for path in find_paths\n    ]"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 116433, "num_retrieved": 466, "inference_time": 16.624148219823837, "generate_results": ["import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n:return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM') == 'xterm-256color' and os.environ.get('GIT_BASH') == '1'", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n:return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n:return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n:return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "import os\ndef is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 88259, "num_retrieved": 279, "inference_time": 16.624148219823837, "generate_results": ["import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the configuration to file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 101627, "num_retrieved": 388, "inference_time": 16.624148219823837, "generate_results": ["import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: the one line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within reasonable width\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: the one line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within reasonable width\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join the commands\n    :return: a single line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    command = seperator.join(lines)\n    \n    # Wrap the command to fit within reasonable width if needed\n    wrapped_command = textwrap.fill(command, width=80)\n    \n    return wrapped_command", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: the one line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within reasonable width\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: the one line command\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    # Wrap the command to fit within reasonable width\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped"]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 142368, "num_retrieved": 394, "inference_time": 16.624148219823837, "generate_results": ["import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module_code = f\"\"\"\ndef {func.__name__}(*args, **kwargs):\n    return {func.__name__}(*args, **kwargs)\n\"\"\"\n    \n    # Create a temporary module and add the function to it\n    module = type(module_name, (), {})\n    exec(module_code, module.__dict__)\n    \n    # Create a temporary script to run the function\n    script = f\"\"\"\nimport sys\nimport os\nimport {module_name}\nimport subprocess\n\ndef run_function():\n    try:\n        # Get the function from the module\n        func = {module_name}.{func.__name__}\n        \n        # Prepare the arguments\n        args_list = {args}\n        \n        # Run the function in a subprocess\n        result = func(*args_list)\n        \n        return result\n        \n    except Exception as e:\n        return str(e)\n\nif __name__ == \"__main__\":\n    result = run_function()\n    print(result)\n\"\"\"\n    \n    # Write the script to a temporary file\n    script_path = f\"/tmp/{module_name}.py\"\n    with open(script_path, 'w') as f:\n        f.write(script)\n    \n    # Set up the environment\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the script in a subprocess with timeout\n    try:\n        result = subprocess.run(\n            [sys.executable, script_path],\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            env=env\n        )\n        return result.stdout.strip() if result.stdout else None\n    except subprocess.TimeoutExpired:\n        return \"Timeout expired\"\n    except Exception as e:\n        return str(e)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module_code = f\"\"\"\ndef {func.__name__}(*args, **kwargs):\n    return {func.__name__}(*args, **kwargs)\n\"\"\"\n    \n    # Create a temporary module\n    temp_module = type(module_name, (), {})\n    exec(module_code, temp_module.__dict__)\n    \n    # Get the module path\n    module_path = f\"/tmp/{module_name}.py\"\n    with open(module_path, 'w') as f:\n        f.write(module_code)\n    \n    try:\n        # Set up environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the function in a subprocess\n        cmd = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}(*{args})\"]\n        \n        # Execute with timeout\n        result = subprocess.run(\n            cmd,\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        # Return the result\n        return result.returncode, result.stdout, result.stderr\n        \n    finally:\n        # Clean up the temporary file\n        if os.path.exists(module_path):\n            os.remove(module_path)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module_code = f\"\"\"\ndef {func.__name__}(*args, **kwargs):\n    return {func.__name__}(*args, **kwargs)\n\"\"\"\n    \n    # Create a temporary module and execute the function\n    module = type(module_name, (), {})\n    exec(module_code, module.__dict__)\n    \n    # Get the module path\n    module_path = f\"/tmp/{module_name}.py\"\n    with open(module_path, 'w') as f:\n        f.write(module_code)\n    \n    try:\n        # Set up environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the function in subprocess\n        cmd = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}({', '.join([repr(arg) for arg in args])})\"]\n        \n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            env=env\n        )\n        \n        # Return the output\n        return result.stdout.strip()\n        \n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function {func.__name__} timed out after {timeout} seconds\")\n    except Exception as e:\n        raise RuntimeError(f\"Error running function {func.__name__}: {str(e)}\")\n    finally:\n        # Clean up the temporary module\n        if os.path.exists(module_path):\n            os.remove(module_path)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module_code = f\"\"\"\ndef {func.__name__}(*args, **kwargs):\n    return {func.__name__}(*args, **kwargs)\n\"\"\"\n    \n    # Create a temporary module\n    temp_module = type(module_name, (), {})\n    exec(module_code, temp_module.__dict__)\n    \n    # Get the module path\n    module_path = f\"/tmp/{module_name}.py\"\n    \n    # Write the module code to a temporary file\n    with open(module_path, 'w') as f:\n        f.write(module_code)\n    \n    try:\n        # Set up the environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Create the command to run\n        cmd = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.{func.__name__}({', '.join([repr(arg) for arg in args])})\"]\n        \n        # Run the command with timeout\n        result = subprocess.run(\n            cmd,\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        # Return the result\n        return result.returncode, result.stdout, result.stderr\n        \n    finally:\n        # Clean up the temporary file\n        if os.path.exists(module_path):\n            os.remove(module_path)", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module = importlib.util.module_from_spec(type('Module', (object,), {'__name__': module_name, '__file__': '<temporary>'}))\n    \n    # Add the function to the module\n    module.func = func\n    \n    # Create a namespace for the module\n    sys.modules[module_name] = module\n    \n    # Import the module to make it available\n    try:\n        importlib.import_module(module_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module {module_name}: {e}\")\n    \n    # Create the command to run\n    command = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.func(*{args})\"]\n    \n    # Add extra environment variables\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the command with timeout\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode, result.stdout, result.stderr\n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Command timed out\"\n    except Exception as e:\n        return -1, \"\", str(e)"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 91501, "num_retrieved": 288, "inference_time": 16.624148219823837, "generate_results": ["import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    var_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(var_name)\n    \n    # If value is not found and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)\\}(\\s*[:\\-](.+))?', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    var_name = match.group(1)\n    default_value = match.group(3) if match.group(3) else None\n    \n    # Get the value from environment\n    value = os.environ.get(var_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    var_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(var_name)\n    \n    # If value is not found and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Pattern: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    variable_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(variable_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{variable_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = re.match(r'\\$\\{([^:]+)(?::(-?[^}]+))?\\}', matcher.group(0))\n    if not match:\n        raise ValueError(f\"Invalid environment variable pattern: {matcher.group(0)}\")\n    \n    var_name = match.group(1)\n    default_value = match.group(2) if match.group(2) else None\n    \n    # Get the value from environment\n    value = os.environ.get(var_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default value)\n    return value if value is not None else default_value"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 100261, "num_retrieved": 500, "inference_time": 16.624148219823837, "generate_results": ["import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after the last slash)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path\")\n            \n        # Get the last part of the path as image_id\n        image_id = path.split('/')[-1]\n        \n        # Validate image_id is not empty\n        if not image_id:\n            raise ValueError(\"Invalid image href: missing image_id\")\n            \n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after the last slash)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path\")\n            \n        # Get the last part of the path as image_id\n        image_id = path.split('/')[-1]\n        \n        # Validate image_id is a valid UUID\n        if not uuidutils.is_uuid_like(image_id):\n            raise ValueError(\"Invalid image ID format\")\n            \n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after the last slash)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path component\")\n            \n        # Get the last part of the path as image_id\n        image_id = path.split('/')[-1]\n        \n        # Validate image_id is not empty\n        if not image_id:\n            raise ValueError(\"Invalid image href: missing image_id\")\n            \n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after any leading path components)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path component\")\n            \n        # Extract the last path component as image_id\n        image_id = path.split('/')[-1]\n        \n        # Validate image_id is not empty\n        if not image_id:\n            raise ValueError(\"Invalid image href: missing image_id in path\")\n            \n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parts = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parts.scheme or not parts.netloc:\n            raise ValueError(\"Invalid image href: missing scheme or netloc\")\n            \n        # Determine if SSL is used (https vs http)\n        use_ssl = parts.scheme == 'https'\n        \n        # Extract image_id from path (after the last slash)\n        path = parts.path\n        if not path:\n            raise ValueError(\"Invalid image href: missing path component\")\n            \n        # Split path by slashes and take the last part\n        path_parts = path.strip('/').split('/')\n        image_id = path_parts[-1] if path_parts else ''\n        \n        if not image_id:\n            raise ValueError(\"Invalid image href: missing image_id in path\")\n            \n        # Return the parsed components\n        return (image_id, parts.netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 142800, "num_retrieved": 405, "inference_time": 16.624148219823837, "generate_results": ["def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in os.path.sep:\n            processed_list.append(item[:-1])\n        else:\n            processed_list.append(item)\n    \n    return processed_list", "def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in os.path.sep:\n            processed_list.append(item[:-1])\n        else:\n            processed_list.append(item)\n    \n    return processed_list", "def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in os.path.sep:\n            processed_list.append(item[:-1])\n        else:\n            processed_list.append(item)\n    \n    return processed_list", "def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in os.path.sep:\n            processed_list.append(item[:-1])\n        else:\n            processed_list.append(item)\n    \n    return processed_list", "def remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in os.path.sep:\n            processed_list.append(item[:-1])\n        else:\n            processed_list.append(item)\n    \n    return processed_list"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 2440, "num_retrieved": 10, "inference_time": 16.624148219823837, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space pattern\n    pattern = pattern.replace(' ', r'\\s*')\n    # Replace special regex characters with escaped versions\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literally\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    # and ensure proper regex escaping for special characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = re.escape(pattern)\n    # Add word boundaries to match exact phrases\n    pattern = r'\\b' + pattern + r'\\b'\n    return pattern"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140933, "num_retrieved": 387, "inference_time": 16.624148219823837, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env_dict = env or {}\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command execution\n    try:\n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env_dict,\n            shell=False,\n            timeout=None\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode]\n            \n    except subprocess.TimeoutExpired:\n        return [124]\n    except FileNotFoundError:\n        return [127]\n    except PermissionError:\n        return [137]\n    except OSError as error:\n        return [error.errno]", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env_dict = env or {}\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command arguments\n    cmd_args = args if args is not None else []\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd + cmd_args,\n            cwd=cwd,\n            env=env_dict,\n            capture_output=not hide_stderr,\n            text=True,\n            check=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode, result.stderr if result.stderr else result.stdout]\n            \n    except subprocess.SubprocessError as error:\n        return [error.returncode, str(error)]", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env_dict = env or {}\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command arguments\n    cmd_args = args if args is not None else []\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd + cmd_args,\n            cwd=cwd,\n            env=env_dict,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            timeout=None\n        )\n        \n        # Return the result\n        return result.returncode, result.stdout, result.stderr\n        \n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Command timed out\"\n    except FileNotFoundError:\n        return -1, \"\", \"Command not found\"\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env = env or os.environ.copy()\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command arguments\n    cmd_args = []\n    for arg in args:\n        if isinstance(arg, (list, tuple)):\n            cmd_args.extend(arg)\n        else:\n            cmd_args.append(arg)\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd + cmd_args,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.STDOUT if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode, result.stdout]\n            \n    except subprocess.SubprocessError as error:\n        return [error.returncode, str(error)]", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Prepare the command list\n    cmd = commands if isinstance(commands, (list, tuple)) else [commands]\n    \n    # Prepare the environment\n    env_dict = env or {}\n    \n    # Prepare the working directory\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Prepare the command arguments\n    cmd_args = args if args is not None else []\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd + cmd_args,\n            cwd=cwd,\n            env=env_dict,\n            capture_output=not hide_stderr,\n            text=True,\n            check=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return [0]\n        else:\n            return [result.returncode, result.stderr if result.stderr else result.stdout]\n            \n    except subprocess.SubprocessError as e:\n        return [e.returncode, str(e)]"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 16.624148219823837, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140222, "num_retrieved": 500, "inference_time": 16.624148219823837, "generate_results": ["def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no parent\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object has no parent (i.e., it's not a child of anything), it's a root\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(predicate, object_)):\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no parent (i.e., roots)\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If this subject has no parent (i.e., no triple with (subject, prop, parent))\n        # then it is a root\n        if not any((subject, prop, parent) in graph for parent in graph.objects(subject, prop)):\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no parent\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object has no parent (i.e., it's not a child of anything), it's a root\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(predicate, object_)):\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property as their predicate\n    # These are the potential roots (nodes with no parent)\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # Check if this subject has no parent (i.e., no triple with (subject, prop, parent))\n        # If it doesn't have a parent, it's a root\n        has_parent = False\n        for s, p, o in graph.triples((subject, prop, None)):\n            has_parent = True\n            break\n        \n        if not has_parent:\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no parent (i.e., roots)\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If this subject has no parent (i.e., no triple with (subject, prop, parent))\n        # then it is a root\n        if not any((subject, prop, parent) in graph for parent in graph.subjects(predicate=prop)):\n            roots.add(subject)\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 16464, "num_retrieved": 45, "inference_time": 16.624148219823837, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 7074, "num_retrieved": 39, "inference_time": 16.624148219823837, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 20418, "num_retrieved": 67, "inference_time": 16.624148219823837, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 128706, "num_retrieved": 399, "inference_time": 16.624148219823837, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 87523, "num_retrieved": 290, "inference_time": 22.14847196638584, "generate_results": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config = parse_configuration(config_filename, schema_filename(), overrides=overrides, resolve_env=resolve_env)\n            configurations[config_filename] = config\n        except Validation_error as error:\n            logs.extend(error.errors)\n    \n    return configurations, logs"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 92026, "num_retrieved": 500, "inference_time": 22.14847196638584, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 143386, "num_retrieved": 396, "inference_time": 22.14847196638584, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n\n    :rtype: Time\n\n    :raises ValueError: if ticks is out of bounds\n        (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    hours, minutes = divmod(seconds, 3600)\n    minutes, seconds = divmod(minutes, 60)\n    \n    # Create the time object\n    time_obj = cls(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n\n    :rtype: Time\n\n    :raises ValueError: if ticks is out of bounds\n        (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    hours, minutes = divmod(seconds, 3600)\n    minutes, seconds = divmod(minutes, 60)\n    \n    # Create a Time object with the parsed components\n    time_obj = cls(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n\n    :rtype: Time\n\n    :raises ValueError: if ticks is out of bounds\n        (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    hours, minutes = divmod(seconds, 3600)\n    minutes, seconds = divmod(minutes, 60)\n    \n    # Create a Time object with the extracted components\n    time_obj = cls(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n\n    :rtype: Time\n\n    :raises ValueError: if ticks is out of bounds\n        (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    hours, minutes = divmod(seconds, 3600)\n    minutes, seconds = divmod(minutes, 60)\n    \n    # Create the time object\n    time_obj = cls(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n\n    :rtype: Time\n\n    :raises ValueError: if ticks is out of bounds\n        (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    hours, minutes = divmod(seconds, 3600)\n    minutes, seconds = divmod(minutes, 60)\n    \n    # Create the time object\n    time_obj = cls(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        time_obj = time_obj.replace(tzinfo=tz)\n    \n    return time_obj"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 140246, "num_retrieved": 299, "inference_time": 22.14847196638584, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the protocol handlers\n        handlers = {}\n        for handler in cls._protocol_handlers.values():\n            if handler.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler\n                break\n        return handlers\n    \n    # Return all supported protocol versions\n    return {version: handler for version, handler in cls._protocol_handlers.items()}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the protocol handlers\n        handlers = {}\n        for version in [(3, 0), (3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 0), (5, 1)]:\n            if version == protocol_version:\n                handlers[version] = cls._get_handler_for_version(version)\n        return handlers\n    \n    # Return all supported versions\n    handlers = {\n        (3, 0): Bolt3,\n        (3, 5): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 0): AsyncBolt5x0,\n        (5, 1): AsyncBolt5x1\n    }\n    return handlers\n\ndef _get_handler_for_version(version):\n    \"\"\"Helper method to get the appropriate handler for a given version.\"\"\"\n    if version == (3, 0):\n        return Bolt3\n    elif version == (3, 5):\n        return Bolt3\n    elif version == (4, 0):\n        return Bolt4x0\n    elif version == (4, 1):\n        return Bolt4x1\n    elif version == (4, 2):\n        return Bolt", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any of the protocol handlers\n        handlers = {}\n        for handler in cls._protocol_handlers.values():\n            if handler.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler\n                break\n        return handlers\n    \n    # Return all available protocol handlers if no specific version is provided\n    return {version: handler for version, handler in cls._protocol_handlers.items()}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any protocol handler\n        handlers = {}\n        for handler_class in cls._handlers.values():\n            if handler_class.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler_class\n                break\n        return handlers\n    \n    # Return all supported versions if no specific version is requested\n    handlers = {}\n    for handler_class in cls._handlers.values():\n        version = handler_class.PROTOCOL_VERSION\n        handlers[version] = handler_class\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple) or len(protocol_version) != 2:\n            raise TypeError(\"protocol_version must be a tuple of length 2\")\n        \n        # Check if the version is supported by any protocol handler\n        handlers = {}\n        for version in [(3, 0), (3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 0), (5, 1)]:\n            if version == protocol_version:\n                handlers[version] = cls._get_handler_for_version(version)\n        return handlers\n    \n    # Return all supported versions\n    handlers = {}\n    for version in [(3, 0), (3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (5, 0), (5, 1)]:\n        handlers[version] = cls._get_handler_for_version(version)\n    return handlers\n\ndef _get_handler_for_version(version):\n    \"\"\"Helper method to get the appropriate handler class for a version.\"\"\"\n    if version == (3, 0):\n        return Bolt3\n    elif version == (3, 5):\n        return Bolt3\n    elif version == (4, 0):\n        return Bolt4x0\n    elif version == (4, 1):\n        return Bolt4x1\n    elif version == (4, 2):\n        return Bolt4x2\n    elif version == (4, 3):\n        return Bolt4x3\n    elif version == (4, 4):\n        return Bolt4x4"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 142157, "num_retrieved": 361, "inference_time": 22.14847196638584, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(tuple(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(tuple(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(tuple(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(tuple(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return Bookmarks(tuple(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 118829, "num_retrieved": 376, "inference_time": 22.14847196638584, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if hasattr(seq, \"run\") and callable(seq.run):\n        # seq is already a sequence\n        seq_type = \"sequence\"\n        return seq, seq_type\n    \n    if hasattr(seq, \"__iter__\") and not isinstance(seq, (str, bytes)):\n        # seq is an iterable, try to convert to a sequence\n        try:\n            # Try to create a Sequence from the iterable\n            from . import Sequence\n            seq_obj = Sequence(seq)\n            seq_type = \"sequence\"\n            return seq_obj, seq_type\n        except Exception:\n            # If conversion fails, fall back to the original seq\n            pass\n    \n    # If seq is a callable, try to convert it to a sequence\n    if callable(seq):\n        try:\n            # Try to create a Sequence from the callable\n            from . import Sequence\n            seq_obj = Sequence(seq)\n            seq_type = \"sequence\"\n            return seq_obj, seq_type\n        except Exception:\n            # If conversion fails, fall back to the original seq\n            pass\n    \n    # If none of the above, return the original seq as a sequence\n    # with a type of \"unknown\"\n    seq_type = \"unknown\"\n    return seq, seq_type", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return (seq, \"sequence\")\n    elif isinstance(seq, (tuple, list)):\n        # Convert to a sequence if it's a tuple or list\n        seq_obj = lena.core.Sequence(*seq)\n        return (seq_obj, \"sequence\")\n    elif callable(seq):\n        # If it's callable, it's likely a generator or function\n        return (seq, \"callable\")\n    else:\n        # For any other type, treat it as a simple element\n        return (seq, \"element\")", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return (seq, \"sequence\")\n    elif isinstance(seq, (tuple, list)):\n        # Convert to a sequence if it's a tuple or list\n        seq_obj = lena.core.Sequence(*seq)\n        return (seq_obj, \"sequence\")\n    elif callable(seq):\n        # If it's callable, it's likely a generator or function\n        # Wrap it in a Sequence\n        seq_obj = lena.core.Sequence(seq)\n        return (seq_obj, \"callable\")\n    else:\n        # For other types, try to convert to a sequence\n        # If it's a single element, wrap it in a Sequence\n        seq_obj = lena.core.Sequence(seq)\n        return (seq_obj, \"element\")", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if hasattr(seq, \"run\") and callable(seq.run):\n        # seq is already a sequence\n        seq_type = \"sequence\"\n        return seq, seq_type\n    \n    if isinstance(seq, tuple):\n        # unpack tuple if it's a single element\n        if len(seq) == 1:\n            seq = seq[0]\n        else:\n            # create a sequence from the tuple\n            seq = Sequence(*seq)\n            seq_type = \"sequence\"\n            return seq, seq_type\n    \n    # If seq is a callable, try to convert it to a sequence\n    if callable(seq):\n        # Try to create a sequence from the callable\n        try:\n            seq = Sequence(seq)\n            seq_type = \"sequence\"\n            return seq, seq_type\n        except Exception:\n            # If conversion fails, raise an error\n            raise lena.core.LenaTypeError(\n                \"arguments must implement run method, or be callable generators (convertible to Run), \"\n                \"{} given\".format(seq)\n            )\n    \n    # If seq is not a sequence or callable, raise an error\n    raise lena.core.LenaTypeError(\n        \"arguments must implement run method, or be callable generators (convertible to Run), \"\n        \"{} given\".format(seq)\n    )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if hasattr(seq, \"run\") and callable(seq.run):\n        # seq is already a sequence\n        seq_type = \"sequence\"\n        return seq, seq_type\n    \n    if isinstance(seq, tuple):\n        # unpack tuple if it's a single element\n        if len(seq) == 1:\n            seq = seq[0]\n        else:\n            # create a sequence from the tuple\n            seq_type = \"sequence\"\n            return Sequence(*seq), seq_type\n    \n    # If seq is a callable, try to create a sequence from it\n    if callable(seq):\n        # Create a sequence with a single element that is a callable\n        seq_type = \"callable\"\n        return Sequence(seq), seq_type\n    \n    # If seq is a generator or iterable, create a sequence from it\n    if hasattr(seq, \"__iter__\"):\n        seq_type = \"iterable\"\n        return Sequence(seq), seq_type\n    \n    # Default case - treat as a sequence\n    seq_type = \"sequence\"\n    return Sequence(seq), seq_type"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 116964, "num_retrieved": 378, "inference_time": 22.14847196638584, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # If scale was not computed before, compute it\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute the current integral to get the current scale\n        current_scale = self.integral(self.bins, self.edges)\n        # Calculate the scaling factor\n        scale_factor = other / current_scale\n        # Apply the scaling factor to the bins\n        for bin_content in self.bins:\n            bin_content *= scale_factor\n        # Update the scale\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale if not computed or if recompute is True\n        if self._scale is None and not recompute:\n            # Compute scale if not already computed\n            if self._scale is None:\n                self._scale = integral(self.bins, self.edges)\n        return self._scale\n    \n    # If other is a float, rescale to that value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Check if scale is zero (can't be rescaled)\n    if self._scale == 0:\n        raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n    \n    # Rescale the histogram\n    scale_factor = other / self._scale\n    # Apply scale factor to bins (assuming bins are values)\n    for bin_content in self.bins:\n        bin_content *= scale_factor\n    \n    # Update the scale\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # Only compute if not already computed\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute current integral\n        current_integral = self.integral(self.bins, self.edges)\n        # Rescale by multiplying by other/current_integral\n        scale_factor = other / current_integral\n        # Apply scale factor to bins\n        for bin_content in self.bins:\n            bin_content *= scale_factor\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # Only compute if not already computed\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute current integral\n        current_integral = self.integral(self.bins, self.edges)\n        # Calculate scaling factor\n        scale_factor = other / current_integral\n        # Apply scaling to bins\n        for bin_content in self.bins:\n            bin_content *= scale_factor\n        # Update scale\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # Only compute if scale was not computed before\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute current integral to get current scale\n        current_scale = self.integral(self.bins, self.edges)\n        # Rescale by multiplying by (other / current_scale)\n        if current_scale != 0:\n            scale_factor = other / current_scale\n            # Apply scale factor to bins\n            for bin_content in self.bins:\n                bin_content *= scale_factor\n            self._scale = other\n        else:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 114494, "num_retrieved": 363, "inference_time": 22.14847196638584, "generate_results": ["def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        # Assuming the graph has a scale attribute or method to get scale\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    # Get the field names from the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) > 0:\n        last_field = field_names[-1]\n        \n        # Get the current data for the last coordinate\n        data = self.coords[1] if len(self.coords) > 1 else self.coords[0]\n        \n        # Rescale the data to the new value\n        # This is a simplified implementation - actual scaling logic would depend on the graph implementation\n        if not hasattr(self, '_scale') or self._scale == 0:\n            raise lena.core.LenaValueError(\"graph has zero scale\")\n            \n        # Apply scaling to the last coordinate\n        # In a real implementation, this would modify the graph's data\n        # For now, we just return the scaled value\n        scaled_value = other\n        \n        # Update the graph's scale\n        self._scale = scaled_value\n        \n        return scaled_value\n    \n    # If no field names, return None\n    return None", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaTypeError(\"scale must be a number\")\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Rescale the last coordinate (and its corresponding errors)\n    if len(field_names) > 0:\n        # Get the last field name\n        last_field = field_names[-1]\n        \n        # Get the coordinate values and errors\n        coords = self.coords\n        errors = self._errors\n        \n        # Rescale the last coordinate\n        if len(coords) > 0:\n            # Get the last coordinate\n            last_coord = coords[-1]\n            \n            # Scale the coordinate\n            scaled_coord = [val * other for val in last_coord]\n            \n            # Update the coordinates\n            coords[-1] = scaled_coord\n            \n            # If errors exist, scale them too\n            if errors is not None and len(errors) > 0:\n                # Get the last error\n                last_error = errors[-1]\n                if last_error is not None:\n                    scaled_error = [val * other for val in last_error]\n                    errors[-1] = scaled_error\n    \n    # Update the scale attribute\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaTypeError(\"scale must be a number\")\n    \n    # Get the field names from the graph\n    field_names = self.field_names\n    \n    # Determine which coordinate to rescale (last one)\n    if len(field_names) > 0:\n        last_field = field_names[-1]\n        \n        # Get the current data and scale\n        data = self.coords\n        scale = other\n        \n        # Rescale the last coordinate\n        if len(data) > 0:\n            # Rescale the last coordinate\n            new_data = []\n            for i in range(len(data[0])):\n                new_data.append(data[0][i] * scale)\n            \n            # Update the graph with new data\n            self.coords = [new_data]\n            \n            # If errors exist, rescale them too\n            if hasattr(self, 'errors') and self.errors:\n                new_errors = []\n                for i in range(len(self.errors[0])):\n                    new_errors.append(self.errors[0][i] * scale)\n                self.errors = [new_errors]\n    \n    # Update the scale attribute\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        # Assuming the graph has a scale attribute or method to get scale\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    # Get the field names from the graph\n    field_names = self.field_names\n    \n    # Only rescale the last coordinate\n    last_field = field_names[-1]\n    \n    # Get the current scale of the graph\n    current_scale = self._scale\n    \n    # If the current scale is unknown or zero, raise an error\n    if current_scale is None or current_scale == 0:\n        raise lena.core.LenaValueError(\n            \"graph has unknown or zero scale, cannot rescale\"\n        )\n    \n    # Calculate the new scale\n    new_scale = other / current_scale\n    \n    # Apply the new scale to the last coordinate\n    # This would typically involve modifying the graph's data\n    # and updating the scale attribute\n    self._scale = other\n    \n    # If the graph has error fields, rescale them as well\n    # This is a simplified implementation - actual implementation\n    # would depend on the specific graph structure\n    if hasattr(self, 'error'):\n        for error_field in self.error:\n            # Rescale the error field with the same factor\n            # as the coordinate field\n            pass\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaTypeError(\"scale must be a number\")\n    \n    # Get the field names from the graph\n    field_names = self.field_names\n    \n    # Only rescale the last coordinate\n    last_field = field_names[-1]\n    \n    # Rescale the last coordinate and its associated errors\n    if hasattr(self, 'coords') and len(self.coords) > 0:\n        # Get the last coordinate array\n        last_coords = self.coords[-1]\n        \n        # Scale the coordinate values\n        scaled_coords = [val * other for val in last_coords]\n        \n        # If errors exist, scale them as well\n        if hasattr(self, 'errors') and self.errors:\n            error_coords = self.errors[-1]\n            scaled_errors = [val * other for val in error_coords]\n            \n            # Update the graph with scaled values\n            self.coords[-1] = scaled_coords\n            self.errors[-1] = scaled_errors\n    \n    # Update the graph's scale\n    self._scale = other\n    \n    return self._scale"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 117516, "num_retrieved": 395, "inference_time": 22.14847196638584, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(len(edges))]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(len(edges))]\n    elif get_coordinate == \"middle\":\n        bin_coords = [(edges[i][0] + edges[i][-1]) / 2 for i in range(len(edges))]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Get bin content and create points\n    points = []\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # For 2D histograms, we need to get the bin content\n            # For 1D histograms,", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import copy\n    \n    # Default make_value is the bin content\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n    \n    # Validate get_coordinate\n    valid_coordinates = [\"left\", \"right\", \"middle\"]\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Get bin edges\n    edges = hist.edges\n    \n    # Determine coordinates based on get_coordinate\n    coords = []\n    for i in range(len(edges)):\n        if get_coordinate == \"left\":\n            coords.append(edges[i][0])\n        elif get_coordinate == \"right\":\n            coords.append(edges[i][-1])\n        elif get_coordinate == \"middle\":\n            coords.append((edges[i][0] + edges[i][-1]) / 2)\n    \n    # Get bin values\n    values = []\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n        \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(len(edges))]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(len(edges))]\n    elif get_coordinate == \"middle\":\n        bin_coords = [edges[i][len(edges[i]) // 2] for i in range(len(edges))]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Create points from histogram bins\n    points = []\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # For 2D histogram, use bin coordinates\n            x_coord = edges[0][i]\n            y_coord = edges", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import copy\n    \n    # Default make_value is the bin content\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n    \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    dim = len(edges)\n    \n    # Get bin coordinates based on get_coordinate\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(dim)]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(dim)]\n    elif get_coordinate == \"middle\":\n        bin_coords = [edges[i][len(edges[i]) // 2] for i in range(dim)]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Create points list\n    points = []\n    \n    # Process each bin\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Determine the coordinate based on get_coordinate\n    if get_coordinate == \"left\":\n        bin_edges = hist.edges\n        coords = [edges[0] for edges in bin_edges]\n        coords = [edges[0] for edges in bin_edges]\n    elif get_coordinate == \"right\":\n        coords = [edges[-1] for edges in bin_edges]\n    elif get_coordinate == \"middle\":\n        coords = [(edges[0] + edges[-1]) / 2 for edges in bin_edges]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right' or 'middle', got {get_coordinate}\")\n    \n    # Create points from histogram bins\n    points = []\n    for i, bin_content in enumerate(hist.bins):\n        # Apply make_value to get the value\n        value = make_value(bin_content)\n        \n        # Add the coordinate and value to points\n        points.append((coords[i],"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 99881, "num_retrieved": 469, "inference_time": 22.14847196638584, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"method\", \"missing method\", iface, candidate))\n\n    # Step 3: Check if methods have correct signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__signature__'):\n                    # Try to get signature from function\n                    try:\n                        sig = method.__code__.co_varnames\n                        # Check if signature matches expected\n                        if not desc.getSignatureString():\n                            errors.append(BrokenMethodImplementation(name, \"invalid signature\", iface, candidate))\n                    except (AttributeError, TypeError):\n                        errors.append(BrokenMethodImplementation(name, \"invalid signature\",", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"method\", \"missing method\", iface, candidate))\n\n    # Step 3: Check if methods have correct signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n                # Check method signature if possible\n                try:\n                    method_signature = method.getSignatureString()\n                    if not method_signature:\n                        errors.append(BrokenMethodImplementation(name, \"invalid signature\", iface, candidate))\n                except AttributeError:\n                    errors.append(BrokenMethodImplementation(name, \"invalid signature\",", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"method\", \"missing method\", iface, candidate))\n\n    # Step 3: Check that methods have correct signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n                # Check method signature if possible\n                try:\n                    method_signature = method.getSignatureString()\n                    if not method_signature:\n                        errors.append(BrokenMethodImplementation(name, \"invalid signature\", iface, candidate))\n                except AttributeError:\n                    errors.append(BrokenMethodImplementation(name, \"invalid signature\",", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"method\", \"missing method\", iface, candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n                # Check signature if possible\n                try:\n                    if hasattr(method, '__signature__'):\n                        # Compare signature with interface definition\n                        pass\n                except AttributeError:\n                    pass\n    except AttributeError:\n        pass\n\n    # Step 4: Check necessary attributes\n    try:\n        for name, desc in", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n    except AttributeError:\n        errors.append(BrokenMethodImplementation(\"methods\", \"missing methods\", iface, candidate))\n\n    # Step 3: Check if methods have correct signature\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n                # Check signature if possible\n                try:\n                    # This is a simplified check - in practice, we'd need to compare\n                    # the actual signature of the method with the interface definition\n                    pass\n                except Exception as e:\n                    errors.append(BrokenMethodImplementation(name, str(e"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 99879, "num_retrieved": 469, "inference_time": 22.14847196638584, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n        else:\n            method = getattr(candidate, name)\n            if not hasattr(method, '__call__'):\n                errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n            try:\n                # Check method signature\n                if hasattr(desc, 'getSignatureString'):\n                    method_sig = desc.getSignatureString()\n                    method_sig_actual = method.__code__.co_varnames\n                    # This is a simplified check - in practice, we'd need to compare\n                    # the actual signature with the expected one\n                    pass\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, \"invalid method signature\", iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate,", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n        else:\n            method = getattr(candidate, name)\n            if not hasattr(method, '__call__'):\n                errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n            try:\n                # Check method signature\n                if hasattr(desc, 'getSignatureString'):\n                    method_sig = method.getSignatureString()\n                    desc_sig = desc.getSignatureString()\n                    if method_sig != desc_sig:\n                        errors.append(BrokenMethodImplementation(name, \"signature mismatch\", iface, candidate))\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, \"signature error\", iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n        else:\n            method = getattr(candidate, name)\n            if not hasattr(method, '__call__'):\n                errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n            try:\n                # Check method signature\n                if hasattr(desc, 'getSignatureString'):\n                    method_sig = desc.getSignatureString()\n                    method_sig_actual = method.__code__.co_varnames\n                    # This is a simplified check - in practice, we'd need to compare\n                    # the actual signature with the expected one\n                    pass\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, \"invalid method signature\", iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate,", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name in iface.names(all=True):\n        method = getattr(candidate, name, None)\n        if method is None:\n            errors.append(BrokenMethodImplementation(name, \"missing\", iface, candidate))\n        else:\n            # Check method signature\n            try:\n                # Get the method's signature from the interface\n                method_spec = iface.direct(name)\n                if method_spec is None:\n                    continue\n                \n                # For methods, we need to check the signature\n                # This is a simplified check - in practice, we'd need\n                # to compare the actual method signature with the interface\n                # specification\n                pass\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, str(e), iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name in iface.names(all=True):\n        attr = getattr(candidate, name, None)\n        if attr is None:\n            errors.append(BrokenImplementation(iface, name, candidate))\n\n    #", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenMethodImplementation(name, \"missing method\", iface, candidate))\n        else:\n            method = getattr(candidate, name)\n            if not hasattr(method, '__call__'):\n                errors.append(BrokenMethodImplementation(name, \"not callable\", iface, candidate))\n            try:\n                # Check method signature\n                if hasattr(desc, 'getSignatureString'):\n                    signature = desc.getSignatureString()\n                    # This is a simplified check - in practice, we'd need to compare\n                    # the actual method signature with the expected one\n                    pass\n            except Exception as e:\n                errors.append(BrokenMethodImplementation(name, \"signature error\", iface, candidate))\n\n    # Step 3: Check all necessary attributes\n    for name, desc in iface.namesAndDescriptions(all=True):\n        if not hasattr(candidate, name):\n            errors.append(BrokenAttributeImplementation(name, \"missing attribute"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 83123, "num_retrieved": 340, "inference_time": 22.14847196638584, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative)"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 80344, "num_retrieved": 319, "inference_time": 22.14847196638584, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # If all bases are the same, use that base's metaclass\n    base = bases[0]\n    if all(b is base for b in bases):\n        return base.__class__\n    \n    # Otherwise, use the metaclass of the first base\n    return bases[0].__class__", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 11633, "num_retrieved": 62, "inference_time": 22.14847196638584, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 9729, "num_retrieved": 47, "inference_time": 22.14847196638584, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 22.14847196638584, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 12.159215271472931, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 10009, "num_retrieved": 48, "inference_time": 12.159215271472931, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    value = self[key]\n    del self[key]\n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 28011, "num_retrieved": 72, "inference_time": 12.159215271472931, "generate_results": ["def _create_in_regex(self) -> Pattern[str]:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the in-style parameter format\n    # This pattern will match in-style parameters according to the style's param_regex\n    # and escape sequence rules\n    pattern = self._in_style.param_regex\n    \n    # Ensure the pattern is properly escaped and handles escape sequences\n    # The pattern should match parameter placeholders in the SQL string\n    # and capture the parameter value for conversion\n    \n    # Use the escape_regex to handle escape sequences that prevent parameter matching\n    # The pattern should be constructed to match the specific parameter format\n    # defined by the in-style parameter style\n    \n    # Return the compiled regular expression pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is defined by the param_regex attribute\n    # of the style instance, which is set during initialization\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is not explicitly defined in the context\n    # However, based on the pattern of parameter matching in the code, we can infer\n    # that it would match parameters based on the in-style format (named, numeric, or ordinal)\n    # and would use the param_regex attribute of the style class\n    \n    # Since the exact regex is not provided in the context, we return a placeholder\n    # that would be used to match parameters in the SQL string\n    import re\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regex is not explicitly defined in the context\n    # However, based on the pattern of parameter matching in the code, \n    # we can infer that it's constructed from the param_regex attribute of the style\n    # which is used to extract parameters from the SQL string.\n    \n    # Since the exact regex pattern is not provided in the context, \n    # we return a placeholder that would be used in the actual implementation\n    # This would typically be a compiled regex pattern based on the param_regex\n    # attribute of the style instance.\n    \n    # In a real implementation, this would use the param_regex attribute\n    # from the style instance to create the pattern.\n    \n    # Placeholder implementation - in actual code, this would use self.param_regex\n    import re\n    return re.compile(self.param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # The in-style parameter regular expression is not explicitly defined in the context\n    # However, based on the pattern of parameter matching in the code, we can infer\n    # that it would match parameter placeholders in the SQL query.\n    # Since the exact pattern is not provided in the context, we return a placeholder\n    # that would match named, numeric, or ordinal parameters based on the style.\n    \n    # This is a generic implementation that would match parameter patterns\n    # based on the style's param_regex attribute\n    return self.param_regex"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 27900, "num_retrieved": 71, "inference_time": 12.159215271472931, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 56503, "num_retrieved": 217, "inference_time": 12.159215271472931, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 50336, "num_retrieved": 190, "inference_time": 12.159215271472931, "generate_results": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 53147, "num_retrieved": 194, "inference_time": 12.159215271472931, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For unambiguous datetimes, we can directly convert from UTC\n    # First, check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine which fold to use\n        fold = self._fold(dt)\n        # Use the fold state to determine which time to return\n        # The fold state indicates whether we're in the first or second occurrence\n        # of an ambiguous time\n        if fold:\n            # Use the second occurrence (fold=1)\n            return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)\n        else:\n            # Use the first occurrence (fold=0)\n            return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)\n    \n    # For non-ambiguous datetimes, simply convert\n    return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine which fold to use\n        # The fold is determined by the fold attribute of the datetime\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the \"forward\" fold (second occurrence)\n        # If fold is 0, we're in the \"backward\" fold (first occurrence)\n        # We need to resolve to the correct fold based on the timezone rules\n        idx = self._find_last_transition(dt)\n        if idx is None or idx == 0:\n            # No transitions found, just return the original dt\n            return dt\n        \n        # Determine the correct fold based on the timezone rules\n        # If we're in a fold, shift to the different index\n        idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n        resolved_idx = idx - idx_offset\n        \n        # Create a new datetime with the resolved index\n        # This effectively resolves the ambiguous time to the correct fold\n        new_dt = dt.replace(tzinfo=None)\n        new_dt = new_dt.replace(tzinfo=self)\n        \n        # If we have a fold, we need to adjust the datetime\n        if fold == 1:\n            # We're in the forward fold, so we need to adjust to the standard time\n            # This is handled by the timezone's transition rules\n            pass\n        \n        return new_dt\n    else:\n        # If not ambiguous, simply return the original datetime\n        return dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For unambiguous datetimes, we can directly convert from UTC\n    # First, check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine which fold to use\n        fold = self._fold(dt)\n        # Use the fold state to determine which time to return\n        # The fold state indicates whether we're in the first or second occurrence\n        # of an ambiguous time\n        if fold:\n            # Return the second occurrence (fold=1)\n            return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)\n        else:\n            # Return the first occurrence (fold=0)\n            return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)\n    \n    # If not ambiguous, simply convert\n    return dt.replace(tzinfo=None).astimezone(self).replace(tzinfo=None)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine which fold to use\n        # The fold state is determined by the fold attribute\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence (the \"fold\" state)\n        # We need to adjust to the correct fold\n        if fold == 1:\n            # For fold=1, we want the second occurrence\n            # This is typically handled by the tzinfo implementation\n            # For now, we just return the original dt with fold=1\n            return dt.replace(fold=1)\n        else:\n            # For fold=0, we're in the first occurrence\n            return dt.replace(fold=0)\n    \n    # If not ambiguous, simply return the datetime\n    return dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For UTC-aware timezones, return the original dt\n    if self is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # If ambiguous, determine the fold state\n        fold = self._fold(dt)\n        # Return the appropriate time based on fold state\n        # The fold state indicates which of the two possible times to use\n        # (the first or second occurrence of the ambiguous time)\n        return dt.replace(fold=not fold)\n    \n    # If not ambiguous, simply return the original datetime\n    return dt"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 60209, "num_retrieved": 230, "inference_time": 12.159215271472931, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 19049, "num_retrieved": 67, "inference_time": 12.159215271472931, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._before_cut = before\n    self._after_cut = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 107425, "num_retrieved": 382, "inference_time": 12.159215271472931, "generate_results": ["def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') or request.headers.get('X-Diaspora-Private-Message'):\n        # Check for legacy Diaspora message headers\n        if request.headers.get('X-Diaspora-Message'):\n            return \"diaspora\"\n    \n    # Check if the request body contains a legacy Diaspora magic envelope\n    body = request.body\n    if isinstance(body, str):\n        # Check for magic envelope signature pattern\n        if b\"application/xml\" in body.encode(\"utf-8\") or b\"magic-env\" in body.encode(\"utf-8\"):\n            return \"diaspora\"\n    \n    # Check if the request URL contains a Diaspora path pattern\n    url = request.url\n    if url and (\"diaspora\" in url.lower() or \"receive\" in url.lower()):\n        return \"diaspora\"\n    \n    # Check if the request has a Diaspora-specific header\n    if request.headers.get('X-Diaspora-Handle'):\n        return \"diaspora\"\n    \n    # Check if the request has a Diaspora-specific content type\n    content_type = request.headers.get('Content-Type', '')\n    if 'diaspora' in content_type.lower():\n        return \"diaspora\"\n    \n    # Default to no protocol found\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'private':\n        return \"diaspora\"\n    \n    # Check if the request body contains a legacy Diaspora magic envelope signature\n    body = request.body\n    if isinstance(body, str):\n        # Look for the magic envelope signature pattern\n        if 'magic-env' in body.lower() or 'diaspora' in body.lower():\n            return \"diaspora\"\n    \n    # Check if the request URL contains a Diaspora-specific path\n    if request.url and ('diaspora' in request.url.lower() or '/receive/' in request.url):\n        return \"diaspora\"\n    \n    # Check if the request method is a Diaspora-specific method\n    if request.method in ['POST', 'PUT'] and request.headers.get('Content-Type', '').startswith('application/xml'):\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'private':\n        return \"diaspora\"\n    \n    # Check if the request body contains a legacy Diaspora magic envelope\n    body = request.body\n    if isinstance(body, str):\n        # Look for the magic envelope signature pattern\n        if b\"application/xml\" in body.encode(\"utf-8\") and b\"magic-env\" in body.encode(\"utf-8\"):\n            return \"diaspora\"\n    \n    # Check if the request URL contains a Diaspora-specific path\n    if request.url.startswith(\"/diaspora/\"):\n        return \"diaspora\"\n    \n    # Check if the request method is a Diaspora-specific method\n    if request.method in [\"POST\", \"PUT\"] and request.url.endswith(\"/receive\"):\n        return \"diaspora\"\n    \n    # Default to no protocol found\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # First check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'public':\n        return \"diaspora\"\n    \n    # Check if the request has a private message\n    if request.headers.get('X-Diaspora-Message') == 'private':\n        return \"diaspora\"\n    \n    # Check if the request has a legacy payload (public message with specific headers)\n    if request.headers.get('X-Diaspora-Message') == 'legacy':\n        return \"diaspora\"\n    \n    # Check if the request body contains a public message (legacy format)\n    body = request.body\n    if isinstance(body, str) and 'public' in body.lower():\n        return \"diaspora\"\n    \n    # Check if the request body contains a private message (legacy format)\n    if isinstance(body, str) and 'private' in body.lower():\n        return \"diaspora\"\n    \n    # If none of the above conditions are met, return None\n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') or request.headers.get('X-Diaspora-Private-Message'):\n        # Legacy Diaspora message headers\n        return \"diaspora\"\n    \n    # Check if the request body contains a public message (legacy format)\n    body = request.body\n    if isinstance(body, str):\n        # Check for legacy public message format\n        if \"public\" in body.lower() or \"diaspora\" in body.lower():\n            return \"diaspora\"\n    \n    # Check if the request has a private message header\n    if request.headers.get('X-Diaspora-Private-Message'):\n        return \"diaspora\"\n    \n    # Check if the request URL contains a diaspora pattern\n    url = request.url\n    if url and ('diaspora' in url.lower() or 'diaspora.' in url.lower()):\n        return \"diaspora\"\n    \n    # Check if the request method is POST and has a specific content type\n    if request.method == 'POST' and request.headers.get('Content-Type', '').startswith('application/diaspora'):\n        return \"diaspora\"\n    \n    # Default to no protocol found\n    return None"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 94313, "num_retrieved": 297, "inference_time": 12.159215271472931, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths\n    if request.url.startswith('/_matrix/'):\n        return True\n    \n    # Check if the request headers contain Matrix-specific headers\n    if request.headers.get('X-Matrix-Server') or request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check if the request method is a Matrix-specific method\n    if request.method in ['POST', 'PUT', 'DELETE'] and request.url.startswith('/_matrix/'):\n        return True\n    \n    # Check if the request contains Matrix-specific parameters\n    if 'matrix' in request.url or 'matrix' in request.headers.get('X-Request-Path', ''):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Check for matrix-specific paths like /_matrix/client/r0\n    if '/_matrix/client/' in url or '/_matrix/federation/' in url:\n        return True\n    \n    # Check for matrix-specific domains or subdomains\n    if 'matrix' in url.lower():\n        return True\n    \n    # Check if the request method is a known Matrix endpoint\n    if request.method in ['POST', 'GET'] and '/_matrix/' in url:\n        return True\n    \n    # Check if the request headers contain matrix-specific information\n    if request.headers.get('X-Matrix-Client-Id'):\n        return True\n    \n    # Check if the request headers contain matrix-specific authentication\n    if request.headers.get('Authorization') and 'matrix' in request.headers.get('Authorization', '').lower():\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Check for common Matrix endpoints like /_matrix/client/r0\n    if '/_matrix/client/' in url or '/_matrix/federation/' in url:\n        return True\n    \n    # Check for Matrix-specific query parameters\n    if 'access_token' in request.headers.get('Authorization', '') or 'matrix' in request.headers.get('X-Matrix-Client', ''):\n        return True\n    \n    # Check if the request method is one that Matrix typically uses\n    if request.method in ['POST', 'GET']:\n        # Matrix typically uses specific endpoints\n        if '/_matrix/client/r0' in url or '/_matrix/federation/v1' in url:\n            return True\n    \n    # Check if the request headers contain Matrix-specific information\n    if request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check if the request URL contains a known Matrix domain pattern\n    if 'matrix' in url.lower() or 'matrix.org' in url.lower():\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths\n    if request.url.startswith('/_matrix/'):\n        return True\n    \n    # Check if the request headers contain Matrix-specific headers\n    if request.headers.get('X-Matrix-Server') or request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check if the request method is one of the Matrix-specific methods\n    if request.method in ['POST', 'PUT', 'DELETE']:\n        # Matrix requests often use specific endpoints\n        if request.url.endswith('/_matrix/client/r0/') or request.url.endswith('/_matrix/federation/'):\n            return True\n    \n    # Check if the request contains Matrix-specific parameters\n    if request.url.find('matrix') != -1 or request.url.find('federation') != -1:\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Matrix requests typically have paths like /_matrix/client/r0 or /_matrix/federation\n    matrix_paths = [\n        '/_matrix/client/r0',\n        '/_matrix/federation/v1',\n        '/_matrix/client/api/v1',\n        '/_matrix/client/r0/register',\n        '/_matrix/client/r0/rooms',\n        '/_matrix/client/r0/rooms/invite',\n        '/_matrix/client/r0/rooms/send',\n        '/_matrix/client/r0/rooms/invite',\n        '/_matrix/client/r0/rooms/leave',\n        '/_matrix/client/r0/rooms/join',\n        '/_matrix/client/r0/rooms/create',\n        '/_matrix/client/r0/rooms/leave',\n        '/_matrix/client/r0/rooms/invite',\n        '/_matrix/client/r0/rooms/leave',\n        '/_matrix/client/r0/rooms/join',\n        '/_matrix/client/r0/rooms/create',\n        '/_matrix/client/r0/rooms/invite',\n        '/_matrix/client/r0/rooms/leave',\n        '/_matrix/client/r0/rooms/join',\n        '/_matrix/client/r0/rooms/create',\n    ]\n    \n    # Check if the request URL contains any matrix paths\n    for path in matrix_paths:\n        if path in url:\n            return True\n    \n    # Check if the request URL contains matrix domains or subdomains\n    matrix_domains = [\n        'matrix',\n        'matrix.org',\n        'matrix.org',\n        'matrix.example.com',\n        'matrix.domain.tld'\n    ]\n    \n    # Check if the domain contains any matrix domain\n    domain = url.split('//')[-1].split('/')[0]\n    if any(domain.startswith(m) for m in matrix_domains):\n        return True\n    \n    # Check if the request headers contain matrix-related information\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check if the request method is a matrix-specific method\n    if request.method in ['POST', 'GET']:\n        # Matrix requests often use POST for creating resources\n        if request.method == 'POST' and '/"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 93957, "num_retrieved": 298, "inference_time": 12.159215271472931, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 113699, "num_retrieved": 426, "inference_time": 12.159215271472931, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the code block\n            continue\n            \n        # Check if line is inside a code block (starts with code block marker)\n        if line.strip().startswith('```') or line.strip().startswith('```'):\n            # Skip the code block\n            continue\n            \n        # Find tags in the line\n        # Use regex to find hashtags\n        import re\n        tag_matches = re.finditer(r'#(\\w+)', line)\n        \n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                replacement = replacer(tag)\n                result = result.replace(f'#{tag}', replacement)\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the code block\n            continue\n            \n        # Check if line is inside a code block (starts with code block marker)\n        if line.strip().startswith('```') or line.strip().startswith('```'):\n            # Skip the code block\n            continue\n            \n        # Find all hashtags in the line\n        # Use a regex to find hashtags that are not inside code blocks\n        import re\n        tag_matches = re.finditer(r'#([^\\s\\W]+)', line)\n        \n        for match in tag_matches:\n            tag = match.group(1).lower()\n            tags.add(tag)\n            \n            # If a replacer is provided, replace the tag in the text\n            if replacer:\n                result = result.replace(f'#{tag}', replacer(tag))\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Toggle code block state\n            in_code_block = not in_code_block\n            continue\n            \n        # If inside a code block, skip processing\n        if in_code_block:\n            continue\n            \n        # Find all hashtags in the line\n        tag_matches = re.finditer(r'#(\\w+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                result = result.replace(f'#{tag}', replacer(tag))\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Toggle code block state\n            in_code_block = not in_code_block\n            continue\n            \n        # If inside a code block, skip processing\n        if in_code_block:\n            continue\n            \n        # Find all hashtags in the line\n        # Use regex to match hashtags that are not inside code blocks\n        import re\n        tag_matches = re.finditer(r'#([^\\s\\W]+)', line)\n        \n        for match in tag_matches:\n            tag = match.group(1).lower()\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag with the result of the replacer function\n            if replacer:\n                replacement = replacer(tag)\n                result = result.replace(f'#{tag}', replacement)\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Pattern to match hashtags and mentions\n    # This pattern will match #tag and @mention, but we need to avoid matching inside code blocks\n    # We'll use a regex that matches hashtags and mentions outside of code blocks\n    pattern = r'(?<!`)(?:#|@)(\\w+)(?!\\w)'\n    \n    # Find all matches\n    matches = list(re.finditer(pattern, text))\n    \n    # Process each match\n    for match in matches:\n        tag = match.group(1)\n        # Add to tags set\n        tags.add(tag)\n        \n        # If replacer is provided, replace the tag with the result of the replacer function\n        if replacer:\n            result = result.replace(f\"#{tag}\", replacer(tag))\n            result = result.replace(f\"@{tag}\", replacer(tag))\n    \n    return tags, result"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 88754, "num_retrieved": 280, "inference_time": 12.159215271472931, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in the text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with mailto links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\">\\1</a>', text)\n    \n    # Replace hashtags with links\n    hashtag_pattern = r'#([a-zA-Z0-9_]+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"/tag/\\1\" class=\"hashtag\">\\1</a>', text)\n    \n    # Replace mentions with links\n    mention_pattern = r'@([a-zA-Z0-9_]+)'\n    text = re.sub(mention_pattern, r'<a href=\"/user/\\1\" class=\"mention\">@\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs in the text with HTML links\n    import re\n    \n    # Pattern to match URLs (including http, https, and other protocols)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    \n    # Replace URLs with HTML anchor tags\n    def replace_url(match):\n        url = match.group(1)\n        # Add attributes like target=\"_blank\" and rel=\"noopener\"\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{url}</a>'\n    \n    # Replace URLs in the text\n    processed_text = re.sub(url_pattern, replace_url, text)\n    \n    return processed_text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Match email addresses\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Match hashtags\n    hashtag_pattern = r'(#\\w+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"/tags/\\1\" class=\"hashtag\">\\1</a>', text)\n    \n    # Match mentions\n    mention_pattern = r'(@\\w+)'\n    text = re.sub(mention_pattern, r'<a href=\"/users/\\1\" class=\"mention\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in the text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\">\\1</a>', text)\n    \n    # Replace email addresses with mailto links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\">\\1</a>', text)\n    \n    # Replace hashtags with HTML links\n    hashtag_pattern = r'#([a-zA-Z0-9_]+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"/tag/\\1\" class=\"hashtag\">\\1</a>', text)\n    \n    # Replace mentions with HTML links\n    mention_pattern = r'@([a-zA-Z0-9_]+)'\n    text = re.sub(mention_pattern, r'<a href=\"/user/\\1\" class=\"mention\">@\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in the text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with mailto links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\">\\1</a>', text)\n    \n    # Replace hashtags with HTML links\n    hashtag_pattern = r'#([a-zA-Z0-9_]+)'\n    text = re.sub(hashtag_pattern, r'<a href=\"/tag/\\1\" class=\"hashtag\">#\\1</a>', text)\n    \n    # Replace mentions with HTML links\n    mention_pattern = r'@([a-zA-Z0-9_]+)'\n    text = re.sub(mention_pattern, r'<a href=\"/user/\\1\" class=\"mention\">@\\1</a>', text)\n    \n    return text"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 83747, "num_retrieved": 322, "inference_time": 12.159215271472931, "generate_results": ["def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except requests.exceptions.RequestException:\n        pass\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 82020, "num_retrieved": 257, "inference_time": 12.159215271472931, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain special characters except for alphanumeric and common punctuation\n    # Remove any non-alphanumeric characters except for hyphens and underscores\n    cleaned_tag = re.sub(r'[^a-zA-Z0-9\\-_]', '', tag.strip())\n    \n    # Tags should not be just whitespace\n    if not cleaned_tag:\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(cleaned_tag) > 50:\n        return False\n    \n    # Tags should not start or end with a hyphen or underscore\n    if cleaned_tag.startswith('-') or cleaned_tag.startswith('_') or \\\n       cleaned_tag.endswith('-') or cleaned_tag.endswith('_'):\n        return False\n    \n    # Tags should not contain consecutive hyphens or underscores\n    if '--' in cleaned_tag or '___' in cleaned_tag:\n        return False\n    \n    # Tags should not contain only numbers\n    if cleaned_tag.isdigit():\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (only alphanumeric and hyphens)\n    # This is a simplified rule - in practice, tags might allow other characters\n    # but we'll keep it simple for now\n    if not tag.replace('-', '').isalnum():\n        return False\n    \n    # Tags should be at least 1 character long\n    if len(tag) < 1:\n        return False\n    \n    # Tags should not start or end with a hyphen\n    if tag.startswith('-') or tag.endswith('-'):\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (only alphanumeric and hashtags)\n    # Remove any non-alphanumeric characters except for the hash symbol\n    cleaned_tag = tag.strip().replace('#', '')\n    \n    # Tags should only contain alphanumeric characters\n    if not cleaned_tag.isalnum():\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(tag) > 50:\n        return False\n    \n    # Tags should not start or end with a hash\n    if tag.startswith('#') or tag.endswith('#'):\n        return False\n    \n    # Tags should not contain multiple consecutive hashtags\n    if '##' in tag:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain invalid characters (only alphanumeric, underscores, and hash)\n    # Tags should not start or end with a hash\n    if tag.startswith('#') or tag.endswith('#'):\n        return False\n    \n    # Tags should not contain spaces or special characters\n    if not re.match(r'^[a-zA-Z0-9_]+$', tag.strip()):\n        return False\n    \n    # Tags should be at least 1 character long\n    if len(tag.strip()) < 1:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (only alphanumeric and hyphens)\n    # This is a simplified rule - in practice, tags might allow other characters\n    # but we'll keep it simple for now\n    if not tag.replace('-', '').isalnum():\n        return False\n    \n    # Tags should not start or end with a hyphen\n    if tag.startswith('-') or tag.endswith('-'):\n        return False\n    \n    # Tags should not be too long (arbitrary limit of 50 characters)\n    if len(tag) > 50:\n        return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 107219, "num_retrieved": 406, "inference_time": 19.053937152028084, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    children_dict = {}\n    for child in node:\n        children_dict[child.tag] = child.text\n    return children_dict", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 102132, "num_retrieved": 378, "inference_time": 19.053937152028084, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any extra whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Check if the handles match exactly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles to lowercase for comparison\n    sender_lower = sender_handle.lower()\n    entity_lower = entity_handle.lower()\n    \n    # Check if the handles match (case-insensitive)\n    return sender_lower == entity_lower", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles to lowercase for comparison\n    sender_normalized = sender_handle.lower()\n    entity_normalized = entity_handle.lower()\n    \n    # Check if the handles match (case-insensitive)\n    return sender_normalized == entity_normalized", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Check if the sender and entity handles match\n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 111389, "num_retrieved": 442, "inference_time": 19.053937152028084, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    path = document_path or \"/.well-known/nodeinfo\"\n    \n    # Create the NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": url.split(\"//\")[1].split(\"/\")[0],\n            \"adminAccount\": f\"admin@{url.split('//')[1].split('/')[0]}\"\n        },\n        \"protocols\": [\n            \"diaspora\"\n        ],\n        \"services\": {\n            \"inbound\": [\n                {\"rel\": \"http://diaspora.github.io/diaspora_federation/federation/inbound\", \"href\": f\"{url}/inbound\"}\n            ],\n            \"outbound\": [\n                {\"rel\": \"http://diaspora.github.io/diaspora_federation/federation/outbound\", \"href\": f\"{url}/outbound\"}\n            ]\n        },\n        \"openRegistrations\": True,\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 100,\n                \"activeWeek\": 20\n            },\n            \"localPosts\": 5000,\n            \"localComments\": 2000\n        }\n    }\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    # Create the NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Node\",\n            \"adminAccount\": \"admin@domain.tld\"\n        },\n        \"protocols\": [\n            \"activitypub\",\n            \"diaspora\"\n        ],\n        \"openRegistrations\": True,\n        \"services\": {\n            \"inbound\": [\n                \"https://example.com/inbox\"\n            ],\n            \"outbound\": [\n                \"https://example.com/outbox\"\n            ]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 150,\n                \"activeWeek\": 30\n            },\n            \"localPosts\": 200,\n            \"localComments\": 50\n        }\n    }\n    \n    # Add custom path if specified\n    if document_path != \"/.well-known/nodeinfo\":\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    # Create the NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Node\",\n            \"adminAccount\": \"admin@domain.tld\"\n        },\n        \"protocols\": [\n            \"diaspora\"\n        ],\n        \"openRegistrations\": True,\n        \"services\": {\n            \"inbound\": [\n                \"https://example.com/receive/public\"\n            ],\n            \"outbound\": [\n                \"https://example.com/receive/users/{guid}\"\n            ]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 100,\n                \"activeWeek\": 20\n            },\n            \"localPosts\": 500,\n            \"localComments\": 200\n        }\n    }\n    \n    # Add custom document path if specified\n    if document_path != \"/.well-known/nodeinfo\":\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    # Base NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Node\",\n            \"adminAccount\": \"admin@domain.tld\"\n        },\n        \"protocols\": [\n            \"diaspora\"\n        ],\n        \"openRegistrations\": True,\n        \"services\": {\n            \"inbound\": [\n                \"https://example.com/receive/public\"\n            ],\n            \"outbound\": [\n                \"https://example.com/send/public\"\n            ]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 100,\n                \"activeWeek\": 20\n            },\n            \"localPosts\": 500,\n            \"localComments\": 100\n        }\n    }\n    \n    # Add custom document path if specified\n    if document_path != \"/.well-known/nodeinfo\":\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    if document_path is None:\n        document_path = \"/.well-known/nodeinfo\"\n    \n    # Create the NodeInfo document structure\n    nodeinfo_document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"0.2.2\"\n        },\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Node\",\n            \"adminAccount\": \"admin@domain.tld\"\n        },\n        \"protocols\": [\n            \"diaspora\"\n        ],\n        \"openRegistrations\": True,\n        \"services\": {\n            \"inbound\": [\n                \"https://example.com/receive/public\"\n            ],\n            \"outbound\": [\n                \"https://example.com/send/public\"\n            ]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 1000,\n                \"activeHalfyear\": 500,\n                \"activeMonth\": 100,\n                \"activeWeek\": 20\n            },\n            \"localPosts\": 500,\n            \"localComments\": 100\n        }\n    }\n    \n    # Add custom document path if specified\n    if document_path != \"/.well-known/nodeinfo\":\n        nodeinfo_document[\"path\"] = document_path\n    \n    return nodeinfo_document"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 74034, "num_retrieved": 236, "inference_time": 19.053937152028084, "generate_results": ["def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Construct the signature content as per Diaspora protocol\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create the hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Import the public key\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create the verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Construct the signature content: data + application/xml + base64url + RSA-SHA256\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create a hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Import the public key\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create a PKCS1 v1.5 verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Create the signature content string\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create the hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Import the public key\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create the verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Construct the signature content: data + application/xml + base64url + RSA-SHA256\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create a SHA256 hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Import the public key\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create a PKCS1 v1.5 verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    import base64\n    from Crypto.Hash import SHA256\n    from Crypto.PublicKey import RSA\n    from Crypto.Signature import PKCS1_v1_5\n    \n    # Extract the data from the XML document\n    data = doc.find(\".//{http://salmon-protocol.org/ns/magic-env}data\").text\n    \n    # Create the signature content string\n    sig_contents = '.'.join([\n        data,\n        base64.b64encode(b\"application/xml\").decode(\"ascii\"),\n        base64.b64encode(b\"base64url\").decode(\"ascii\"),\n        base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n    ])\n    \n    # Create the hash of the signature content\n    sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n    \n    # Create the public key object\n    public_key_obj = RSA.importKey(public_key)\n    \n    # Create the verifier\n    verifier = PKCS1_v1_5.new(public_key_obj)\n    \n    # Verify the signature\n    try:\n        verifier.verify(sig_hash, base64.b64decode(signature))\n        return True\n    except Exception:\n        return False"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 100693, "num_retrieved": 351, "inference_time": 19.053937152028084, "generate_results": ["def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    import json\n    from lxml import etree\n    from typing import Dict, Any\n    \n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        # Extract subject and aliases\n        subject = data.get(\"subject\")\n        aliases = data.get(\"aliases\", [])\n        \n        # Extract links\n        links = []\n        for link in data.get(\"links\", []):\n            links.append({\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            })\n            \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        root = etree.fromstring(document)\n        \n        # Extract subject\n        subject_elem = root.find(\".//{http://webfingerprint.org/ns/xrd}subject\")\n        subject = None\n        if subject_elem is not None:\n            subject = subject_elem.text\n        \n        # Extract aliases\n        aliases = []\n        alias_elems = root.findall(\".//{http://webfingerprint.org/ns/xrd}alias\")\n        for alias in alias_elems:\n            aliases.append(alias.text)\n            \n        # Extract links\n        links = []\n        link_elems = root.findall(\".//{http://webfingerprint.org/ns/xrd}link\")\n        for link in link_elems:\n            link_dict = {\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            }\n            links.append(link_dict)\n            \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except Exception:\n        # If parsing fails, return empty dict\n        pass\n    \n    # If both parsing attempts fail, return empty dict\n    return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Extract subject and aliases from JSON webfinger\n        subject = doc.get(\"subject\")\n        aliases = doc.get(\"aliases\", [])\n        \n        # Extract links from JSON webfinger\n        links = []\n        for link in doc.get(\"links\", []):\n            links.append({\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            })\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except json.JSONDecodeError:\n        # If JSON parsing fails, try parsing as XRD (old format)\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            for element in root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\"):\n                subject = element.text\n                break\n                \n            # Extract aliases from XRD\n            aliases = []\n            for element in root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\"):\n                aliases.append(element.text)\n                \n            # Extract links from XRD\n            links = []\n            for element in root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\"):\n                link = {\n                    \"rel\": element.get(\"rel\"),\n                    \"type\": element.get(\"type\"),\n                    \"href\": element.get(\"href\")\n                }\n                links.append(link)\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Extract subject and aliases from JSON webfinger\n        subject = doc.get(\"subject\")\n        aliases = doc.get(\"aliases\", [])\n        \n        # Extract links from JSON webfinger\n        links = []\n        for link in doc.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and type_ and href:\n                links.append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}subject\"):\n                subject = element.text\n                break\n                \n            # Extract aliases from XRD\n            aliases = []\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}alias\"):\n                aliases.append(element.text)\n                \n            # Extract links from XRD\n            links = []\n            for element in root.findall(\".//{http://webfingerprint.org/ns/xrd}link\"):\n                rel = element.get(\"rel\")\n                type_ = element.get(\"type\")\n                href = element.get(\"href\")\n                if rel and type_ and href:\n                    links.append({\n                        \"rel\": rel,\n                        \"type\": type_,\n                        \"href\": href\n                    })\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        # Try to parse as JSON first (new format)\n        data = json.loads(document)\n        \n        # Extract required fields from JSON\n        result = {}\n        \n        # Extract subject (acct:handle)\n        subject = data.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"handle\"] = subject.replace(\"acct:\", \"\")\n        \n        # Extract aliases\n        aliases = data.get(\"aliases\", [])\n        result[\"aliases\"] = []\n        for alias in aliases:\n            if isinstance(alias, str):\n                result[\"aliases\"].append(alias)\n        \n        # Extract links\n        links = data.get(\"links\", [])\n        result[\"links\"] = []\n        for link in links:\n            if isinstance(link, dict):\n                result[\"links\"].append({\n                    \"rel\": link.get(\"rel\"),\n                    \"type\": link.get(\"type\"),\n                    \"href\": link.get(\"href\")\n                })\n        \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            result = {}\n            \n            # Extract subject\n            subject_elem = root.find(\".//{http://webfingerprint.org/ns/xrd}subject\")\n            if subject_elem is not None:\n                subject = subject_elem.text\n                if subject and subject.startswith(\"acct:\"):\n                    result[\"handle\"] = subject.replace(\"acct:\", \"\")\n            \n            # Extract aliases\n            aliases = []\n            for alias in root.findall(\".//{http://webfingerprint.org/ns/xrd}alias\"):\n                aliases.append(alias.text)\n            result[\"aliases\"] = aliases\n            \n            # Extract links\n            links = []\n            for link in root.findall(\".//{http://webfingerprint.org/ns/xrd}link\"):\n                link_dict = {\n                    \"rel\": link.get(\"rel\"),\n                    \"type\": link.get(\"type\"),\n                    \"href\": link.get(\"href\")\n                }\n                links.append(link_dict)\n            result[\"links\"] = links\n            \n            return result\n            \n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Extract subject and aliases from JSON webfinger\n        subject = doc.get(\"subject\")\n        aliases = doc.get(\"aliases\", [])\n        \n        # Extract links from JSON webfinger\n        links = []\n        for link in doc.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and type_ and href:\n                links.append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            for element in root.findall(\".//{http://webfinger.net/rel/profile}subject\"):\n                subject = element.text\n                break\n                \n            # Extract aliases from XRD\n            aliases = []\n            for element in root.findall(\".//{http://webfinger.net/rel/profile}alias\"):\n                aliases.append(element.text)\n                \n            # Extract links from XRD\n            links = []\n            for link in root.findall(\".//{http://webfinger.net/rel/profile}link\"):\n                rel = link.get(\"rel\")\n                type_ = link.get(\"type\")\n                href = link.get(\"href\")\n                if rel and type_ and href:\n                    links.append({\n                        \"rel\": rel,\n                        \"type\": type_,\n                        \"href\": href\n                    })\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n        except Exception:\n            # If both parsing methods fail, return empty dict\n            return {}"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 105967, "num_retrieved": 377, "inference_time": 19.053937152028084, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path=f\".well-known/webfinger\", cache=False)\n        if document:\n            return document\n    except Exception:\n        pass\n    return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path='/.well-known/webfinger')\n        if document:\n            return document\n    except Exception:\n        pass\n    return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path=f\"/.well-known/webfinger\")\n        if document:\n            return document\n        # Try with the full handle as a query parameter\n        document, _, _ = fetch_document(host=handle.split(\"@\")[1], path=f\"/.well-known/webfinger?q={handle}\")\n        if document:\n            return document\n    except Exception:\n        pass\n    return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path=f\"/.well-known/webfinger\", cache=False)\n        if document:\n            return document\n    except Exception:\n        pass\n    return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        document = fetch_document(host=handle.split(\"@\")[1], path=f\".well-known/webfinger\", cache=False)\n        if document:\n            return document\n    except Exception:\n        pass\n    return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 99289, "num_retrieved": 350, "inference_time": 19.053937152028084, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:\" + handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"type\": \"application/activity+json\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host and handle parts\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:\" + handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"type\": \"application/activity+json\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"type\": \"application/activity+json\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:\" + handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"type\": \"application/activity+json\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"type\": \"text/html\",\n                \"href\": link[\"href\"]\n            })\n        elif", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Extract host and handle parts\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        logger.warning(\"retrieve_and_parse_diaspora_webfinger: invalid handle given: %s\", handle)\n        return None\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=f\"/.well-known/webfinger?resource=acct:{quote(handle)}\"\n    )\n    \n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document\n    webfinger = {\n        \"subject\": \"acct:\" + handle,\n        \"aliases\": [\n            f\"{host}/profile/{handle.split('@')[0]}\",\n            f\"{host}/u/{handle.split('@')[0]}\"\n        ],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/activity+json\"\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://microformats.org/profile/hcard\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://joindiaspora.com/seed_location\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"http://webfinger.net/rel/profile-page\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 77903, "num_retrieved": 312, "inference_time": 19.053937152028084, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n        \n    try:\n        # Parse the document as XRD\n        xrd = XRD()\n        xrd.from_xml(document)\n        return xrd\n    except Exception as e:\n        logger.warning(\"Failed to parse host-meta document from %s: %s\", host, e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    doc, status_code, error = fetch_document(host=host, path='/host-meta')\n    if not doc:\n        return None\n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    xrd = XRD()\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"lrdd\":\n            xrd.links.append(Link(\n                rel='lrdd',\n                type_='application/xrd+xml',\n                template=link.get(\"template\")\n            ))\n    return xrd", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as XRD\n        xrd = XRD()\n        xrd.from_xml(document)\n        return xrd\n    except Exception as ex:\n        logger.warning(\"Failed to parse host-meta document from %s: %s\", host, ex)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n        \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n        \n    # Create an XRD instance and populate it with the retrieved data\n    xrd = XRD()\n    \n    # Add links from the retrieved document\n    for link in doc.get('links', []):\n        xrd.links.append(Link(\n            rel=link.get('rel'),\n            type_=link.get('type'),\n            href=link.get('href')\n        ))\n        \n    # Add elements from the retrieved document\n    for element in doc.get('elements', []):\n        xrd.elements.append(Element(\n            name=element.get('name'),\n            href=element.get('href')\n        ))\n        \n    return xrd", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    doc, status_code, error = fetch_document(host=host, path='/host-meta')\n    if not doc:\n        return None\n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    xrd = XRD()\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"lrdd\":\n            xrd.links.append(Link(\n                rel=link[\"rel\"],\n                type_=link[\"type_\"],\n                template=link[\"template\"]\n            ))\n    return xrd"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 117249, "num_retrieved": 500, "inference_time": 19.053937152028084, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 104300, "num_retrieved": 352, "inference_time": 19.053937152028084, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual parts\n    parts = [key] + list(keys)\n    \n    for i, part in enumerate(parts):\n        if i == len(parts) - 1:\n            # Last part - assign the value\n            current[part] = val\n        else:\n            # Intermediate part - create nested dictionary if it doesn't exist\n            if part not in current:\n                current[part] = {}\n            current = current[part]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual parts\n    parts = [key] + list(keys)\n    \n    for i, part in enumerate(parts):\n        if i == len(parts) - 1:\n            # Last part - assign the value\n            current[part] = val\n        else:\n            # Intermediate part - create nested dictionary if it doesn't exist\n            if part not in current:\n                current[part] = {}\n            current = current[part]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual components\n    components = [key] + list(keys)\n    \n    for i, component in enumerate(components):\n        if i == len(components) - 1:\n            # Last component - set the value\n            current[component] = val\n        else:\n            # Intermediate component - create nested dictionary if it doesn't exist\n            if component not in current:\n                current[component] = {}\n            current = current[component]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into a list of keys\n    key_parts = [key] + list(keys)\n    \n    # Iterate through each key part\n    for i, k in enumerate(key_parts):\n        if i == len(key_parts) - 1:\n            # Last key - set the value\n            current[k] = val\n        else:\n            # Intermediate key - create the nested dictionary if it doesn't exist\n            if k not in current:\n                current[k] = {}\n            current = current[k]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual components\n    components = [key] + list(keys)\n    \n    for i, component in enumerate(components):\n        if i == len(components) - 1:\n            # Last component - set the value\n            current[component] = val\n        else:\n            # Intermediate component - create nested dictionary if needed\n            if component not in current:\n                current[component] = {}\n            current = current[component]"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 90550, "num_retrieved": 250, "inference_time": 19.053937152028084, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n    \n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n    \n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n    \n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n    \n    return ListOfFileNames(settings_dirs, spec_option)"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 85281, "num_retrieved": 275, "inference_time": 19.053937152028084, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 77354, "num_retrieved": 249, "inference_time": 19.053937152028084, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or initialized once\n    # Based on the context, it's likely initialized in the CoreServices class\n    # and returned via a class method\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # In a real implementation, this might return an instance of WorkspaceManager\n    # or a similar class that manages workspaces.\n    return WorkspaceManager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Initialize workspace manager with workspaces base directory\n    # The base directory is typically defined in the configuration\n    # For now, we'll return a placeholder instance\n    # In a real implementation, this would retrieve the actual workspace manager instance\n    # from a configuration or service registry\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # In a real implementation, this might return an instance of WorkspaceManager\n    # or a similar class that manages workspaces.\n    return WorkspaceManager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # Based on the context, it appears that the workspace manager is\n    # likely a singleton or a service that is initialized once and reused.\n    # The actual implementation would typically involve retrieving an\n    # instance from a service registry or configuration.\n    return WorkspaceManager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 81235, "num_retrieved": 268, "inference_time": 19.053937152028084, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton or instance that can be accessed\n    # through a class-level attribute or service registry\n    # Based on context, this likely returns an instance of InfraredPluginManager\n    # which is initialized in the core services\n    from infrared.core.services.plugins import InfraredPluginManager\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and dependencies\n    # In the context of the retrieved code, this would likely return\n    # an instance of InfraredPluginManager or a similar class\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and should be initialized\n    # based on the context provided in the retrieved context\n    # The context shows that CoreServices.plugins_manager() is used to get the plugin manager\n    # and that it's likely a class method that returns an instance of InfraredPluginManager\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and dependencies\n    # In the context of the retrieved code, this would likely return\n    # an instance of InfraredPluginManager or a similar class\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and dependencies\n    # In the context of the retrieved code, this would likely return\n    # an instance of InfraredPluginManager or a similar class\n    return InfraredPluginManager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 103247, "num_retrieved": 362, "inference_time": 19.053937152028084, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': {'type': dict, 'required': True},\n        'config': {'type': dict, 'required': False},\n        'plugin_type': {'type': str, 'required': True},\n        'description': {'type': str, 'required': True},\n        'plugin.spec': {'type': str, 'required': False},\n        'entry_point': {'type': str, 'required': False},\n        'roles_path': {'type': str, 'required': False},\n        'playbook': {'type': str, 'required': False},\n    }\n\n    # Validate required fields\n    for field, field_info in required_fields.items():\n        if field_info['required'] and field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n        \n        if field_info['required'] and field in spec_data:\n            if field_info['type'] == dict and not isinstance(spec_data[field], dict):\n                raise IRValidatorException(f\"Field '{field}' must be a dictionary\")\n            elif field_info['type'] == str and not isinstance(spec_data[field], str):\n                raise IRValidatorException(f\"Field '{field}' must be a string\")\n\n    # Validate subparsers structure\n    if 'subparsers' in spec_data and not isinstance(spec_data['subparsers'], dict):\n        raise IRValidatorException(\"Field 'subparsers' must be a dictionary\")\n\n    # Validate config structure\n    if 'config' in spec_data and not isinstance(spec_data['config'], dict):\n        raise IRValidatorException(\"Field 'config' must be a dictionary\")\n\n    # Validate plugin_type\n    if 'plugin_type' in spec_data and not isinstance(spec_data['plugin_type'], str):\n        raise IRValidatorException(\"Field '", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': dict,\n        'config': dict,\n        'plugin_type': str,\n        'description': str,\n        'plugin.spec': str,\n        'plugin.path': str,\n        'plugin.entry_point': str,\n        'plugin.playbook': str,\n        'plugin.roles_path': str,\n        'plugin.defaults_dir': str,\n        'plugin.vars_dir': str\n    }\n\n    # Validate required fields\n    for field, expected_type in required_fields.items():\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n        \n        if not isinstance(spec_data[field], expected_type):\n            raise IRValidatorException(f\"Field '{field}' must be of type {expected_type.__name__}, got {type(spec_data[field]).__name__}\")\n\n    # Validate subparsers structure\n    if not spec_data.get('subparsers'):\n        raise IRValidatorException(\"Missing 'subparsers' field in spec file\")\n    \n    # Validate config structure\n    config = spec_data.get('config', {})\n    if not config:\n        raise IRValidatorException(\"Missing 'config' field in spec file\")\n    \n    # Validate plugin_type\n    plugin_type = spec_data.get('plugin_type')\n    if not plugin_type:\n        raise IRValidatorException(\"Missing 'plugin_type' field in spec file\")\n    \n    # Validate description\n    description = spec_data.get('description')\n    if not description:\n        raise IRValidatorException(\"Missing 'description' field in spec file\")\n    \n    # Validate plugin.spec\n    plugin_spec = spec_data.get('plugin.spec')\n    if not plugin_spec:\n        raise IRValidatorException(\"Missing 'plugin.spec' field in spec file\")\n    \n    # Validate plugin.path\n    plugin_path = spec", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': {'type': dict, 'required': True},\n        'config': {'type': dict, 'required': False},\n        'plugin_type': {'type': str, 'required': True},\n        'description': {'type': str, 'required': True},\n        'plugin.spec': {'type': str, 'required': False}\n    }\n\n    # Validate required fields\n    for field, field_info in required_fields.items():\n        if field_info['required'] and field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate field types\n    for field, field_info in required_fields.items():\n        if field_info['required'] and field in spec_data:\n            if field_info['type'] == dict and not isinstance(spec_data[field], dict):\n                raise IRValidatorException(f\"Field '{field}' must be a dictionary\")\n            elif field_info['type'] == str and not isinstance(spec_data[field], str):\n                raise IRValidatorException(f\"Field '{field}' must be a string\")\n\n    # Validate subparsers has at least one parser\n    if spec_data.get('subparsers'):\n        if not isinstance(spec_data['subparsers'], dict):\n            raise IRValidatorException(\"subparsers must be a dictionary\")\n        if not spec_data['subparsers']:\n            raise IRValidatorException(\"subparsers must contain at least one parser\")\n\n    # Validate plugin_type\n    if not spec_data.get('plugin_type'):\n        raise IRValidatorException(\"plugin_type is required\")\n\n    # Validate description\n    if not spec_data.get('description'):\n        raise IRValidatorException(\"description is required\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML content: {str(e)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': dict,\n        'config': dict,\n        'plugin_type': str,\n        'description': str,\n        'plugin.spec': str,\n        'plugin.path': str,\n        'plugin.entry_point': str,\n        'plugin.playbook': str,\n        'plugin.roles_path': str,\n        'plugin.defaults_dir': str,\n        'plugin.vars_dir': str,\n    }\n\n    # Validate required fields\n    for field, expected_type in required_fields.items():\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n        \n        if not isinstance(spec_data[field], expected_type):\n            raise IRValidatorException(f\"Field '{field}' must be of type {expected_type.__name__}, got {type(spec_data[field]).__name__}\")\n\n    # Validate subparsers structure\n    if not isinstance(spec_data['subparsers'], dict):\n        raise IRValidatorException(\"subparsers must be a dictionary\")\n    \n    # Validate config structure\n    if not isinstance(spec_data['config'], dict):\n        raise IRValidatorException(\"config must be a dictionary\")\n    \n    # Validate plugin_type\n    if not isinstance(spec_data['plugin_type'], str):\n        raise IRValidatorException(\"plugin_type must be a string\")\n    \n    # Validate description\n    if not isinstance(spec_data['description'], str):\n        raise IRValidatorException(\"description must be a string\")\n    \n    # Validate plugin.spec\n    if not isinstance(spec_data['plugin.spec'], str):\n        raise IRValidatorException(\"plugin.spec must be a string\")\n    \n    # Validate plugin.path\n    if not isinstance(spec_data['plugin.path'], str):\n        raise IRValidatorException(\"plugin.path must be a string\")\n    \n    # Validate plugin.entry_point\n    if", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Invalid YAML content: {str(exc)}\")\n\n    # Define required fields and their expected types\n    required_fields = {\n        'subparsers': dict,\n        'config': dict,\n        'plugin_type': str,\n        'description': str,\n        'plugin.spec': str,\n        'plugin.path': str,\n        'plugin.entry_point': str,\n        'plugin.playbook': str,\n        'plugin.roles_path': str,\n        'plugin.defaults_dir': str,\n        'plugin.vars_dir': str,\n    }\n\n    # Validate required fields\n    for field, expected_type in required_fields.items():\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n        \n        if not isinstance(spec_data[field], expected_type):\n            raise IRValidatorException(f\"Field '{field}' must be of type {expected_type.__name__}, got {type(spec_data[field]).__name__}\")\n\n    # Validate subparsers structure\n    if not isinstance(spec_data['subparsers'], dict):\n        raise IRValidatorException(\"subparsers must be a dictionary\")\n    \n    # Validate config structure\n    if not isinstance(spec_data['config'], dict):\n        raise IRValidatorException(\"config must be a dictionary\")\n    \n    # Validate plugin_type\n    if not isinstance(spec_data['plugin_type'], str):\n        raise IRValidatorException(\"plugin_type must be a string\")\n    \n    # Validate description\n    if not isinstance(spec_data['description'], str):\n        raise IRValidatorException(\"description must be a string\")\n    \n    # Validate plugin.spec\n    if not isinstance(spec_data['plugin.spec'], str):\n        raise IRValidatorException(\"plugin.spec must be a string\")\n    \n    # Validate plugin.path\n    if not isinstance(spec_data['plugin.path'], str):\n        raise IRValidatorException(\"plugin.path must be a string\")\n    \n    # Validate plugin.entry_point\n    if"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 103909, "num_retrieved": 369, "inference_time": 19.053937152028084, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 96227, "num_retrieved": 292, "inference_time": 14.174695372581482, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Add the group to the parser dictionary\n        parser_dict[group['name']] = group.get('value', {})\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Add the group to the parser dictionary\n        parser_dict[group['name']] = group.get('value', {})\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Add the group to the parser dictionary\n        parser_dict[group['name']] = group.get('value', {})\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Get the group name and its configuration\n        group_name = group.get('name')\n        group_config = group.get('config', {})\n        \n        # Add the group to the parser dictionary\n        if group_name not in parser_dict:\n            parser_dict[group_name] = group_config\n        else:\n            # Merge the new group configuration with existing one\n            # using dict_merge from dict_utils\n            dict_utils.dict_merge(parser_dict[group_name], group_config)", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # Extract the include groups from the spec file\n    include_groups = self.spec_helper.spec_dict.get('include_groups', [])\n    \n    # Process each group in the include_groups list\n    for group in include_groups:\n        # Check if the group is already defined in the parser_dict\n        if group not in parser_dict:\n            # Add the group to the parser_dict\n            parser_dict[group] = {}\n    \n    # Return the updated parser_dict\n    return parser_dict"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 79560, "num_retrieved": 241, "inference_time": 14.174695372581482, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    for parser in self.iterate_parsers():\n        for spec_option in self._get_all_options_spec(parser):\n            option_name = spec_option['name']\n            option_spec = spec_option\n            \n            # Get default value from option spec\n            if option_spec.get('default') is not None:\n                defaults[option_name] = option_spec['default']\n                \n            # Get default value from parser options\n            if option_spec.get('action') == 'store_true':\n                defaults[option_name] = False\n                \n            # Handle nested options\n            if option_spec.get('nested'):\n                # Handle nested structure based on spec\n                pass\n                \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize defaults dictionary\n    defaults = {}\n    \n    # Get the main spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process the main parser and subparsers to extract default values\n    for parser_name, parser_dict in spec_dict.get('subparsers', {}).items():\n        # Extract default values from options in the parser\n        for group in parser_dict.get('groups', []):\n            for option_name, option_dict in group.get('options', {}).items():\n                # Set default value from option spec if available\n                if 'default' in option_dict:\n                    defaults[option_name] = option_dict['default']\n        \n        # Extract default values from flat options\n        for option_name, option_dict in parser_dict.get('options', {}).items():\n            if 'default' in option_dict:\n                defaults[option_name] = option_dict['default']\n    \n    # Process the main parser for defaults\n    for group in spec_dict.get('groups', []):\n        for option_name, option_dict in group.get('options', {}).items():\n            if 'default' in option_dict:\n                defaults[option_name] = option_dict['default']\n    \n    # Add defaults from the main parser options\n    for option_name, option_dict in spec_dict.get('options', {}).items():\n        if 'default' in option_dict:\n            defaults[option_name] = option_dict['default']\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize defaults dictionary\n    defaults = {}\n    \n    # Get the main spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process the main parser options\n    for parser in self.iterate_parsers():\n        parser_name = parser['name']\n        parser_options = parser.get('options', {})\n        \n        # Process options in the parser\n        for option_name, option_spec in parser_options.items():\n            # Get default value from option spec\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n                \n        # Process options in groups\n        for group in parser.get('groups', []):\n            group_options = group.get('options', {})\n            for option_name, option_spec in group_options.items():\n                default_value = option_spec.get('default')\n                if default_value is not None:\n                    defaults[option_name] = default_value\n    \n    # Process subparsers options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        subparser_options = subparser_dict.get('options', {})\n        for option_name, option_spec in subparser_options.items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n                \n        # Process group options in subparsers\n        for group in subparser_dict.get('groups', []):\n            group_options = group.get('options', {})\n            for option_name, option_spec in group_options.items():\n                default_value = option_spec.get('default')\n                if default_value is not None:\n                    defaults[option_name] = default_value\n    \n    # Add defaults from shared groups\n    shared_groups = spec_dict.get('shared_groups', [])\n    for group in shared_groups:\n        group_options = group.get('options', {})\n        for option_name, option_spec in group_options.items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    for parser in self.iterate_parsers():\n        for spec_option in self._get_all_options_spec(parser):\n            option_name = spec_option['name']\n            option_spec = spec_option\n            \n            # Check if option has a default value defined in spec\n            if 'default' in option_spec:\n                defaults[option_name] = option_spec['default']\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    defaults = {}\n    \n    # Get the spec dictionary\n    spec_dict = self.spec_dict\n    \n    # Process the main parser and subparsers\n    for parser_name, parser_dict in spec_dict.get('subparsers', {}).items():\n        # Get all option specs for this parser\n        option_specs = self._get_all_options_spec(parser_dict)\n        \n        # Process each option spec\n        for option_spec in option_specs:\n            option_name = option_spec['name']\n            option_type = option_spec.get('type', 'value')\n            \n            # Resolve the default value based on option type\n            if option_type == 'bool':\n                # For boolean options, default to False\n                defaults[option_name] = False\n            elif option_type == 'str':\n                # For string options, default to empty string\n                defaults[option_name] = ''\n            elif option_type == 'int':\n                # For integer options, default to 0\n                defaults[option_name] = 0\n            elif option_type == 'list':\n                # For list options, default to empty list\n                defaults[option_name] = []\n            elif option_type == 'dict':\n                # For dictionary options, default to empty dictionary\n                defaults[option_name] = {}\n            else:\n                # For other types, default to None\n                defaults[option_name] = None\n    \n    # Add any default values from the main parser\n    main_parser_dict = spec_dict.get('main', {})\n    if main_parser_dict:\n        for option_name, option_spec in main_parser_dict.get('options', {}).items():\n            option_type = option_spec.get('type', 'value')\n            if option_type == 'bool':\n                defaults[option_name] = False\n            elif option_type == 'str':\n                defaults[option_name] = ''\n            elif option_type == 'int':\n                defaults[option_name] = 0\n            elif option_type == 'list':\n                defaults[option_name] = []\n            elif option_type == 'dict':\n                defaults[option_name] = {}\n            else:\n                defaults[option_name] = None\n    \n    return defaults"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 78342, "num_retrieved": 230, "inference_time": 14.174695372581482, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 90135, "num_retrieved": 305, "inference_time": 14.174695372581482, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append((arg_name, arg_value, 'cli'))\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append((arg_name, arg_value, 'answer_file'))\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Deprecated arguments found:\")\n        for arg_name, arg_value, source in deprecated_args:\n            print(f\"  {source}: {arg_name}={arg_value}\")\n        return True\n    \n    return False", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_arg']:\n            deprecated_args.append((arg_name, arg_value, \"deprecated\"))\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_arg']:\n            deprecated_args.append((arg_name, arg_value, \"deprecated\"))\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"The following arguments are deprecated and will be removed in a future version:\")\n        for arg_name, arg_value, source in deprecated_args:\n            print(f\"  - {arg_name} ({source}): {arg_value}\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_arg']:\n            deprecated_args.append((arg_name, arg_value, 'cli'))\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_arg']:\n            deprecated_args.append((arg_name, arg_value, 'answer_file'))\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Deprecated arguments found:\")\n        for arg_name, arg_value, source in deprecated_args:\n            print(f\"  {source}: {arg_name}={arg_value}\")\n        return True\n    \n    return False", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_arg1', 'old_arg2', 'old_arg3']:  # Example deprecated arguments\n            deprecated_args.append((arg_name, arg_value))\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_arg1', 'old_arg2', 'old_arg3']:  # Example deprecated arguments\n            deprecated_args.append((arg_name, arg_value))\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Deprecated arguments found:\")\n        for arg_name, arg_value in deprecated_args:\n            print(f\"  Argument '{arg_name}' with value '{arg_value}' is deprecated.\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    deprecated_args = []\n    \n    # Check for deprecated arguments in cli_args\n    for arg_name, arg_value in cli_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(f\"--{arg_name}={arg_value}\")\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg_name, arg_value in answer_file_args.items():\n        if arg_name in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(f\"--{arg_name}={arg_value}\")\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"The following arguments are deprecated and will be removed in a future version:\")\n        for arg in deprecated_args:\n            print(f\"  {arg}\")\n        return True\n    \n    return False"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 103173, "num_retrieved": 341, "inference_time": 14.174695372581482, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 98501, "num_retrieved": 301, "inference_time": 14.174695372581482, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser in self.spec_helper.iterate_parsers():\n        if parser['name'] == command_name:\n            for option in parser.get('options', []):\n                if option['name'] == argument_name:\n                    return option\n    return None"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 90292, "num_retrieved": 304, "inference_time": 14.174695372581482, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 88563, "num_retrieved": 275, "inference_time": 14.174695372581482, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    result = {}\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n        else:\n            result[arg] = val\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))\n    return result"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 93401, "num_retrieved": 319, "inference_time": 14.174695372581482, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if any part of the condition is met\n        condition_met = False\n        \n        for part in condition_parts:\n            # Format: \"arg=value\"\n            if '=' not in part:\n                continue\n                \n            condition_arg, condition_value = part.split('=', 1)\n            \n            # Get the value of the condition argument from args\n            condition_value_from_args = args.get(command_name, {}).get(condition_arg)\n            \n            # Check if the value matches the required condition\n            if condition_value_from_args == condition_value:\n                condition_met = True\n                break\n                \n        # If condition is met, add the option name to required_args\n        if condition_met:\n            required_args.append(option_spec['name'])\n            \n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if any part of the condition is met\n        condition_met = False\n        \n        for part in condition_parts:\n            # Format: \"arg=value\"\n            if '=' in part:\n                condition_arg, condition_value = part.split('=', 1)\n                # Check if the argument exists in args and has the expected value\n                arg_value = args.get(command_name, {}).get(condition_arg)\n                if arg_value is not None and str(arg_value) == condition_value:\n                    condition_met = True\n                    break\n            else:\n                # Check if the argument exists in args\n                arg_value = args.get(command_name, {}).get(part)\n                if arg_value is not None:\n                    condition_met = True\n                    break\n        \n        # If condition is met, add the option name to matched_args\n        if condition_met:\n            matched_args.append(option_spec['name'])\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if any of the conditions are met\n        condition_met = False\n        \n        for part in condition_parts:\n            # Format: \"arg=value\" or \"arg\"\n            if '=' in part:\n                condition_arg, condition_value = part.split('=', 1)\n            else:\n                condition_arg = part\n                condition_value = None\n                \n            # Get the value of the condition argument from args\n            condition_value_from_args = args.get(command_name, {}).get(condition_arg)\n            \n            # Check if the condition is met\n            if condition_value is None:\n                # If no value is specified in required_when, check if the argument is present\n                if condition_value_from_args is not None:\n                    condition_met = True\n                    break\n            else:\n                # Check if the value matches\n                if str(condition_value_from_args) == condition_value:\n                    condition_met = True\n                    break\n                    \n        if condition_met:\n            required_args.append(option_spec['name'])\n            \n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    \n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if any part of the condition is met\n        condition_met = False\n        \n        for part in condition_parts:\n            # Format: \"arg=value\"\n            if '=' not in part:\n                continue\n                \n            condition_arg, condition_value = part.split('=', 1)\n            \n            # Get the value of the condition argument from args\n            condition_arg_value = args.get(command_name, {}).get(condition_arg)\n            \n            # Check if the value matches the condition\n            if condition_arg_value == condition_value:\n                condition_met = True\n                break\n                \n        if condition_met:\n            required_args.append(option_spec['name'])\n            \n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    required_args = []\n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if required_when:\n            # Extract the condition from required_when\n            # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n            condition_parts = required_when.split(',')\n            condition_matched = False\n            \n            # Check if any part of the condition is met\n            for part in condition_parts:\n                part = part.strip()\n                if '=' in part:\n                    condition_key, condition_value = part.split('=', 1)\n                else:\n                    condition_key = part\n                    condition_value = None\n                \n                # Get the value from args for the condition key\n                condition_value_from_args = args.get(command_name, {}).get(condition_key)\n                \n                # Check if the condition is met\n                if condition_value is None:\n                    # If no value is specified in condition, check if key exists\n                    if condition_value_from_args is not None:\n                        condition_matched = True\n                        break\n                else:\n                    # Check if the value matches\n                    if condition_value_from_args == condition_value:\n                        condition_matched = True\n                        break\n            \n            # If condition is matched, add the option name to required_args\n            if condition_matched:\n                required_args.append(option_spec['name'])\n    \n    return required_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 91657, "num_retrieved": 298, "inference_time": 14.174695372581482, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str):\n                # Check if the value exceeds the specified length\n                # This is a placeholder - actual length validation would depend on context\n                if len(arg_value) > 100:  # Example length limit\n                    raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds length limit of 100 characters\")", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str):\n                # Check if the value exceeds the specified length\n                # This is a placeholder - actual length validation would depend on context\n                if len(arg_value) > 100:  # Example length limit\n                    raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds length limit of 100 characters\")\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str):\n                # Check if the value exceeds the specified length\n                # This is a placeholder - actual length validation would depend on context\n                if len(arg_value) > 100:  # Example length limit\n                    raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds length limit of 100 characters\")\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str):\n                # Check if the value exceeds the specified length\n                # This is a placeholder - actual length validation would depend on context\n                if len(arg_value) > 100:  # Example length limit\n                    raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds length limit of 100 characters\")", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            if isinstance(arg_value, str):\n                # Check if the value exceeds the specified length\n                # This is a placeholder - actual length validation would depend on context\n                # For example, if there were a length specification in the option spec\n                if len(arg_value) > 100:  # Example length limit\n                    raise ValueError(f\"Argument '{arg_name}' in command '{command_name}' exceeds length limit of 100 characters\")"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 91212, "num_retrieved": 294, "inference_time": 14.174695372581482, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        if 'choices' in option_spec:\n            choices = option_spec['choices']\n            if option_name in args:\n                value = args[option_name]\n                if value not in choices:\n                    raise exceptions.IRInvalidChoiceException([\n                        (option_name, value, choices)\n                    ])", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        choices = option_spec.get('choices', [])\n        if not choices:\n            continue\n            \n        # Get the value for this option from args\n        if option_name in args:\n            value = args[option_name]\n            if value not in choices:\n                raise exceptions.IRInvalidChoiceException([\n                    (option_name, value, choices)\n                ])", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        if option_name in args:\n            choices = option_spec.get('choices', [])\n            if choices and args[option_name] not in choices:\n                raise exceptions.IRInvalidChoiceException([\n                    (option_name, args[option_name], choices)\n                ])", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        if 'choices' in option_spec:\n            option_name = option_spec['name']\n            if option_name in args:\n                option_value = args[option_name]\n                if option_value not in option_spec['choices']:\n                    raise exceptions.IRInvalidChoiceException(\n                        [(option_name, option_value, option_spec['choices'])])", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs()\n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        if option_name in args:\n            choices = option_spec.get('choices', [])\n            if choices and args[option_name] not in choices:\n                raise exceptions.IRInvalidChoiceException([\n                    (option_name, args[option_name], choices)\n                ])"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 91281, "num_retrieved": 295, "inference_time": 14.174695372581482, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Check if the option has min and max values defined\n            min_value = option_spec.get('min_value')\n            max_value = option_spec.get('max_value')\n            \n            if min_value is not None and max_value is not None:\n                # Convert option value to int if it's a number\n                try:\n                    value = int(option_value)\n                except (ValueError, TypeError):\n                    continue\n                    \n                # Check if value is within the range\n                if value < min_value or value > max_value:\n                    raise exceptions.IRInvalidMinMaxRangeException([\n                        (command_name, option_name, min_value, max_value, value)\n                    ])", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Check if the option has min and max values defined\n            min_value = option_spec.get('min_value')\n            max_value = option_spec.get('max_value')\n            \n            if min_value is not None and max_value is not None:\n                # Convert option value to int if it's a number\n                try:\n                    value = int(option_value)\n                except (ValueError, TypeError):\n                    continue\n                    \n                # Check if value is within the range\n                if value < min_value or value > max_value:\n                    raise exceptions.IRInvalidMinMaxRangeException([\n                        (command_name, option_name, min_value, max_value, value)\n                    ])", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Extract min and max values from option specification\n            min_value = option_spec.get('min_value')\n            max_value = option_spec.get('max_value')\n            \n            # Skip validation if min or max is not specified\n            if min_value is None and max_value is None:\n                continue\n                \n            # Convert option value to integer if it's a number\n            try:\n                value = int(option_value)\n            except (ValueError, TypeError):\n                continue\n                \n            # Check if value is within the specified range\n            if min_value is not None and value < min_value:\n                raise exceptions.IRInvalidMinMaxRangeException([\n                    (command_name, option_name, min_value, value)\n                ])\n                \n            if max_value is not None and value > max_value:\n                raise exceptions.IRInvalidMinMaxRangeException([\n                    (command_name, option_name, max_value, value)\n                ])", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Extract min and max values from option specification\n            min_value = option_spec.get('min', None)\n            max_value = option_spec.get('max', None)\n            \n            # Skip if no min/max values are defined\n            if min_value is None and max_value is None:\n                continue\n                \n            # Convert option value to integer if it's a number\n            try:\n                value = int(option_value)\n            except (ValueError, TypeError):\n                continue\n                \n            # Check if value is within the specified range\n            if min_value is not None and value < min_value:\n                raise exceptions.IRInvalidMinMaxRangeException([\n                    (command_name, option_name, min_value, value)\n                ])\n                \n            if max_value is not None and value > max_value:\n                raise exceptions.IRInvalidMinMaxRangeException([\n                    (command_name, option_name, max_value, value)\n                ])", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification\n            option_spec = self.get_option_spec(command_name, option_name)\n            \n            if not option_spec:\n                continue\n                \n            # Check if the option has min and max values defined\n            min_value = option_spec.get('min_value')\n            max_value = option_spec.get('max_value')\n            \n            if min_value is not None and max_value is not None:\n                # Convert option value to int if it's a number\n                try:\n                    value = int(option_value)\n                except (ValueError, TypeError):\n                    continue\n                    \n                # Check if value is within the range\n                if value < min_value or value > max_value:\n                    raise exceptions.IRInvalidMinMaxRangeException([\n                        (command_name, option_name, min_value, max_value, value)\n                    ])"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 92770, "num_retrieved": 314, "inference_time": 14.174695372581482, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 96389, "num_retrieved": 347, "inference_time": 14.174695372581482, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Process all arguments\n    for arg_name, arg_value in args.items():\n        # Check if the argument starts with a control prefix\n        if arg_name.startswith('control_') or arg_name.startswith('extra-vars'):\n            # Handle control arguments\n            if arg_name.startswith('control_'):\n                control_args[arg_name[len('control_'):]] = arg_value\n            elif arg_name == 'extra-vars':\n                # Handle extra-vars as custom arguments\n                custom_args[arg_value] = None\n        else:\n            # Handle nested arguments\n            # Check if the argument is a custom variable (starts with 'custom_')\n            if arg_name.startswith('custom_'):\n                custom_args[arg_name[len('custom_'):]] = arg_value\n            else:\n                # Default to nested arguments\n                nested_args[arg_name] = arg_value\n    \n    return control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process arguments to separate control and nested\n    for arg_name, arg_value in args.items():\n        # Check if the argument is a control argument (typically starts with 'control_' or is a known control option)\n        # Control arguments are typically those that control IR behavior and are not passed to Ansible playbooks\n        if arg_name.startswith('control_') or arg_name in ['debug', 'verbose', 'dry-run', 'output', 'extra-vars', 'ansible-args']:\n            control_args[arg_name] = arg_value\n        else:\n            # For all other arguments, treat them as nested arguments\n            # These will be passed to Ansible playbooks and stored in the spec yml file\n            nested_args[arg_name] = arg_value\n    \n    # Handle custom arguments (custom ansible variables) which are typically passed as key=value pairs\n    # These are not part of the nested structure but are used as custom variables\n    # In the context of infrared, custom arguments are typically passed as extra-vars\n    # We'll check if any argument is explicitly marked as custom or if it's in a custom format\n    # Since the context doesn't specify a clear marker, we'll assume that any argument not in control or nested is custom\n    # However, based on the typical infrared behavior, custom arguments are usually passed as extra-vars\n    # We'll add any argument that is not in control or nested to custom_args\n    custom_args = {}\n    for arg_name, arg_value in args.items():\n        if arg_name not in control_args and arg_name not in nested_args:\n            custom_args[arg_name] = arg_value\n    \n    return control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through all arguments\n    for arg_name, arg_value in args.items():\n        # Check if the argument starts with a control prefix (commonly '--')\n        # and if it's a known control argument\n        if arg_name.startswith('--'):\n            # Check if it's a control argument (typically those that control IR behavior)\n            # These are arguments that are not nested and are used to control IR behavior\n            # Examples: --debug, --verbose, --dry-run, --output, etc.\n            if arg_name in ['--debug', '--verbose', '--dry-run', '--output', '--extra-vars', '--inventory', '--ansible-args']:\n                control_args[arg_name] = arg_value\n            else:\n                # For other arguments, assume they are nested arguments\n                # These will be processed and put into the nested_args dict\n                # We need to determine if they are nested or custom\n                # Custom arguments are typically those that are not part of the nested structure\n                # and are used as custom ansible variables\n                if arg_name.startswith('--additional-args'):\n                    # Handle additional args which are passed as key=value pairs\n                    if isinstance(arg_value, str):\n                        # Parse the additional args string\n                        additional_args = arg_value.split(';')\n                        for additional_arg in additional_args:\n                            additional_arg = additional_arg.strip()\n                            if '=' in additional_arg:\n                                key, value = additional_arg.split('=', 1)\n                                custom_args[key] = value\n                    else:\n                        custom_args[arg_name] = arg_value\n                else:\n                    # For all other arguments, treat them as nested arguments\n                    # These will be processed and put into the nested_args dict\n                    # We need to split the key if it contains dots (nested keys)\n                    if '.' in arg_name:", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Process arguments to separate control, nested, and custom\n    for arg_name, arg_value in args.items():\n        # Check if argument starts with 'extra-vars' or contains 'extra-vars'\n        if arg_name.startswith('extra-vars') or 'extra-vars' in arg_name:\n            # Handle custom ansible variables\n            custom_args[arg_name] = arg_value\n        elif arg_name.startswith('debug') or arg_name.startswith('verbose'):\n            # Control arguments that affect IR behavior\n            control_args[arg_name] = arg_value\n        elif arg_name.startswith('output') or arg_name.startswith('dry-run'):\n            # Control arguments that affect IR behavior\n            control_args[arg_name] = arg_value\n        else:\n            # Default to nested arguments (used by Ansible playbooks)\n            nested_args[arg_name] = arg_value\n\n    return control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    custom_args = {}\n\n    # Iterate through all arguments\n    for arg_name, arg_value in args.items():\n        # Check if the argument starts with a control prefix (common control args)\n        # These are typically flags or options that control IR behavior\n        if arg_name.startswith('debug') or arg_name.startswith('verbose') or \\\n           arg_name.startswith('dry-run') or arg_name.startswith('output') or \\\n           arg_name.startswith('extra-vars') or arg_name.startswith('ansible-args'):\n            control_args[arg_name] = arg_value\n        elif arg_name.startswith('additional-args'):\n            # Handle additional-args which are passed as key=value pairs\n            # These are typically passed to Ansible\n            additional_args = arg_value.split(';')\n            for additional_arg in additional_args:\n                if '=' in additional_arg:\n                    key, value = additional_arg.split('=', 1)\n                    nested_args[key] = value\n                else:\n                    nested_args[arg_name] = arg_value\n        else:\n            # Default to nested arguments (used by Ansible playbooks)\n            # These will be put into the spec yml file\n            nested_args[arg_name] = arg_value\n\n    return control_args, nested_args"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 94443, "num_retrieved": 285, "inference_time": 14.174695372581482, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder for future implementation\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder or error condition\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder for future expansion\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder or error case\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder or error case\n            pass\n    \n    return vars_dict"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 104356, "num_retrieved": 360, "inference_time": 14.174695372581482, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register plugins_dir path otherwise roles introduced by the plugin are not found during the plugin execution\n    plugin_roles_path = ir_plugin.roles_path\n    if plugin_roles_path:\n        role_path = os.path.join(ir_plugin.path, plugin_roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore the original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register plugins_dir path otherwise roles introduced by the plugin\n    # are not found during the plugin execution\n    plugin_roles_path = ir_plugin.roles_path\n    if plugin_roles_path:\n        role_path = os.path.join(ir_plugin.path, plugin_roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\",\n                        role_path)\n        # roles path points to the dir which contains installed plugins\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register plugins_dir path otherwise roles introduced by the plugin\n    # are not found during the plugin execution\n    plugin_roles_path = ir_plugin.roles_path\n    if plugin_roles_path:\n        role_path = os.path.join(ir_plugin.path, plugin_roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\",\n                        role_path)\n        # roles path points to the dir which contains installed plugins\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n    \n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register plugins_dir path otherwise roles introduced by the plugin\n    # are not found during the plugin execution\n    plugin_roles_path = ir_plugin.roles_path\n    if plugin_roles_path:\n        role_path = os.path.join(ir_plugin.path, plugin_roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\",\n                        role_path)\n        # roles path points to the dir which contains installed plugins\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n    \"\"\"\n    # Save the current ANSIBLE_ROLES_PATH so that it can be restored later\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Register the plugin's roles path if provided\n    plugin_roles_path = ir_plugin.roles_path\n    if plugin_roles_path:\n        role_path = os.path.join(ir_plugin.path, plugin_roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        \n        # Roles path points to the dir which contains installed plugins\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible-playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore the original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 105546, "num_retrieved": 371, "inference_time": 13.231294870376587, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook execution parameters\n    playbook_path = ir_plugin.playbook\n    verbose = None\n    extra_vars = vars_dict\n    ansible_args = cli_args\n    \n    # Execute the Ansible playbook with the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 92205, "num_retrieved": 311, "inference_time": 13.231294870376587, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for non-CLI arguments\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the spec\n    return values_dict"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 106267, "num_retrieved": 372, "inference_time": 13.231294870376587, "generate_results": ["def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create a path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Load the plugin spec file\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    # Create a flat dictionary from the spec content\n    # Flatten the nested structure by removing the 'subparsers' key and flattening the rest\n    flat_dict = {}\n    \n    # Extract the plugin type\n    plugin_type = spec_content.get('config', {}).get('plugin_type', spec_content.get('plugin_type'))\n    flat_dict['plugin_type'] = plugin_type\n    \n    # Extract the entry point\n    entry_point = spec_content.get('config', {}).get('entry_point', spec_content.get('entry_point'))\n    flat_dict['entry_point'] = entry_point\n    \n    # Extract the roles path\n    roles_path = spec_content.get('config', {}).get('roles_path', spec_content.get('roles_path'))\n    flat_dict['roles_path'] = roles_path\n    \n    # Extract the description\n    description = spec_content.get('description')\n    flat_dict['description'] = description\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create a path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Load the plugin spec file\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    # Create a flat dictionary from the spec content\n    # Flatten the nested structure by removing the subparsers key\n    flat_dict = {}\n    \n    # Extract the plugin type\n    plugin_type = spec_content.get('config', {}).get('plugin_type', spec_content.get('plugin_type'))\n    flat_dict['plugin_type'] = plugin_type\n    \n    # Extract the entry point\n    entry_point = spec_content.get('config', {}).get('entry_point', spec_content.get('entry_point'))\n    flat_dict['entry_point'] = entry_point\n    \n    # Extract the roles path\n    roles_path = spec_content.get('config', {}).get('roles_path', spec_content.get('roles_path'))\n    flat_dict['roles_path'] = roles_path\n    \n    # Extract the description\n    description = spec_content.get('description')\n    flat_dict['description'] = description\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create plugin instance\n    plugin = InfraredPlugin(plugin_dir)\n    \n    # Create a flat dictionary from the plugin's spec\n    spec_dict = {}\n    \n    # Add plugin type\n    spec_dict['plugin_type'] = plugin.type()\n    \n    # Add entry point\n    spec_dict['entry_point'] = plugin.entry_point()\n    \n    # Add roles path\n    spec_dict['roles_path'] = plugin.roles_path()\n    \n    # Add description\n    spec_dict['description'] = plugin.description()\n    \n    # Add config section if available\n    config = {}\n    try:\n        config['plugin_type'] = plugin.type()\n        config['entry_point'] = plugin.entry_point()\n        config['roles_path'] = plugin.roles_path()\n    except Exception:\n        pass\n    \n    # Add config to spec dict\n    if config:\n        spec_dict['config'] = config\n    \n    return spec_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create a path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Load the plugin spec file\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    # Create a flat dictionary from the spec content\n    # Flatten the nested structure by iterating through the spec\n    flat_dict = {}\n    \n    # Extract the plugin type\n    plugin_type = spec_content.get('config', {}).get('plugin_type', spec_content.get('plugin_type'))\n    flat_dict['plugin_type'] = plugin_type\n    \n    # Extract the entry point\n    entry_point = spec_content.get('config', {}).get('entry_point', spec_content.get('entry_point'))\n    flat_dict['entry_point'] = entry_point\n    \n    # Extract the roles path\n    roles_path = spec_content.get('config', {}).get('roles_path', spec_content.get('roles_path'))\n    flat_dict['roles_path'] = roles_path\n    \n    # Extract the description\n    description = spec_content.get('description')\n    flat_dict['description'] = description\n    \n    # Extract the subparsers (if any)\n    subparsers = spec_content.get('subparsers', {})\n    flat_dict['subparsers'] = subparsers\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    # Create a path to the plugin spec file\n    spec_file_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    # Load the plugin spec file\n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    # Create a flat dictionary from the spec content\n    # Flatten the nested structure by converting the dictionary to a flat structure\n    # using the keys as the keys in the flat dictionary\n    flat_dict = {}\n    \n    # Process the spec content to create a flat dictionary\n    for key, value in spec_content.items():\n        if isinstance(value, dict):\n            # If the value is a dictionary, recursively flatten it\n            for sub_key, sub_value in value.items():\n                flat_dict[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            # If the value is not a dictionary, add it directly to the flat dictionary\n            flat_dict[key] = value\n    \n    return flat_dict"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 103033, "num_retrieved": 340, "inference_time": 13.231294870376587, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG', '')\n    if not config_path:\n        # Use the common/ directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        # Use the common/ directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        # Use the common directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        # Use the common/ directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        # Use the common directory path from infrared core\n        version_info = version.VersionInfo('infrared')\n        common_path = pkg.resource_filename(version_info.package, 'common')\n        config_path = os.path.join(common_path, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 86437, "num_retrieved": 272, "inference_time": 13.231294870376587, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Implementation based on context\n    # This function is part of the SpecObject abstract base class\n    # It extends the CLI by adding options from the spec to the subparser\n    # The actual implementation would involve parsing the spec and adding\n    # appropriate argparse arguments to the subparser\n    \n    # Since the specific implementation details are not provided in the context,\n    # this is a placeholder that follows the expected pattern\n    # In a real implementation, this would parse the spec and add arguments\n    # to the subparser based on the spec definition\n    \n    # The function is expected to be implemented in the specific spec class\n    # that inherits from SpecObject\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This implementation is based on the context provided\n    # The actual implementation would depend on the specific spec\n    # and how it defines its CLI options\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This implementation is based on the context provided\n    # The actual implementation would depend on the specific spec\n    # and how it defines its CLI options\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a subparser for the plugin\n    plugin_subparser = root_subparsers.add_parser(\n        self.name,\n        help=self.specification.get_help(),\n        description=self.specification.get_description()\n    )\n    \n    # Add all the options from the plugin specification\n    self.specification.add_options(plugin_subparser)\n    \n    # Add any shared groups defined in the plugin specification\n    if hasattr(self.specification, 'shared_groups'):\n        for group in self.specification.shared_groups:\n            plugin_subparser.add_argument_group(group['title'])\n            for opt_name, opt_dict in group.get('options', {}).items():\n                plugin_subparser.add_argument(\n                    opt_name,\n                    **opt_dict\n                )", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This implementation is based on the context provided\n    # The actual implementation would depend on the specific spec\n    # and how it defines its CLI options\n    pass"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 131072, "actual_tokens": 49368, "num_retrieved": 127, "inference_time": 13.231294870376587, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")"]}
