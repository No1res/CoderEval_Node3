{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4284, "num_retrieved": 11, "inference_time": 0.9184942245483398, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 <= nanoseconds < 86400000000000)\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"nanoseconds must be int\")\n    if not (0 <= nanoseconds < 86400000000000):\n        raise ValueError(\"nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time.from_ticks(nanoseconds, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 <= nanoseconds < 86400000000000)\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"nanoseconds must be int\")\n    if not (0 <= nanoseconds < 86400000000000):\n        raise ValueError(\"nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time.from_ticks(nanoseconds, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 <= nanoseconds < 86400000000000)\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"nanoseconds must be int\")\n    if not (0 <= nanoseconds < 86400000000000):\n        raise ValueError(\"nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time.from_ticks(nanoseconds, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 <= nanoseconds < 86400000000000)\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"nanoseconds must be int\")\n    if not (0 <= nanoseconds < 86400000000000):\n        raise ValueError(\"nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time.from_ticks(nanoseconds, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 <= nanoseconds < 86400000000000)\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"nanoseconds must be int\")\n    if not (0 <= nanoseconds < 86400000000000):\n        raise ValueError(\"nanoseconds out of range (0..86400000000000)\")\n    \n    second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n    minute, second = divmod(second, 60)\n    hour, minute = divmod(minute, 60)\n    \n    return Time.from_ticks(nanoseconds, tz)"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4356, "num_retrieved": 13, "inference_time": 0.9184942245483398, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\"Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    if value is None:\n        return None\n    seconds = value.total_seconds()\n    if not (-86400 < seconds < 86400):\n        raise ValueError(\"utcoffset must be less than a day\")\n    if seconds % 60 != 0 or value.microseconds != 0:\n        raise ValueError(\"utcoffset must be a whole number of minutes\")\n    return int(seconds // 60)", "def dehydrate_timedelta(value):\n    \"\"\"Dehydrator for `timedelta` values.\n\n    :param value: The timedelta value to dehydrate.\n    :type value: timedelta\n    :return: A value of type understood by packstream.\n    \"\"\"\n    if value is None:\n        return None\n    if value.days != 0:\n        raise ValueError(\"utcoffset must be less than a day\")\n    if value.seconds % 60 != 0 or value.microseconds != 0:\n        raise ValueError(\"utcoffset must be a whole number of minutes\")\n    return value.total_seconds()", "def dehydrate_timedelta(value):\n    \"\"\"Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    if value is None:\n        return None\n    seconds = value.total_seconds()\n    if seconds == 0:\n        return Structure(b\"0\", 0)\n    # Convert to days, seconds, and nanoseconds\n    days = int(seconds // 86400)\n    remaining_seconds = int(seconds % 86400)\n    nanoseconds = int((seconds - int(seconds)) * 1000000000)\n    return Structure(b\"d\", days, remaining_seconds, nanoseconds)", "def dehydrate_timedelta(value):\n    \"\"\"Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    if value is None:\n        return None\n    seconds = value.total_seconds()\n    if not (-86400 < seconds < 86400):\n        raise ValueError(\"utcoffset must be less than a day\")\n    if seconds % 60 != 0 or value.microseconds != 0:\n        raise ValueError(\"utcoffset must be a whole number of minutes\")\n    return int(seconds // 60)", "def dehydrate_timedelta(value):\n    \"\"\"Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    if value is None:\n        return None\n    seconds = value.total_seconds()\n    if not (-86400 < seconds < 86400):\n        raise ValueError(\"utcoffset must be less than a day\")\n    if seconds % 60 != 0 or value.microseconds != 0:\n        raise ValueError(\"utcoffset must be a whole number of minutes\")\n    return int(seconds // 60)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4411, "num_retrieved": 15, "inference_time": 0.9184942245483398, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for time values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    hour, minute, second, nanosecond, tzinfo = value\n    return Structure(b\"t\", hour, minute, second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for time values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    hour, minute, second, nanosecond, tzinfo = value\n    return Structure(b\"t\", hour, minute, second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for time values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    hour, minute, second, nanosecond, tzinfo = value\n    return Structure(b\"t\", hour, minute, second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for time values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    hour, minute, second, nanosecond, tzinfo = value\n    return Structure(b\"t\", hour, minute, second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for time values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    hour, minute, second, nanosecond, tzinfo = value\n    return Structure(b\"t\", hour, minute, second, nanosecond)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4440, "num_retrieved": 15, "inference_time": 0.9184942245483398, "generate_results": ["def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"X\", value[0], value[1])", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"X\", value[0], value[1])", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"X\", value[0], value[1])", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"X\", value[0], value[1])", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    return Structure(b\"X\", value[0], value[1])"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4356, "num_retrieved": 15, "inference_time": 0.9184942245483398, "generate_results": ["def keys(self):\n    \"\"\"Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return self.keys", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\"\"\"\n    return self.keys", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\"\"\"\n    return self.keys", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return self.keys", "def keys(self):\n    \"\"\"Return the keys of the record.\n\n:return: list of key names\"\"\"\n    return self.keys"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4227, "num_retrieved": 11, "inference_time": 0.9184942245483398, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Bolt 3.0 and above\n    handlers[Version(3, 0)] = Bolt4x0\n    handlers[Version(3, 1)] = Bolt4x1\n    handlers[Version(3, 2)] = Bolt4x2\n    handlers[Version(3, 3)] = Bolt4x3\n    handlers[Version(3, 4)] = Bolt4x4\n\n    # Bolt 4.0 and above\n    handlers[Version(4, 0)] = AsyncBolt4x0\n    handlers[Version(4, 1)] = AsyncBolt4x1\n    handlers[Version(4, 2)] = AsyncBolt4x2\n    handlers[Version(4, 3)] = AsyncBolt4x3\n    handlers[Version(4, 4)] = AsyncBolt4x4\n\n    if protocol_version is not None:\n        return {k: v for k, v in handlers.items() if k == protocol_version}\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Bolt 3.0 and above\n    handlers[Version(3, 0)] = Bolt4x0\n    handlers[Version(3, 1)] = Bolt4x1\n    handlers[Version(3, 2)] = Bolt4x2\n    handlers[Version(3, 3)] = Bolt4x3\n    handlers[Version(3, 4)] = Bolt4x4\n\n    # Bolt 4.0 and above\n    handlers[Version(4, 0)] = AsyncBolt4x0\n    handlers[Version(4, 1)] = AsyncBolt4x1\n    handlers[Version(4, 2)] = AsyncBolt4x2\n    handlers[Version(4, 3)] = AsyncBolt4x3\n    handlers[Version(4, 4)] = AsyncBolt4x4\n\n    if protocol_version is not None:\n        return {k: v for k, v in handlers.items() if k == protocol_version}\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Bolt 3.0\n    handlers[Version(3, 0)] = Bolt4x0\n\n    # Bolt 3.1 - not explicitly defined in context\n    # Bolt 3.5 - not explicitly defined in context\n    # Bolt 4.0 - not explicitly defined in context\n    # Bolt 4.1\n    handlers[Version(4, 1)] = Bolt4x1\n    # Bolt 4.2\n    handlers[Version(4, 2)] = Bolt4x2\n    # Bolt 4.4\n    handlers[Version(4, 4)] = Bolt4x4\n\n    if protocol_version is None:\n        return handlers\n\n    if protocol_version in handlers:\n        return {protocol_version: handlers[protocol_version]}\n\n    return {}", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Bolt 3.0 and above\n    handlers[Version(3, 0)] = Bolt4x0\n    handlers[Version(3, 1)] = Bolt4x1\n    handlers[Version(3, 2)] = Bolt4x2\n    handlers[Version(3, 3)] = Bolt4x3\n    handlers[Version(3, 4)] = Bolt4x4\n\n    # Bolt 4.0 and above\n    handlers[Version(4, 0)] = AsyncBolt4x0\n    handlers[Version(4, 1)] = AsyncBolt4x1\n    handlers[Version(4, 2)] = AsyncBolt4x2\n    handlers[Version(4, 3)] = AsyncBolt4x3\n    handlers[Version(4, 4)] = AsyncBolt4x4\n\n    if protocol_version is not None:\n        return {k: v for k, v in handlers.items() if k == protocol_version}\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple\")\n\n    handlers = {}\n\n    # Bolt 3.0\n    handlers[Version(3, 0)] = Bolt4x0\n\n    # Bolt 3.1\n    handlers[Version(3, 1)] = Bolt4x1\n\n    # Bolt 3.5\n    handlers[Version(3, 5)] = Bolt4x5\n\n    # Bolt 4.0\n    handlers[Version(4, 0)] = AsyncBolt4x0\n\n    # Bolt 4.1\n    handlers[Version(4, 1)] = AsyncBolt4x1\n\n    # Bolt 4.2\n    handlers[Version(4, 2)] = AsyncBolt4x2\n\n    # Bolt 4.4\n    handlers[Version(4, 4)] = AsyncBolt4x4\n\n    if protocol_version is not None:\n        return {k: v for k, v in handlers.items() if k == protocol_version}\n\n    return handlers"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4118, "num_retrieved": 6, "inference_time": 0.9184942245483398, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4316, "num_retrieved": 12, "inference_time": 0.9184942245483398, "generate_results": ["def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        raise KeyError(f\"Key '{key}' not found in keys\")", "def index(self, key):\n    \"\"\"Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4176, "num_retrieved": 8, "inference_time": 0.9184942245483398, "generate_results": ["def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4242, "num_retrieved": 10, "inference_time": 0.9184942245483398, "generate_results": ["def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    record_dict = {}\n    for key in keys:\n        if isinstance(key, int):\n            try:\n                record_dict[key] = self.value(key)\n            except IndexError:\n                raise IndexError(f\"Index {key} is out of bounds\")\n        else:\n            record_dict[key] = self.value(key)\n    \n    # Fill in missing keys with None\n    record_keys = self.keys\n    for key in record_keys:\n        if key not in record_dict:\n            record_dict[key] = None\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_keys = self.keys\n    record_values = self.values()\n    \n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            result[record_keys[key]] = record_values[key]\n        else:\n            # key is a string (field name)\n            if key not in record_keys:\n                result[key] = None\n            else:\n                result[key] = record_values[record_keys.index(key)]\n    \n    # If no keys are provided, return all values\n    if not keys:\n        return dict(zip(record_keys, record_values))\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self._values)\n    \n    record_values = self.values(*keys)\n    record_keys = self.keys()\n    \n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"index {key} is out of bounds\")\n            result[record_keys[key]] = record_values[key]\n        else:\n            # key is a string (field name)\n            try:\n                index = record_keys.index(key)\n                result[key] = record_values[index]\n            except ValueError:\n                # key not found in record keys\n                result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self._values)\n    \n    record_values = self.values(*keys)\n    record_keys = self.keys()\n    \n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"index {key} is out of bounds\")\n            result[record_keys[key]] = record_values[key]\n        else:\n            # key is a string (field name)\n            try:\n                index = record_keys.index(key)\n                result[key] = record_values[index]\n            except ValueError:\n                # key not found in record, insert with None\n                result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self._values)\n    \n    record_values = self.values(*keys)\n    record_keys = self.keys()\n    \n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"index {key} is out of bounds\")\n            result[record_keys[key]] = record_values[key]\n        else:\n            # key is a string (field name)\n            if key not in record_keys:\n                result[key] = None\n            else:\n                idx = record_keys.index(key)\n                result[key] = record_values[idx]\n    \n    return result"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4418, "num_retrieved": 16, "inference_time": 0.9184942245483398, "generate_results": ["def pop_u16(self):\n    \"\"\" Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    value = self.data[-2] << 8 | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\" Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    b = self.data[-2:]\n    return (b[0] << 8) | b[1]", "def pop_u16(self):\n    \"\"\" Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    return (self.data[-2] << 8) | self.data[-1]", "def pop_u16(self):\n    \"\"\" Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\n    \"\"\"\n    if len(self) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    return (self[-2] << 8) | self[-1]", "def pop_u16(self):\n    \"\"\" Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    bytes_data = self.data[-2:]\n    return (bytes_data[0] << 8) | bytes_data[1]"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4363, "num_retrieved": 14, "inference_time": 0.9184942245483398, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation would append a DISCARD message to the output queue\n    # with the specified parameters\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation would append a DISCARD message to the output queue\n    # with the specified parameters\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation would append a DISCARD message to the output queue\n    # with the specified parameters\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation follows the pattern of other message methods in the driver\n    # Appends a DISCARD message to the output queue with specified parameters\n    # This is a placeholder implementation based on the context and expected behavior\n    pass", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    # Implementation would append a DISCARD message to the output queue\n    # with the specified parameters\n    pass"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4239, "num_retrieved": 10, "inference_time": 0.9184942245483398, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation follows the pattern of other Bolt message methods\n    # in the driver, which typically send the message and return a Response\n    return self._send_message(\"BEGIN\", mode=mode, bookmarks=bookmarks, \n                             metadata=metadata, timeout=timeout, \n                             db=db, imp_user=imp_user, \n                             dehydration_hooks=dehydration_hooks, \n                             hydration_hooks=hydration_hooks, **handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation follows the pattern of other Bolt message methods\n    # in the driver, such as run, commit, rollback, etc.\n    \n    # Since the actual implementation details are not provided in the context,\n    # this is a placeholder that matches the expected signature and behavior.\n    # In a real implementation, this would send a BEGIN message to the server\n    # and return a Response object with the given handlers.\n    \n    # Construct the response with the provided handlers\n    response = self._create_response(handlers)\n    \n    # Append the BEGIN message to the output queue with the specified parameters\n    # This would involve sending the message over the network and handling the response\n    # The actual network communication and message formatting would be handled by the driver\n    \n    return response", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation follows the pattern of other Bolt message methods\n    # in the driver, which typically send the message and return a Response\n    return self._send_message(\"BEGIN\", mode=mode, bookmarks=bookmarks, \n                             metadata=metadata, timeout=timeout, \n                             db=db, imp_user=imp_user, \n                             dehydration_hooks=dehydration_hooks, \n                             hydration_hooks=hydration_hooks, **handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation follows the pattern of other Bolt message methods\n    # in the driver, which typically send the message and return a Response\n    return self._send_message(\"BEGIN\", mode=mode, bookmarks=bookmarks, \n                             metadata=metadata, timeout=timeout, \n                             db=db, imp_user=imp_user, \n                             dehydration_hooks=dehydration_hooks, \n                             hydration_hooks=hydration_hooks, **handlers)", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    # This function would append a BEGIN message to the output queue\n    # and return a Response object with the provided handlers\n    # Implementation follows the pattern of other Bolt message methods\n    # in the driver, which typically use internal message construction\n    # and return a Response object with handlers attached\n    \n    # Since the actual implementation details are not provided in the context,\n    # this is a placeholder that matches the expected signature and behavior\n    # of the begin method based on the pattern of other methods in the driver\n    \n    # In a real implementation, this would construct a BEGIN message with\n    # the provided parameters and send it to the server\n    # Then return a Response object with the handlers\n    \n    # For now, we return a Response object with the handlers\n    # The actual response construction would be handled by the driver internals\n    return self._send_message(\"BEGIN\", mode=mode, bookmarks=bookmarks, \n                             metadata=metadata, timeout=timeout, db=db, \n                             imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n                             hydration_hooks=hydration_hooks, **handlers)"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4449, "num_retrieved": 17, "inference_time": 0.9184942245483398, "generate_results": ["def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    if n >= 0:\n        return int(n + (0.5 if n - int(n) >= 0.5 else -0.5))\n    else:\n        return int(n - (0.5 if n - int(n) <= -0.5 else 0.5))", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    if n >= 0:\n        return int(n + (0.5 if n - int(n) >= 0.5 else -0.5))\n    else:\n        return int(n - (0.5 if n - int(n) <= -0.5 else 0.5))", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    if n == int(n):\n        return int(n)\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    # If fractional part is exactly 0.5, round to the nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    if n >= 0:\n        return int(n + (0.5 if n - int(n) >= 0.5 else -0.5))\n    else:\n        return int(n - (0.5 if n - int(n) <= -0.5 else 0.5))", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with .5 rounding to the nearest even integer.\"\"\"\n    if n >= 0:\n        return int(n + (0.5 if n - int(n) >= 0.5 else -0.5))\n    else:\n        return int(n - (0.5 if n - int(n) <= -0.5 else 0.5))"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4319, "num_retrieved": 13, "inference_time": 0.9184942245483398, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base class for the new Point subclass\n    base_class = Point\n    \n    # Create a new class dynamically\n    class PointSubclass(base_class):\n        def __new__(cls, *args, **kwargs):\n            # Ensure the correct number of coordinates are provided\n            if len(args) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates, {len(args)} provided\")\n            return super().__new__(cls)\n        \n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # Store the field names for this point type\n            self._fields = fields\n            \n        @property\n        def srid(self):\n            return self._srid\n        \n        @srid.setter\n        def srid(self, value):\n            self._srid = value\n            # Update srid_map if needed\n            if value in srid_map:\n                srid_table[value] = (self.__class__, len(fields))\n    \n    # Register the new class in srid_map\n    for srid, (point_class, dim) in srid_map.items():\n        if point_class == PointSubclass:\n            srid_map[srid] = (PointSubclass, len(fields))\n    \n    # Set the class name and return it\n    PointSubclass.__name__ = name\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(type):\n        def __new__(cls, name, bases, namespace, *args, **kwargs):\n            # Create the new class\n            new_class = super().__new__(cls, name, bases, namespace)\n            \n            # Add the fields attribute\n            new_class.fields = fields\n            \n            # Add the srid_map attribute\n            new_class.srid_map = srid_map\n            \n            # Add a method to hydrate the point from raw data\n            def hydrate_point(srid, *coordinates):\n                try:\n                    point_class, dim = srid_map[srid]\n                except KeyError:\n                    point = Point(coordinates)\n                    point.srid = srid\n                    return point\n                else:\n                    if len(coordinates) != dim:\n                        raise ValueError(\"SRID %d requires %d coordinates (%d provided)\" % (srid, dim, len(coordinates)))\n                    return point_class(coordinates)\n            \n            new_class.hydrate_point = hydrate_point\n            \n            return new_class\n    \n    # Create the class using the metaclass\n    return PointSubclass(name, (Point,), {\"__doc__\": f\"Point subclass for {name}\"})", "def point_type(name, fields, srid_map):\n    \"\"\" Dynamically create a Point subclass. \"\"\"\n    # Define the base class for the new Point subclass\n    base_class = Point\n    \n    # Create a new class dynamically\n    class PointSubclass(base_class):\n        def __new__(cls, *args, **kwargs):\n            # Ensure the correct number of coordinates are provided\n            if len(args) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates\")\n            return super().__new__(cls)\n        \n        def __init__(self, *args, **kwargs):\n            super().__init__(args)\n            # Store the name and fields for metadata\n            self._name = name\n            self._fields = fields\n    \n    # Register the new class in srid_map if needed\n    if srid_map:\n        # Assign SRID mapping based on fields\n        if len(fields) == 2:\n            srid_map[4326] = (PointSubclass, 2)\n        elif len(fields) == 3:\n            srid_map[4978] = (PointSubclass, 3)\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base class for the new Point subclass\n    base_class = Point\n    \n    # Create a new class dynamically\n    class PointSubclass(base_class):\n        def __new__(cls, *args, **kwargs):\n            # Ensure the correct number of coordinates are provided\n            if len(args) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates\")\n            return super().__new__(cls)\n        \n        def __init__(self, *args, **kwargs):\n            super().__init__(args)\n            # Store the name and fields for identification\n            self.name = name\n            self.fields = fields\n    \n    # Register the new class with srid_map for SRID lookup\n    srid_map[name] = (PointSubclass, len(fields))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the base class for the new Point subclass\n    base_class = Point\n    \n    # Create a new class dynamically\n    class PointSubclass(base_class):\n        def __new__(cls, *args, **kwargs):\n            # Ensure the correct number of coordinates is provided\n            if len(args) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates\")\n            return super().__new__(cls)\n        \n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            # Store the field names for this point type\n            self._fields = fields\n            \n        @property\n        def srid(self):\n            return self._srid\n        \n        @srid.setter\n        def srid(self, value):\n            self._srid = value\n            # Update the srid_map if needed\n            if value in srid_map:\n                srid_table[value] = (self.__class__, len(fields))\n    \n    # Register the new point type in the srid_map\n    for srid, (point_class, dim) in srid_map.items():\n        if point_class == PointSubclass:\n            srid_map[srid] = (PointSubclass, len(fields))\n    \n    # Set the class name and return it\n    PointSubclass.__name__ = name\n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4426, "num_retrieved": 17, "inference_time": 0.9184942245483398, "generate_results": ["def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, category=FutureWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4508, "num_retrieved": 20, "inference_time": 0.9895247966051102, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set inline in the R code to ensure they take effect\n    # when R starts\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set inline within the R code execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be configured via environment variables\n    # These options are set inline in the R code to ensure they take effect\n    # when R starts\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be configured via environment variables\n    # These options are set inline within the R code execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set inline within the R code execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\""]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4566, "num_retrieved": 22, "inference_time": 0.9895247966051102, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual number of jobs based on target_concurrency\n    jobs = target_concurrency\n    \n    # If no files are provided, return success with empty output\n    if not varargs:\n        return 0, b''\n    \n    # Create the command with file arguments\n    # This is a simplified version - in practice, xargs would spawn processes\n    # and collect results, but here we simulate the behavior\n    try:\n        # Use subprocess to run the command with each file argument\n        # This is a simplified implementation that assumes we can run the command\n        # with each file as an argument in parallel\n        import subprocess\n        import os\n        \n        # If color is enabled, we might want to use a pty\n        if color and os.name != 'nt':\n            # On Unix-like systems, use a pty if supported\n            pass\n            \n        # Run the command with each file argument\n        # This is a simplified version - in reality, xargs would use a pool\n        # of processes to run the command in parallel\n        results = []\n        for file_arg in varargs:\n            # Create a new command with the file argument\n            new_cmd = list(cmd)\n            new_cmd.extend(['--file', file_arg])\n            \n            # Run the command\n            result = subprocess.run(new_cmd, capture_output=True, text=True)\n            results.append(result)\n        \n        # Determine the exit code - take the maximum of all exit codes\n        exit_code = max(r.returncode for r in results) if results else 0\n        \n        # Combine all outputs\n        output = b''.join(r.stdout.encode() for r in results)\n        output += b''.join(r.stderr.encode() for r in results)\n        \n        return exit_code, output\n        \n    except Exception as e:\n        # Return error code and error message\n        return 1, str(e).encode()", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Determine the number of jobs based on target_concurrency\n    jobs = target_concurrency\n    \n    # If we're on Windows and using a shell that doesn't support xargs, \n    # we need to handle it differently\n    if sys.platform == 'win32':\n        # On Windows, we can't use xargs directly, so we use a shell loop\n        # to process files one by one\n        result = []\n        for file_arg in varargs:\n            # Run the command with the current file argument\n            try:\n                proc = subprocess.run(\n                    cmd,\n                    input=file_arg,\n                    text=True,\n                    capture_output=True,\n                    timeout=300,\n                    shell=True,\n                    check=False,\n                    env=os.environ.copy()\n                )\n                result.append(proc)\n            except subprocess.TimeoutExpired:\n                result.append(subprocess.CompletedProcess(\n                    args=cmd, returncode=1, stdout='', stderr='Timeout'\n                ))\n        # Aggregate results\n        return_code = 0\n        for proc in result:\n            if proc.returncode != 0:\n                return_code = proc.returncode\n                break\n        return (return_code, b''.join([proc.stdout.encode() if proc.stdout else b'' for proc in result]))\n    \n    # For Unix-like systems, use xargs directly\n    # We need to ensure the command is properly formatted\n    # and that we handle the file arguments correctly\n    \n    # Create the xargs command with the appropriate number of jobs\n    xargs_cmd = ['xargs', '-P', str(jobs)]\n    \n    # If color is enabled, we need to pass the color option\n    if color:\n        xargs_cmd.extend(['--color=always'])\n    \n    # Combine the command with the file arguments\n    full_cmd = xargs_cmd + list(cmd)\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            full_cmd", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Determine the number of jobs based on target_concurrency\n    jobs = target_concurrency\n    \n    # If we're on Windows and using a shell that doesn't support xargs,\n    # we need to handle it differently\n    if sys.platform == 'win32':\n        # On Windows, we can't use xargs directly, so we use a simple loop\n        # with a single job to avoid issues with shell interpretation\n        jobs = 1\n    \n    # If no files are provided, return success with empty output\n    if not varargs:\n        return 0, b''\n    \n    # Build the command with the files as arguments\n    # We use a shell to handle the file arguments properly\n    if jobs == 1:\n        # Single job - run the command directly\n        process = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n',\n            capture_output=True,\n            text=False,\n            shell=True if sys.platform == 'win32' else False,\n            **kwargs\n        )\n        return process.returncode, process.stdout\n    else:\n        # Multiple jobs - use xargs to distribute the work\n        # We need to ensure the command is properly formatted\n        # and that we handle the file arguments correctly\n        \n        # On platforms that support pty, enable color if requested\n        if color and sys.platform not in ('win32', 'cygwin'):\n            # Use a pty to enable color output\n            pass\n        \n        # Use subprocess to run xargs with the specified concurrency\n        # This is a simplified version - in practice, xargs would be invoked\n        # with the appropriate number of jobs\n        process = subprocess.run(\n            ['xargs', '-P', str(jobs)] + list(cmd),\n            input='\\n'.join(varargs) + '\\n',\n            capture_output=True,\n            text=False,\n            **kwargs", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    import subprocess\n    import os\n    \n    # Determine the number of jobs based on target_concurrency\n    jobs = target_concurrency\n    \n    # If we're on a platform that supports pty and color is enabled, use it\n    if color and os.name != 'nt':  # Avoid pty on Windows\n        # Use pty for color output on supported platforms\n        pass\n    \n    # Build the command with file arguments\n    full_cmd = list(cmd)\n    if varargs:\n        # Join file arguments with spaces and pass as arguments\n        full_cmd.extend(varargs)\n    \n    # Execute the command with the specified concurrency\n    try:\n        result = subprocess.run(\n            full_cmd,\n            capture_output=True,\n            text=False,\n            check=False,\n            **kwargs\n        )\n        return result.returncode, result.stdout\n    except Exception as e:\n        return 1, e.__class__.__name__.encode('utf-8')", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # Determine the actual number of jobs based on target_concurrency\n    jobs = target_concurrency\n    \n    # If no files are provided, return success with empty output\n    if not varargs:\n        return 0, b''\n    \n    # Create the command with file arguments\n    # This is a simplified version - in practice, this would spawn multiple processes\n    # and collect results, but here we just return a dummy implementation\n    # For actual implementation, this would use subprocess and multiprocessing\n    \n    # In a real implementation, we would:\n    # 1. Split varargs into chunks based on jobs\n    # 2. Run each chunk in parallel\n    # 3. Collect results and return the final exit code and output\n    \n    # Since we don't have access to the full implementation details,\n    # we return a placeholder that mimics the expected behavior\n    \n    # For now, we'll just run the command once with all files\n    # This is a simplified version that doesn't actually parallelize\n    \n    # Use subprocess to run the command with the files\n    import subprocess\n    \n    # Build the command with file arguments\n    full_cmd = list(cmd) + varargs\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_cmd,\n            capture_output=True,\n            text=False,\n            timeout=300,  # 5 minute timeout\n            env=os.environ.copy(),\n            **kwargs\n        )\n        return result.returncode, result.stdout\n    except subprocess.TimeoutExpired:\n        return 1, b'timeout'\n    except Exception as e:\n        return 1, str(e).encode()"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4497, "num_retrieved": 19, "inference_time": 0.9895247966051102, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result\n    # for the same input sequence\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle result each time\n    random.seed(0)\n    return random.sample(seq, len(seq))"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4630, "num_retrieved": 24, "inference_time": 0.9895247966051102, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if s == '0':\n        return (0,)\n    \n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros to make all versions the same length\n    # This is a simple approach to ensure consistent comparison\n    max_parts = 3\n    while len(version_tuple) < max_parts:\n        version_tuple += (0,)\n    \n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if s == '0':\n        return (0,)\n    \n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros to ensure consistent length (e.g., '1' becomes (1, 0, 0, ...))\n    # This is a simple implementation that assumes version strings are in the format x.y.z\n    # and compares them lexicographically by splitting on dots\n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if s == '0':\n        return (0,)\n    \n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros to make all versions the same length\n    # This is a simple heuristic for comparison\n    max_parts = 3\n    while len(version_tuple) < max_parts:\n        version_tuple += (0,)\n    \n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if s == '0':\n        return (0,)\n    \n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    \n    # Pad with zeros to ensure consistent length (e.g., '1' becomes (1, 0, 0, ...))\n    # This is a simple implementation that assumes version strings are in the format 'x.y.z'\n    # and compares them lexicographically by splitting on '.'\n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if s == '0':\n        return (0,)\n    \n    parts = s.split('.')\n    result = []\n    for part in parts:\n        # Handle version parts that might contain pre-release tags (like alpha, beta)\n        # We only take the numeric part\n        numeric_part = ''\n        for char in part:\n            if char.isdigit():\n                numeric_part += char\n            elif char.isalpha():\n                # If we encounter a letter, it's part of a pre-release tag\n                # We skip it since we only want numeric version numbers\n                break\n        if numeric_part:\n            result.append(int(numeric_part))\n        else:\n            # If no digits found, treat as 0\n            result.append(0)\n    \n    # Pad with zeros to ensure consistent length\n    while len(result) < 3:\n        result.append(0)\n    \n    return tuple(result)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4296, "num_retrieved": 13, "inference_time": 0.9895247966051102, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure the command is properly normalized\n        # to handle deep-path shebangs and avoid issues with shebang parsing\n        # This is a workaround for Windows not supporting shebangs natively\n        # and for issue8557 where Python fails to parse certain paths\n        cmd = tuple(os.path.normpath(os.path.abspath(path)) for path in cmd)\n    \n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure the command is properly normalized\n        # to handle shebangs and deep-path executables\n        # This is a simplified version that ensures the command is properly\n        # resolved and handles the case where shebangs might not be parsed\n        # correctly on Windows\n        return tuple(\n            os.path.normpath(os.path.abspath(os.path.expanduser(arg))) \n            if arg.startswith('#!') else arg \n            for arg in cmd\n        )\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure the command is properly normalized\n        # to handle deep-path shebangs and avoid issues with shebang parsing\n        # This is a workaround for Windows not parsing shebangs properly\n        # and for issue https://bugs.python.org/issue8557\n        \n        # If the first element is a shebang line, we need to handle it specially\n        # For Windows, we convert the command to a proper executable path\n        # by ensuring it's fully qualified and can be executed\n        \n        # Normalize the command by ensuring it's a full path if needed\n        # and that it's properly formatted for Windows\n        first_cmd = cmd[0]\n        \n        # If the command starts with a shebang, we need to handle it\n        if first_cmd.startswith('#!'):\n            # Extract the executable path from the shebang\n            # and replace the shebang with the actual executable\n            # This handles deep-path shebangs properly\n            executable_path = first_cmd[2:].strip()\n            # Replace the first element with the executable path\n            new_cmd = (executable_path,) + cmd[1:]\n            return new_cmd\n        \n        # For regular commands, ensure they are properly formatted\n        # and that they can be executed on Windows\n        return cmd\n    \n    # On non-Windows platforms, just return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure the command is properly formatted\n        # to handle shebangs and deep-path executables\n        # The shebang is not parsed by Python on Windows, so we need to\n        # normalize the command to ensure it's executable\n        # We also need to handle the case where the command is a deep path\n        # by ensuring it's properly resolved\n        \n        # If the first element is a shebang, we need to handle it specially\n        # For Windows, we convert the shebang to a proper executable path\n        if cmd and cmd[0].startswith('#!'):\n            # Extract the executable path from the shebang\n            # For example, '#!/usr/bin/env python' -> 'python'\n            # We need to find the actual executable path\n            # This is handled by parse_shebang.find_executable\n            # So we just normalize the command to use the executable\n            # found by the shebang parser\n            pass\n        \n        # Normalize the command to ensure it's a valid path\n        # This is done by parse_shebang.normalize_cmd\n        # which handles both shebangs and deep paths\n        return parse_shebang.normalize_cmd(cmd)\n    \n    # On non-Windows systems, just use the standard normalization\n    return parse_shebang.normalize_cmd(cmd)", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure the command is properly normalized\n        # to handle deep-path shebangs and avoid issues with shebang parsing\n        # The shebang is not parsed on Windows, so we need to ensure the\n        # command is properly formatted for Windows execution\n        # We also need to handle the case where the command is a path\n        # that might not be directly executable\n        \n        # Normalize the command to ensure it's in a format that Windows can handle\n        # This includes ensuring that paths are properly quoted and that\n        # the command is not split incorrectly\n        \n        # If the first element is a path, we need to ensure it's properly\n        # formatted for Windows\n        if isinstance(cmd[0], str) and os.path.exists(cmd[0]):\n            # Ensure the command is properly formatted for Windows\n            # by ensuring it's a full path and not relative\n            return tuple([os.path.abspath(cmd[0])] + list(cmd[1:]))\n        \n        # If the command is a shebang, we need to ensure it's properly\n        # formatted for Windows\n        # Since Windows doesn't parse shebangs, we just return the command\n        # as is, but ensure it's properly formatted\n        return cmd\n    \n    # On non-Windows systems, we can use the standard normalization\n    # which handles deep-path shebangs properly\n    return cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4431, "num_retrieved": 18, "inference_time": 0.9895247966051102, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        cache_instance.key = key\n        cache_instance.lock = lock\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_obj = key(*args, **kwargs)\n            try:\n                return cache_instance[key_obj]\n            except KeyError:\n                result = func(*args, **kwargs)\n                cache_instance[key_obj] = result\n                return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        cache_instance.key = key\n        cache_instance.lock = lock\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_args = args\n            key_kwargs = kwargs\n            if key is not hashkey:\n                key_args = key(*args, **kwargs)\n            else:\n                key_args = hashkey(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set the wrapper's __name__ and __doc__ to match the original function\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        cache_instance = cache_instance if cache_instance is not None else _UnboundCache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_val = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            if key_val in cache_instance:\n                return cache_instance[key_val]\n            \n            # If not in cache, compute the value and store it\n            result = func(*args, **kwargs)\n            cache_instance[key_val] = result\n            return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        cache_instance = cache_instance if cache_instance is not None else _UnboundCache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_value = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            if key_value in cache_instance:\n                return cache_instance[key_value]\n            \n            # If not in cache, compute the value and store it\n            result = func(*args, **kwargs)\n            cache_instance[key_value] = result\n            return result\n        \n        # Set the __name__ and __doc__ attributes to preserve original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        cache_instance.key = key\n        cache_instance.lock = lock\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_val = key(*args, **kwargs)\n            try:\n                return cache_instance[key_val]\n            except KeyError:\n                # If key not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_val] = result\n                return result\n        \n        # Set the __name__ and __doc__ attributes to preserve original function metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4295, "num_retrieved": 13, "inference_time": 0.9895247966051102, "generate_results": ["import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use the cache to store results with TTL\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(type(x) for x in args) + tuple(type(v) for v in kwargs.values())\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use typed=False to avoid type-based key hashing\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(key)\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use typed=True to ensure the cache key is properly typed\n            # This is a simplified implementation assuming the cache handles typing\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(key)\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        return wrapper\n    \n    return decorator", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use typed=False to avoid type-based key hashing\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(key)\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Use typed=True to ensure type stability in key hashing\n            key = args + tuple(sorted(kwargs.items()))\n            if typed:\n                key = tuple(x if isinstance(x, (int, float, str, bool)) else hash(x) for x in key)\n            \n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            \n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            \n            return result\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4271, "num_retrieved": 14, "inference_time": 0.9895247966051102, "generate_results": ["def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    if typed:\n        key_func = typedkey\n    else:\n        key_func = hashkey\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    if typed:\n        key_func = typedkey\n    else:\n        key_func = hashkey\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    cache = MRUCache(maxsize=maxsize)\n    \n    if typed:\n        key_func = typedkey\n    else:\n        key_func = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    if typed:\n        key_func = typedkey\n    else:\n        key_func = hashkey\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    if typed:\n        key_func = typedkey\n    else:\n        key_func = hashkey\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4351, "num_retrieved": 15, "inference_time": 0.9895247966051102, "generate_results": ["def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4352, "num_retrieved": 15, "inference_time": 0.9895247966051102, "generate_results": ["def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize)\n        \n        if typed:\n            key_func = typedkey\n        else:\n            key_func = hashkey\n            \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize)\n        \n        if typed:\n            key_func = typedkey\n        else:\n            key_func = hashkey\n            \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize)\n        if typed:\n            key_func = typedkey\n        else:\n            key_func = hashkey\n            \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize)\n        \n        if typed:\n            key_func = typedkey\n        else:\n            key_func = hashkey\n            \n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import LFUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize)\n        \n        if typed:\n            key_func = typedkey\n        else:\n            key_func = hashkey\n            \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4380, "num_retrieved": 16, "inference_time": 0.9895247966051102, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self.__data))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self.__data))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4322, "num_retrieved": 14, "inference_time": 0.9895247966051102, "generate_results": ["def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4321, "num_retrieved": 14, "inference_time": 0.9895247966051102, "generate_results": ["def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4349, "num_retrieved": 15, "inference_time": 0.9895247966051102, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        # Store the original method\n        original_method = method\n        \n        def wrapper(self, *args, **kwargs):\n            # Create the cache key\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    return cache[k]\n                except KeyError:\n                    pass\n            # Call the original method and store the result\n            result = original_method(self, *args, **kwargs)\n            with lock:\n                try:\n                    cache.setdefault(k, result)\n                except ValueError:\n                    # Value too large\n                    pass\n            return result\n        \n        # Add cache info and clear methods\n        wrapper.cache_info = lambda: cache.cache_info() if hasattr(cache, 'cache_info') else None\n        wrapper.cache_clear = lambda: cache.cache_clear() if hasattr(cache, 'cache_clear') else None\n        wrapper.cache_parameters = lambda: cache.cache_parameters() if hasattr(cache, 'cache_parameters') else {}\n        \n        # Update wrapper with original method metadata\n        functools.update_wrapper(wrapper, original_method)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        cache_key = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the cache based on the instance and arguments\n            k = cache_key(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            v = method(self, *args, **kwargs)\n            # in case of a race, prefer the item already in the cache\n            try:\n                with lock:\n                    cache[k] = v\n            except ValueError:\n                # value too large\n                pass\n            return v\n        \n        # Add cache_info and cache_clear methods if available\n        if hasattr(cache, 'cache_info'):\n            wrapper.cache_info = cache.cache_info\n        if hasattr(cache, 'cache_clear'):\n            wrapper.cache_clear = cache.cache_clear\n            \n        # Update wrapper with method metadata\n        functools.update_wrapper(wrapper, method)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        key = key if callable(key) else hashkey\n        \n        def wrapper(self, *args, **kwargs):\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            v = method(self, *args, **kwargs)\n            with lock:\n                try:\n                    cache.setdefault(k, v)\n                except ValueError:\n                    return v  # value too large\n            return v\n        \n        # Copy method metadata\n        wrapper.__name__ = method.__name__\n        wrapper.__doc__ = method.__doc__\n        wrapper.__module__ = method.__module__\n        wrapper.__qualname__ = method.__qualname__\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        key = key if callable(key) else hashkey\n        \n        def wrapper(self, *args, **kwargs):\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            v = method(self, *args, **kwargs)\n            with lock:\n                try:\n                    cache.setdefault(k, v)\n                except ValueError:\n                    return v  # value too large\n            return v\n        \n        # Preserve the original method's metadata\n        wrapper.__name__ = method.__name__\n        wrapper.__doc__ = method.__doc__\n        wrapper.__module__ = method.__module__\n        wrapper.__qualname__ = method.__qualname__\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        # Store the original method\n        original_method = method\n        \n        def wrapper(self, *args, **kwargs):\n            # Create the cache key\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    return cache[k]\n                except KeyError:\n                    pass\n            # Call the original method and store the result\n            result = original_method(self, *args, **kwargs)\n            with lock:\n                try:\n                    cache.setdefault(k, result)\n                except ValueError:\n                    # Value too large\n                    pass\n            return result\n        \n        # Add cache info and clear methods\n        wrapper.cache_info = lambda: cache.cache_info() if hasattr(cache, 'cache_info') else None\n        wrapper.cache_clear = lambda: cache.cache_clear() if hasattr(cache, 'cache_clear') else None\n        wrapper.cache_parameters = lambda: cache.cache_parameters() if hasattr(cache, 'cache_parameters') else {}\n        \n        # Update wrapper with original method metadata\n        functools.update_wrapper(wrapper, original_method)\n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4253, "num_retrieved": 12, "inference_time": 0.9895247966051102, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception type and message\n        exc_type = e.__class__.__name__\n        exc_msg = str(e)\n        \n        # Format the traceback\n        tb = traceback.format_exc(max_level=max_level)\n        \n        # Limit path level in traceback\n        tb_lines = tb.split('\\n')\n        filtered_lines = []\n        for line in tb_lines:\n            if 'File' in line and 'in' in line:\n                # Check if path level exceeds max_path_level\n                path_level = 0\n                for p in line.split(' '):\n                    if p.startswith('File'):\n                        path_level += 1\n                if path_level <= max_path_level:\n                    filtered_lines.append(line)\n            else:\n                filtered_lines.append(line)\n        \n        # Reconstruct traceback\n        formatted_tb = '\\n'.join(filtered_lines)\n        \n        # Return formatted exception string\n        return f\"{exc_type}: {exc_msg}\\n{formatted_tb}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception type and message\n        exc_type = e.__class__.__name__\n        exc_msg = str(e)\n        \n        # Format the traceback\n        tb = traceback.format_exception(e, e, e.__traceback__, max_level=max_level)\n        tb_str = ''.join(tb)\n        \n        # Limit path level if needed\n        if max_path_level > 0:\n            # We don't have direct access to path level in standard traceback, \n            # so we just return the formatted trace\n            pass\n            \n        return f\"{exc_type}: {exc_msg}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception type and message\n        exc_type = e.__class__.__name__\n        exc_msg = str(e)\n        \n        # Format the traceback\n        tb = traceback.format_exception(e, e, e.__traceback__, max_level=max_level)\n        tb_str = ''.join(tb)\n        \n        # Limit path level in the traceback\n        tb_str = tb_str.replace('\\n', '\\n' + ' ' * 4 * max_path_level)\n        \n        # Return formatted string\n        return f\"{exc_type}: {exc_msg}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the traceback\n        tb_str = ''.join(traceback.format_list(tb))\n        \n        # Get the exception type and value\n        exc_type = e.__class__.__name__\n        exc_value = str(e)\n        \n        # Combine into a readable string\n        result = f\"{exc_type}: {exc_value}\\n\"\n        result += tb_str\n        \n        return result.strip()\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the exception type and message\n        exc_type = e.__class__.__name__\n        exc_msg = str(e)\n        \n        # Format the traceback\n        tb = traceback.format_exception(e, e, e.__traceback__, max_level=max_level)\n        tb_str = ''.join(tb)\n        \n        # Limit path level in stack trace\n        tb_str = tb_str.replace('  File \"', '  File \"').replace(' in ', ' in ')\n        \n        # Return formatted string\n        return f\"{exc_type}: {exc_msg}\\n{tb_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4141, "num_retrieved": 10, "inference_time": 0.9895247966051102, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Return negative value on error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Failed to write to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Return negative value on error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        mode = 'w' if overwrite else 'a'\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(text_buffer)\n    except Exception as e:\n        logger.error(\"Failed to write to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using the specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Return negative value on error\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4160, "num_retrieved": 9, "inference_time": 0.8769158869981766, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Failed to read file %s with encoding %s: %s\", file_name, encoding, e)\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, e)\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Failed to read file %s with encoding %s: %s\", file_name, encoding, e)\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Failed to read file %s with encoding %s: %s\", file_name, encoding, e)\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Failed to read file %s with encoding %s: %s\", file_name, encoding, e)\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4292, "num_retrieved": 14, "inference_time": 0.8769158869981766, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    \n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4285, "num_retrieved": 13, "inference_time": 0.8769158869981766, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # Clear any existing handlers, filters, and loggers\n    # This is a placeholder implementation - actual logic depends on the application's logging setup\n    import logging\n    \n    # Reset root logger\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Remove all handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters if needed\n    from SolBase import SolBase\n    if hasattr(SolBase, 'CONTEXT_FILTER'):\n        SolBase.CONTEXT_FILTER.clear()\n    \n    # Optionally reset any platform-specific logging state\n    # This should be expanded based on actual usage", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # Clear any existing handlers, filters, and log levels\n    # This function should be implemented to restore logging to a clean state\n    # Implementation details depend on the specific logging setup in the application\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    import logging\n    logging.getLogger().setLevel(logging.NOTSET)\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    # Reset any context filters or other platform-specific logging state if needed\n    # This is a placeholder - actual implementation may depend on specific requirements\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # Clear any existing handlers, filters, and loggers\n    # This function should be implemented to reset the logging system\n    # to a clean state, potentially removing all handlers and resetting\n    # configuration to default values\n    \n    # Example implementation (specifics may vary based on actual logging setup)\n    import logging\n    \n    # Remove all existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset the root logger level to default\n    logging.root.setLevel(logging.NOTSET)\n    \n    # Optionally reset any custom loggers or configurations\n    # This is a placeholder - actual implementation depends on context", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # Clear any existing handlers, filters, and loggers\n    # This implementation assumes a default reset to basic logging configuration\n    import logging\n    \n    # Reset root logger\n    logging.getLogger().setLevel(logging.WARNING)\n    \n    # Remove all handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Optionally reset any custom loggers or context filters\n    # This is a minimal implementation based on available context\n    # In a real implementation, additional cleanup might be needed"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4221, "num_retrieved": 10, "inference_time": 0.8769158869981766, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    raise NotImplementedError"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4162, "num_retrieved": 8, "inference_time": 0.8769158869981766, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the result\n        # We need to find the longest suffix of result that matches a prefix of ordering\n        i = 0\n        while i < len(ordering) and i < len(result) and result[i] == ordering[i]:\n            i += 1\n        \n        # If we found a match, we need to insert the rest of the ordering\n        # after the matched prefix\n        if i < len(ordering):\n            # Insert the remaining elements of ordering after the matched prefix\n            result = result[:i] + ordering[i:]\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Find the common suffix for each ordering\n    # We'll build a mapping from each element to the list of orderings it appears in\n    element_to_orderings = {}\n    for ordering in orderings:\n        for elem in ordering:\n            if elem not in element_to_orderings:\n                element_to_orderings[elem] = []\n            element_to_orderings[elem].append(ordering)\n    \n    # For each element, find the suffix that appears in all orderings containing it\n    # We'll build the result list by processing elements in order of first appearance\n    result = []\n    seen = set()\n    \n    # Process each ordering to build the merged result\n    # We'll use a greedy approach: for each ordering, extend the result with elements\n    # that are not yet seen and that appear in the ordering\n    for ordering in orderings:\n        # For each element in the ordering, if it's not already in result,\n        # and if it appears in all orderings that contain it, add it\n        for elem in ordering:\n            if elem not in seen:\n                # Check if this element appears in all orderings that contain it\n                # If it does, we can add it to the result\n                if elem in element_to_orderings:\n                    # Find the suffix of each ordering that starts with elem\n                    suffixes = [o for o in element_to_orderings[elem] if o.startswith(elem)]\n                    # If all orderings containing elem have the same suffix, we can add elem\n                    # But we need to ensure the suffix is preserved\n                    # Instead, we'll just add elem and then later ensure the suffix is preserved\n                    # by checking the orderings\n                    pass\n                seen.add(elem", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Find the common suffix for each prefix\n    result = []\n    seen = set()\n    \n    # Process each ordering\n    for ordering in orderings:\n        # For each element in the ordering, find the longest suffix that matches\n        # a prefix in the result\n        for i, item in enumerate(ordering):\n            if item not in seen:\n                # Find the longest suffix of the current ordering that starts with item\n                suffix_start = i\n                suffix = ordering[suffix_start:]\n                \n                # Check if this suffix is already in the result\n                # We need to find the position where this suffix should be inserted\n                found = False\n                for j, existing in enumerate(result):\n                    if existing == item:\n                        # We found the item, now check if the suffix matches\n                        if len(existing) == len(suffix) and existing == suffix:\n                            found = True\n                            break\n                    elif existing > item:\n                        # Insert before this existing element\n                        result.insert(j, item)\n                        found = True\n                        break\n                \n                if not found:\n                    # Insert at the end\n                    result.append(item)\n                \n                seen.add(item)\n    \n    # Reconstruct the result by merging all orderings with the constraint\n    # that if an object appears in multiple orderings, the suffix must match\n    # We'll use a different approach: build the result by processing each ordering\n    # and ensuring that the suffixes are preserved\n    \n    # Start with the first ordering\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it in\n    for ordering in orderings[1:]:\n        # Find the position to insert the ordering\n        # We need to preserve the order within each ordering", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the result\n        # We need to find the longest suffix of result that matches a prefix of ordering\n        i = 0\n        while i < len(ordering) and i < len(result) and result[i] == ordering[i]:\n            i += 1\n        \n        # If we found a match, we need to insert the rest of the ordering\n        # after the matched prefix\n        if i < len(ordering):\n            # Insert the remaining elements of ordering after the matched prefix\n            result = result[:i] + ordering[i:]\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the result\n        # We need to find the longest suffix of result that matches a prefix of ordering\n        i = 0\n        while i < len(ordering) and i < len(result) and result[i] == ordering[i]:\n            i += 1\n        \n        # If we found a match, insert the rest of the ordering after the match\n        if i < len(ordering):\n            # Insert the part of ordering from i onwards\n            result = result[:i] + ordering[i:]\n    \n    return result"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4297, "num_retrieved": 13, "inference_time": 0.8769158869981766, "generate_results": ["def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides specified, return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides specified, return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides specified, return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides specified, return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    return provides", "def directlyProvidedBy(object):\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides specified, return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4311, "num_retrieved": 13, "inference_time": 0.8769158869981766, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track which classes we've seen\n    seen = set()\n    result = []\n    \n    # Process classes in the order they appear\n    for cls in classes:\n        # If we haven't seen this class before, add it to result\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track which classes we've seen\n    seen = set()\n    result = []\n    \n    # Process classes in the order they appear\n    for cls in classes:\n        # If we haven't seen this class before, add it to result\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track which classes we've seen\n    seen = set()\n    result = []\n    \n    # Process classes in the order they appear\n    for cls in classes:\n        # If we haven't seen this class before, add it to result\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track which classes we've seen\n    seen = set()\n    result = []\n    \n    # Process classes in the order they appear\n    for cls in classes:\n        # If we haven't seen this class before, add it to result\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return tuple(result)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Use a set to track which classes we've seen\n    seen = set()\n    result = []\n    \n    # Process classes in the order they appear\n    for cls in classes:\n        # If we haven't seen this class before, add it to result\n        if cls not in seen:\n            seen.add(cls)\n            result.append(cls)\n    \n    return tuple(result)"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4320, "num_retrieved": 15, "inference_time": 0.8769158869981766, "generate_results": ["def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(all=True)\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(all=True)\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(all=True)\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(all=True)\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attributes\n        names = self.names()\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions\n    else:\n        # Return all attributes including those from base interfaces\n        names = self.names(all=True)\n        descriptions = []\n        for name in names:\n            desc = self.direct(name)\n            if desc is not None:\n                descriptions.append((name, desc))\n        return descriptions"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4474, "num_retrieved": 19, "inference_time": 0.8769158869981766, "generate_results": ["def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include names from direct and indirect base interfaces\n        names = set()\n        for interface in self.__sro__:\n            names.update(interface.__dict__.keys())\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include names from direct and indirect base interfaces\n        names = set()\n        for interface in self.__sro__:\n            names.update(interface.__dict__.keys())\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include names from direct and indirect base interfaces\n        names = set()\n        for interface in self.__sro__:\n            names.update(interface.names(True))\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include names from direct and indirect base interfaces\n        names = set()\n        for interface in self.__sro__:\n            names.update(interface.__dict__.keys())\n        return list(names)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include names from direct and indirect base interfaces\n        names = set()\n        for interface in self.__sro__:\n            names.update(interface.__dict__.keys())\n        return list(names)"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4239, "num_retrieved": 12, "inference_time": 0.8769158869981766, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.extend(item.flattened())\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.extend(item.flattened())\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.extend(item.flattened())\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.extend(item.flattened())\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif isinstance(item, Declaration):\n            output.extend(item.flattened())\n        else:\n            output.append(item)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4087, "num_retrieved": 7, "inference_time": 0.8769158869981766, "generate_results": ["def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if not _should_attempt_c_optimizations():\n            return False\n        # Attempt to import the C optimization module\n        # This is a placeholder - actual module import would depend on context\n        # For now, we just return True if optimizations are attempted\n        return True\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            # This is a placeholder - actual module import would depend on context\n            # For now, we just return True if optimizations are attempted\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            import zope.interface._c_optimizations\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Try to import the C optimization module\n            import zope.interface._c_optimizations\n            return True\n        return False\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        if _should_attempt_c_optimizations():\n            # Attempt to import the C optimization module\n            # This is a placeholder - actual module import would depend on context\n            # For example, if the C extension is available, return it\n            # Otherwise, return False\n            return True\n        else:\n            return False\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4186, "num_retrieved": 9, "inference_time": 0.8769158869981766, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import sys\n    from zope.interface._compat import _use_c_impl\n    \n    # If running on PyPy, we don't use C optimizations\n    if sys.implementation.name == 'pypy':\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    # Otherwise, attempt to use C optimizations\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import sys\n    from zope.interface._compat import _use_c_impl\n    \n    # If running on PyPy, we don't use C optimizations\n    if sys.implementation.name == 'pypy':\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    import os\n    if os.environ.get('PURE_PYTHON') == '1':\n        return False\n    \n    # Otherwise, attempt to use C optimizations\n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import sys\n    from zope.interface._compat import _use_c_impl\n    \n    # If running on PyPy, we should not use C optimizations\n    if sys.implementation.name == 'pypy':\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    # Otherwise, attempt to use C optimizations\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    import os\n    import sys\n    \n    # Check if running on PyPy\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check if PURE_PYTHON environment variable is set\n    pure_python = os.environ.get('PURE_PYTHON')\n    if pure_python is not None and pure_python.lower() == '1':\n        return False\n    \n    return True"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4411, "num_retrieved": 17, "inference_time": 0.8769158869981766, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4291, "num_retrieved": 13, "inference_time": 0.8769158869981766, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4123, "num_retrieved": 7, "inference_time": 0.8769158869981766, "generate_results": ["def reset(self):\n    \"\"\"Reset the histogram.\n\n    Current context is reset to an empty dict.\n    Bins are reinitialized with the *initial_value*\n    or with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = [self.initial_value] * self._num_bins\n    elif hasattr(self, 'make_bins'):\n        self._bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\n    Current context is reset to an empty dict.\n    Bins are reinitialized with the *initial_value*\n    or with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = [self.initial_value] * len(self._bins)\n    else:\n        self._bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\n    Current context is reset to an empty dict.\n    Bins are reinitialized with the *initial_value*\n    or with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = [self.initial_value] * len(self._bins)\n    else:\n        self._bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\n    Current context is reset to an empty dict.\n    Bins are reinitialized with the *initial_value*\n    or with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = [self.initial_value] * len(self._bins) if self._bins else []\n    elif hasattr(self, 'make_bins'):\n        self._bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\n    Current context is reset to an empty dict.\n    Bins are reinitialized with the *initial_value*\n    or with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self._bins = [self.initial_value] * len(self._bins) if self._bins else []\n    else:\n        self._bins = self.make_bins()"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4067, "num_retrieved": 5, "inference_time": 0.8769158869981766, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import itertools\n    from lena.flow import get_data_context\n\n    def convert_graph_to_csv(graph, sep=separator, header=header):\n        # Extract coordinates and values from graph\n        coords = []\n        values = []\n        \n        # Process each point in the graph\n        for coord, value in zip(graph.coords, graph.values):\n            # Convert coordinates to strings\n            coord_str = sep.join(map(str, coord))\n            # Convert value to string (if value is iterable, join with separator)\n            if hasattr(value, \"__iter__\") and not isinstance(value, (str, bytes)):\n                value_str = sep.join(map(str, value))\n            else:\n                value_str = str(value)\n            coords.append(coord_str)\n            values.append(value_str)\n        \n        # Create CSV rows\n        rows = []\n        if header is not None:\n            rows.append(header)\n        \n        # Add data rows\n        for coord_str, value_str in zip(coords, values):\n            rows.append(coord_str + sep + value_str)\n        \n        return \"\\n\".join(rows)\n    \n    # This function is deprecated and will be removed in future versions\n    # Use lena.output.ToCSV for converting graphs to CSV in Lena sequences\n    raise DeprecationWarning(\"to_csv is deprecated. Use lena.output.ToCSV instead.\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import itertools\n    from lena.core import LenaValueError\n\n    def convert_graph_to_csv(graph, sep=separator, header=header):\n        if header is not None:\n            yield header\n\n        # Get coordinates and values from the graph\n        coords = []\n        values = []\n        \n        for coord, value in zip(graph.coordinates, graph.values):\n            # Convert coordinates to strings\n            coord_str = sep.join(map(str, coord))\n            # Convert value to string\n            if hasattr(value, \"__iter__\"):\n                value_str = sep.join(map(str, value))\n            else:\n                value_str = str(value)\n            \n            row = coord_str + sep + value_str\n            yield row\n\n    # This function is deprecated and should not be used directly\n    # Instead, use lena.output.ToCSV for converting graphs to CSV\n    raise DeprecationWarning(\"to_csv is not used. Iterables are converted to tables. \"\n                            \"Use lena.output.ToCSV for converting graphs to CSV.\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import itertools\n    import lena.core\n\n    def convert_graph_to_csv(graph, sep=separator, header=header):\n        # Extract coordinates and values from the graph\n        coords = []\n        values = []\n        \n        # Get the field names from the graph\n        field_names = graph.field_names\n        \n        # Iterate over the graph points\n        for point in graph.points:\n            # Extract coordinates\n            coord = tuple(point[0])  # Coordinates from the point\n            coords.append(coord)\n            \n            # Extract values\n            value = tuple(point[1])  # Values from the point\n            values.append(value)\n        \n        # Create CSV rows\n        rows = []\n        \n        # Add header if provided\n        if header is not None:\n            rows.append(header)\n        \n        # For each point, create a row with coordinates and values\n        for coord, value in zip(coords, values):\n            # Convert coordinates to string\n            coord_str = sep.join(map(str, coord))\n            \n            # Convert values to string\n            value_str = sep.join(map(str, value))\n            \n            # Combine coordinate and value\n            row = coord_str + sep + value_str\n            rows.append(row)\n        \n        return \"\\n\".join(rows)\n    \n    # This function is deprecated and should not be used directly\n    # Instead, use lena.output.ToCSV for converting graphs to CSV\n    raise lena.core.LenaDeprecationError(\n        \"to_csv is deprecated in Lena 0.5. \"\n        \"Use lena.output.ToCSV instead to convert graphs to CSV.\"\n    )", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import itertools\n    from lena.core import LenaValueError\n\n    def convert_graph_to_csv(graph, sep=separator, header=header):\n        if header is not None:\n            yield header\n\n        # Get coordinates and values from the graph\n        coords = []\n        values = []\n        \n        for coord, value in zip(graph.coords, graph.values):\n            # Convert coordinates to strings\n            coord_str = sep.join(map(str, coord))\n            # Convert value to string (if value is iterable, convert each part)\n            if hasattr(value, \"__iter__\") and not isinstance(value, (str, bytes)):\n                value_str = sep.join(map(str, value))\n            else:\n                value_str = str(value)\n            yield f\"{coord_str},{value_str}\"\n\n    # This function is deprecated and should not be used directly\n    # Instead, use ToCSV output element\n    raise DeprecationWarning(\"to_csv is deprecated in Lena 0.5. Use ToCSV instead.\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    (separated by *separator*), then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    import itertools\n    from lena.flow import get_data_context\n\n    def convert_graph_to_csv(graph, sep=separator, header=header):\n        # Extract coordinates and values from the graph\n        coords = []\n        values = []\n        \n        # Process each point in the graph\n        for coord, value in zip(graph.coordinates, graph.values):\n            # Convert coordinates to strings\n            coord_str = sep.join(map(str, coord))\n            # Convert value to string (if value is iterable, convert each part)\n            if hasattr(value, \"__iter__\"):\n                value_str = sep.join(map(str, value))\n            else:\n                value_str = str(value)\n            coords.append(coord_str)\n            values.append(value_str)\n        \n        # Create CSV rows\n        rows = []\n        if header is not None:\n            rows.append(header)\n        \n        # Add data rows\n        for coord_str, value_str in zip(coords, values):\n            rows.append(coord_str + sep + value_str)\n        \n        return \"\\n\".join(rows)\n    \n    # This function is deprecated and not used directly\n    # In practice, use ToCSV output element instead\n    # For compatibility, return a placeholder implementation\n    return convert_graph_to_csv"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4073, "num_retrieved": 5, "inference_time": 1.0833002924919128, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph.GetErrorX() if coord_name == \"x\" else self.root_graph.GetErrorY()\n    # This is a simplified implementation based on the context\n    # In a real implementation, this would parse the error fields\n    # and return the index corresponding to the given coordinate\n    # Since the exact implementation details are not fully provided,\n    # this returns a placeholder.\n    return 0", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph.GetErrorX() if coord_name == \"x\" else self.root_graph.GetErrorY()\n    # This is a simplified implementation based on the context\n    # In a real implementation, we would need to parse the error fields properly\n    # For now, assuming we have access to error indices through the root graph\n    error_indices = {}\n    for err in self._parsed_error_names:\n        if err[1] == coord_name:\n            error_indices[err[0]] = err[3]\n    return error_indices.get(coord_name, -1)", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph.GetErrorX() if coord_name == \"x\" else self.root_graph.GetErrorY()\n    # This is a simplified implementation based on the context\n    # In a real implementation, it would parse the error fields\n    # and return the index for the given coordinate\n    error_indices = {}\n    for err in self._parsed_error_names:\n        if err[1] == coord_name:\n            error_indices[err[0]] = err[3]\n    return error_indices.get(coord_name, -1)", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph.GetErrorX() if coord_name == \"x\" else self.root_graph.GetErrorY()\n    # This is a simplified implementation based on the context\n    # In a real implementation, we would need to parse the error fields properly\n    # For now, assuming we have error indices stored or can be derived\n    # This is a placeholder - actual implementation would depend on the full context\n    return 0  # Placeholder return value", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph.GetErrorX() if coord_name == \"x\" else self.root_graph.GetErrorY()\n    # This is a simplified implementation based on the context\n    # In a real implementation, it would parse the error fields and find the index\n    # For now, assuming we have access to error indices through the root graph\n    # and we're looking for the index of the error corresponding to coord_name\n    error_x_ind = 0\n    error_y_ind = 0\n    for err in self._parsed_error_names:\n        if err[1] == coord_name:\n            if coord_name == \"x\":\n                error_x_ind = err[3]\n            else:\n                error_y_ind = err[3]\n    return error_x_ind if coord_name == \"x\" else error_y_ind"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4071, "num_retrieved": 5, "inference_time": 1.0833002924919128, "generate_results": ["def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        error_name = \"x\"\n        context.error = context.error or {}\n        if error_name not in context.error:\n            context.error[error_name] = {}\n        context.error[error_name][\"index\"] = self._error_x_ind\n    \n    if self._error_y_ind != -1:\n        error_name = \"y\"\n        context.error = context.error or {}\n        if error_name not in context.error:\n            context.error[error_name] = {}\n        context.error[error_name][\"index\"] = self._error_y_ind", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        error_name = \"x\"\n        context.error = context.error or {}\n        if self._error_x_ind != -1:\n            context.error[error_name] = context.error.get(error_name, {})\n            context.error[error_name][\"index\"] = self._error_x_ind\n    \n    if self._error_y_ind != -1:\n        error_name = \"y\"\n        context.error = context.error or {}\n        if self._error_y_ind != -1:\n            context.error[error_name] = context.error.get(error_name, {})\n            context.error[error_name][\"index\"] = self._error_y_ind", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        error_name = \"x\"\n        error_index = self._error_x_ind\n        context.error = context.error or {}\n        context.error[error_name] = context.error.get(error_name, {})\n        context.error[error_name][\"index\"] = error_index\n    \n    if self._error_y_ind != -1:\n        error_name = \"y\"\n        error_index = self._error_y_ind\n        context.error = context.error or {}\n        context.error[error_name] = context.error.get(error_name, {})\n        context.error[error_name][\"index\"] = error_index", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        error_name = \"x\"\n        context.error = context.error or {}\n        if error_name not in context.error:\n            context.error[error_name] = {}\n        context.error[error_name][\"index\"] = self._error_x_ind\n    \n    if self._error_y_ind != -1:\n        error_name = \"y\"\n        context.error = context.error or {}\n        if error_name not in context.error:\n            context.error[error_name] = {}\n        context.error[error_name][\"index\"] = self._error_y_ind", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    if self._error_x_ind != -1:\n        error_name = \"x\"\n        context.error = context.error or {}\n        if self._error_x_ind != -1:\n            context.error[error_name] = context.error.get(error_name, {})\n            context.error[error_name][\"index\"] = self._error_x_ind\n    if self._error_y_ind != -1:\n        error_name = \"y\"\n        context.error = context.error or {}\n        context.error[error_name] = context.error.get(error_name, {})\n        context.error[error_name][\"index\"] = self._error_y_ind"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4102, "num_retrieved": 6, "inference_time": 1.0833002924919128, "generate_results": ["def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier computation\n    edges = [np.array(edge) for edge in edges]\n    \n    # Compute the volume of each bin (product of edge differences)\n    bin_volumes = []\n    for i in range(len(edges)):\n        # For each dimension, compute the width of each bin\n        widths = [edges[j][k+1] - edges[j][k] for k in range(len(edges[j])-1)]\n        # The volume of each bin is the product of the widths\n        volume = np.prod(widths)\n        bin_volumes.append(volume)\n    \n    # Compute the integral as the sum of (bin value * bin volume)\n    total_integral = 0\n    for bin_idx, bin_value in enumerate(bins):\n        if isinstance(bin_value, (int, float)):\n            total_integral += bin_value * bin_volumes[bin_idx]\n        else:\n            # If bin_value is a collection, sum over its elements\n            total_integral += sum(bin_value * bin_volumes[bin_idx])\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier computation\n    edges = [np.array(e) for e in edges]\n    \n    # Compute the volume of each bin\n    bin_volumes = []\n    for i in range(len(edges)):\n        # For each dimension, compute the width of each bin\n        widths = [edges[j][i+1] - edges[j][i] for j in range(len(edges))]\n        # The volume of each bin is the product of the widths\n        volume = np.prod(widths)\n        bin_volumes.append(volume)\n    \n    # Compute the integral as the sum of (bin value * bin volume)\n    total_integral = 0\n    for bin_idx, bin_value in enumerate(bins):\n        total_integral += bin_value * bin_volumes[bin_idx]\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier computation\n    edges = [np.array(e) for e in edges]\n    \n    # Compute the volume of each bin\n    bin_volumes = []\n    for i in range(len(edges)):\n        # For each dimension, compute the width of each bin\n        widths = [edges[j][i+1] - edges[j][i] for j in range(len(edges))]\n        # The volume of each bin is the product of the widths\n        volume = np.prod(widths)\n        bin_volumes.append(volume)\n    \n    # Compute the integral as the sum of (bin value * bin volume)\n    total_integral = 0\n    for bin_idx, bin_value in enumerate(bins):\n        total_integral += bin_value * bin_volumes[bin_idx]\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Unify the bins and edges to handle multidimensional cases\n    bins, edges = hf.unify_1_md(bins, edges)\n    \n    # Compute the volume of each bin (product of edge differences)\n    volumes = []\n    for i in range(len(bins)):\n        bin_volume = 1.0\n        for j in range(len(edges)):\n            if len(edges[j]) > 0:\n                bin_volume *= edges[j][i+1] - edges[j][i]\n        volumes.append(bin_volume)\n    \n    # Compute the integral as sum of bin values times bin volumes\n    total_integral = 0.0\n    for i in range(len(bins)):\n        total_integral += bins[i] * volumes[i]\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Convert edges to numpy arrays for easier computation\n    edges = [np.array(e) for e in edges]\n    \n    # Compute the volume of each bin\n    # For each dimension, the bin volume is the difference between consecutive edges\n    volumes = []\n    for i in range(len(edges)):\n        edge_diff = edges[i][1:] - edges[i][:-1]\n        volumes.append(edge_diff)\n    \n    # Compute the integral by summing the product of bin values and bin volumes\n    # Use numpy to handle multidimensional array operations\n    bin_volumes = np.prod(np.array(volumes), axis=0)\n    \n    # Multiply each bin value by its corresponding volume and sum\n    result = np.sum(bins * bin_volumes)\n    \n    return result"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4058, "num_retrieved": 5, "inference_time": 1.0833002924919128, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for el in seq:\n            if isinstance(el, fill_request_seq.FillRequest):\n                return True\n    if isinstance(seq, source.Source):\n        return False\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for el in seq:\n            if isinstance(el, fill_request_seq.FillRequest):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for el in seq:\n            if isinstance(el, fill_request_seq.FillRequest):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for el in seq:\n            if isinstance(el, fill_request_seq.FillRequest):\n                return True\n    if isinstance(seq, source.Source):\n        return False\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for el in seq:\n            if isinstance(el, fill_request_seq.FillRequest):\n                return True\n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4109, "num_retrieved": 7, "inference_time": 1.0833002924919128, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(obj.fill) and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(obj.fill) and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4091, "num_retrieved": 6, "inference_time": 1.0833002924919128, "generate_results": ["def is_run_el(obj):\n    \"\"\"Object contains executable method 'run'.\"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"Object contains executable method 'run'.\"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"Object contains executable method 'run'.\"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"Object contains executable method 'run'.\"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"Object contains executable method 'run'.\"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4162, "num_retrieved": 9, "inference_time": 1.0833002924919128, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute') and callable(obj.fill) and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'compute') and callable(obj.fill) and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4059, "num_retrieved": 5, "inference_time": 1.0833002924919128, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for k, v in d1.items():\n            if k not in d2:\n                result[k] = v\n            elif isinstance(v, dict) and isinstance(d2[k], dict):\n                result[k] = difference(v, d2[k], level=-1)\n            else:\n                # If keys exist but values are different\n                if v != d2[k]:\n                    result[k] = v\n    else:\n        # Level 1: only check direct keys and values\n        for k, v in d1.items():\n            if k not in d2:\n                result[k] = v\n            elif isinstance(v, dict) and isinstance(d2[k], dict):\n                # Recursively process nested dictionaries at level 1\n                result[k] = difference(v, d2[k], level=level - 1)\n            else:\n                # If keys exist but values are different\n                if v != d2[k]:\n                    result[k] = v\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for k, v in d1.items():\n            if k not in d2:\n                result[k] = v\n            elif isinstance(v, dict) and isinstance(d2[k], dict):\n                result[k] = difference(v, d2[k], level=-1)\n            else:\n                if v != d2[k]:\n                    result[k] = v\n    else:\n        # Level 1: only check direct keys and values\n        for k, v in d1.items():\n            if k not in d2:\n                result[k] = v\n            elif isinstance(v, dict) and isinstance(d2[k], dict):\n                result[k] = difference(v, d2[k], level=level - 1)\n            else:\n                if v != d2[k]:\n                    result[k] = v\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for k, v in d1.items():\n            if k not in d2:\n                result[k] = v\n            elif isinstance(v, dict) and isinstance(d2[k], dict):\n                result[k] = difference(v, d2[k], level=-1)\n            else:\n                # If keys exist but values are different\n                if v != d2[k]:\n                    result[k] = v\n    else:\n        # Level 1: only check direct keys and values\n        for k, v in d1.items():\n            if k not in d2:\n                result[k] = v\n            elif isinstance(v, dict) and isinstance(d2[k], dict):\n                result[k] = difference(v, d2[k], level=level - 1)\n            else:\n                if v != d2[k]:\n                    result[k] = v\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=-1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    else:\n        # Level 1: only check direct keys and values\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=level - 1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    if level == -1:\n        # Infinite recursion: check all keys and values recursively\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=-1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    else:\n        # Level 1: only check direct keys and values\n        for key in d1:\n            if key not in d2:\n                result[key] = d1[key]\n            elif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                result[key] = difference(d1[key], d2[key], level=level-1)\n            elif d1[key] != d2[key]:\n                result[key] = d1[key]\n    \n    return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4108, "num_retrieved": 6, "inference_time": 1.0833002924919128, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Check if coord is within histogram edges\n    if self.edges is None or len(self.edges) == 0:\n        return\n    \n    # Validate that coord has the same dimension as edges\n    try:\n        # Check if coord is within bounds for each dimension\n        for dim, edges in enumerate(self.edges):\n            if not (edges[0] <= coord[dim] <= edges[1]):\n                return  # Outside histogram edges\n    except (IndexError, TypeError):\n        return  # Invalid coordinate\n    \n    # Create a bin value (weight) at the given coordinate\n    # In a real implementation, this would update the bins array\n    # For now, we just store the weight at the coordinate\n    # This is a simplified version - actual implementation would\n    # depend on how the histogram bins are structured\n    bin_value = weight\n    \n    # Add to internal data structure (this would depend on actual histogram structure)\n    # For now, we just store the value\n    if not hasattr(self, '_data'):\n        self._data = []\n    \n    # In a real implementation, we would find the bin corresponding to coord\n    # and update its content with weight\n    # This is a placeholder implementation\n    self._data.append((coord, bin_value))", "def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Check if coord is within the histogram edges\n    if self.edges is None or len(self.edges) == 0:\n        return\n    \n    # Validate that coord has the same dimension as edges\n    try:\n        # Check if coord is within the bounds defined by edges\n        for i, edge in enumerate(self.edges):\n            if not (edge[0] <= coord[i] <= edge[1]):\n                return  # Coordinate outside edges, ignore\n    except (IndexError, TypeError):\n        return  # Invalid coordinate dimension or format\n    \n    # Create a data point with the given weight\n    data_point = weight\n    \n    # Add the data point to the internal storage\n    self._data.append(data_point)\n    self._cur_context = self._cur_context  # Keep context as is", "def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Check if coord is within the histogram edges\n    if self.edges is None or len(self.edges) == 0:\n        return\n    \n    # Validate that coord has the correct dimension\n    try:\n        # Get the dimensions of the histogram\n        dim = len(self.edges)\n        if len(coord) != dim:\n            return\n    except:\n        return\n    \n    # Check if coord is within the edges\n    for i, edge in enumerate(self.edges):\n        if not (edge[0] <= coord[i] <= edge[1]):\n            return\n    \n    # If we reach here, coord is within edges\n    # Add the weight to the corresponding bin\n    # We need to find the bin index based on coord\n    bin_index = []\n    for i, edge in enumerate(self.edges):\n        # Find the bin index for this coordinate\n        # This is a simplified approach - in practice, this would require\n        # binning logic based on the histogram's binning scheme\n        bin_index.append(0)  # Placeholder\n    \n    # In a real implementation, we would use the histogram's binning logic\n    # to determine the correct bin index for the given coord\n    # For now, we just store the weight in a placeholder\n    # This would be replaced with actual binning logic\n    \n    # Since we don't have access to the full binning logic in this context,\n    # we'll just store the weight in a data structure that can be used\n    # by the histogram's compute method\n    if not hasattr(self, '_data'):\n        self._data = []\n    \n    # Append the weight to the data\n    self._data.append((coord, weight))", "def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Check if coord is within the histogram edges\n    if self.edges is None or len(self.edges) == 0:\n        return\n    \n    # Validate that coord has the correct dimension\n    try:\n        # Get the dimensions of the histogram\n        dim = len(self.edges)\n        if len(coord) != dim:\n            return\n    except:\n        return\n    \n    # Check if coord is within the edges\n    for i, edge in enumerate(self.edges):\n        if not (edge[0] <= coord[i] <= edge[1]):\n            return\n    \n    # If we reach here, coord is within edges\n    # Add the weight to the corresponding bin\n    # We need to find the bin index based on coord\n    bin_index = []\n    for i, edge in enumerate(self.edges):\n        # Find the bin that contains coord[i]\n        # This is a simplified approach - in practice, we'd need to use\n        # a more sophisticated binning algorithm\n        bin_index.append(0)  # Placeholder\n    \n    # In a real implementation, we would use the binning logic\n    # to determine the correct bin index based on coord\n    # For now, we just store the weight in the bin\n    # This is a simplified version - actual implementation would\n    # depend on the specific binning scheme used\n    \n    # Since we don't have access to the full binning logic,\n    # we'll just store the weight in a placeholder\n    # In a real implementation, we would update the bins array\n    # based on the coord and weight\n    \n    # For now, we just return without doing anything\n    # This is a placeholder implementation\n    pass", "def fill(self, coord, weight=1):\n    \"\"\"Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Check if coord is within the histogram edges\n    if self.edges is None or len(self.edges) == 0:\n        return\n    \n    # Validate that coord has the correct dimension\n    try:\n        # Get the dimensions of the histogram\n        dim = len(self.edges)\n        if len(coord) != dim:\n            return\n    except:\n        return\n    \n    # Check if coord is within the edges\n    for i, edges in enumerate(self.edges):\n        if not (edges[0] <= coord[i] <= edges[1]):\n            return\n    \n    # Add the weight to the appropriate bin\n    # This assumes that the histogram has a method to get the bin index from coordinates\n    # and that the bin content can be updated with a weight\n    bin_index = self._get_bin_index(coord)\n    if bin_index is not None:\n        self.bins[bin_index] += weight"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4228, "num_retrieved": 10, "inference_time": 1.0833002924919128, "generate_results": ["def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    errors = []\n    \n    # Validate label keys\n    for key in labels.keys():\n        # Check if key is a string and matches the expected regex pattern\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        else:\n            # Assuming there's a regex pattern defined for label keys (e.g., in a global config)\n            # This is a placeholder - actual regex would be defined elsewhere\n            import re\n            # Example regex pattern for label keys (alphanumeric, hyphens, underscores)\n            key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n            if not re.match(key_pattern, key):\n                errors.append({str(key): \"Label key does not match the regex pattern\"})\n    \n    # Validate label values\n    for key, value in labels.items():\n        if not isinstance(value, (str, list)):\n            # If value is not string or list, it's invalid\n            if isinstance(value, (int, float)):\n                errors.append({str(value): \"Label value must be a string or list\"})\n            else:\n                errors.append({str(value): \"Label value must be a string or list\"})\n        \n        # If value is a list, validate each element\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"Label value must be a string\"})\n        \n        # If value is a string, validate it (could be extended with regex)\n        if isinstance(value, str):\n            # Example: ensure string is not empty\n            if not value.strip():\n                errors.append({str(value): \"Label value cannot be empty\"})\n    \n    # If any errors occurred, raise ValidationError\n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    from krake.data.constraints import LabelKeyRegex, LabelValueRegex\n    from krake.exceptions import ValidationError\n\n    errors = []\n\n    # Validate label keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        elif not LabelKeyRegex.match(key):\n            errors.append({str(key): \"Label key does not match the regex\"})\n\n    # Validate label values\n    for key, value in labels.items():\n        if value is None:\n            continue\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n        elif not LabelValueRegex.match(value):\n            errors.append({str(value): \"Label value does not match the regex\"})\n\n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    from krake.data.constraints import LabelKeyRegex, LabelValueRegex\n    from krake.exceptions import ValidationError\n\n    errors = []\n\n    # Validate label keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        elif not LabelKeyRegex.match(key):\n            errors.append({str(key): \"Label key does not match the regex\"})\n\n    # Validate label values\n    for key, value in labels.items():\n        if value is None:\n            continue\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n        else:\n            if not LabelValueRegex.match(value):\n                errors.append({str(value): \"Label value does not match the regex\"})\n\n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    from krake.data.constraints import ValidationError\n    \n    errors = []\n    \n    # Validate label keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        else:\n            # Check if key matches the expected regex pattern\n            # Assuming regex pattern is defined in a global or class variable\n            # For example, if regex is defined as LABEL_KEY_REGEX\n            # We'll use a placeholder here - in real implementation, use actual regex\n            import re\n            # Example regex pattern for label keys (alphanumeric, hyphens, underscores)\n            label_key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n            if not re.match(label_key_pattern, key):\n                errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate label values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            # If value is not a string, check if it's a list or other iterable\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): \"expected string", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    from krake.data.constraints import LabelKeyRegex, LabelValueRegex\n    from krake.exceptions import ValidationError\n\n    errors = []\n\n    # Validate label keys\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            errors.append({\n                str(key): \"Label key must be a string\"\n            })\n        elif not LabelKeyRegex.match(key):\n            errors.append({\n                str(key): f\"Label key '{key}' does not match the regex {LabelKeyRegex.pattern}\"\n            })\n\n    # Validate label values\n    for key, value in labels.items():\n        if value is None:\n            continue\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({\n                        str(item): \"Label value must be a string\"\n                    })\n                elif not LabelValueRegex.match(item):\n                    errors.append({\n                        str(item): f\"Label value '{item}' does not match the regex {LabelValueRegex.pattern}\"\n                    })\n        elif not isinstance(value, str):\n            errors.append({\n                str(value): \"Label value must be a string\"\n            })\n        elif not LabelValueRegex.match(value):\n            errors.append({\n                str(value): f\"Label value '{value}' does not match the regex {LabelValueRegex.pattern}\"\n            })\n\n    if errors:\n        raise ValidationError(errors)"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4154, "num_retrieved": 8, "inference_time": 1.0833002924919128, "generate_results": ["import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Kubernetes resource names must follow these rules:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and dots\n    # - Must start with a letter or number\n    # - Must not end with a hyphen or dot\n    # - Cannot have consecutive hyphens or dots\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Kubernetes resource names must follow these rules:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and dots\n    # - Must start with a letter or number\n    # - Must not end with a hyphen or dot\n    # - Cannot contain consecutive hyphens or dots\n    \n    pattern = r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$'\n    return re.compile(pattern)", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Kubernetes resource names must follow these rules:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and dots\n    # - Must start with a letter or number\n    # - Must end with a letter or number\n    # - Cannot start or end with a hyphen or dot\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Kubernetes resource names must follow these rules:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and dots\n    # - Must start with a letter or number\n    # - Must not end with a hyphen or dot\n    # - Cannot have consecutive hyphens or dots\n    \n    pattern = r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$'\n    return re.compile(pattern)", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Kubernetes resource names must follow these rules:\n    # - Must be 1-253 characters long\n    # - Can only contain lowercase letters, numbers, hyphens, and dots\n    # - Must start with a letter or number\n    # - Must end with a letter or number\n    # - Cannot start or end with a hyphen or dot\n    pattern = re.compile(r'^[a-z0-9]([a-z0-9.-]*[a-z0-9])?$')\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4184, "num_retrieved": 9, "inference_time": 1.0833002924919128, "generate_results": ["def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Define a regular expression pattern for valid values (example: alphanumeric with optional underscores, dots, hyphens)\n    # This pattern can be adjusted based on specific requirements\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Define a regular expression pattern for valid values (example: alphanumeric with optional underscores, dots, or hyphens)\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Define a regular expression pattern for valid values (example: alphanumeric with optional underscores, dots, or hyphens)\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Define a regular expression pattern for valid values (example: alphanumeric with optional underscores, dots, or hyphens)\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Define a regular expression pattern for valid values (example: alphanumeric with optional underscores, dots, hyphens)\n    # This pattern can be adjusted based on actual requirements\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, str(value)):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression\")"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4161, "num_retrieved": 10, "inference_time": 1.0833002924919128, "generate_results": ["def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Regular expression for valid key (e.g., alphanumeric, underscore, hyphen, dot)\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Regular expression for valid keys (example: alphanumeric, underscore, dash, dot)\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Regular expression for valid key (e.g., alphanumeric with underscores, starting with letter or digit)\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Regular expression for valid keys (example: alphanumeric, underscore, dash, dot)\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    # Regular expression for valid key (e.g., alphanumeric with underscores, starting with letter or digit)\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key}\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4062, "num_retrieved": 5, "inference_time": 1.0833002924919128, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        # Copy identifying fields from manifest_dict\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                result[\"metadata\"] = {}\n                try:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                except KeyError:\n                    pass\n                # Recursively process metadata\n                result[\"metadata\"] = generate_default_observer_schema_dict(\n                    metadata, first_level=False\n                )\n        except (KeyError, AssertionError):\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"] and first_level:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        # Copy identifying fields from manifest\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                result[\"metadata\"] = {}\n                try:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                except KeyError:\n                    pass\n                # Recursively process metadata\n                result[\"metadata\"] = generate_default_observer_schema_dict(\n                    metadata, first_level=False\n                )\n        except (KeyError, AssertionError):\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"] and first_level:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:`generate_default_observer_schema_list`, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                result[\"metadata\"] = {}\n                try:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                except KeyError:\n                    pass\n                result[\"metadata\"] = generate_default_observer_schema_dict(\n                    result[\"metadata\"], first_level=False\n                )\n        except (KeyError, AssertionError):\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                result[\"metadata\"] = {}\n                try:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                except KeyError:\n                    pass\n                # Recursively process metadata\n                result[\"metadata\"] = generate_default_observer_schema_dict(\n                    metadata, first_level=False\n                )\n        except (KeyError, AssertionError):\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            if isinstance(metadata, dict):\n                result[\"metadata\"] = {}\n                try:\n                    result[\"metadata\"][\"name\"] = metadata[\"name\"]\n                except KeyError:\n                    pass\n                # Recursively process metadata\n                result[\"metadata\"] = generate_default_observer_schema_dict(\n                    metadata, first_level=False\n                )\n        except (KeyError, AssertionError):\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n            \n    return result"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4195, "num_retrieved": 7, "inference_time": 1.0833002924919128, "generate_results": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(last_applied_manifest):\n            last_applied_manifest.append(deepcopy(response[idx]))\n        elif idx < len(last_applied_manifest):\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n    # Handle the last element (length constraints)\n    if len(observer_schema) > 0:\n        last_element = observer_schema[-1]\n        current_length = response[-1][\"observer_schema_list_current_length\"] if len(response) > 0 else 0\n        if \"observer_schema_list_min_length\" in last_element:\n            min_length = last_element[\"observer_schema_list_min_length\"]\n            if current_length < min_length:\n                # Extend the list to meet minimum length\n                while len(last_applied_manifest) < min_length:\n                    last_applied_manifest.append({})\n        if \"observer_schema_list_max_length\" in last_element:\n            max_length = last_element[\"observer_schema_list_max_length\"]\n            if current_length > max_length:\n                # Truncate the list to meet maximum length\n                last_applied_manifest = last_applied_manifest[:max_length]", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            continue\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        else:\n            if idx < len(last_applied_manifest):\n                last_applied_manifest[idx] = value\n            else:\n                last_applied_manifest.append(value)\n    \n    # Handle list length constraints\n    if len(response) > 0:\n        last_length = response[-1].get(\"observer_schema_list_current_length\", 0)\n        schema_length = observer_schema[-1].get(\"observer_schema_list_current_length\", 0)\n        \n        # Ensure list length is within bounds\n        if last_length < observer_schema[-1][\"observer_schema_list_min_length\"]:\n            # Extend if needed\n            while len(last_applied_manifest) < observer_schema[-1][\"observer_schema_list_min_length\"]:\n                last_applied_manifest.append(None)\n        elif last_length > observer_schema[-1][\"observer_schema_list_max_length\"]:\n            # Truncate if needed\n            last_applied_manifest = last_applied_manifest[:observer_schema[-1][\"observer_schema_list_max_length\"]]", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(last_applied_manifest):\n            last_applied_manifest.append(deepcopy(response[idx]))\n        elif idx < len(last_applied_manifest) and idx < len(response):\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n        else:\n            # Handle case where response is shorter than observer_schema\n            if idx >= len(response):\n                last_applied_manifest[idx] = deepcopy(response[idx])\n    \n    # Check list length constraints\n    if len(response) > 0:\n        last_length = response[-1].get(\"observer_schema_list_current_length\", 0)\n        schema_min = observer_schema[-1].get(\"observer_schema_list_min_length\", 0)\n        schema_max = observer_schema[-1].get(\"observer_schema_list_max_length\", 0)\n        \n        if last_length < schema_min or last_length > schema_max:\n            # Log or handle invalid length as needed\n            pass", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            continue\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        else:\n            if idx < len(last_applied_manifest):\n                last_applied_manifest[idx] = value\n            else:\n                last_applied_manifest.append(value)\n    \n    # Handle the last element (length constraints)\n    if len(observer_schema) > 0 and len(response) > 0:\n        last_schema = observer_schema[-1]\n        last_response = response[-1]\n        \n        # Check length constraints\n        if \"observer_schema_list_min_length\" in last_schema and \\\n           \"observer_schema_list_max_length\" in last_schema:\n            current_length = last_response.get(\"observer_schema_list_current_length\", 0)\n            min_length = last_schema[\"observer_schema_list_min_length\"]\n            max_length = last_schema[\"observer_schema_list_max_length\"]\n            \n            if current_length < min_length or current_length > max_length:\n                # This would be an error condition, but we just update the manifest\n                # based on the response\n                pass", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            continue\n            \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        else:\n            if idx < len(last_applied_manifest):\n                last_applied_manifest[idx] = value\n            else:\n                last_applied_manifest.append(value)\n    \n    # Handle list length constraints\n    if len(response) > 0:\n        last_applied_manifest[-1] = response[-1]"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4185, "num_retrieved": 7, "inference_time": 1.0833002924919128, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function goes through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4116, "num_retrieved": 6, "inference_time": 0.9516016840934753, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = app.spec.observer_schema\n    \n    # If custom observer schema is already provided, return early\n    if observer_schema:\n        return observer_schema\n    \n    # Generate default observer schema for each resource in manifest\n    default_schema = []\n    \n    for resource in manifest:\n        resource_name = resource.get(\"metadata\", {}).get(\"name\")\n        resource_kind = resource.get(\"kind\")\n        \n        if not resource_name or not resource_kind:\n            continue\n            \n        # Default observer schema entry\n        default_entry = {\n            \"kind\": resource_kind,\n            \"name\": resource_name,\n            \"fields\": [\n                \"metadata.name\",\n                \"metadata.namespace\",\n                \"metadata.labels\",\n                \"metadata.annotations\",\n                \"spec\",\n                \"status\"\n            ]\n        }\n        \n        default_schema.append(default_entry)\n    \n    return default_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = app.spec.observer_schema\n    \n    # If custom observer schema is already provided, return early\n    if observer_schema:\n        return observer_schema\n    \n    # Generate default observer schema for each resource in manifest\n    default_schema = []\n    \n    for resource in manifest:\n        resource_name = resource.get(\"metadata\", {}).get(\"name\")\n        resource_kind = resource.get(\"kind\")\n        \n        if not resource_name or not resource_kind:\n            continue\n            \n        # Default observer schema entry\n        default_entry = {\n            \"kind\": resource_kind,\n            \"name\": resource_name,\n            \"fields\": [\n                \"metadata.name\",\n                \"metadata.namespace\",\n                \"metadata.labels\",\n                \"metadata.annotations\",\n                \"spec\",\n                \"status\"\n            ]\n        }\n        \n        default_schema.append(default_entry)\n    \n    return default_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = app.spec.observer_schema\n    \n    # If custom observer schema is already provided, return it\n    if observer_schema:\n        return observer_schema\n    \n    # Generate default observer schema for each resource in manifest\n    default_schema = []\n    \n    for resource in manifest:\n        resource_type = resource.get(\"kind\")\n        if not resource_type:\n            continue\n            \n        # Default observer schema fields for Kubernetes resources\n        default_observer = {\n            \"kind\": resource_type,\n            \"api_version\": resource.get(\"apiVersion\"),\n            \"name\": resource.get(\"metadata\", {}).get(\"name\"),\n            \"namespace\": resource.get(\"metadata\", {}).get(\"namespace\"),\n            \"fields\": {\n                \"metadata\": {\n                    \"name\": True,\n                    \"namespace\": True,\n                    \"labels\": True,\n                    \"annotations\": True\n                },\n                \"spec\": {\n                    \"replicas\": True,\n                    \"resources\": True,\n                    \"selector\": True,\n                    \"template\": True\n                },\n                \"status\": {\n                    \"conditions\": True,\n                    \"replicas\": True\n                }\n            }\n        }\n        \n        # Add resource-specific fields if available\n        if resource_type == \"Deployment\":\n            default_observer[\"fields\"][\"spec\"][\"template\"] = {\n                \"metadata\": True,\n                \"spec\": {\n                    \"containers\": True,\n                    \"imagePullPolicy\": True\n                }\n            }\n        elif resource_type == \"Service\":\n            default_observer[\"fields\"][\"spec\"][\"ports\"] = True\n            default_observer[\"fields\"][\"spec\"][\"selector\"] = True\n        elif resource_type == \"Pod\":\n            default_observer[\"fields\"][\"spec\"][\"containers\"] = True\n        elif resource_type == \"ConfigMap\":\n            default_observer[\"fields\"][\"data\"] = True\n        elif resource_type == \"Secret\":\n            default_observer[\"fields\"][\"stringData\"] = True\n        \n        default_schema.append(default_observer)\n    \n    return default_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = app.spec.observer_schema\n    \n    # If a custom observer schema is already provided, don't generate a default one\n    if observer_schema:\n        return observer_schema\n    \n    # Generate default observer schema for each resource in manifest\n    default_schema = []\n    \n    for resource in manifest:\n        resource_name = resource.get(\"metadata\", {}).get(\"name\")\n        resource_kind = resource.get(\"kind\")\n        \n        if not resource_name or not resource_kind:\n            continue\n            \n        # Default observer schema entry\n        default_entry = {\n            \"kind\": resource_kind,\n            \"name\": resource_name,\n            \"fields\": [\n                \"metadata.name\",\n                \"metadata.namespace\",\n                \"metadata.labels\",\n                \"metadata.annotations\",\n                \"spec\",\n                \"status\"\n            ]\n        }\n        \n        default_schema.append(default_entry)\n    \n    return default_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest\n    observer_schema = app.spec.observer_schema\n    \n    # If custom observer schema is already provided, return it\n    if observer_schema:\n        return observer_schema\n    \n    # Generate default observer schema for each resource in manifest\n    default_schema = []\n    \n    for resource in manifest:\n        resource_name = resource.get(\"metadata\", {}).get(\"name\")\n        resource_kind = resource.get(\"kind\")\n        \n        if not resource_name or not resource_kind:\n            continue\n            \n        # Default observer schema entry\n        default_entry = {\n            \"kind\": resource_kind,\n            \"name\": resource_name,\n            \"fields\": [\n                \"metadata.name\",\n                \"metadata.namespace\",\n                \"metadata.labels\",\n                \"metadata.annotations\",\n                \"spec\",\n                \"status\"\n            ]\n        }\n        \n        default_schema.append(default_entry)\n    \n    return default_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4147, "num_retrieved": 7, "inference_time": 0.9516016840934753, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = self._convert_params(params)\n    else:\n        out_params = self._convert_params(params)\n    \n    return self.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = self._convert_params(params)\n    else:\n        out_params = self._convert_params(params)\n    \n    return self.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = self._convert_params(params)\n    else:\n        out_params = self._convert_params(params)\n    \n    return self.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = self._convert_params(params)\n    else:\n        out_params = self._convert_params(params)\n    \n    return self.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        out_params = self._convert_params(params)\n    else:\n        out_params = self._convert_params(params)\n    \n    return self.convert(sql, params)"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4147, "num_retrieved": 7, "inference_time": 0.9516016840934753, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    in_counter = itertools.count()\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions, in_counter), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    in_counter = itertools.count()\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions, in_counter), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4089, "num_retrieved": 7, "inference_time": 0.9516016840934753, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, check_digests=True)\n    try:\n        if path:\n            (inv_dir, inv_file) = fs.path.split(path)\n            validator.obj_fs = open_fs(inv_dir, create=False)\n            validator.validate_inventory(inv_file, where='standalone')\n        else:\n            # Validate root filesystem\n            validator.validate(self.root_fs)\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, check_digests=True)\n    try:\n        if path:\n            (inv_dir, inv_file) = fs.path.split(path)\n            validator.obj_fs = open_fs(inv_dir, create=False)\n            validator.validate_inventory(inv_file, where='standalone')\n        else:\n            # Validate root filesystem directly\n            validator.validate(self.root_fs)\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    except ValidatorAbortException:\n        return False\n    \n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    \n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, check_digests=True)\n    try:\n        if path:\n            (inv_dir, inv_file) = fs.path.split(path)\n            validator.obj_fs = open_fs(inv_dir, create=False)\n            validator.validate_inventory(inv_file, where='standalone')\n        else:\n            # Validate root filesystem\n            validator.validate(self.root_fs)\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, check_digests=True)\n    try:\n        if path:\n            (inv_dir, inv_file) = fs.path.split(path)\n            validator.obj_fs = open_fs(inv_dir, create=False)\n            validator.validate_inventory(inv_file, where='standalone')\n        else:\n            # Validate root filesystem\n            validator.validate(self.root_fs)\n    except fs.errors.ResourceNotFound:\n        validator.log.error('E033', where='standalone', explanation='failed to open directory')\n    passed = (validator.log.num_errors == 0)\n    messages = str(validator)\n    if messages != '':\n        print(messages)\n    return passed", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=False, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        if fs.path.isdir(path):\n            validator.obj_fs = open_fs(path, create=False)\n        else:\n            # If path is a file, try to open its parent directory\n            (parent, filename) = fs.path.split(path)\n            validator.obj_fs = open_fs(parent, create=False)\n            if not validator.obj_fs.exists(filename):\n                raise fs.errors.ResourceNotFound()\n        \n        validator.validate(path)\n        return True\n    except ValidatorAbortException:\n        return False\n    except fs.errors.ResourceNotFound:\n        return False"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4214, "num_retrieved": 12, "inference_time": 0.9516016840934753, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for error in self.errors:\n        messages.append(f\"{prefix}E{error.code}: {error.desc}\")\n    for warning in self.warnings:\n        messages.append(f\"{prefix}W{warning.code}: {warning.desc}\")\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    messages = []\n    for error in self.errors:\n        messages.append(f\"{prefix}E{error.code}: {error.desc}\")\n    for warning in self.warnings:\n        messages.append(f\"{prefix}W{warning.code}: {warning.desc}\")\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    messages = []\n    for error in self.errors:\n        messages.append(f\"{prefix}[E{error.code}] {error.desc}\")\n    for warning in self.warnings:\n        messages.append(f\"{prefix}[W{warning.code}] {warning.desc}\")\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    messages = []\n    for error in self.errors:\n        messages.append(f\"{prefix}E{error.code}: {error.desc}\")\n    for warning in self.warnings:\n        messages.append(f\"{prefix}W{warning.code}: {warning.desc}\")\n    return \"\\n\".join(messages) if messages else \"\"", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    messages = []\n    for error in self.errors:\n        messages.append(f\"{prefix}[E{error.code}] {error.desc}\")\n    for warning in self.warnings:\n        messages.append(f\"{prefix}[W{warning.code}] {warning.desc}\")\n    return \"\\n\".join(messages) if messages else \"\""]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4121, "num_retrieved": 8, "inference_time": 0.9516016840934753, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4116, "num_retrieved": 8, "inference_time": 0.9516016840934753, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4025, "num_retrieved": 4, "inference_time": 0.9516016840934753, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    digest_algorithm = self.digest_algorithm\n    lax_digests = self.lax_digests\n    \n    # Extract spec version from inventory if requested\n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type value (e.g., \"ocfl_1.0\" -> \"1.0\")\n            match = re.match(r'ocfl_(\\d+\\.\\d+)', inventory['type'])\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If type value doesn't match expected pattern, fall back to self.spec_version\n                pass\n    \n    # Validate inventory structure and content\n    inv_validator = InventoryValidator(\n        log=self.log,\n        where='root',\n        lax_digests=lax_digests,\n        spec_version=spec_version\n    )\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    digest_algorithm = self.digest_algorithm\n    lax_digests = self.lax_digests\n    \n    # Extract spec_version from inventory if requested\n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type field (e.g., \"ocfl_1.0\" -> \"1.0\")\n            match = re.match(r'ocfl_(\\d+\\.\\d+)', inventory['type'])\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If type doesn't match expected format, fall back to self.spec_version\n                pass\n    \n    # Validate inventory structure and content\n    inv_validator = InventoryValidator(\n        log=self.log,\n        where='root',\n        lax_digests=lax_digests,\n        spec_version=spec_version\n    )\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    digest_algorithm = self.digest_algorithm\n    lax_digests = self.lax_digests\n    \n    # Extract spec_version from inventory if requested\n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type field (assumed to be in format 'ocfl_1.0' or similar)\n            match = re.match(r'ocfl_(\\d+\\.\\d+)', inventory['type'])\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If type doesn't match expected format, fall back to self.spec_version\n                pass\n    \n    # Validate inventory structure and content\n    inv_validator = InventoryValidator(\n        log=self.log,\n        where='root',\n        lax_digests=lax_digests,\n        spec_version=spec_version\n    )\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    digest_algorithm = self.digest_algorithm\n    lax_digests = self.lax_digests\n    \n    # Extract spec version from inventory if requested\n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract version from type value (e.g., \"ocfl_1.0\" -> \"1.0\")\n            match = re.match(r'ocfl_(\\d+\\.\\d+)', inventory['type'])\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If type value doesn't match expected pattern, fall back to self.spec_version\n                pass\n    \n    # Validate inventory structure and content\n    inv_validator = InventoryValidator(\n        log=self.log,\n        where='root',\n        lax_digests=lax_digests,\n        spec_version=spec_version\n    )\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    spec_version = self.spec_version\n    if extract_spec_version:\n        if 'type' in inventory and inventory['type']:\n            # Extract spec version from type value (e.g., \"ocfl_1.0\" -> \"1.0\")\n            match = re.match(r'ocfl_(\\d+\\.\\d+)', inventory['type'])\n            if match:\n                spec_version = match.group(1)\n            else:\n                # If type value doesn't match expected format, fall back to self.spec_version\n                pass\n        else:\n            # No type value or invalid type value, fall back to self.spec_version\n            pass\n    \n    # Validate the inventory structure and content\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=spec_version)\n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 3945, "num_retrieved": 2, "inference_time": 0.9516016840934753, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used.get(filepath, []):\n            if digest not in manifest_files[filepath]:\n                self.log.error('E094', where='root', filepath=filepath, missing_digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used.get(filepath, []):\n            if digest not in manifest_files[filepath]:\n                self.log.error('E094', where='root', filepath=filepath, missing_digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used.get(filepath, []):\n            if digest not in manifest_files[filepath]:\n                self.log.error('E094', where='root', filepath=filepath, missing_digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used:\n            if filepath not in manifest_files:\n                continue\n            if digest not in manifest_files[filepath]:\n                self.log.error('E094', where='root', filepath=filepath, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used:\n            if filepath not in manifest_files:\n                continue\n            if digest not in manifest_files[filepath]:\n                self.log.error('E094', where='root', filepath=filepath, digest=digest)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 3978, "num_retrieved": 2, "inference_time": 0.9516016840934753, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version uses a specification version that is equal to or earlier than the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=self.spec_version, \n                       prev_version_dir=prior.where, prev_spec_version=prior.spec_version)\n    \n    # Ensure the digest algorithm used in the prior version is compatible with the current version\n    # If the current version uses a different digest algorithm, the prior version must use a compatible one\n    if prior.digest_algorithm != self.digest_algorithm:\n        # Check if the prior version's digest algorithm is supported by the current version\n        # This is a simplified check; in practice, the validator should know about supported algorithms\n        if self.digest_algorithm not in prior.supported_digest_algorithms:\n            self.log.error('E104', where=prior.where, prior_digest_algorithm=prior.digest_algorithm, \n                           current_digest_algorithm=self.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version uses a specification version that is equal to or earlier than the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=self.spec_version, \n                       prev_version_dir=prior.where, prev_spec_version=prior.spec_version)\n    \n    # Ensure the digest algorithm used in the prior version is compatible with the current version\n    # If the current version uses a different digest algorithm, the prior version must use a compatible one\n    if prior.digest_algorithm != self.digest_algorithm:\n        # Check if the prior version's digest algorithm is supported by the current version\n        # This is a simplified check; in practice, the validator should know about supported algorithms\n        if not self.supports_digest_algorithm(prior.digest_algorithm):\n            self.log.error('E104', where=prior.where, prior_digest_algorithm=prior.digest_algorithm, \n                           current_digest_algorithm=self.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version uses a specification version that is equal to or earlier than the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=self.spec_version, \n                       prev_version_dir=prior.where, prev_spec_version=prior.spec_version)\n    \n    # Ensure the digest algorithm used in the prior version is compatible with the current version\n    # If the current version uses a different digest algorithm, the prior version must use a compatible one\n    if prior.digest_algorithm != self.digest_algorithm:\n        # Check if the prior version's digest algorithm is supported by the current version\n        # This is a simplified check; in practice, the validator should know about supported algorithms\n        if not self.supports_digest_algorithm(prior.digest_algorithm):\n            self.log.error('E104', where=prior.where, prior_digest_algorithm=prior.digest_algorithm, \n                           current_digest_algorithm=self.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version uses a specification version that is equal to or earlier than the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=self.spec_version, \n                       prev_version_dir=prior.where, prev_spec_version=prior.spec_version)\n    \n    # Ensure the digest algorithm used in the prior version is compatible with the current version\n    # If the current version uses a different digest algorithm, the prior version must use a compatible one\n    if prior.digest_algorithm != self.digest_algorithm:\n        # Check if the prior version's digest algorithm is supported by the current version\n        # This is a simplified check; in practice, the validator would need to know about supported algorithms\n        if self.digest_algorithm not in prior.supported_digests:\n            self.log.error('E104', where=prior.where, prior_digest_algorithm=prior.digest_algorithm, \n                           current_digest_algorithm=self.digest_algorithm)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version uses a specification version that is equal to or earlier than the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=self.spec_version, \n                       prev_version_dir=prior.where, prev_spec_version=prior.spec_version)\n    \n    # Ensure the digest algorithm used in the prior version is compatible with the current version\n    if prior.digest_algorithm != self.digest_algorithm:\n        # If the digest algorithm differs, we need to ensure that the content is still valid\n        # and that the digest values are consistent with the current version\n        # For now, we only log a warning if the algorithms differ, as the spec allows different algorithms\n        # in prior versions, but they must be consistent with the current version\n        self.log.warning('W011', where=prior.where, prior_digest_algorithm=prior.digest_algorithm, \n                         current_digest_algorithm=self.digest_algorithm)"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4021, "num_retrieved": 4, "inference_time": 0.9516016840934753, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the content files for this digest from the manifest\n        content_files = manifest[digest]\n        # Map each logical path to the corresponding content file(s)\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            logical_path_map[logical_path].update(content_files)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the content files for this digest from the manifest\n        content_files = manifest[digest]\n        # Map each logical file path to the corresponding content file\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            logical_path_map[logical_file].update(content_files)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the content files for this digest from the manifest\n        content_files = manifest[digest]\n        # Map each logical path to the corresponding content file(s)\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            logical_path_map[logical_path].update(content_files)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the content files for this digest from the manifest\n        content_files = manifest[digest]\n        # Map each logical path to the corresponding content file(s)\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            logical_path_map[logical_path].update(content_files)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    manifest = inventory['manifest']\n    state = inventory['versions'][version]['state']\n    \n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the content files for this digest from the manifest\n        content_files = manifest[digest]\n        # Map each logical path to the corresponding content file(s)\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            logical_path_map[logical_path].update(content_files)\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 3964, "num_retrieved": 2, "inference_time": 0.9516016840934753, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all digests in fixity refer to files in manifest_files\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all digests in fixity refer to files in manifest_files\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that all files in fixity are present in manifest_files\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that all files in fixity are present in manifest_files\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all digests in fixity refer to files in manifest_files\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4307, "num_retrieved": 13, "inference_time": 0.9516016840934753, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4212, "num_retrieved": 10, "inference_time": 0.9516016840934753, "generate_results": ["def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = defaultdict(list)\n    \n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(os.path.basename(xml_filename))[0]\n    \n    # Iterate through all files and group them by XML basename\n    for file in files:\n        file_name = os.path.basename(file)\n        file_ext = os.path.splitext(file_name)[1]\n        \n        # If the file is an XML file, use its basename as key\n        if file_ext == \".xml\":\n            file_basename = os.path.splitext(file_name)[0]\n            # Group all files that share the same XML basename\n            grouped_files[file_basename].append(file)\n        else:\n            # For non-XML files, check if they match the XML basename\n            if file_basename == xml_basename:\n                grouped_files[xml_basename].append(file)\n    \n    # Convert to regular dict and return\n    return dict(grouped_files)", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = defaultdict(list)\n    \n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(os.path.basename(xml_filename))[0]\n    \n    # Iterate through all files and group them by XML basename\n    for file in files:\n        file_name = os.path.basename(file)\n        file_ext = os.path.splitext(file_name)[1]\n        \n        # If the file is an XML file, use its basename as key\n        if file_ext == \".xml\":\n            file_basename = os.path.splitext(file_name)[0]\n            # Group all files that share the same XML basename\n            grouped_files[file_basename].append(file)\n        else:\n            # For non-XML files, check if they match the XML basename\n            if file_basename == xml_basename:\n                grouped_files[xml_basename].append(file)\n    \n    # Convert defaultdict to regular dict and return\n    return dict(grouped_files)", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = defaultdict(list)\n    \n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(os.path.basename(xml_filename))[0]\n    \n    # Process each file to determine if it belongs to the XML\n    for file in files:\n        file_name = os.path.basename(file)\n        file_ext = os.path.splitext(file_name)[1].lower()\n        \n        # Check if the file is an XML file (matching the given xml_filename)\n        if file_ext == \".xml\":\n            # If the file matches the XML filename (case-insensitive), group it\n            if file_name.lower() == xml_filename.lower():\n                grouped_files[xml_basename].append(file)\n        else:\n            # For non-XML files, check if they are related by basename\n            # If the file name starts with the XML basename, group it\n            if file_name.startswith(xml_basename):\n                grouped_files[xml_basename].append(file)\n    \n    # Convert defaultdict to regular dict and return\n    return dict(grouped_files)", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = defaultdict(list)\n    \n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(os.path.basename(xml_filename))[0]\n    \n    # Process each file to determine if it belongs to the XML\n    for file in files:\n        file_path = file\n        file_basename = os.path.basename(file_path)\n        file_ext = os.path.splitext(file_basename)[1].lower()\n        \n        # Check if the file is an XML file\n        if file_ext == '.xml':\n            # If the file matches the XML filename, group it\n            if file_basename == xml_filename:\n                grouped_files[xml_basename].append(file_path)\n        else:\n            # For non-XML files, check if they have the same basename as the XML\n            # This is a simplified approach - we assume files with the same basename belong to the same XML\n            base_name = os.path.splitext(file_basename)[0]\n            if base_name == xml_basename:\n                grouped_files[xml_basename].append(file_path)\n    \n    # Convert to regular dict and return\n    return dict(grouped_files)", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = defaultdict(list)\n    \n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(os.path.basename(xml_filename))[0]\n    \n    # Iterate through all files and group them by XML basename\n    for file in files:\n        file_name = os.path.basename(file)\n        file_ext = os.path.splitext(file_name)[1]\n        \n        # If the file is an XML file, use its basename as key\n        if file_ext == \".xml\":\n            file_basename = os.path.splitext(file_name)[0]\n            # Group all files that match the XML basename\n            grouped_files[file_basename].append(file)\n        else:\n            # For non-XML files, check if they match the XML basename\n            if file_basename == xml_basename:\n                grouped_files[xml_basename].append(file)\n    \n    # Convert to regular dict and return\n    return dict(grouped_files)"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4298, "num_retrieved": 13, "inference_time": 0.9516016840934753, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix + '-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix + '-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix + '-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix + '-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(prefix + '-'))"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4314, "num_retrieved": 14, "inference_time": 0.9516016840934753, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4425, "num_retrieved": 17, "inference_time": 1.2514178901910782, "generate_results": ["def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Group files by XML basename\n    for file in files:\n        if file.is_file():\n            if file.suffix.lower() == '.xml':\n                xml_basename = file.stem\n                package_data[xml_basename] = {\n                    'xml': str(file),\n                    'assets': [],\n                    'renditions': []\n                }\n                \n                # Look for assets and renditions with the same basename\n                for asset_file in files:\n                    if asset_file.is_file() and asset_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.pdf']:\n                        asset_name = asset_file.stem\n                        if xml_basename in asset_name:\n                            package_data[xml_basename]['assets'].append(str(asset_file))\n                \n                for rendition_file in files:\n                    if rendition_file.is_file() and rendition_file.suffix.lower() in ['.pdf', '.epub', '.mobi']:\n                        rendition_name = rendition_file.stem\n                        if xml_basename in rendition_name:\n                            package_data[xml_basename]['renditions'].append(str(rendition_file))\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Group files by XML basename\n    for file in files:\n        if file.is_file():\n            if file.suffix.lower() == '.xml':\n                # Extract XML basename (without extension)\n                xml_basename = file.stem\n                package_data[xml_basename] = {\n                    'xml': str(file),\n                    'assets': [],\n                    'renditions': []\n                }\n                \n                # Look for assets and renditions with the same basename\n                for asset_file in files:\n                    if asset_file.is_file() and asset_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.bmp']:\n                        if asset_file.stem == xml_basename:\n                            package_data[xml_basename]['assets'].append(str(asset_file))\n                \n                for rendition_file in files:\n                    if rendition_file.is_file() and rendition_file.suffix.lower() in ['.pdf', '.epub', '.mobi']:\n                        if rendition_file.stem == xml_basename:\n                            package_data[xml_basename]['renditions'].append(str(rendition_file))\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Group files by XML basename\n    for file in files:\n        if file.is_file():\n            if file.suffix.lower() == '.xml':\n                xml_basename = file.stem\n                package_data[xml_basename] = {\n                    'xml': str(file),\n                    'assets': [],\n                    'renditions': []\n                }\n                \n                # Look for assets and renditions with the same basename\n                for asset_file in files:\n                    if asset_file.is_file() and asset_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.tif']:\n                        asset_name = asset_file.name\n                        if xml_basename in asset_name:\n                            package_data[xml_basename]['assets'].append(str(asset_file))\n                \n                for rendition_file in files:\n                    if rendition_file.is_file() and rendition_file.suffix.lower() in ['.pdf', '.epub', '.mobi']:\n                        rendition_name = rendition_file.name\n                        if xml_basename in rendition_name:\n                            package_data[xml_basename]['renditions'].append(str(rendition_file))\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Group files by XML basename\n    for file in files:\n        if file.is_file():\n            if file.suffix.lower() == '.xml':\n                xml_basename = file.stem\n                package_data[xml_basename] = {\n                    'xml': str(file),\n                    'assets': [],\n                    'renditions': []\n                }\n                \n                # Look for assets and renditions with the same basename\n                for asset_file in files:\n                    if asset_file.is_file() and asset_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.tif']:\n                        if xml_basename in asset_file.stem:\n                            if asset_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.tif']:\n                                package_data[xml_basename]['assets'].append(str(asset_file))\n                            else:\n                                package_data[xml_basename]['renditions'].append(str(asset_file))\n            elif file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.tif']:\n                # Check if file name matches any XML basename\n                for xml_file in files:\n                    if xml_file.is_file() and xml_file.suffix.lower() == '.xml':\n                        if file.stem in xml_file.stem:\n                            if file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.tif']:\n                                package_data[xml_file.stem]['assets'].append(str(file))\n                            else:\n                                package_data[xml_file.stem]['renditions'].append(str(file))\n    \n    return package_data", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from pathlib import Path\n    import re\n    \n    package_data = {}\n    folder_path = Path(folder)\n    \n    # Get all files in the folder\n    files = list(folder_path.iterdir())\n    \n    # Dictionary to store files grouped by XML basename\n    xml_basenames = {}\n    \n    for file in files:\n        if file.is_file():\n            filename = file.name\n            file_ext = file.suffix.lower()\n            \n            # Check if file is XML\n            if file_ext == '.xml':\n                # Extract basename (without extension) for XML file\n                xml_basename = filename[:-4]  # Remove .xml extension\n                xml_basenames[xml_basename] = {\n                    'xml': str(file)\n                }\n            elif file_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.bmp']:\n                # For renditions and assets, we need to find the corresponding XML basename\n                # Look for XML file with the same basename\n                xml_candidate = f\"{filename[:-4]}.xml\"\n                if xml_candidate in xml_basenames:\n                    xml_basename = xml_candidate[:-4]\n                    if xml_basename not in package_data:\n                        package_data[xml_basename] = {\n                            'xml': xml_basenames[xml_basename]['xml'],\n                            'renditions': [],\n                            'assets': []\n                        }\n                    \n                    # Add rendition or asset file\n                    if file_ext in ['.pdf']:\n                        package_data[xml_basename]['renditions'].append(str(file))\n                    else:\n                        package_data[xml_basename]['assets'].append(str(file))\n            else:\n                # Skip unsupported file types\n                continue\n    \n    # If no XML files found, return empty dict\n    if not package_data:\n        return {}\n    \n    return package_data"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4296, "num_retrieved": 12, "inference_time": 1.2514178901910782, "generate_results": ["def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    \n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        \n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    \n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    \n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        \n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    \n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    \n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        \n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    \n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    \n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        \n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    \n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    \n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        \n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    \n    return {'type': 'asset', 'path': file_path}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4449, "num_retrieved": 17, "inference_time": 1.2514178901910782, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao pacote com base na linguagem e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com o nome do arquivo e o caminho, formatado conforme o padro de rendies\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = os.path.splitext(file_name)[0]\n    \n    # Determina o nome da rendio com base na linguagem\n    if lang == 'pt':\n        rendition_name = f\"{prefix}.pdf\"\n    else:\n        rendition_name = f\"{prefix}-{lang}.pdf\"\n    \n    return {\n        \"original\": file_name,\n        lang: rendition_name\n    }", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao pacote com base na linguagem e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome do arquivo e o caminho do arquivo\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = self.xmltree.get('prefix') or self.xmltree.find('.//journal').get('prefix')\n    \n    if not prefix:\n        raise ValueError(\"Prefixo no encontrado no XML\")\n    \n    # Verifica se o arquivo  a verso original ou uma verso por idioma\n    if lang == 'original':\n        rendition_key = 'original'\n    else:\n        rendition_key = lang\n    \n    rendition_info = {\n        'name': f\"{prefix}-{rendition_key}.{os.path.splitext(file_name)[1]}\",\n        'uri': file_path\n    }\n    \n    # Adiciona ao dicionrio de rendies\n    if 'renditions' not in self._renditions:\n        self._renditions = {}\n    self._renditions[rendition_key] = rendition_info\n    \n    return rendition_info", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao pacote com base na linguagem e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome do arquivo e o caminho, formatado conforme o padro:\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\"\n        }\n    \"\"\"\n    # Extrai o nome do arquivo sem a extenso\n    file_name = os.path.basename(file_path)\n    base_name, _ = os.path.splitext(file_name)\n    \n    # Cria o nome da rendio com base na linguagem\n    rendition_name = f\"{base_name}-{lang}{_}\"\n    \n    # Retorna o dicionrio com o nome original e a rendio\n    return {\n        \"original\": f\"{base_name}.pdf\",\n        lang: rendition_name\n    }", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao pacote com base na linguagem e no caminho do arquivo.\n    \n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF de rendio\n        \n    Returns\n    -------\n    dict\n        Dicionrio com o nome do arquivo e o caminho, formatado conforme o padro de rendies\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = os.path.splitext(file_name)[0]\n    \n    # Determina o nome da rendio com base na linguagem\n    if lang == 'pt':\n        rendition_name = f\"{prefix}.pdf\"\n    else:\n        rendition_name = f\"{prefix}-{lang}.pdf\"\n    \n    return {\n        \"original\": file_name,\n        lang: rendition_name\n    }", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao pacote com base na linguagem e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome do arquivo e o caminho do arquivo\n    \"\"\"\n    file_name = os.path.basename(file_path)\n    prefix = self.xmltree.get('prefix') or self.xmltree.get('id')\n    \n    if not prefix:\n        raise ValueError(\"Prefixo no encontrado no XML para identificao da rendio\")\n    \n    # Verifica se o arquivo  a rendio original ou em uma linguagem especfica\n    if lang == 'original':\n        rendition_key = 'original'\n    else:\n        rendition_key = lang\n    \n    rendition_info = {\n        'name': f\"{prefix}-{rendition_key}.{os.path.splitext(file_name)[1]}\",\n        'uri': file_path\n    }\n    \n    # Adiciona ao dicionrio de rendies\n    if 'renditions' not in self._renditions:\n        self._renditions = {}\n    \n    self._renditions[rendition_key] = rendition_info\n    \n    return rendition_info"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4284, "num_retrieved": 12, "inference_time": 1.2514178901910782, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\"\"\"\n    # Extract the file extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the file extension\n    if file_extension in [\".tif\", \".tiff\"]:\n        asset_type = \"original\"\n    elif file_extension in [\".png\"]:\n        asset_type = \"optimised\"\n    elif file_extension in [\".jpg\", \".jpeg\"]:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create the asset entry with the basename and file path\n    asset_entry = {\n        'basename': basename,\n        'file_path': file_path,\n        'type': asset_type\n    }\n    \n    # Add the asset to the assets collection\n    self.assets.append(asset_entry)\n    \n    return asset_entry", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\"\"\"\n    # Extract the extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the extension\n    if file_extension in [\".tif\", \".tiff\"]:\n        asset_type = \"original\"\n    elif file_extension in [\".png\"]:\n        asset_type = \"optimised\"\n    elif file_extension in [\".jpg\", \".jpeg\"]:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create the asset key using the basename and extension\n    asset_key = basename + file_extension\n    \n    # Store the asset in the assets dictionary\n    self.assets[asset_key] = file_path\n    \n    # If the asset is a TIFF file, generate a PNG version (optimised)\n    if file_extension in [\".tif\", \".tiff\"]:\n        try:\n            # Generate PNG from TIFF\n            png_path = self.convert2png(destination_path=None)\n            # Store the PNG path with the same basename but .png extension\n            self.assets[basename + \".png\"] = png_path\n        except Exception as e:\n            # Log or handle error appropriately\n            pass\n    \n    # If the asset is a PNG file, it's already optimised\n    # If the asset is a JPEG file, generate a thumbnail\n    if file_extension in [\".jpg\", \".jpeg\"]:\n        try:\n            # Generate thumbnail\n            thumbnail_path = self.create_thumbnail(destination_path=None)\n            # Store the thumbnail path\n            self.assets[basename + \".thumbnail.jpg\"] = thumbnail_path\n        except Exception as e:\n            # Log or handle error appropriately\n            pass\n    \n    return asset_key", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\"\"\"\n    # Extract the extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the extension\n    if file_extension in ['.tif', '.tiff']:\n        asset_type = \"original\"\n    elif file_extension in ['.png']:\n        asset_type = \"optimised\"\n    elif file_extension in ['.jpg', '.jpeg']:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create the asset entry with the basename and file path\n    asset_entry = {\n        'basename': basename,\n        'file_path': file_path,\n        'type': asset_type\n    }\n    \n    # Add the asset to the assets collection\n    self.assets.append(asset_entry)\n    \n    return asset_entry", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\"\"\"\n    # Extract the file extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the file extension\n    if file_extension in [\".tif\", \".tiff\"]:\n        asset_type = \"original\"\n    elif file_extension in [\".png\"]:\n        asset_type = \"optimised\"\n    elif file_extension in [\".jpg\", \".jpeg\"]:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create the asset entry with the basename and file path\n    asset_entry = {\n        'basename': basename,\n        'file_path': file_path,\n        'type': asset_type\n    }\n    \n    # Add the asset to the assets collection\n    self.assets.append(asset_entry)\n    \n    return asset_entry", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets collection with the given basename and file path.\"\"\"\n    # Extract the file extension from the file path\n    file_extension = os.path.splitext(file_path)[1].lower()\n    \n    # Determine the asset type based on the file extension\n    if file_extension in ['.tif', '.tiff']:\n        asset_type = \"original\"\n    elif file_extension in ['.png']:\n        asset_type = \"optimised\"\n    elif file_extension in ['.jpg', '.jpeg']:\n        asset_type = \"thumbnail\"\n    else:\n        asset_type = \"original\"\n    \n    # Create the asset entry with the basename and file path\n    asset_entry = {\n        'basename': basename,\n        'file_path': file_path,\n        'type': asset_type\n    }\n    \n    # Add the asset to the assets collection\n    self.assets.append(asset_entry)\n    \n    return asset_entry"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4377, "num_retrieved": 15, "inference_time": 1.2514178901910782, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(xml_file)[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files that share the same XML basename\n                for file in filenames:\n                    if basename in file and file.endswith((\".xml\", \".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".bmp\")):\n                        result[basename][\"files\"].append(file)\n            \n            return result\n    except Exception as e:\n        LOGGER.exception(e)\n        raise RuntimeError(_(\"Unable to explore zip file {}: {}\").format(zip_path, e))", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(xml_file)[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files that share the same XML basename\n                for file in filenames:\n                    if basename in file and file.endswith((\".xml\", \".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".zip\", \".txt\")):\n                        result[basename][\"files\"].append(file)\n            \n            return result\n    except Exception as e:\n        LOGGER.exception(e)\n        raise RuntimeError(_(\"Unable to explore zip file {}: {}\").format(zip_path, e))", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(xml_file)[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files that share the same XML basename\n                for file in filenames:\n                    if basename in file and file.endswith(\".xml\"):\n                        continue\n                    if basename in file:\n                        result[basename][\"files\"].append(file)\n            \n            return result\n    except Exception as e:\n        LOGGER.exception(e)\n        raise RuntimeError(_(\"Unable to explore zip file {}: {}\").format(zip_path, e))", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(xml_file)[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files that share the same XML basename\n                for file in filenames:\n                    if basename in file and file.endswith((\".xml\", \".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".zip\", \".txt\", \".html\", \".css\", \".js\")):\n                        result[basename][\"files\"].append(file)\n            \n            return result\n    except Exception as e:\n        LOGGER.exception(e)\n        raise RuntimeError(f\"Unable to explore zip file {zip_path}: {e}\")", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    try:\n        with ZipFile(zip_path) as zf:\n            filenames = zf.namelist()\n            xml_files = [f for f in filenames if f.endswith(\".xml\")]\n            \n            result = {}\n            for xml_file in xml_files:\n                basename = os.path.splitext(xml_file)[0]\n                result[basename] = {\n                    \"xml\": xml_file,\n                    \"files\": []\n                }\n                \n                # Collect all files that share the same basename\n                for file in filenames:\n                    if basename in file and file.endswith((\".xml\", \".pdf\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".bmp\")):\n                        result[basename][\"files\"].append(file)\n            \n            return result\n    except Exception as e:\n        LOGGER.exception(e)\n        raise RuntimeError(f\"Unable to explore zip file {zip_path}: {e}\")"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4204, "num_retrieved": 10, "inference_time": 1.2514178901910782, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    if not file_utils.is_zipfile(zip_path):\n        return []\n    \n    with ZipFile(zip_path, 'r') as zip_file:\n        return zip_file.namelist()"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4812, "num_retrieved": 28, "inference_time": 1.2514178901910782, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4275, "num_retrieved": 12, "inference_time": 1.2514178901910782, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4636, "num_retrieved": 23, "inference_time": 1.2514178901910782, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return None, None\n    \n    # Split the issue string by 's' to separate number and supplement\n    parts = issue.split('s')\n    \n    number = None\n    supplement = None\n    \n    if len(parts) == 1:\n        # No supplement, just extract number\n        number = parts[0]\n        # If number is empty, return None\n        if not number:\n            return None, None\n    elif len(parts) == 2:\n        # Split into number and supplement\n        number = parts[0]\n        supplement = parts[1]\n        # If number is empty, return None\n        if not number:\n            return None, supplement\n        # If supplement is empty, return None\n        if not supplement:\n            return number, None\n    else:\n        # More than two parts, invalid format\n        return None, None\n    \n    # Clean up number and supplement (remove any non-digit characters)\n    import re\n    number = re.sub(r'\\D', '', number)\n    supplement = re.sub(r'\\D', '', supplement)\n    \n    # Return the extracted values\n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return None, None\n    \n    # Split the issue string by 's' to separate number and supplement\n    parts = issue.split('s')\n    number = parts[0].strip() if parts else None\n    \n    # If there's a supplement part, extract it\n    supplement = parts[1].strip() if len(parts) > 1 else None\n    \n    # If no number is found, try to extract from the original issue string\n    if not number:\n        # Try to extract number from the issue string (e.g., \"12\" or \"12s1\")\n        match = re.search(r'(\\d+)', issue)\n        number = match.group(1) if match else None\n    \n    # If supplement is not found, try to extract from the issue string\n    if not supplement:\n        # Look for supplement after 's' in the issue string\n        supplement_match = re.search(r's(\\d+)', issue)\n        supplement = supplement_match.group(1) if supplement_match else None\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return None, None\n    \n    # Split the issue string by 's' to separate number and supplement\n    parts = issue.split('s')\n    \n    number = None\n    supplement = None\n    \n    if len(parts) == 1:\n        # No supplement, just extract number\n        number = parts[0]\n        # If number is empty, return None\n        if not number:\n            return None, None\n    elif len(parts) == 2:\n        # Split into number and supplement\n        number = parts[0]\n        supplement = parts[1]\n        # If number is empty, return None\n        if not number:\n            return None, supplement\n        # If supplement is empty, return None\n        if not supplement:\n            return number, None\n    else:\n        # More than two parts, invalid format\n        return None, None\n    \n    # Validate that number is a digit\n    if not number.isdigit():\n        return None, None\n    \n    # Validate that supplement is a digit\n    if supplement and not supplement.isdigit():\n        return None, None\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return None, None\n    \n    # Split the issue string by 's' to separate number and supplement\n    parts = issue.split('s')\n    number = parts[0].strip() if parts else None\n    \n    # If there's a supplement part, extract it\n    supplement = parts[1].strip() if len(parts) > 1 else None\n    \n    # If no number is found, try to extract from the original issue string\n    if not number and issue:\n        # Try to extract number from the issue string (e.g., \"2020-01\" or \"2020-01s1\")\n        # Look for digits at the end of the string\n        import re\n        match = re.search(r'(\\d+)([sS]\\d+)?', issue)\n        if match:\n            number = match.group(1)\n            supplement = match.group(2) if match.group(2) else None\n    \n    # If still no number, return None for both\n    if not number:\n        number = None\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return None, None\n    \n    # Split the issue string by 's' to separate number and supplement\n    parts = issue.split('s')\n    number = parts[0].strip() if parts[0].strip() else None\n    \n    # Supplement is the part after 's' if exists\n    supplement = parts[1].strip() if len(parts) > 1 else None\n    \n    # If supplement is empty, set it to None\n    if supplement == '0' or supplement == '':\n        supplement = None\n    \n    return number, supplement"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 1.2514178901910782, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return \"NOT \" + self.children[0].pretty(indent + 1, debug)\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expression.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expression.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 1.2514178901910782, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it matches absorption or negative absorption patterns\n        # This is a simplified implementation assuming expressions are in a form that can be evaluated\n        \n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using Boolean algebra rules\n        # We'll assume the expressions are already in a form that can be processed\n        # and that the absorption rules can be applied directly\n        \n        # In practice, this would require parsing and evaluating the expression tree\n        # For now, we return the original expression as a placeholder\n        # A full implementation would require recursive evaluation of the expression tree\n        \n        # Apply absorption: if expr is of form A & (A | B) or A | (A & B), reduce to A\n        # Apply negative absorption: if expr is of form A & (~A | B) or A | (~A & B), reduce to A & B or A | B\n        \n        # Since we don't have full expression parsing/evaluation capabilities here,\n        # we return the original expression as a placeholder\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for i, expr in enumerate(args):\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # For each expression, try to simplify using absorption laws\n        simplified = expr\n        # Apply absorption and negative absorption rules\n        # This is a simplified implementation that assumes the expressions are in a form\n        # where we can check for the patterns directly\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # Check if expression is of form A & (~A | B) or A | (~A & B)\n        \n        # Since we don't have direct access to the internal structure of expressions,\n        # we assume that the expressions are already in a canonical form\n        # and that the simplification can be done through evaluation or pattern matching\n        \n        # For now, we just return the original expressions as no direct simplification\n        # can be applied without knowing the internal structure\n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # These simplifications are typically handled by the Expression class\n            # We assume the Expression class has methods to handle these rules\n            # For now, we just return the expression as is, as the actual simplification\n            # would be implemented in the Expression class\n            result.append(expr)\n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it matches A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the internal structure of the expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            # This is a simplified implementation - actual logic would depend on expression structure\n            # For now, we return the expression as is since full parsing and simplification is complex\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4189, "num_retrieved": 9, "inference_time": 1.2514178901910782, "generate_results": ["def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self._hooks[hook].append(handler)\n        return handler\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook].append(handler)\n        return handler\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook].append(handler)\n        return handler\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook].append(handler)\n        return handler\n    \n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook].append(handler)\n        return handler\n    \n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4232, "num_retrieved": 11, "inference_time": 1.2514178901910782, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"user\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"user\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"user\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"user\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"user\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4152, "num_retrieved": 8, "inference_time": 1.2514178901910782, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4191, "num_retrieved": 9, "inference_time": 1.2514178901910782, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all attributes and convert fractional values to integers\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days' and value != int(value):\n                # For days, convert fractional part to hours\n                days = int(value)\n                hours = int((value - days) * 24)\n                result.days = days\n                result.hours = result.hours + hours\n            elif attr == 'hours' and value != int(value):\n                # For hours, convert fractional part to minutes\n                hours = int(value)\n                minutes = int((value - hours) * 60)\n                result.hours = hours\n                result.minutes = result.minutes + minutes\n            elif attr == 'minutes' and value != int(value):\n                # For minutes, convert fractional part to seconds\n                minutes = int(value)\n                seconds = int((value - minutes) * 60)\n                result.minutes = minutes\n                result.seconds = result.seconds + seconds\n            else:\n                # For integer attributes, just assign the integer value\n                result.__dict__[attr] = int(value)\n    \n    # Normalize the values to ensure they are within valid ranges\n    # This is a simplified version - actual normalization would be more complex\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all attributes and convert fractional values to integers\n    for attr in self._attributes:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Round to nearest integer\n                value = int(round(value))\n            # For attributes like days, hours, minutes, seconds, etc.\n            # we need to ensure the value is an integer\n            if attr == 'days':\n                value = int(value)\n            elif attr == 'hours':\n                value = int(value)\n            elif attr == 'minutes':\n                value = int(value)\n            elif attr == 'seconds':\n                value = int(value)\n            elif attr == 'microseconds':\n                value = int(value)\n            elif attr == 'weeks':\n                value = int(value)\n            elif attr == 'months':\n                value = int(value)\n            elif attr == 'years':\n                value = int(value)\n            \n            # Set the attribute on the result object\n            setattr(result, attr, value)\n    \n    # Ensure the result is properly normalized\n    # This is a simplified version - actual implementation would depend on\n    # the specific logic of relativedelta's normalization\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    \n    # Convert fractional days to whole days and remaining hours\n    if result.days is not None:\n        days = int(result.days)\n        fractional_days = result.days - days\n        if fractional_days > 0:\n            # Convert fractional days to hours\n            fractional_hours = fractional_days * 24\n            result.hours += int(fractional_hours)\n            # Adjust days if hours exceed 24\n            if result.hours >= 24:\n                result.days += result.hours // 24\n                result.hours = result.hours % 24\n    \n    # Convert fractional hours to whole hours\n    if result.hours is not None:\n        hours = int(result.hours)\n        fractional_hours = result.hours - hours\n        if fractional_hours > 0:\n            # Convert fractional hours to minutes\n            fractional_minutes = fractional_hours * 60\n            result.minutes += int(fractional_minutes)\n            # Adjust hours if minutes exceed 60\n            if result.minutes >= 60:\n                result.hours += result.minutes // 60\n                result.minutes = result.minutes % 60\n    \n    # Convert fractional minutes to whole minutes\n    if result.minutes is not None:\n        minutes = int(result.minutes)\n        fractional_minutes = result.minutes - minutes\n        if fractional_minutes > 0:\n            # Convert fractional minutes to seconds\n            fractional_seconds = fractional_minutes * 60\n            result.seconds += int(fractional_seconds)\n            # Adjust minutes if seconds exceed 60\n            if result.seconds >= 60:\n                result.minutes += result.seconds // 60\n                result.seconds = result.seconds % 60\n    \n    # Convert fractional seconds to whole seconds\n    if result.seconds is not None:\n        seconds = int(result.seconds)\n        fractional_seconds = result.seconds - seconds\n        if fractional_seconds > 0:\n            # Convert fractional seconds to microseconds\n            fractional_microseconds = fractional_seconds * 1000000\n            result.microseconds += int(fractional_microseconds)\n            # Adjust seconds if microseconds exceed", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all attributes and convert fractional values to integers\n    for attr in self._attributes:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Round to nearest integer\n                value = int(round(value))\n            # For attributes like days, hours, minutes, seconds, etc.\n            # we need to ensure they are integers\n            if attr == 'days':\n                value = int(value)\n            elif attr == 'hours':\n                value = int(value)\n            elif attr == 'minutes':\n                value = int(value)\n            elif attr == 'seconds':\n                value = int(value)\n            elif attr == 'microseconds':\n                value = int(value)\n            elif attr == 'weeks':\n                value = int(value)\n            elif attr == 'months':\n                value = int(value)\n            elif attr == 'years':\n                value = int(value)\n            \n            # Set the attribute on the result object\n            setattr(result, attr, value)\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.copy()\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(result, attr)\n        if value is not None:\n            if isinstance(value, float):\n                # Convert float to integer by truncating (floor for positive)\n                if value > 0:\n                    setattr(result, attr, int(value))\n                else:\n                    setattr(result, attr, int(value))\n            elif isinstance(value, (int, long)):\n                # Already integer, no change needed\n                pass\n            else:\n                # Handle other types if necessary\n                setattr(result, attr, int(value))\n    \n    # Normalize the values to ensure they are in valid ranges\n    # For example, if days is 1.5, we want days=1 and hours=14\n    if hasattr(result, 'days') and result.days is not None:\n        days = int(result.days)\n        hours = int(result.hours) if result.hours is not None else 0\n        minutes = int(result.minutes) if result.minutes is not None else 0\n        seconds = int(result.seconds) if result.seconds is not None else 0\n        \n        # Adjust hours, minutes, seconds based on days\n        if days > 0:\n            hours += days * 24\n            minutes += hours % 60\n            seconds += minutes % 60\n            \n            # Adjust days to be the integer part\n            result.days = days\n            result.hours = hours % 24\n            result.minutes = minutes % 60\n            result.seconds = seconds % 60\n    \n    return result"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4183, "num_retrieved": 9, "inference_time": 1.2514178901910782, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        return lambda dt: namefunc(dt).encode('ascii')\n    return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    if six.PY2:\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, six.text_type):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4095, "num_retrieved": 8, "inference_time": 1.3569748103618622, "generate_results": ["def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fall back to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fallback to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fall back to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fall back to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\"}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fallback to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\"}"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4295, "num_retrieved": 16, "inference_time": 1.3569748103618622, "generate_results": ["def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-pre\":\n        return render_pep440_pre(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4113, "num_retrieved": 8, "inference_time": 1.3569748103618622, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\" if \"+\" not in \"0\" else \".\"", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\" if \"+\" not in \"0\" else \".\"", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\" if \"+\" not in \"0\" else \".\"", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\" if \"+\" not in \"0\" else \".\"", "def plus_or_dot(pieces):\n    \"\"\"\n    Return a + if we don't already have one, else return a .\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\" if \"+\" not in \"0\" else \".\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4206, "num_retrieved": 12, "inference_time": 1.3569748103618622, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or os.environ.copy()\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose and not hidden\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n        if verbose and result.stderr:\n            print(result.stderr.strip())\n            \n        return result\n        \n    except FileNotFoundError:\n        raise subprocess.CalledProcessError(\n            cmd=full_command,\n            returncode=1,\n            output=\"Command not found: \" + \" \".join(full_command)\n        )", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the complete command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose and command was successful\n        if verbose and result.returncode == 0:\n            print(\"Command output:\", result.stdout.strip())\n            \n        # Print error if command failed\n        if result.returncode != 0:\n            if result.stderr:\n                print(\"Command error:\", result.stderr.strip())\n            else:\n                print(\"Command failed with return code:\", result.returncode)\n                \n        return result\n        \n    except FileNotFoundError:\n        print(f\"Command not found: {' '.join(full_command)}\")\n        raise\n    except Exception as e:\n        print(f\"Error executing command: {str(e)}\")\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n        if verbose and result.stderr:\n            print(result.stderr.strip())\n            \n        # Hide stderr if requested\n        if hide_stderr:\n            result.stderr = \"\"\n            \n        return result\n        \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Command not found: {' '.join(full_command)}\")\n    except Exception as e:\n        raise RuntimeError(f\"Error running command: {' '.join(full_command)} - {str(e)}\")", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n        if verbose and result.stderr:\n            print(result.stderr.strip())\n            \n        # Hide stderr if requested\n        if hide_stderr:\n            result.stderr = \"\"\n            \n        return result\n        \n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Command not found: {' '.join(full_command)}\")\n    except Exception as e:\n        raise RuntimeError(f\"Error running command: {' '.join(full_command)} - {str(e)}\")", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Construct the full command\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.SubprocessError as e:\n        raise e"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4164, "num_retrieved": 10, "inference_time": 1.3569748103618622, "generate_results": ["def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4116, "num_retrieved": 8, "inference_time": 1.3569748103618622, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4144, "num_retrieved": 9, "inference_time": 1.3569748103618622, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for root_dir, dirs, files in ocfl_walk(self.obj_fs, '/', is_storage_root=True):\n        # Check if this is an OCFL object directory\n        if 'inventory.json' in files:\n            # Validate the object\n            if validate_objects:\n                try:\n                    # Validate the object using the validator\n                    self.validate(root_dir)\n                    good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n            \n            num_objects += 1\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for root_dir, dirs, files in ocfl_walk(self.obj_fs, dir='/', is_storage_root=True):\n        # Check if this is an OCFL object directory\n        if 'inventory.json' in files:\n            # Validate the object\n            if validate_objects:\n                try:\n                    # Validate the object using the existing validation logic\n                    self.validate(root_dir)\n                    good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n            \n            num_objects += 1\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for root_path, dirs, files in ocfl_walk(self.obj_fs, '/', is_storage_root=True):\n        # Check if this is an OCFL object directory\n        if 'inventory.json' in files:\n            # Validate the object at this root\n            try:\n                # Validate the object structure and content\n                if validate_objects:\n                    self.validate(root_path)\n                \n                # Check if the object is valid (no errors)\n                if self.log.num_errors == 0:\n                    good_objects += 1\n                \n                num_objects += 1\n                \n                # If check_digests is True, validate digest files\n                if check_digests:\n                    # This would typically involve checking inventory and content digests\n                    # Implementation depends on specific digest validation logic\n                    pass\n                    \n            except Exception as e:\n                # Log any errors during validation\n                self.log.error('E002', error=str(e))\n                continue\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for root_path, dirs, files in ocfl_walk(self.obj_fs, dir='/', is_storage_root=True):\n        # Check for OCFL object declaration files\n        namastes = find_namastes(0, pyfs=self.obj_fs, dir=root_path)\n        \n        if len(namastes) == 0:\n            continue\n            \n        num_objects += 1\n        \n        # Validate the object if requested\n        if validate_objects:\n            try:\n                # Validate the object structure and content\n                self.validate(root_path)\n                good_objects += 1\n            except Exception as e:\n                self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for root_dir, dirs, files in ocfl_walk(self.obj_fs, '/', is_storage_root=True):\n        # Check if this is an OCFL object directory\n        if 'inventory.json' in files:\n            # Validate the object\n            if validate_objects:\n                try:\n                    # Validate the object using the validator\n                    self.validate(root_dir)\n                    good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n            \n            num_objects += 1\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4092, "num_retrieved": 7, "inference_time": 1.3569748103618622, "generate_results": ["def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the storage root with required directories and files\n    # This function sets up the basic structure of an OCFL storage root\n    # including the necessary directories and files as defined by the OCFL specification\n    \n    # Create the root directory structure\n    # The root should contain:\n    # - content/ directory for content files\n    # - inventory/ directory for inventory files\n    # - .ocfl/ directory for metadata and configuration files\n    \n    # Create content directory if it doesn't exist\n    if self.content_directory:\n        self.obj_fs.makedir(self.content_directory)\n    \n    # Create inventory directory\n    self.obj_fs.makedir('inventory')\n    \n    # Create .ocfl directory for metadata\n    self.obj_fs.makedir('.ocfl')\n    \n    # Create the initial inventory file\n    inventory_path = fs.path.join('inventory', 'inventory.json')\n    self.obj_fs.create_file(inventory_path, content='{\"version\": \"1.1\", \"objects\": []}')\n    \n    # Create the initial metadata file\n    metadata_path = fs.path.join('.ocfl', 'metadata.json')\n    self.obj_fs.create_file(metadata_path, content='{\"spec_version\": \"1.1\", \"digest_algorithm\": \"sha512\"}')\n    \n    # Set the object's specification version\n    self.spec_version = self.spec_version or '1.1'\n    \n    # Set the digest algorithm if not already set\n    if self.digest_algorithm is None:\n        self.digest_algorithm = 'sha512'\n    \n    # Initialize the object's ID if not set\n    if self.id is None:\n        self.id = self.generate_id()\n    \n    # Initialize the root inventory validator\n    self.root_inv_validator = InventoryValidator(\n        log=self.log,\n        check_digests=self.check_digests,\n        lax_digests=self.lax_digests\n    )\n    \n    # Initialize the object's inventory digest files\n    self.inventory_digest_files = []\n    \n    # Set the content directory\n    self.content_directory = self.content_directory or 'content'\n    \n    # Log initialization completion\n    if self.log:\n        self.log.info(\"OCFL storage root initialized successfully\")", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Create the root directory structure for OCFL storage\n    root_dir = self.content_directory\n    if self.obj_fs is None:\n        # If no filesystem is provided, create a new one\n        # This would typically involve creating the necessary directories\n        # and setting up the required files (e.g., inventory, namaste files)\n        pass\n    \n    # Initialize the root with required OCFL structure\n    # This includes creating the content directory and setting up\n    # the necessary metadata files (like the inventory and namaste files)\n    \n    # Set up the inventory digest files\n    self.inventory_digest_files = []\n    \n    # Initialize the root inventory validator\n    self.root_inv_validator = None\n    \n    # Set up the object filesystem with the required structure\n    # This would typically involve creating the content directory\n    # and ensuring the necessary OCFL files are present\n    \n    # Initialize the digest algorithm if not already set\n    if self.digest_algorithm is None:\n        self.digest_algorithm = 'sha512'\n    \n    # Set up the specification version\n    if self.spec_version is None:\n        self.spec_version = '1.1'\n    \n    # Initialize the object ID if not already set\n    if self.id is None:\n        self.id = self.generate_id()\n    \n    # Create the root structure with required directories\n    # This would typically involve creating the content directory\n    # and setting up the necessary metadata files\n    \n    # Initialize the root with the required OCFL structure\n    # This includes creating the content directory and setting up\n    # the necessary metadata files (like the inventory and namaste files)\n    \n    # Set up the root inventory validator\n    self.root_inv_validator = self.create_inventory_validator()\n    \n    # Initialize the object filesystem with the required structure\n    # This would typically involve creating the content directory\n    # and ensuring the necessary OCFL files are present\n    \n    # Set up the object with the required metadata\n    # This includes setting up the digest algorithm, specification version,\n    # and other required parameters\n    \n    # Initialize the object with the required structure\n    # This includes creating the content directory and setting up\n    # the necessary metadata files (like the inventory and namaste files)\n    \n    # Set up the root with the required OCFL structure\n    # This includes creating the content directory and setting up\n    # the necessary metadata", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the storage root with required directories and files\n    # This function sets up the basic structure of an OCFL storage root\n    # including the necessary directories and files as defined by the OCFL specification\n    \n    # Create the root directory structure\n    root_dir = self.obj_fs.getdir('/')\n    \n    # Ensure the required directories exist\n    required_dirs = ['inventory', 'content', 'metadata']\n    for dir_name in required_dirs:\n        dir_path = fs.path.join('/', dir_name)\n        if not self.obj_fs.exists(dir_path):\n            self.obj_fs.makedir(dir_path)\n    \n    # Create the inventory directory with the required files\n    inventory_dir = fs.path.join('/', 'inventory')\n    if not self.obj_fs.exists(inventory_dir):\n        self.obj_fs.makedir(inventory_dir)\n    \n    # Create the 0= Namaste file to declare the root as an OCFL storage root\n    namaste_file = fs.path.join('/', '0=ocfl')\n    if not self.obj_fs.exists(namaste_file):\n        self.obj_fs.write(namaste_file, 'ocfl')\n    \n    # Set the specification version if not already set\n    if self.spec_version is None:\n        self.spec_version = '1.1'\n    \n    # Set the digest algorithm if not already set\n    if self.digest_algorithm is None:\n        self.digest_algorithm = 'sha512'\n    \n    # Initialize the root inventory with a basic structure\n    inventory_file = fs.path.join('/', 'inventory', 'inventory.json')\n    if not self.obj_fs.exists(inventory_file):\n        # Create a minimal inventory file\n        inventory_content = {\n            \"version\": self.spec_version,\n            \"digest_algorithm\": self.digest_algorithm,\n            \"object_id\": self.id or \"unknown\"\n        }\n        self.obj_fs.write(inventory_file, json.dumps(inventory_content, indent=2))\n    \n    # Initialize the object metadata\n    metadata_file = fs.path.join('/', 'metadata', 'metadata.json')\n    if not self.obj_fs.exists(metadata_file):\n        metadata_content = {\n            \"spec_version\": self.spec_version,\n            \"digest_algorithm\": self.digest_algorithm,\n            \"created\": datetime.utcnow().isoformat(),\n            \"modified\": datetime.utcnow().isoformat()\n        }\n        self.obj_fs.write(metadata_file, json.dumps(metadata_content, indent=2))\n    \n    #", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Create the root directory structure for OCFL storage\n    root_dir = self.content_directory\n    if self.obj_fs is None:\n        # If no filesystem is provided, create a new one\n        # This would typically involve creating the necessary directories\n        # such as content, inventory, and metadata directories\n        pass\n    \n    # Initialize the root with required directories\n    # This includes creating the content directory and inventory directory\n    # and setting up the necessary metadata\n    \n    # Set up the inventory digest files\n    self.inventory_digest_files = []\n    \n    # Initialize the root inventory validator\n    self.root_inv_validator = None\n    \n    # Set the object's specification version\n    self.spec_version = self.spec_version or '1.1'\n    \n    # Set the digest algorithm\n    self.digest_algorithm = self.digest_algorithm or 'sha512'\n    \n    # Initialize the object's ID if not already set\n    if self.id is None:\n        self.id = self.generate_id()\n    \n    # Initialize the object's content directory\n    self.content_directory = self.content_directory or 'content'\n    \n    # Initialize the object's fixity\n    self.fixity = self.fixity or []\n    \n    # Set up the object's filesystem\n    if self.obj_fs is None:\n        # Create a new filesystem instance\n        # This would typically involve creating the root directory\n        # and setting up the necessary structure\n        pass\n    \n    # Initialize the object's metadata\n    self.metadata = {}\n    \n    # Initialize the object's version tracking\n    self.versions = {}\n    \n    # Initialize the object's file tracking\n    self.src_files = {}\n    \n    # Initialize the object's validation state\n    self.valid = True", "def initialize(self):\n    \"\"\"\n    Create and initialize a new OCFL storage root.\n    \"\"\"\n    # Initialize the storage root with required directories and files\n    # This function sets up the basic structure of an OCFL storage root\n    # including the necessary directories and files as defined by the OCFL specification\n    \n    # Create the root directory structure\n    root_dir = self.obj_fs.root\n    required_dirs = ['inventory', 'content', 'metadata', 'versions']\n    \n    # Create required directories if they don't exist\n    for dir_name in required_dirs:\n        dir_path = fs.path.join(root_dir, dir_name)\n        try:\n            self.obj_fs.makedir(dir_path)\n        except Exception as e:\n            # Log error if directory creation fails\n            if self.log:\n                self.log.error(f\"Failed to create directory {dir_path}: {str(e)}\")\n    \n    # Create the initial inventory file\n    inventory_path = fs.path.join(root_dir, 'inventory', 'inventory.json')\n    try:\n        # Create a basic inventory structure\n        inventory_data = {\n            \"version\": self.spec_version,\n            \"digest_algorithm\": self.digest_algorithm,\n            \"created\": datetime.utcnow().isoformat(),\n            \"modified\": datetime.utcnow().isoformat(),\n            \"object_id\": self.id\n        }\n        # Write inventory data to file\n        with self.obj_fs.open(inventory_path, 'w') as f:\n            json.dump(inventory_data, f, indent=2)\n    except Exception as e:\n        if self.log:\n            self.log.error(f\"Failed to create inventory file at {inventory_path}: {str(e)}\")\n    \n    # Create the initial metadata file\n    metadata_path = fs.path.join(root_dir, 'metadata', 'metadata.json')\n    try:\n        metadata_data = {\n            \"spec_version\": self.spec_version,\n            \"digest_algorithm\": self.digest_algorithm,\n            \"created\": datetime.utcnow().isoformat(),\n            \"modified\": datetime.utcnow().isoformat(),\n            \"object_id\": self.id\n        }\n        with self.obj_fs.open(metadata_path, 'w') as f:\n            json.dump(metadata_data, f, indent=2)\n    except Exception as e:\n        if self.log:\n            self.log.error(f\"Failed to create metadata file at {metadata_path}: {str(e)}\")\n    \n    # Create the initial version directory\n    versions_path = fs.path.join(root_dir, 'versions', '1')\n    try:"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4076, "num_retrieved": 6, "inference_time": 1.3569748103618622, "generate_results": ["def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if it's zero-prefixed by checking if it has leading zeros\n        # We'll assume the pattern is consistent with the existing versions\n        # If we have a zero-prefixed version, we need to find the padding size\n        # by checking the existing versions\n        \n        # For simplicity, if we have a non-zero-prefixed version, just increment\n        # If we have a zero-prefixed version, we need to determine the padding\n        # size from the existing versions (not available here)\n        \n        # Since we don't have access to the full version list, we'll assume\n        # the pattern is just simple incrementation\n        next_num = version_num + 1\n        return f'v{next_num}'\n    \n    return version  # Fallback", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if it's zero-prefixed by checking if it has leading zeros\n        # We need to find the padding size from existing versions\n        # For simplicity, if we have v1 and v2, we assume non-zero-prefixed\n        # If we have v002, v003, etc., we assume zero-prefixed\n        \n        # Since we don't have access to the full version list, we'll assume\n        # the pattern is consistent and just increment the number\n        next_num = version_num + 1\n        # Return with the same padding as the input\n        return f'v{next_num:0*d}'.format(d=len(match.group(1)))\n    \n    # If version doesn't match v<number> pattern, return v2 as fallback\n    return 'v2'", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if it's zero-prefixed by checking if it has leading zeros\n        # We need to find the padding size from existing versions\n        # For simplicity, if we have v1, we know the pattern is non-zero-prefixed\n        # Otherwise, we assume the pattern is zero-prefixed with the same padding\n        # Since we don't have access to the full version list, we'll just increment\n        # and return the next version with the same padding\n        \n        # If version is v2 or higher, we assume it's part of a zero-prefixed sequence\n        # and we need to determine the padding size\n        # For now, we'll just increment the number and return with the same padding\n        # This is a simplified implementation\n        \n        # Try to determine padding size by checking if the version has leading zeros\n        # If it does, we assume the pattern is zero-prefixed\n        # If not, we just increment normally\n        \n        # For simplicity, we'll just increment the number\n        next_num = version_num + 1\n        # Return with the same padding as the input version\n        # This is a simplified implementation that doesn't fully handle zero-padding\n        # in all cases\n        return f'v{next_num:0{len(match.group(1))}d}'\n    \n    # If version doesn't match v<number> pattern, return v2 as fallback\n    return 'v2'", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if zero-prefixed by checking if version has leading zeros\n        # Find the maximum length of any version in the sequence\n        # For now, assume the pattern is consistent and just increment\n        return f'v{version_num + 1}'\n    \n    return version + '1'", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    if version == 'v1':\n        return 'v2'\n    \n    # Check if version is zero-prefixed (e.g., v002, v003)\n    match = re.match(r'v(\\d+)', version)\n    if match:\n        version_num = int(match.group(1))\n        # Determine if it's zero-prefixed by checking if it has leading zeros\n        # We'll assume the pattern is consistent with the existing versions\n        # If we have a zero-prefixed version, we need to find the padding size\n        # by checking the existing versions for the pattern\n        \n        # For simplicity, if we have a non-zero-prefixed version, just increment\n        # If we have a zero-prefixed version, we need to determine the padding\n        # size from the existing versions\n        \n        # Since we don't have access to the full context of existing versions,\n        # we'll assume the pattern is consistent and just increment the number\n        next_num = version_num + 1\n        \n        # Return the version with the same padding as the input\n        # If the input version has leading zeros, we preserve that pattern\n        # For example, v002 -> v003, v003 -> v004, etc.\n        \n        # Convert next_num to string and pad with zeros to match the length of version_num\n        # But we don't know the padding size from just one version\n        # So we'll just return the next number in sequence\n        \n        # This is a simplified implementation - in a real scenario,\n        # we would need to analyze the existing version patterns to determine\n        # the correct padding size\n        \n        # For now, we'll just return the next version number\n        return f'v{next_num}'\n    \n    return f'v{int(version[1:]) + 1}'"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4047, "num_retrieved": 5, "inference_time": 1.3569748103618622, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume we have access to inventory data for each version\n    # We'll validate that each version has an inventory and track digests that differ from root\n    \n    # Start with root inventory (version_dirs[0] is v1)\n    root_inventory = self.inventory\n    \n    # Track digests that differ from root inventory\n    differing_digests = {}\n    \n    # Process each version in sequence\n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        if vdir not in self.inventory['versions']:\n            self.error(\"E012\", version=vdir)\n            return\n        \n        # Get the version inventory\n        version_inventory = self.inventory['versions'][vdir]\n        \n        # Check if the version has a manifest and fixity blocks\n        if 'manifest' not in version_inventory or 'fixity' not in version_inventory:\n            self.error(\"E013\", version=vdir)\n            return\n        \n        # Compare manifest digests with root inventory\n        root_manifest = root_inventory['manifest']\n        version_manifest = version_inventory['manifest']\n        \n        # Track digests that are new in this version\n        new_digests = set(version_manifest.keys()) - set(root_manifest.keys())\n        \n        # Track digests that are different in content paths\n        for digest in new_digests:\n            differing_digests[digest] = version_manifest[digest]\n        \n        # Check if any digests in the version manifest are missing from root\n        for digest in version_manifest:\n            if digest not in root_manifest:\n                differing_digests[digest] = version_manifest[digest]\n        \n        # Check if any digests in root are missing in version\n        for digest in root_manifest:\n            if digest not in version_manifest:\n                # This is a deletion, which is not allowed in OCFL\n                self.error(\"E014\", version=vdir, digest=digest)\n                return\n    \n    # Return the record of differing digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume we have access to the inventory structure through self.inventory\n    # and we can validate each version's inventory against the root inventory\n    \n    root_inventory = self.inventory\n    \n    # Track digests that differ from root inventory\n    differing_digests = {}\n    \n    for vdir in version_dirs:\n        # Validate that this version has an inventory\n        if vdir not in root_inventory['versions']:\n            self.error(\"E065\", version=vdir)\n            continue\n            \n        # Get the version inventory\n        version_inventory = root_inventory['versions'][vdir]\n        \n        # Check if the version inventory has a manifest and state\n        if 'manifest' not in version_inventory or 'state' not in version_inventory:\n            self.error(\"E064\", version=vdir)\n            continue\n            \n        # Compare manifest digests with root inventory\n        root_manifest = root_inventory['manifest']\n        version_manifest = version_inventory['manifest']\n        \n        # Track digests that are new in this version\n        new_digests = set()\n        for digest, paths in version_manifest.items():\n            if digest not in root_manifest:\n                new_digests.add(digest)\n        \n        # Track digests that are removed in this version\n        removed_digests = set()\n        for digest in root_manifest.keys():\n            if digest not in version_manifest:\n                removed_digests.add(digest)\n        \n        # Record differing digests\n        if new_digests:\n            differing_digests[vdir] = {'added': new_digests}\n        if removed_digests:\n            differing_digests[vdir] = {'removed': removed_digests}\n        \n        # Validate state blocks (this is a simplified check)\n        # In a real implementation, we would compare state blocks more thoroughly\n        if 'state' in version_inventory:\n            # Check that state entries are consistent with manifest\n            for digest, file_paths in version_inventory['state'].items():\n                if digest not in version_manifest:\n                    self.warning(\"", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume we have access to inventory data for each version\n    # We'll validate that each version has an inventory and track digests that differ from root\n    \n    # Start with root inventory (version_dirs[0] is v1)\n    root_inventory = self.inventory\n    \n    # Track digests that differ from root inventory\n    differing_digests = {}\n    \n    # Validate each version in sequence\n    for vdir in version_dirs:\n        # Check if inventory exists for this version\n        if vdir not in self.inventory['versions']:\n            self.error(\"E012\", version=vdir)\n            return\n        \n        # Get the version inventory\n        version_inventory = self.inventory['versions'][vdir]\n        \n        # Check if the version inventory has a manifest and fixity blocks\n        if 'manifest' not in version_inventory or 'fixity' not in version_inventory:\n            self.error(\"E013\", version=vdir)\n            return\n        \n        # Compare manifest digests with root inventory\n        root_manifest = root_inventory['manifest']\n        version_manifest = version_inventory['manifest']\n        \n        # Track digests that are new in this version\n        new_digests = set()\n        for digest, paths in version_manifest.items():\n            if digest not in root_manifest:\n                new_digests.add(digest)\n        \n        # Track digests that are removed in this version (not in root)\n        removed_digests = set()\n        for digest in root_manifest:\n            if digest not in version_manifest:\n                removed_digests.add(digest)\n        \n        # Record differing digests\n        if new_digests:\n            differing_digests.update(new_digests)\n        \n        # If there are removed digests, this may indicate a deletion\n        # which is not allowed in OCFL (unless handled by forward deltas)\n        if removed_digests:\n            self.warning(\"W012\", version=vdir, removed_digests=','.join(removed_digests))\n    \n    # Return the", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume the first version is the root inventory\n    root_version = version_dirs[0]\n    \n    # Validate that each version has an inventory\n    for vdir in version_dirs:\n        if vdir == root_version:\n            continue  # Root version is assumed to have inventory\n            \n        # Check if inventory exists for this version\n        # This would typically involve checking the presence of inventory files\n        # For now, we'll assume the existence is validated elsewhere\n        # and just track digests that differ from root\n        \n        # For each version, compare content digests with root inventory\n        # and record differences\n        pass\n    \n    # Record any content digests different from root inventory\n    # This would require accessing the actual inventory data\n    # For now, we'll just return an empty list as placeholder\n    return []", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Assume we have access to inventory data for each version\n    # We'll validate that each version has an inventory and track digests that differ from root\n    \n    # Start with the first version as the root inventory\n    root_inventory = None\n    version_digest_differences = {}\n    \n    for vdir in version_dirs:\n        # Get the inventory for this version (this would normally be loaded from storage)\n        # For this implementation, we assume the inventory is available\n        inventory = self.get_inventory(vdir)  # This method would need to be implemented\n        \n        if not inventory:\n            self.error(\"E009\", version=vdir)\n            return\n            \n        if root_inventory is None:\n            # First version - set as root\n            root_inventory = inventory\n            continue\n            \n        # Validate that the current version's inventory includes all previous versions\n        # and that the version sequence is maintained\n        \n        # Check if the current version has a valid version directory structure\n        if 'versions' not in inventory:\n            self.error(\"E012\", version=vdir)\n            return\n            \n        # Check if the current version includes all previous versions in its versions block\n        current_versions = set(inventory['versions'].keys())\n        previous_versions = set(self.get_previous_versions(version_dirs, vdir))\n        \n        if not previous_versions.issubset(current_versions):\n            self.error(\"E013\", version=vdir, missing_versions=previous_versions - current_versions)\n            return\n            \n        # Compare content digests between root and current version\n        # Get the manifest from root and current version\n        root_manifest = root_inventory.get('manifest', {})\n        current_manifest = inventory.get('manifest', {})\n        \n        # Track digests that differ from root\n        for digest, paths in current_manifest.items():\n            if digest not in root_manifest:\n                # This digest is new in current version\n                version_digest_differences[vdir] = version_digest_differences.get(vdir, []) +"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4072, "num_retrieved": 6, "inference_time": 1.3569748103618622, "generate_results": ["def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\"\"\"\n    try:\n        pyfs = open_fs(path)\n    except Exception:\n        return 'error: cannot open path'\n    \n    namastes = find_namastes(0, pyfs=pyfs)\n    \n    if len(namastes) == 0:\n        return 'error: no 0=* declaration file found'\n    \n    if len(namastes) > 1:\n        return 'error: multiple 0=* declaration files found'\n    \n    namaste = namastes[0]\n    declaration = namaste.filename\n    \n    # Check if it's a root (storage root) - should have a specific declaration\n    if declaration.startswith('0=ocfl_object_'):\n        # Check if it's a root or an object\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', declaration)\n        if match:\n            version = match.group(1)\n            if version == '1.0':\n                return 'root'\n            else:\n                return 'object'\n        else:\n            return 'error: invalid 0= declaration format'\n    \n    return 'file'", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\"\"\"\n    try:\n        pyfs = open_fs(path)\n    except Exception:\n        return 'error: cannot open path'\n    \n    namastes = find_namastes(0, pyfs=pyfs)\n    \n    if len(namastes) == 0:\n        return 'error: no 0=* declaration file found'\n    \n    if len(namastes) > 1:\n        return 'error: multiple 0=* declaration files found'\n    \n    namaste = namastes[0]\n    declaration = namaste.filename\n    \n    # Check if it's a root (storage root) - should have a specific structure\n    if declaration.startswith('0=ocfl_object_'):\n        # Check if it's a root or object\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', declaration)\n        if match:\n            version = match.group(1)\n            if version == '1.0':\n                return 'root'\n            else:\n                return 'object'\n    \n    # If it's not a 0=ocfl_object_ file, it might be an inventory file\n    if declaration.startswith('0='):\n        return 'file'\n    \n    return 'error: invalid 0=* declaration file'", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        pyfs = open_fs(path)\n    except Exception:\n        return 'error: cannot open path'\n    \n    namastes = find_namastes(0, pyfs=pyfs)\n    \n    if len(namastes) == 0:\n        return 'error: no 0=* declaration file found'\n    \n    if len(namastes) > 1:\n        return 'error: multiple 0=* declaration files found'\n    \n    declaration = namastes[0]\n    declaration_name = declaration.filename\n    \n    # Check if it's a root (storage root) - should have a specific structure\n    if declaration_name.startswith('0=ocfl_object_') and declaration_name.endswith('.json'):\n        # This is likely a root declaration\n        return 'root'\n    \n    # Check if it's an object - should have a 0=ocfl_object_x.x declaration\n    match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', declaration_name)\n    if match:\n        # This is an object declaration\n        return 'object'\n    \n    # If it's a file but not a valid 0= declaration\n    if declaration_name.startswith('0='):\n        return 'file'\n    \n    # Otherwise, it's not a valid namaste file\n    return 'error: invalid 0=* file name'", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.\"\"\"\n    try:\n        pyfs = open_fs(path)\n    except Exception:\n        return 'error: cannot open path'\n    \n    namastes = find_namastes(0, pyfs=pyfs)\n    \n    if len(namastes) == 0:\n        return 'error: no 0=* declaration file found'\n    \n    if len(namastes) > 1:\n        return 'error: multiple 0=* declaration files found'\n    \n    namaste = namastes[0]\n    declaration = namaste.filename\n    \n    # Check if it's a root (storage root) - should have a specific structure\n    if declaration.startswith('0=ocfl_object_'):\n        # Check if it's a root declaration (no version)\n        if '1.0' in declaration:\n            return 'root'\n        \n        # Check if it's an object declaration with version\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', declaration)\n        if match:\n            version = match.group(1)\n            if version == '1.0':\n                return 'object'\n            else:\n                return 'object'\n    \n    # If it's not a 0=ocfl_object_ file, it's likely a file\n    if declaration.startswith('0='):\n        return 'file'\n    \n    return 'error: invalid 0=* declaration file'", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        pyfs = open_fs(path)\n    except Exception:\n        return 'error: cannot open path'\n    \n    namastes = find_namastes(0, pyfs=pyfs)\n    \n    if len(namastes) == 0:\n        return 'error: no 0=* declaration file found'\n    \n    if len(namastes) > 1:\n        return 'error: multiple 0=* declaration files found'\n    \n    declaration = namastes[0]\n    filename = declaration.filename\n    \n    # Check if it's a root (storage root) - should have a specific structure\n    if filename.startswith('0=ocfl_object_') and 'inventory.json' in filename:\n        # This could be a root or object, need to check structure\n        # But if it's a root, it should have specific files\n        # Check if it's a root by looking at the path structure\n        # If we can't determine, assume it's an object\n        pass\n    \n    # Check if it's an object (has 0=ocfl_object_x.x declaration)\n    match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', filename)\n    if match:\n        version = match.group(1)\n        if version == '1.0':\n            # This is a valid object declaration\n            return 'object'\n        else:\n            # This is a versioned object\n            return 'object'\n    \n    # If it's a file (like inventory.json), it's a file\n    if filename.endswith('.json'):\n        return 'file'\n    \n    # Otherwise, it's not a valid declaration\n    return 'error: invalid 0=* declaration file'"]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4244, "num_retrieved": 11, "inference_time": 1.3569748103618622, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        # Add a field to filter by bug ID\n        n = int(utils.get_last_field_num(params))\n        params[f\"f{n+1}\"] = \"id\"\n        params[f\"o{n+1}\"] = \"exact\"\n        params[f\"v{n+1}\"] = bug_ids", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        # Add the bug IDs to the params if provided\n        n = int(utils.get_last_field_num(params))\n        params[f\"f{n+1}\"] = \"bug_id\"\n        params[f\"o{n+1}\"] = \"exact\"\n        params[f\"v{n+1}\"] = bug_ids", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        n = int(utils.get_last_field_num(params))\n        params[f\"f{n+1}\"] = \"bug_id\"\n        params[f\"o{n+1}\"] = \"in\"\n        params[f\"v{n+1}\"] = bug_ids", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        n = int(utils.get_last_field_num(params))\n        params[f\"f{n+1}\"] = \"bug_id\"\n        params[f\"o{n+1}\"] = \"in\"\n        params[f\"v{n+1}\"] = bug_ids", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    if bug_ids:\n        n = int(utils.get_last_field_num(params))\n        params[f\"f{n+1}\"] = \"bug_id\"\n        params[f\"o{n+1}\"] = \"exact\"\n        params[f\"v{n+1}\"] = bug_ids"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4239, "num_retrieved": 11, "inference_time": 1.3569748103618622, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4129, "num_retrieved": 7, "inference_time": 1.3569748103618622, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This function is not directly implementable as described in the context\n    # because the context does not contain the actual implementation of parse_arguments\n    # or the necessary parser setup. The context shows related functions like\n    # parse_subparser_arguments, make_flags_from_arguments, and main, but not parse_arguments.\n    # Without the actual parser definition or setup, a complete implementation cannot be provided.\n    raise NotImplementedError(\"parse_arguments implementation not available in context\")", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This function is not fully implemented in the provided context.\n    # Based on the context, it appears that argument parsing is handled by other functions\n    # such as parse_subparser_arguments and parse_arguments in the borgmatic module.\n    # Since the exact implementation of parse_arguments is not provided in the context,\n    # we return a placeholder implementation that would typically be found in the actual codebase.\n    \n    # The actual implementation would likely involve creating an ArgumentParser,\n    # setting up subparsers, and parsing the arguments accordingly.\n    \n    # Placeholder implementation - in a real scenario, this would be implemented\n    # using the actual argument parser setup from the borgmatic codebase.\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--no-color', action='store_true')\n    parser.add_argument('--verbosity', type=int, default=0)\n    parser.add_argument('--syslog-verbosity', type=int, default=0)\n    parser.add_argument('--log-file-verbosity', type=int, default=0)\n    parser.add_argument('--monitoring-verbosity', type=int, default=0)\n    parser.add_argument('--log-file', help='Log file path')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action')\n    \n    # Example subparsers\n    list_parser = subparsers.add_parser('list')\n    list_parser.add_argument('--archive', help='Archive name')\n    list_parser.add_argument('--paths', nargs='*', help='Paths to list')\n    list_parser.add_argument('--json', action='store_true')\n    \n    extract_parser = subparsers.add_parser('extract')\n    extract_parser.add_argument('--archive', help='Archive name')\n    extract_parser.add_argument('--paths', nargs='*', help='Paths to extract')\n    extract_parser.add_argument('--json', action='store_true')\n    \n    # Parse the arguments\n    parsed_args = parser.parse_args(arguments)\n    \n    # Return the parsed arguments as a namespace\n    return parsed_args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This function is not directly implementable as stated in the docstring\n    # because it references an ArgumentParser instance, but the context does not\n    # provide the actual implementation of the argument parsing logic.\n    # The context shows functions like parse_subparser_arguments and main,\n    # but not the actual parse_arguments function.\n    # Therefore, based on the available context, we cannot fully implement\n    # this function as requested.\n    raise NotImplementedError(\"parse_arguments implementation not available in context\")", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This function is not directly implementable as stated in the docstring\n    # because it references an ArgumentParser instance, but the context does not\n    # provide the actual implementation of the argument parsing logic.\n    # The context shows functions like parse_subparser_arguments and main,\n    # but not a complete implementation of parse_arguments.\n    # Therefore, based on the available context, we cannot fully implement\n    # this function as required.\n    raise NotImplementedError(\"Implementation not available in provided context\")", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This function is not fully implemented in the context provided.\n    # Based on the context, it appears that argument parsing is handled by other functions\n    # such as parse_subparser_arguments and parse_arguments in the borgmatic module.\n    # Since the exact implementation of parse_arguments is not available in the context,\n    # we return a placeholder that would typically be implemented using argparse.\n    \n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='action')\n    \n    # Add common arguments\n    common_parser = parser.add_argument_group('common arguments')\n    common_parser.add_argument('--repository', help='Repository path')\n    common_parser.add_argument('--config', help='Configuration file path')\n    common_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    common_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different actions\n    prune_parser = subparsers.add_parser('prune', help='Prune archives')\n    compact_parser = subparsers.add_parser('compact', help='Compact archives')\n    create_parser = subparsers.add_parser('create', help='Create archives')\n    check_parser = subparsers.add_parser('check', help='Check archives')\n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    mount_parser = subparsers.add_parser('mount', help='Mount archives')\n    list_parser = subparsers.add_parser('list', help='List archives')\n    rlist_parser = subparsers.add_parser('rlist', help='List archives with repository')\n    \n    # Add common arguments to each subparser\n    for parser in [prune_parser, compact_parser, create_parser, check_parser, extract_parser, mount_parser, list_parser, rlist_parser]:\n        parser.add_argument('--archive', help='Archive name')\n        parser.add_argument('--paths', nargs='+', help='Paths to list')\n        parser.add_argument('--json', action='store_true', help='Output in JSON format')\n        parser.add_argument('--no-color', action='store_true', help='Disable color output')\n        parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    \n    # Parse the arguments\n    try:"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4310, "num_retrieved": 13, "inference_time": 1.3569748103618622, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    import argparse\n    from argparse import ArgumentParser\n    \n    # Get all action groups and arguments\n    actions = parser._actions\n    \n    flags = []\n    for action in actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    \n    # Remove duplicates and sort\n    unique_flags = sorted(set(flags))\n    \n    return ' '.join(unique_flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    import argparse\n    from argparse import _StoreAction\n    \n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                for option_string in action.option_strings:\n                    if option_string.startswith('--'):\n                        flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    import argparse\n    from argparse import ArgumentParser\n    \n    # Get all action groups and arguments\n    actions = parser._actions\n    flags = []\n    \n    for action in actions:\n        if isinstance(action, argparse._StoreAction):\n            # For store actions, add the flag\n            if action.option_strings:\n                for option_string in action.option_strings:\n                    flags.append(option_string)\n        elif isinstance(action, argparse._AppendAction):\n            # For append actions, add the flag\n            if action.option_strings:\n                for option_string in action.option_strings:\n                    flags.append(option_string)\n    \n    # Join all flags with spaces\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    import sys\n    from argparse import ArgumentParser\n    \n    # Get all action names and their help texts\n    actions = parser._actions\n    flags = []\n    \n    for action in actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                # Convert option string to flag format\n                if option_string.startswith('--'):\n                    flags.append(option_string)\n    \n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    import argparse\n    from argparse import _StoreAction\n    \n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                for option_string in action.option_strings:\n                    if option_string.startswith('--'):\n                        flags.append(option_string)\n    return ' '.join(flags)"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4125, "num_retrieved": 7, "inference_time": 1.3569748103618622, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--config-paths', nargs='+', default=['~/.borgmatic/config'], help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Path to log file')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different actions\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--paths', nargs='+', help='Paths to backup')\n    \n    extract_parser = subparsers.add_parser('extract', help='Extract a backup')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name')\n    extract_parser.add_argument('--paths', nargs='+', help='Paths to extract')\n    \n    mount_parser = subparsers.add_parser('mount', help='Mount a backup')\n    mount_parser.add_argument('--repository', help='Repository path')\n    mount_parser.add_argument('--archive', help='Archive name')\n    mount", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--config-path', action='append', help='Path to configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Path to log file')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different actions\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument('repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--paths', nargs='+', help='Paths to include in archive')\n    \n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    extract_parser.add_argument('repository', help='Repository path')\n    extract_parser.add_argument('archive', help='Archive name')\n    extract_parser.add_argument('--paths', nargs='+', help='Paths to extract')\n    \n    mount_parser = subparsers.add_parser('mount', help='Mount an archive')\n    mount_parser.add_argument('repository', help='Repository path')\n    mount_parser.add_argument('archive', help='Archive name')\n    mount_parser.add_argument('--mount-point", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--config-path', action='append', help='Path to a configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for each action\n    # Example: create parser\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--paths', nargs='+', help='Paths to backup')\n    create_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Example: extract parser\n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name')\n    extract_parser.add_argument('--paths', nargs='+', help='Paths to extract')\n    extract_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    #", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--config-path', action='append', help='Path to configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Path to log file')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different actions\n    # Example: create parser\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--paths', nargs='+', help='Paths to backup')\n    create_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Example: extract parser\n    extract_parser = subparsers.add_parser('extract', help='Extract files from archive')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name')\n    extract_parser.add_argument('--paths', nargs='+', help='Paths to extract')\n    extract_parser.add_argument('--json', action='store_true', help='Output in JSON format", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='command', help='sub-command help')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--config-path', action='append', help='Configuration file path')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different commands\n    # Example: create subparser for 'create'\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--paths', nargs='+', help='Paths to backup')\n    create_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Example: extract subparser\n    extract_parser = subparsers.add_parser('extract', help='Extract archives')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name')\n    extract_parser.add_argument('--paths', nargs='+', help='Paths to extract')\n    extract_parser.add_argument('--json', action='store_true', help='Output in JSON format"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4125, "num_retrieved": 7, "inference_time": 1.422885149717331, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for action_name, subparser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_args)\n            arguments[action_name] = parsed_args\n        except SystemExit:\n            # If parsing fails, break to avoid further processing\n            break\n    \n    return arguments, remaining_args", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for action_name, subparser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_args)\n            arguments[action_name] = parsed_args\n        except SystemExit:\n            # If parsing fails, break and return what we have\n            break\n    \n    return arguments, remaining_args", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for action_name, subparser in subparsers.items():\n        # Parse the arguments using the subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_args)\n            arguments[action_name] = parsed_args\n        except SystemExit:\n            # If parsing fails, break and return what we have\n            break\n    \n    return arguments, remaining_args", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for action_name, subparser in subparsers.items():\n        # Parse the arguments using the subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_args)\n            arguments[action_name] = parsed_args\n        except SystemExit:\n            # If parsing fails, break and return what we have\n            break\n    \n    return arguments, remaining_args", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_args = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for action_name, subparser in subparsers.items():\n        # Parse the arguments using the subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_args)\n            arguments[action_name] = parsed_args\n            # Remove the parsed arguments from remaining_args\n            remaining_args = remaining_args[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If parsing fails, break out and return what we have\n            break\n    \n    return arguments, remaining_args"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4140, "num_retrieved": 9, "inference_time": 1.422885149717331, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.borg.flags as flags\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool to manage Borg backups',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global flags\n    parser.add_argument('--version', action='version', version='borgmatic ' + pkg_resources.require('borgmatic')[0].version)\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add 'list' command\n    list_parser = subparsers.add_parser('list', help='List archives or paths in a repository')\n    list_parser.add_argument('archive', nargs='?', help='Archive to list')\n    list_parser.add_argument('paths', nargs='*', help='Paths to list within an archive')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    list_parser.add_argument('--info', action='store_true', help='Show info about the operation')\n    list_parser.add_argument('--debug', action='store_true', help='Enable debug output')\n    list_parser.add_argument('--show-rc', action='store_true', help='Show repository configuration')\n    list_parser.add_argument('--remote-path', help='Remote path to use')\n    list_parser.add_argument('--lock-wait', type=int, help='Wait time for lock')\n    \n    # Add 'rlist' command\n    rlist_parser = subparsers.add_parser('rlist', help='List archives with a repository')\n    rlist_parser.add_argument('prefix', nargs='?', help='Prefix to match archives')\n    rlist_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    rlist_parser.add_argument('--info', action='store_true', help='Show info about the operation')\n    rlist_parser.add_argument('--debug', action='store_true', help='Enable debug output')\n    rlist_parser.add_argument('--show-rc', action='store_true', help='Show repository configuration')\n    rlist_parser.add_argument('--remote-path', help='Remote path to use')\n    rlist_parser.add_argument('--lock-wait', type=int, help='", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.borgmatic as borgmatic\n    import borgmatic.commands.borg as borg\n    import borgmatic.commands.info as info\n    import borgmatic.commands.list as list_cmd\n    import borgmatic.commands.rlist as rlist_cmd\n    import borgmatic.commands.create as create\n    import borgmatic.commands.prune as prune\n    import borgmatic.commands.restore as restore\n    import borgmatic.commands.backup as backup\n    import borgmatic.commands.hook as hook\n    import borgmatic.commands.version as version\n\n    parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool')\n    \n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add subparsers for each command\n    subparsers.add_parser('backup', help='Create a backup')\n    subparsers.add_parser('restore', help='Restore a backup')\n    subparsers.add_parser('create', help='Create a backup')\n    subparsers.add_parser('prune', help='Prune old backups')\n    subparsers.add_parser('list', help='List archives')\n    subparsers.add_parser('rlist', help='List archives with a repository')\n    subparsers.add_parser('info', help='Display information about archives')\n    subparsers.add_parser('version', help='Show version information')\n    subparsers.add_parser('hook', help='Execute hooks')\n    \n    # Add global flags\n    global_parser = parser.add_argument_group('global options')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--config-path', help='Path to configuration file')\n    global_parser.add_argument('--verbosity', type=int, help='Verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--version', action='store_true', help='Show version')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.borg.flags as flags\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool for managing Borg backups',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global flags\n    parser.add_argument('--version', action='version', version='borgmatic ' + pkg_resources.require('borgmatic')[0].version)\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add 'list' action\n    list_parser = subparsers.add_parser('list', help='List archives or paths within an archive')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    list_parser.add_argument('--info', action='store_true', help='Show info about the archive')\n    list_parser.add_argument('--debug', action='store_true', help='Enable debug output')\n    list_parser.add_argument('--show-rc', action='store_true', help='Show repository configuration')\n    list_parser.add_argument('--remote-path', help='Remote path to use')\n    list_parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    list_parser.add_argument('archive', nargs='?', help='Archive name')\n    list_parser.add_argument('paths', nargs='*', help='Paths to list')\n    \n    # Add 'rlist' action\n    rlist_parser = subparsers.add_parser('rlist', help='List archives with a repository')\n    rlist_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    rlist_parser.add_argument('--info', action='store_true', help='Show info about the archive')\n    rlist_parser.add_argument('--debug', action='store_true', help='Enable debug output')\n    rlist_parser.add_argument('--show-rc', action='store_true', help='Show repository configuration')\n    rlist_parser.add_argument('--remote-path', help='Remote path to use')\n    rlist_parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    rlist_parser.add_argument('--prefix', help='Prefix to match archives with", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.borgmatic as borgmatic\n    import borgmatic.commands.borg.list as list_command\n    import borgmatic.commands.borg.info as info_command\n    import borgmatic.commands.borg.create as create_command\n    import borgmatic.commands.borg.prune as prune_command\n    import borgmatic.commands.borg.rlist as rlist_command\n    import borgmatic.commands.borg.check as check_command\n    import borgmatic.commands.borg.mount as mount_command\n    import borgmatic.commands.borg.status as status_command\n    import borgmatic.commands.borg.backup as backup_command\n    import borgmatic.commands.borg.restore as restore_command\n    import borgmatic.commands.borg.hooks as hooks_command\n    import borgmatic.commands.borg.cleanup as cleanup_command\n\n    parser = argparse.ArgumentParser(description='Borgmatic - A tool for managing Borg backups')\n    \n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add subparsers for each command\n    subparsers.add_parser('list', help='List archives in a repository')\n    subparsers.add_parser('info', help='Display information about archives in a repository')\n    subparsers.add_parser('create', help='Create a backup')\n    subparsers.add_parser('prune', help='Prune old backups')\n    subparsers.add_parser('rlist', help='List archives with a repository')\n    subparsers.add_parser('check', help='Check repository integrity')\n    subparsers.add_parser('mount', help='Mount a repository')\n    subparsers.add_parser('status', help='Display repository status')\n    subparsers.add_parser('backup', help='Create a backup')\n    subparsers.add_parser('restore', help='Restore a backup')\n    subparsers.add_parser('hooks', help='Manage hooks')\n    subparsers.add_parser('cleanup', help='Cleanup old backups')\n    \n    # Add global flags\n    global_parser = parser.add_argument_group('global options')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(prog)s')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.borgmatic as borgmatic\n    import borgmatic.commands.borg.list as list_command\n    import borgmatic.commands.borg.info as info_command\n    import borgmatic.commands.borg.create as create_command\n    import borgmatic.commands.borg.delete as delete_command\n    import borgmatic.commands.borg.rlist as rlist_command\n    import borgmatic.commands.borg.restore as restore_command\n    import borgmatic.commands.borg.backup as backup_command\n    import borgmatic.commands.borg.check as check_command\n    import borgmatic.commands.borg.prune as prune_command\n    import borgmatic.commands.borg.status as status_command\n    import borgmatic.commands.borg.mount as mount_command\n    import borgmatic.commands.borg.unmount as unmount_command\n    import borgmatic.commands.borg.misc as misc_command\n\n    parser = argparse.ArgumentParser(description='Borgmatic - a tool for managing Borg backups')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Add subparsers for each command\n    subparsers.add_parser('list', help='List archives or paths in a repository')\n    subparsers.add_parser('info', help='Display information about archives in a repository')\n    subparsers.add_parser('create', help='Create a backup')\n    subparsers.add_parser('delete', help='Delete archives')\n    subparsers.add_parser('rlist', help='List archives with a repository (requires Borg RLIST feature)')\n    subparsers.add_parser('restore', help='Restore archives')\n    subparsers.add_parser('backup', help='Create a backup')\n    subparsers.add_parser('check', help='Check the integrity of archives')\n    subparsers.add_parser('prune', help='Prune old archives')\n    subparsers.add_parser('status', help='Show repository status')\n    subparsers.add_parser('mount', help='Mount a repository')\n    subparsers.add_parser('unmount', help='Unmount a repository')\n    subparsers.add_parser('misc', help='Miscellaneous commands')\n\n    # Add global flags to the top-level parser\n    global_parser = parser.add_argument_group('global options')\n    global_parser.add_argument('--version', action='version', version='borgmatic %(prog)s')\n    global"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4158, "num_retrieved": 10, "inference_time": 1.422885149717331, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4092, "num_retrieved": 7, "inference_time": 1.422885149717331, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser(description='Borgmatic - A tool for managing Borg backups')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level (0-3)')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level (0-3)')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level (0-3)')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level (0-3)')\n    global_parser.add_argument('--log-file', type=str, default=None, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, default=None, help='Format for log file')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add config arguments\n    config_parser = parser.add_argument_group('config')\n    config_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    config_parser.add_argument('--overrides', nargs='+', default=[], help='Configuration overrides')\n    config_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    \n    # Add action parsers\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('--bootstrap', type=str, required=True, help='Bootstrap configuration')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source-filename',", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the actual parser setup and argument parsing\n    # logic is not provided in the context.\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace or a dictionary of parsed arguments.\n    \n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subcommands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Example subcommands (these would be fully defined in the actual implementation)\n    config_parser = subparsers.add_parser('config', help='Configuration related commands')\n    config_subparsers = config_parser.add_subparsers(dest='action', help='Configuration actions')\n    \n    bootstrap_parser = config_subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--bootstrap', help='Bootstrap configuration')\n    \n    generate_parser = config_subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('--destination-filename', help='Destination filename')\n    generate_parser.add_argument('--source-filename', help='Source filename')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file')\n    \n    validate_parser = config_subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('--validate', help='Validate configuration')\n    \n    # Add other subcommands as needed\n    \n    # Parse the arguments\n    args = parser", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser(description='Borgmatic - A tool for managing Borg backups')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level (0-3)')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level (0-3)')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level (0-3)')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level (0-3)')\n    global_parser.add_argument('--log-file', type=str, default=None, help='Log file path')\n    global_parser.add_argument('--log-file-format', type=str, default=None, help='Log file format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add config arguments\n    config_parser = parser.add_argument_group('config')\n    config_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    config_parser.add_argument('--overrides', nargs='+', default=[], help='Configuration overrides')\n    config_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    \n    # Add action parsers\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap Borgmatic')\n    bootstrap_parser.add_argument('--bootstrap', type=str, help='Bootstrap configuration')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source-filename', type=str, help='Source configuration", "import argparse\nimport sys\nimport importlib_metadata\nfrom borgmatic.commands.arguments import make_parsers\n\ndef parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # Create the argument parser\n    (global_parser, action_parsers, global_plus_action_parser) = make_parsers()\n    \n    # Parse the arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--version', action='version', version=importlib_metadata.version('borgmatic'))\n    \n    # Add global arguments\n    global_args = global_parser.parse_args(arguments)\n    \n    # Create a namespace to hold all arguments\n    args = argparse.Namespace()\n    \n    # Set global arguments\n    for attr in dir(global_args):\n        if not attr.startswith('_'):\n            setattr(args, attr, getattr(global_args, attr))\n    \n    # Process action-specific arguments\n    for action_name, action_parser in action_parsers.choices.items():\n        # Parse the action-specific arguments\n        action_args = action_parser.parse_args(arguments)\n        \n        # Update the main arguments namespace\n        for attr in dir(action_args):\n            if not attr.startswith('_'):\n                setattr(args, attr, getattr(action_args, attr))\n    \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg backups',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument(\n        '--version', action='version', version='borgmatic %(prog)s 1.0'\n    )\n    global_parser.add_argument(\n        '--bash-completion', action='store_true', help='Print bash completion script'\n    )\n    global_parser.add_argument(\n        '--fish-completion', action='store_true', help='Print fish completion script'\n    )\n    global_parser.add_argument(\n        '--verbosity', type=int, default=0, help='Verbosity level (0-3)'\n    )\n    global_parser.add_argument(\n        '--syslog-verbosity', type=int, default=0, help='Syslog verbosity level (0-3)'\n    )\n    global_parser.add_argument(\n        '--log-file-verbosity', type=int, default=0, help='Log file verbosity level (0-3)'\n    )\n    global_parser.add_argument(\n        '--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level (0-3)'\n    )\n    global_parser.add_argument(\n        '--log-file', type=str, default=None, help='Log file path'\n    )\n    global_parser.add_argument(\n        '--log-file-format', type=str, default=None, help='Log file format'\n    )\n    global_parser.add_argument(\n        '--no-color', action='store_true', help='Disable color output'\n    )\n    global_parser.add_argument(\n        '--resolve-env', action='store_true', help='Resolve environment variables in configuration files'\n    )\n    global_parser.add_argument(\n        '--overrides', type=str, nargs='*', help='Configuration overrides'\n    )\n    \n    # Add subparsers for actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the configuration')\n    bootstrap_parser.add_argument(\n        '--bootstrap', type=str, required=True, help='Bootstrap configuration file path'\n    )"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4196, "num_retrieved": 9, "inference_time": 1.422885149717331, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4171, "num_retrieved": 8, "inference_time": 1.422885149717331, "generate_results": ["def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(\"$@\")\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! _fish_seen_subcommand_from ${all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg '${exact_option_args}'\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion -n \"$action_parser_condition\" -n \"$exact_option_condition\" -a '{action_name}' -d '{shlex", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(\"$@\")\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! _fish_seen_subcommand_from ${all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg '${exact_option_args}'\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion -n \"$action_parser_condition\" -n \"$exact_option_condition\" -a '{action_name}' -d '{shlex", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(\"$@\")\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! _fish_seen_subcommand_from ${all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg '${exact_option_args}'\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion -n \"$action_parser_condition\" -n \"$exact_option_condition\" -a '{action_name}' -d '{shlex", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! _fish_seen_subcommand_from {all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_borgmatic_completion_{action_name}() {{\n                local cur=\"${COMP_WORDS", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                            \"$this_filename\"\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            local action_parser_condition=\"! __fish_seen_subcommand_from {all_action_parsers}\"\n            local exact_option_condition=\"! _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_borgmatic_{action_name}_completion() {{\n                local cur=\"${COMP_WORDS"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4124, "num_retrieved": 7, "inference_time": 1.422885149717331, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--log-file-format', help='Log file format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('bootstrap', help='Bootstrap arguments')\n    \n    generate_parser = subparsers.add_parser('generate', help='Generate configuration files')\n    generate_parser.add_argument('generate', help='Generate arguments')\n    \n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('validate', help='Validate arguments')\n    \n    list_parser = subparsers.add_parser('list', help='List archives in a repository')\n    list_parser.add_argument('repository', help='Repository path')\n    list_parser.add_argument('--prefix', help='Prefix to match archives')\n    list_parser.add_argument('--match-archives', help='Match archives with glob pattern')\n    list_parser.add_argument('--glob-archives', help='Glob archives with pattern')\n    list_parser.add_argument('--json', action", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--log-file-format', help='Log file format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # bootstrap subparser\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--global', help='Global configuration')\n    \n    # generate subparser\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('generate', help='Generate configuration')\n    generate_parser.add_argument('--global', help='Global configuration')\n    \n    # validate subparser\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('validate', help='Validate configuration')\n    validate_parser.add_argument('--global', help='Global configuration')\n    \n    # list subparser\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--json', action='store_true', help='Output", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log to file')\n    global_parser.add_argument('--log-file-format', help='Log file format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # bootstrap subparser\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--global', help='Global configuration')\n    \n    # generate subparser\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration')\n    generate_parser.add_argument('generate', help='Generate configuration')\n    generate_parser.add_argument('--global', help='Global configuration')\n    \n    # validate subparser\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration')\n    validate_parser.add_argument('validate', help='Validate configuration')\n    validate_parser.add_argument('--global', help='Global configuration')\n    \n    # list subparser\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('repository', help='Repository path')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--log-file-format', help='Log file format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('bootstrap', help='Bootstrap arguments')\n    \n    generate_parser = subparsers.add_parser('generate', help='Generate configuration files')\n    generate_parser.add_argument('generate', help='Generate arguments')\n    \n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('validate', help='Validate arguments')\n    \n    # Add other subparsers as needed\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to argparse.Namespace\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', help='Log file path')\n    global_parser.add_argument('--log-file-format', help='Log file format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # bootstrap subparser\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap the system')\n    bootstrap_parser.add_argument('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--global', help='Global configuration')\n    \n    # generate subparser\n    generate_parser = subparsers.add_parser('generate', help='Generate configuration files')\n    generate_parser.add_argument('generate', help='Generate configuration')\n    generate_parser.add_argument('--global', help='Global configuration')\n    \n    # validate subparser\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('validate', help='Validate configuration')\n    validate_parser.add_argument('--global', help='Global configuration')\n    \n    # list subparser\n    list_parser = subparsers.add_parser('list', help='List archives in a repository')\n    list_parser.add_argument('repository', help='Repository"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4134, "num_retrieved": 7, "inference_time": 1.422885149717331, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: None)\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order they appear in subparsers\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser instance for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments with the current subparser\n        try:\n            # Parse arguments using the subparser\n            parsed_args = parser.parse_args(remaining_arguments, namespace=argparse.Namespace())\n            \n            # Store the parsed arguments under the subparser name\n            parsed_arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining list\n            remaining_arguments = []\n            \n            # If there are still arguments left, continue to the next subparser\n            # Note: This is a simplified implementation; in practice, we'd need to handle\n            # the actual argument parsing more carefully.\n            \n        except SystemExit:\n            # If parsing fails, we don't want to exit the program\n            # Instead, we just skip this subparser and continue\n            pass\n    \n    # Return the parsed arguments dictionary and the remaining arguments\n    return dict(parsed_arguments), remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: None)\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order they appear in subparsers\n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            # Create a namespace for the arguments\n            parsed_args = subparser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = parsed_args\n        except SystemExit:\n            # If parsing fails, we don't want to exit the program\n            # Just skip this subparser and continue\n            pass\n        except Exception:\n            # If any other error occurs, just skip this subparser\n            pass\n    \n    # After parsing, any remaining arguments are not claimed by any subparser\n    return dict(parsed_arguments), remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: None)\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order they appear in subparsers\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser instance for the current subparser\n        parser = subparser\n        \n        # Try to parse the remaining arguments using the current subparser\n        try:\n            # Parse arguments with the current subparser\n            parsed_args = parser.parse_args(remaining_arguments)\n            \n            # Store the parsed arguments under the subparser name\n            parsed_arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining list\n            remaining_arguments = []\n            \n            # If there are still arguments left, continue to the next subparser\n            # Note: This is a simplified implementation - in practice, we'd need to\n            # handle the case where arguments are consumed by multiple subparsers\n            break\n            \n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    # If no subparser was able to parse arguments, return empty dict and original arguments\n    if not parsed_arguments:\n        parsed_arguments = {}\n    \n    return dict(parsed_arguments), remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Initialize result dictionary\n    parsed_arguments = defaultdict(lambda: None)\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Create a namespace to hold parsed arguments for this subparser\n        parsed_args = parser.parse_args(remaining_arguments, namespace=argparse.Namespace())\n        \n        # If arguments were parsed, store them in the result dictionary\n        if parsed_args is not None:\n            parsed_arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining list\n            # Find the indices of the parsed arguments and remove them\n            indices_to_remove = []\n            for i, arg in enumerate(remaining_arguments):\n                if arg in [str(getattr(parsed_args, k)) for k in dir(parsed_args) if not k.startswith('_')]:\n                    indices_to_remove.append(i)\n            \n            # Remove the arguments from the list\n            for i in reversed(indices_to_remove):\n                remaining_arguments.pop(i)\n    \n    # Convert defaultdict to regular dict\n    parsed_arguments = dict(parsed_arguments)\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: None)\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Iterate over subparsers to try to parse arguments\n    for subparser_name, parser in subparsers.items():\n        # Create a namespace to hold parsed arguments\n        namespace = parser.parse_args(remaining_arguments)\n        \n        # If we successfully parsed arguments, store them\n        if namespace is not None:\n            parsed_arguments[subparser_name] = namespace\n            # Remove the parsed arguments from the remaining list\n            # Find the indices of the parsed arguments and remove them\n            indices_to_remove = []\n            for i, arg in enumerate(remaining_arguments):\n                if arg in [str(getattr(namespace, key)) for key in dir(namespace) if not key.startswith('_')]:\n                    indices_to_remove.append(i)\n            \n            # Remove the arguments from the list\n            for i in sorted(indices_to_remove, reverse=True):\n                remaining_arguments.pop(i)\n    \n    return dict(parsed_arguments), remaining_arguments"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4163, "num_retrieved": 8, "inference_time": 1.422885149717331, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - backup and restore with Borg')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Define subparsers for each action\n    make_subparsers(subparsers)\n    \n    # Return the top-level parser, subparsers, and the combined parser with global arguments\n    global_plus_action_parser = argparse.ArgumentParser(add_help=False)\n    global_plus_action_parser.add_argument('--config', help='Path to configuration file')\n    global_plus_action_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO', help='Log level')\n    global_plus_action_parser.add_argument('--dry-run', action='store_true', help='Only print what would be done')\n    \n    # Combine global arguments with action-specific parsers\n    combined_parser = parser\n    \n    return (parser, subparsers, combined_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n\n    parser = argparse.ArgumentParser(description='Borgmatic - A tool to manage Borg backups')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define subparsers for each action\n    make_subparsers(subparsers)\n\n    # Separate the global parser from the action parsers\n    global_parser = parser\n    action_parsers = subparsers\n    \n    # Create a combined parser with global and action options\n    global_plus_action_parser = argparse.ArgumentParser(\n        parents=[global_parser],\n        add_help=False\n    )\n    \n    return (global_parser, action_parsers, global_plus_action_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n\n    parser = argparse.ArgumentParser(description='Borgmatic - A tool for managing Borg backups')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define subparsers for each action\n    actions = {\n        'backup': make_subparsers.make_backup_parser(),\n        'config': make_subparsers.make_config_parser(),\n        'list': make_subparsers.make_list_parser(),\n        'rlist': make_subparsers.make_rlist_parser(),\n        'prune': make_subparsers.make_prune_parser(),\n        'verify': make_subparsers.make_verify_parser(),\n        'check': make_subparsers.make_check_parser(),\n        'init': make_subparsers.make_init_parser(),\n        'create': make_subparsers.make_create_parser(),\n        'restore': make_subparsers.make_restore_parser(),\n        'archive': make_subparsers.make_archive_parser(),\n        'cleanup': make_subparsers.make_cleanup_parser(),\n        'status': make_subparsers.make_status_parser(),\n        'version': make_subparsers.make_version_parser(),\n    }\n\n    for action_name, action_parser in actions.items():\n        subparsers.add_parser(action_name, help=action_parser.description, **action_parser.kwargs)\n\n    # Create global parser with common arguments\n    global_parser = argparse.ArgumentParser(add_help=False)\n    global_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO')\n    global_parser.add_argument('--log-json', action='store_true', help='Log in JSON format')\n    global_parser.add_argument('--config', help='Path to configuration file')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n\n    # Add global arguments to the main parser\n    parser.add_argument('--global-args', nargs='*', help='Global arguments')\n\n    # Return the top-level parser, subparsers, and the combined parser with global arguments\n    return (parser, actions, parser.add_parser('borgmatic', parents=[global_parser], help='Borgmatic command line tool'))", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n\n    parser = argparse.ArgumentParser(description='Borgmatic - backup and restore with Borg')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define subparsers for each action\n    make_subparsers(subparsers)\n\n    # Return the top-level parser, subparsers, and the combined parser with global arguments\n    global_plus_action_parser = argparse.ArgumentParser()\n    global_plus_action_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_plus_action_parser.add_argument('--config', help='Path to configuration file')\n    global_plus_action_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO', help='Log level')\n    global_plus_action_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_plus_action_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n\n    # Add all actions to the global parser\n    for action_name, action_parser in subparsers.choices.items():\n        action_parser._action_groups[0].add_argument('--' + action_name, action='store_true', help=f'Enable {action_name} action')\n\n    return (parser, subparsers, global_plus_action_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    from borgmatic.commands.arguments import make_subparsers\n\n    parser = argparse.ArgumentParser(description='Borgmatic - backup and restore with Borg')\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n\n    # Define subparsers for each action\n    make_subparsers(subparsers)\n\n    # Return the top-level parser, subparsers, and the combined parser with global arguments\n    global_plus_action_parser = argparse.ArgumentParser()\n    global_plus_action_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_plus_action_parser.add_argument('--config', help='Path to configuration file')\n    global_plus_action_parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO', help='Log level')\n    global_plus_action_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_plus_action_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    \n    # Add all actions from subparsers to the global parser\n    for action_name, action_parser in subparsers.choices.items():\n        action_parser._action_groups[0].add_argument('--action', choices=[action_name], help=f'Choose action: {action_name}')\n\n    return (parser, subparsers, global_plus_action_parser)"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4090, "num_retrieved": 5, "inference_time": 1.422885149717331, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not isinstance(response, list):\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # If max_results is 0 or less, return empty generator\n    if max_results <= 0:\n        return\n    \n    # Yield chunks of response based on max_results\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not isinstance(response, list):\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # If max_results is 0 or less, return empty generator\n    if max_results <= 0:\n        return\n    \n    # Yield chunks of response based on max_results\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not isinstance(response, list):\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # If max_results is 0 or less, return empty generator\n    if max_results <= 0:\n        return\n    \n    # Split the response into pages of max_results\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not isinstance(response, list):\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # If max_results is 0 or less, return empty generator\n    if max_results <= 0:\n        return\n    \n    # Yield chunks of response based on max_results\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not isinstance(response, list):\n        return\n    \n    # If max_results is negative, we treat it as 1000 for pagination\n    if max_results < 0:\n        max_results = 1000\n    \n    # If max_results is 0 or less, return empty generator\n    if max_results <= 0:\n        return\n    \n    # Yield the first page\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4293, "num_retrieved": 12, "inference_time": 1.422885149717331, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 and size[-1].isdigit() == False else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4188, "num_retrieved": 10, "inference_time": 1.422885149717331, "generate_results": ["def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n    \n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4159, "num_retrieved": 8, "inference_time": 1.422885149717331, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{{{key}}}}}}}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{key}}}}}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{{{key}}}}}}}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{{{{{key}}}}}}}\"\n        url = url.replace(placeholder, str(value))\n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4239, "num_retrieved": 11, "inference_time": 1.422885149717331, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"Check if a string represents a None value.\"\"\"\n    return val is None or (isinstance(val, str) and val.lower() == 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"Check if a string represents a None value.\"\"\"\n    return val is None or (isinstance(val, str) and val.lower() == 'none')", "def is_none_string(val: any) -> bool:\n    \"\"\"Check if a string represents a None value.\"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"Check if a string represents a None value.\"\"\"\n    return val is None or (isinstance(val, str) and val.lower() in ['none', 'null', 'nil', ''])", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    return val is None or (isinstance(val, str) and val.lower() == 'none')"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4196, "num_retrieved": 9, "inference_time": 1.422885149717331, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if hasattr(action, 'option_strings') and action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4061, "num_retrieved": 5, "inference_time": 1.422885149717331, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Warning: {} has already been processed.\".format(path_name))\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Skipping already processed path: {}\".format(abs_path))\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Warning: {} has already been processed.\".format(path_name))\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Warning: {} has already been processed.\".format(path_name))\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(\"Warning: {} has already been processed.\".format(path_name))\n        return True\n    processed.add(abs_path)\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4314, "num_retrieved": 14, "inference_time": 0.982825756072998, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        \n        # Create a 3-tuple: (previous, current, next)\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and the two adjacent vertices\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        \n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and the two adjacent vertices\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        \n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the vertex and its two neighbors (with wraparound)\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        result.append((prev, curr, next_))\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev = vertices[(i - 1) % n]\n        curr = vertices[i]\n        next_ = vertices[(i + 1) % n]\n        \n        result.append((prev, curr, next_))\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 1167, "num_retrieved": 5, "inference_time": 0.982825756072998, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    \n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    \n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4349, "num_retrieved": 14, "inference_time": 0.982825756072998, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n    \n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n    \n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number\n    \n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'\n    \n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    if register_value in flow_params:\n        flow_params[register_value] = register_number"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4431, "num_retrieved": 18, "inference_time": 0.982825756072998, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    if value is None:\n        return None\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    if value is None:\n        return None\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    if value is None:\n        return None\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    if value is None:\n        return None\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    if value is None:\n        return None\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4304, "num_retrieved": 15, "inference_time": 0.982825756072998, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 1161, "num_retrieved": 5, "inference_time": 0.982825756072998, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4202, "num_retrieved": 9, "inference_time": 0.982825756072998, "generate_results": ["import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the .tar.gz archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the archive to a temporary file\n    temp_file = destination_path / \"archive.tar.gz\"\n    with open(temp_file, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    try:\n        subprocess.check_output(\n            [\"tar\", \"xf\", str(temp_file), \"-C\", str(destination_path)],\n            stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e.output.decode()}\")\n    \n    # Remove the temporary archive file\n    temp_file.unlink()\n    \n    return destination_path", "import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the archive to a temporary file\n    temp_file = destination_path / \"archive.tar.gz\"\n    with open(temp_file, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    try:\n        subprocess.check_output(\n            [\"tar\", \"xf\", str(temp_file), \"-C\", str(destination_path)],\n            stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Error extracting archive: {e.output.decode()}\")\n    \n    # Remove the temporary archive file\n    temp_file.unlink()\n    \n    return destination_path", "import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination_path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the .tar.gz archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the archive to a temporary file\n    temp_file = destination_path / \"archive.tar.gz\"\n    with open(temp_file, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    try:\n        subprocess.check_output(\n            [\"tar\", \"xf\", str(temp_file), \"-C\", str(destination_path)],\n            stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e.output.decode()}\")\n    \n    # Remove the temporary archive file\n    temp_file.unlink()\n    \n    return destination_path", "import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination_path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the .tar.gz archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the archive to a temporary file\n    temp_file = destination_path / \"archive.tar.gz\"\n    with open(temp_file, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    try:\n        subprocess.check_output(\n            [\"tar\", \"xf\", str(temp_file), \"-C\", str(destination_path)],\n            stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Error extracting archive: {e.output.decode()}\") from e\n    \n    # Remove the temporary archive file\n    temp_file.unlink()\n    \n    return destination_path", "import requests\nimport gzip\nimport os\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the .tar.gz archive\n    with gzip.open(response.content, 'rb') as gz_file:\n        tar_data = gz_file.read()\n    \n    # Extract the archive to the destination path\n    with subprocess.Popen(\n        [\"tar\", \"xf\", \"-\", \"-C\", str(destination_path)],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL\n    ) as proc:\n        proc.stdin.write(tar_data)\n        proc.stdin.close()\n        proc.wait()\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4330, "num_retrieved": 13, "inference_time": 0.982825756072998, "generate_results": ["import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.uname().sysname == 'Darwin'", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.uname().sysname == 'Darwin'", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.uname().sysname == 'Darwin'", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.uname().sysname == 'Darwin'", "import os\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.name == 'posix' and os.path.basename(os.uname().sysname) == 'Darwin'"]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4294, "num_retrieved": 12, "inference_time": 0.982825756072998, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        regex_key = re.compile(key.replace('.', r'\\.').replace('*', '.*'))\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        regex_key = re.compile(key.replace('*', '.*'))\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        regex_key = re.compile(key.replace('.', r'\\.').replace('*', '.*'))\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        regex_key = re.compile(key.replace('.', r'\\.').replace('*', '.*'))\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        regex_key = re.compile(key.replace('*', '.*'))\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4806, "num_retrieved": 31, "inference_time": 0.982825756072998, "generate_results": ["import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    else:\n        return name", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)", "import re\n\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    else:\n        return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if name is None:\n        return None\n    # Remove quotes from the beginning and end of the string\n    return name.strip('\"\\'')", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if name is None:\n        return None\n    # Remove quotes from the beginning and end of the string\n    return name.strip('\"\\'')"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4176, "num_retrieved": 8, "inference_time": 0.982825756072998, "generate_results": ["import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin'] else 0\n    \n    if platform == 1:  # POSIX\n        # POSIX shell splitting using regex\n        # Matches quoted strings and unquoted tokens separated by whitespace\n        pattern = r'([^\"\\'\\s]+|\"[^\"]*\"|\\'[^\\']*\\')'\n        tokens = re.findall(pattern, s)\n        # Remove empty tokens and strip quotes\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            elif token.startswith(\"'\") and token.endswith(\"'\"):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        return result\n    \n    elif platform == 0:  # Windows/CMD\n        # Windows command line splitting - handles spaces in quoted strings\n        # Split on spaces, but preserve quoted strings\n        pattern = r'([^\" ]+|\"[^\"]*\")'\n        tokens = re.findall(pattern, s)\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        return result\n    \n    else:\n        raise ValueError(f\"Unsupported platform: {platform}\")", "import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin'] else 0\n    \n    if platform == 1:  # POSIX\n        # POSIX shell splitting pattern\n        pattern = r'([^\\s\"\\'`]+|\"[^\"]*\"|\\'[^\\']*\\'|\\`[^\\`]*\\`|\\s+)'\n        return [part.strip() for part in re.split(r'(\\s+)', s) if part.strip()]\n    \n    elif platform == 0:  # Windows/CMD\n        # Windows command line splitting pattern\n        # Handle quoted strings and spaces\n        pattern = r'([^\"\\s]+|\"[^\"]*\")'\n        parts = re.findall(pattern, s)\n        result = []\n        for part in parts:\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            else:\n                result.append(part)\n        return result\n    \n    else:\n        raise ValueError(f\"Unsupported platform: {platform}\")", "import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin'] else 0\n    \n    if platform == 1:  # POSIX\n        # POSIX shell splitting using regex\n        # Matches quoted strings and unquoted tokens separated by whitespace\n        pattern = r'([^\"\\'\\s]+|\"[^\"]*\"|\\'[^\\']*\\')'\n        tokens = re.findall(pattern, s)\n        # Remove empty tokens and strip quotes\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            elif token.startswith(\"'\") and token.endswith(\"'\"):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        return result\n    \n    elif platform == 0:  # Windows/CMD\n        # Windows CMD splitting - handles spaces and quotes\n        # Replace double quotes with a delimiter and split\n        # Also handle spaces within quotes\n        # Use regex to split on spaces, but preserve quoted strings\n        pattern = r'([^\" ]+|\"[^\"]*\")'\n        tokens = re.findall(pattern, s)\n        result = []\n        for token in tokens:\n            if token.startswith('\"') and token.endswith('\"'):\n                result.append(token[1:-1])\n            else:\n                result.append(token)\n        return result\n    \n    else:\n        raise ValueError(f\"Unsupported platform: {platform}\")", "import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin'] else 0\n    \n    if platform == 1:  # POSIX\n        # POSIX shell splitting pattern\n        pattern = r'([^\\s\"\\'`]|\"(?:[^\"\\\\]|\\\\.)*\"|\\'(?:[^\\'\\\\]|\\\\.)*\\')'\n        return re.findall(pattern, s)\n    \n    elif platform == 0:  # Windows/CMD\n        # Windows CMD splitting pattern - handles spaces, quotes, and escaped characters\n        # This pattern matches quoted strings and unquoted tokens\n        pattern = r'(?:(?:^|(?<=[^\"\\']))(?:[^\"\\']|\"(?:[^\"\\\\]|\\\\.)*\"|\\'(?:[^\\'\\\\]|\\\\.)*\\')+(?=\\s|$))'\n        tokens = re.findall(pattern, s)\n        result = []\n        for token in tokens:\n            # Remove surrounding quotes\n            token = token.strip('\"\\'')\n            result.append(token)\n        return result\n    \n    else:\n        raise ValueError(f\"Unsupported platform: {platform}\")", "import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin'] else 0\n    \n    if platform == 1:  # POSIX\n        # POSIX shell splitting using regex\n        # This handles quotes, escaped quotes, and spaces\n        pattern = r'([^\"\\'\\\\]|\\\\.)+'\n        return re.findall(pattern, s)\n    \n    elif platform == 0:  # Windows/CMD\n        # Windows CMD splitting - handles spaces, quotes, and escaping\n        # This is a simplified version that handles basic cases\n        # In Windows CMD, spaces are split unless enclosed in quotes\n        # We use a regex to split on spaces, but preserve quoted strings\n        pattern = r'([^\"\\'\\\\]|\\\\.)+'\n        result = re.findall(pattern, s)\n        # For Windows, we need to handle the fact that CMD doesn't handle\n        # complex quoting like POSIX shells\n        # This is a minimal implementation\n        return result\n    \n    else:\n        raise ValueError(f\"Unsupported platform: {platform}\")"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4240, "num_retrieved": 10, "inference_time": 0.982825756072998, "generate_results": ["import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / archive_path.stem\n    \n    # Use tar to extract the archive\n    if archive_path.suffix.lower() in ['.tar.gz', '.tgz']:\n        cmd = ['tar', '-xzf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.xz']:\n        cmd = ['tar', '-xJf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.bz2']:\n        cmd = ['tar', '-xjf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.zip']:\n        cmd = ['unzip', '-q', str(archive_path), '-d', str(extract_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path.suffix}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repo URL based on the extracted files\n    # If filename is provided, use it; otherwise, use the archive stem\n    if filename:\n        repo_url = f\"file://{str(extract_dir)}/{filename}\"\n    else:\n        repo_url = f\"file://{str(extract_dir)}\"\n    \n    return repo_url", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / archive_path.stem\n    \n    # Use tar to extract the archive\n    if archive_path.suffix.lower() in ['.tar.gz', '.tgz']:\n        cmd = ['tar', '-xzf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.xz']:\n        cmd = ['tar', '-xJf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.bz2']:\n        cmd = ['tar', '-xjf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.zip']:\n        cmd = ['unzip', '-q', str(archive_path), '-d', str(extract_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path.suffix}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repo URL based on the extracted files\n    # If filename is provided, use it; otherwise, use the archive stem\n    if filename:\n        repo_url = f\"file://{str(extract_dir)}/{filename}\"\n    else:\n        repo_url = f\"file://{str(extract_dir)}\"\n    \n    return repo_url", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / archive_path.stem\n    \n    # Use tar to extract the archive\n    if archive_path.suffix.lower() in ['.tar.gz', '.tgz']:\n        cmd = ['tar', '-xzf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.xz']:\n        cmd = ['tar', '-xJf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.bz2']:\n        cmd = ['tar', '-xjf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.zip']:\n        cmd = ['unzip', '-q', str(archive_path), '-d', str(extract_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path.suffix}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repository URL based on the extracted files\n    # For simplicity, assume the repository URL is derived from the extracted directory\n    # and the original archive name if provided\n    if filename:\n        repo_url = f\"file://{extract_dir}/{filename}\"\n    else:\n        repo_url = f\"file://{extract_dir}\"\n    \n    return repo_url", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / archive_path.stem\n    \n    # Use tar to extract the archive\n    if archive_path.suffix.lower() in ['.tar.gz', '.tgz']:\n        cmd = ['tar', '-xzf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.xz']:\n        cmd = ['tar', '-xJf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.bz2']:\n        cmd = ['tar', '-xjf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.zip']:\n        cmd = ['unzip', '-q', str(archive_path), '-d', str(extract_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path.suffix}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repository URL based on the extracted files\n    # For simplicity, assume the repository URL is the path to the extracted directory\n    # with a specific naming convention\n    if filename:\n        repo_url = f\"file://{str(extract_dir)}/{filename}\"\n    else:\n        # Default to the directory name\n        repo_url = f\"file://{str(extract_dir)}\"\n    \n    return repo_url", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    archive_path = Path(archive_path)\n    tmp_path = Path(tmp_path)\n    \n    # Ensure the temporary directory exists\n    tmp_path.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_path / archive_path.stem\n    \n    # Use tar to extract the archive\n    if archive_path.suffix.lower() in ['.tar.gz', '.tgz']:\n        cmd = ['tar', '-xzf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.xz']:\n        cmd = ['tar', '-xJf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.tar.bz2']:\n        cmd = ['tar', '-xjf', str(archive_path), '-C', str(extract_dir)]\n    elif archive_path.suffix.lower() in ['.zip']:\n        cmd = ['unzip', '-q', str(archive_path), '-d', str(extract_dir)]\n    else:\n        raise ValueError(f\"Unsupported archive format: {archive_path.suffix}\")\n    \n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Determine the repo URL based on the extracted files\n    # This is a simplified implementation - in practice, the actual repo URL\n    # would be derived from the content of the extracted files\n    if filename:\n        # If a specific filename is provided, use it to construct the URL\n        # This is a placeholder - actual URL construction would depend on the archive content\n        return f\"file://{extract_dir}/{filename}\"\n    else:\n        # Default to the first file in the extracted directory\n        files = list(extract_dir.glob(\"*\"))\n        if not files:\n            raise FileNotFoundError(f\"No files found in extracted directory: {"]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4198, "num_retrieved": 9, "inference_time": 0.982825756072998, "generate_results": ["import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter files based on the ignored list (if ignored is provided)\n        if ignored:\n            ignored_files = [f for f in ignored_files if any(ignore in f for ignore in ignored)]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n    \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except Exception as e:\n        # Handle any other unexpected errors\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    try:\n        # Run git check-ignore to get the list of ignored files\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                                capture_output=True, text=True, check=False)\n        \n        if result.returncode != 0:\n            return \"\"\n            \n        # Split the output into lines and filter out empty lines\n        ignored_files = []\n        for line in result.stdout.strip().splitlines():\n            if line:\n                ignored_files.append(line.strip())\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return \",\".join(ignored_files)\n        \n    except Exception as e:\n        return \"\"", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    \n    # Filter files based on the ignored list (if provided)\n    if ignored:\n        ignored_files = [f for f in ignored_files if any(ignore in f for ignore in ignored)]\n    \n    # Sort the list of ignored files\n    ignored_files.sort()\n    \n    # Join the files with commas\n    return ','.join(ignored_files)", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    \n    # Filter files based on the ignored list (if provided)\n    if ignored:\n        ignored_files = [f for f in ignored_files if any(ignored_file in f for ignored_file in ignored)]\n    \n    # Sort the list of ignored files\n    ignored_files.sort()\n    \n    # Join the files with commas\n    return ','.join(ignored_files)", "import subprocess\nimport os\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    \n    # Filter files based on the ignored list (if provided)\n    if ignored:\n        ignored_files = [f for f in ignored_files if any(ignore in f for ignore in ignored)]\n    \n    # Sort the list of ignored files\n    ignored_files.sort()\n    \n    # Return as a comma-separated string\n    return \",\".join(ignored_files) if ignored_files else \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 3282, "num_retrieved": 30, "inference_time": 0.982825756072998, "generate_results": ["def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    supported_extensions = ['.docopt', '.py', '.ini', '.json']\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in supported_extensions)"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4125, "num_retrieved": 7, "inference_time": 0.982825756072998, "generate_results": ["import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f'Invalid frequency format: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Map units to timedelta components\n    unit_map = {\n        'second': datetime.timedelta(seconds=number),\n        'seconds': datetime.timedelta(seconds=number),\n        'minute': datetime.timedelta(minutes=number),\n        'minutes': datetime.timedelta(minutes=number),\n        'hour': datetime.timedelta(hours=number),\n        'hours': datetime.timedelta(hours=number),\n        'day': datetime.timedelta(days=number),\n        'days': datetime.timedelta(days=number),\n        'week': datetime.timedelta(weeks=number),\n        'weeks': datetime.timedelta(weeks=number),\n        'month': None,  # Not supported - months vary in days\n        'months': None,  # Not supported - months vary in days\n        'year': None,  # Not supported - years vary in days\n        'years': None,  # Not supported - years vary in days\n    }\n\n    if unit not in unit_map:\n        raise ValueError(f'Unsupported unit: {unit}')\n\n    # For months, years - we don't support them as they vary in days\n    if unit_map[unit] is None:\n        raise ValueError(f'Unsupported unit: {unit}')\n\n    return unit_map[unit]", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (supporting common time units)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency.strip())\n    if not match:\n        raise ValueError(f'Cannot parse frequency string: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return datetime.timedelta(weeks=number)\n    elif unit == 'days':\n        return datetime.timedelta(days=number)\n    elif unit == 'hours':\n        return datetime.timedelta(hours=number)\n    elif unit == 'minutes':\n        return datetime.timedelta(minutes=number)\n    elif unit == 'seconds':\n        return datetime.timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unsupported time unit: {unit}')", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (supporting common time units)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    if not match:\n        raise ValueError(f'Cannot parse frequency \"{frequency}\"')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return datetime.timedelta(weeks=number)\n    elif unit == 'days':\n        return datetime.timedelta(days=number)\n    elif unit == 'hours':\n        return datetime.timedelta(hours=number)\n    elif unit == 'minutes':\n        return datetime.timedelta(minutes=number)\n    elif unit == 'seconds':\n        return datetime.timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unsupported time unit: {unit}')", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f'Invalid frequency format: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Map units to timedelta components\n    unit_map = {\n        'second': datetime.timedelta(seconds=number),\n        'seconds': datetime.timedelta(seconds=number),\n        'minute': datetime.timedelta(minutes=number),\n        'minutes': datetime.timedelta(minutes=number),\n        'hour': datetime.timedelta(hours=number),\n        'hours': datetime.timedelta(hours=number),\n        'day': datetime.timedelta(days=number),\n        'days': datetime.timedelta(days=number),\n        'week': datetime.timedelta(weeks=number),\n        'weeks': datetime.timedelta(weeks=number),\n        'month': None,  # Not supported - months vary in days\n        'months': None,  # Not supported - months vary in days\n        'year': None,  # Not supported - years vary in days\n        'years': None,  # Not supported - years vary in days\n    }\n\n    if unit not in unit_map:\n        raise ValueError(f'Unsupported unit: {unit}')\n\n    # For months, years, and other unsupported units, we don't have a direct mapping\n    # Since the problem specifies only number and unit of time, and examples show weeks,\n    # we assume only the supported units are valid.\n    if unit_map[unit] is None:\n        raise ValueError(f'Unsupported unit: {unit}')\n\n    return unit_map[unit]", "import datetime\nimport re\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    if not match:\n        raise ValueError(f'Cannot parse frequency \"{frequency}\"')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return datetime.timedelta(weeks=number)\n    elif unit == 'days':\n        return datetime.timedelta(days=number)\n    elif unit == 'hours':\n        return datetime.timedelta(hours=number)\n    elif unit == 'minutes':\n        return datetime.timedelta(minutes=number)\n    elif unit == 'seconds':\n        return datetime.timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unknown time unit: {unit}')"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4245, "num_retrieved": 10, "inference_time": 0.982825756072998, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to see if it's localhost\n        # localhost can be represented as 'localhost' or '127.0.0.1'\n        if host.lower() == 'localhost':\n            return True\n        \n        # Check if the host resolves to 127.0.0.1\n        ip_address = socket.gethostbyname(host)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to see if it's localhost\n        # localhost can be represented as 'localhost' or '127.0.0.1'\n        if host.lower() == 'localhost':\n            return True\n        \n        # Check if the host resolves to 127.0.0.1\n        ip = socket.gethostbyname(host)\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to see if it's localhost\n        # localhost can be represented as 'localhost' or '127.0.0.1'\n        if host.lower() == 'localhost':\n            return True\n        \n        # Check if the host resolves to 127.0.0.1\n        ip = socket.gethostbyname(host)\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to see if it matches localhost\n        # localhost can be represented as 'localhost' or '127.0.0.1'\n        if host.lower() == 'localhost':\n            return True\n        \n        # Check if the host resolves to 127.0.0.1\n        ip = socket.gethostbyname(host)\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to see if it's localhost\n        # localhost can be resolved to 127.0.0.1 or ::1\n        ip = socket.gethostbyname(host)\n        # Check if the resolved IP is the loopback address\n        return ip in ['127.0.0.1', '127.0.0.0', '::1']\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4307, "num_retrieved": 14, "inference_time": 0.9565519839525223, "generate_results": ["import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragments into glob patterns\n            # Use a simple rule: convert path to a glob pattern with **/*path*/**\n            # This assumes that the path is a file or directory name\n            # and we want to match it anywhere in the tree\n            path_parts = path.split(os.path.sep)\n            # Create a glob pattern that matches the path anywhere\n            glob_pattern = 'sh:**/*' + '*'.join(re.escape(part) for part in path_parts) + '*/**'\n            result.append(glob_pattern)\n    return result", "import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    if not find_paths:\n        return []\n\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            # Already a Borg pattern, pass through untouched\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use sh: prefix and create a recursive glob pattern\n            # Replace any path separators with / and add wildcards around\n            # Use PurePath to handle path components properly\n            path_parts = PurePath(path).parts\n            if not path_parts:\n                # Empty path, use default pattern\n                result.append('sh:**/*')\n            else:\n                # Create a glob pattern with wildcards around the path\n                # Use ** to match any directory depth\n                pattern_parts = ['**']\n                for part in path_parts:\n                    pattern_parts.append(f'*{part}*')\n                # Join with / to form the full glob pattern\n                glob_pattern = '/'.join(pattern_parts)\n                result.append(f'sh:{glob_pattern}')\n    \n    return result", "import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use a simple rule: replace any directory separator with '/' and wrap with sh:**/*path*/**\n            # This assumes that the path is a file or directory name that should be matched recursively\n            # We use ** to match any directory depth\n            path = path.strip('/')\n            # Replace any path separator with '/' and ensure it's properly formatted\n            path_parts = path.split('/')\n            # Join with '/' to get the full path\n            path_str = '/'.join(path_parts)\n            # Create a glob pattern that matches recursively\n            glob_pattern = f'sh:**/*{path_str}*/**'\n            result.append(glob_pattern)\n    return result", "import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    if not find_paths:\n        return []\n\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            # Already a Borg pattern, pass through untouched\n            result.append(path)\n        else:\n            # Transform path fragment into a glob pattern\n            # Use sh: prefix and use **/*path*/** pattern\n            # Replace any path separators with / and ensure proper glob syntax\n            path = path.strip()\n            if not path:\n                continue\n                \n            # Convert path to a glob pattern with **/*path*/**\n            # Split by path separators and build the glob\n            path_parts = path.split(os.path.sep)\n            # Create a glob pattern with wildcards\n            glob_pattern = 'sh:**/*' + '*'.join(re.escape(part) for part in path_parts) + '*/**'\n            result.append(glob_pattern)\n    \n    return result", "import re\nimport os\nfrom pathlib import PurePath\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    if not find_paths:\n        return []\n\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            # Already a Borg pattern, pass through untouched\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use sh: prefix and create a recursive glob pattern\n            # Replace any existing slashes with the glob pattern structure\n            # Use ** to match any directory depth\n            path = path.lstrip(os.path.sep)\n            # Create a glob pattern with **/*path*/* to match recursively\n            # This ensures the path is matched at any depth\n            glob_pattern = f\"sh:**/*{path}*/**\"\n            result.append(glob_pattern)\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4552, "num_retrieved": 22, "inference_time": 0.9565519839525223, "generate_results": ["def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return platform.system() == \"Windows\" and \"GIT_BASH\" in os.environ.get(\"SYSTEMROOT\", \"\")", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return platform.system() == \"Windows\" and \"GIT_BASH\" in os.environ", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return platform.system() == \"Windows\" and \"GIT_BASH\" in os.environ", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return platform.system() == \"Windows\" and \"GIT_BASH\" in os.environ", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return platform.system() == \"Windows\" and \"GIT_BASH\" in os.environ"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4111, "num_retrieved": 7, "inference_time": 0.9565519839525223, "generate_results": ["import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    # Check if file exists and overwrite is False\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    # Write the rendered config to the file\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    # Set file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\nimport tempfile\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return\n\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n    with open(config_filename, 'w', encoding='utf-8') as file:\n        file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4240, "num_retrieved": 10, "inference_time": 0.9565519839525223, "generate_results": ["import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Remove leading and trailing whitespace and split into lines\n    lines = script.strip().splitlines()\n    \n    # Filter out empty lines and comments\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the specified separator\n    return seperator.join(commands)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Remove leading and trailing whitespace and split into lines\n    lines = script.strip().splitlines()\n    \n    # Filter out empty lines and comments\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the specified separator\n    return seperator.join(commands)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Remove leading and trailing whitespace and split into lines\n    lines = script.strip().splitlines()\n    \n    # Filter out empty lines and comments\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the specified separator\n    return seperator.join(commands)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Remove leading and trailing whitespace and split into lines\n    lines = script.strip().splitlines()\n    \n    # Filter out empty lines and comments\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the specified separator\n    return seperator.join(commands)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: a multi-line script string\n    :param seperator: the separator to join commands (default \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Remove leading and trailing whitespace and split into lines\n    lines = script.strip().splitlines()\n    \n    # Filter out empty lines and comments\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the specified separator\n    return seperator.join(commands)"]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4261, "num_retrieved": 11, "inference_time": 0.9565519839525223, "generate_results": ["import subprocess\nimport sys\nimport os\nfrom pathlib import Path\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Prepare the environment\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Create a temporary script file to hold the function call\n    temp_script = Path(\"/tmp/matplotlib_subprocess_{}.py\".format(hash(func)))\n    try:\n        # Write the function call to the script\n        script_content = f\"import {func.__module__}\\n{func.__name__}({', '.join([repr(arg) for arg in args])})\"\n        temp_script.write_text(script_content, encoding=\"utf-8\")\n        \n        # Execute the script using subprocess.run\n        command = [sys.executable, str(temp_script)]\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n    finally:\n        # Clean up the temporary script file\n        if temp_script.exists():\n            temp_script.unlink()", "import subprocess\nimport sys\nimport os\nfrom typing import Any, Dict, Tuple, List\n\ndef subprocess_run_helper(func: Any, *args: str, timeout: float, extra_env: Dict[str, str] = None) -> Tuple[Any, Any]:\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the return code and the stdout of the subprocess.\n    \"\"\"\n    # Prepare the environment\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Create the command to run\n    command = [sys.executable] + list(args)\n    \n    # Execute the subprocess\n    try:\n        result = subprocess.run(\n            command,\n            env=env,\n            timeout=timeout,\n            capture_output=True,\n            text=True\n        )\n        return result.returncode, result.stdout\n    except subprocess.TimeoutExpired:\n        return -1, \"Process timed out\"\n    except Exception as e:\n        return -1, f\"Error: {str(e)}\"", "import subprocess\nimport sys\nimport os\nfrom functools import partial\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the command to run\n    command = [sys.executable] + list(args)\n    \n    # Create environment variables\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the subprocess\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        return result\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Command timed out after {timeout} seconds: {command}\")\n    except Exception as e:\n        raise RuntimeError(f\"Subprocess failed: {e}\")", "import subprocess\nimport sys\nimport os\nfrom functools import partial\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the command to run\n    command = [sys.executable] + list(args)\n    \n    # Set up environment variables\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the subprocess\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        return result\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Command timed out after {timeout} seconds: {command}\")\n    except Exception as e:\n        raise RuntimeError(f\"Subprocess failed: {str(e)}\") from e", "import subprocess\nimport sys\nimport os\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script that imports and runs the function\n    script = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Prepare environment\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Execute the script using subprocess\n    try:\n        result = subprocess.run(\n            [sys.executable, \"-c\", script],\n            env=env,\n            timeout=timeout,\n            capture_output=True\n        )\n        return result.returncode, result.stdout.decode('utf-8'), result.stderr.decode('utf-8')\n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Timeout expired\"\n    except Exception as e:\n        return -1, \"\", str(e)"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4233, "num_retrieved": 11, "inference_time": 0.9565519839525223, "generate_results": ["import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    match = matcher.group(0)\n    # Remove the ${ and } to get the variable name\n    var_name = match[1:-1]\n    \n    # Check for optional default value (e.g., ${FOO:-bar})\n    if ':' in var_name:\n        var_name, default_value = var_name.split(':', 1)\n        default_value = default_value.strip()\n    else:\n        default_value = None\n    \n    # Get the environment variable value\n    env_value = os.getenv(var_name)\n    \n    # If no default value is provided and the environment variable is not set, raise an error\n    if default_value is None and env_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = matcher.group(0)\n    variable_name = match[1:-1]  # Remove the ${} around the variable name\n    \n    # Check for optional default value (e.g., ${FOO:-bar})\n    if ':' in variable_name:\n        variable_name, default_value = variable_name.split(':', 1)\n        default_value = default_value.strip()\n    else:\n        default_value = None\n    \n    # Get the environment variable value\n    env_value = os.getenv(variable_name)\n    \n    # If no default value is provided and the variable is not set, raise an error\n    if default_value is None and env_value is None:\n        raise EnvironmentError(f\"Environment variable '{variable_name}' is not set and no default value provided\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = matcher.group(0)\n    variable_name = match[1:-1]  # Remove the ${ and } around the variable name\n    \n    # Check for optional default value (e.g., ${FOO:-bar})\n    if ':' in variable_name:\n        variable_name, default_value = variable_name.split(':', 1)\n        default_value = default_value.strip()\n    else:\n        default_value = None\n    \n    # Get the environment variable value\n    env_value = os.environ.get(variable_name)\n    \n    # If no value is found and no default is provided, raise an error\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{variable_name}' is not set and no default value provided\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    match = matcher.group(0)\n    # Remove the ${} and split on :- to get name and default\n    if ':' in match[2:-1]:\n        var_name, default = match[2:-1].split(':', 1)\n    else:\n        var_name = match[2:-1]\n        default = None\n    \n    # Get the environment variable value\n    env_value = os.getenv(var_name)\n    \n    # If no default is provided and the variable is not set, raise an error\n    if default is None and env_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default", "import os\nimport logging\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    match = matcher.group(0)\n    # Remove the ${} and split on :- to get name and default\n    if ':' in match[2:-1]:\n        # Format is ${FOO:-bar}\n        parts = match[2:-1].split(':', 1)\n        var_name = parts[0]\n        default_value = parts[1]\n    else:\n        # Format is ${FOO}\n        var_name = match[2:-1]\n        default_value = None\n    \n    # Get the environment variable value\n    env_value = os.environ.get(var_name)\n    \n    # If no value is found and no default is provided, raise an error\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4154, "num_retrieved": 8, "inference_time": 0.9565519839525223, "generate_results": ["import urllib.parse\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed = urllib.parse.urlsplit(image_href)\n    \n    # Validate the URL structure\n    if not parsed.netloc:\n        raise ValueError(f\"Invalid image href: {image_href} (missing netloc)\")\n    \n    # Extract the image_id from the path (last part after the last '/')\n    path_parts = parsed.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError(f\"Invalid image href: {image_href} (empty path)\")\n    \n    image_id = path_parts[-1]\n    \n    # Determine if SSL is used (https vs http)\n    use_ssl = parsed.scheme == 'https'\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib.parse\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed = urllib.parse.urlsplit(image_href)\n    \n    # Validate the URL structure\n    if not parsed.netloc:\n        raise ValueError(f\"Invalid image href: {image_href} (missing netloc)\")\n    \n    # Extract the image_id from the path (last part after the last '/')\n    path_parts = parsed.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError(f\"Invalid image href: {image_href} (empty path)\")\n    \n    image_id = path_parts[-1]\n    \n    # Determine if SSL is used (https vs http)\n    use_ssl = parsed.scheme == 'https'\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib.parse\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed = urllib.parse.urlsplit(image_href)\n    \n    # Validate the URL structure\n    if not parsed.netloc:\n        raise ValueError(f\"Invalid image href: {image_href} (missing netloc)\")\n    \n    # Determine if SSL is used (https vs http)\n    use_ssl = parsed.scheme == 'https'\n    \n    # Extract the path and split to get the image ID\n    path_parts = parsed.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError(f\"Invalid image href: {image_href} (empty path)\")\n    \n    # The image ID is typically the last part of the path\n    # or the last part after splitting by '/'\n    image_id = path_parts[-1] if path_parts else ''\n    \n    # If the image ID is empty, try to extract from the URL\n    if not image_id:\n        # Try to extract from the netloc or other parts\n        # In some cases, the image ID might be in the path\n        # or the netloc might contain it\n        # For now, if no image ID is found, raise an error\n        raise ValueError(f\"Invalid image href: {image_href} (no image ID found)\")\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib.parse\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed = urllib.parse.urlsplit(image_href)\n    \n    # Validate the URL structure\n    if not parsed.netloc:\n        raise ValueError('Invalid image href: missing netloc')\n    \n    # Determine if SSL is used (https vs http)\n    use_ssl = parsed.scheme == 'https'\n    \n    # Extract the path and split to get the image ID\n    path_parts = parsed.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError('Invalid image href: empty path')\n    \n    # The image ID is typically the last part of the path\n    # or the last part after splitting on '/'\n    image_id = path_parts[-1]\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib.parse\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed = urllib.parse.urlsplit(image_href)\n    \n    # Validate the URL structure\n    if not parsed.netloc:\n        raise ValueError(f\"Invalid image href: {image_href} (missing netloc)\")\n    \n    # Extract the image_id from the path (last part after the last '/')\n    path_parts = parsed.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError(f\"Invalid image href: {image_href} (empty path)\")\n    \n    image_id = path_parts[-1]\n    \n    # Determine if SSL is used (https vs http)\n    use_ssl = parsed.scheme == 'https'\n    \n    return (image_id, parsed.netloc, use_ssl)"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4168, "num_retrieved": 8, "inference_time": 0.9565519839525223, "generate_results": ["import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '/'):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 2440, "num_retrieved": 10, "inference_time": 0.9565519839525223, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for safe matching\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words only\n    return r'\\b' + escaped_pattern + r'\\b'", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    return escaped_pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for safe matching\n    escaped_pattern = re.escape(pattern)\n    return escaped_pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching whole words\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words only\n    return r'\\b' + escaped_pattern + r'\\b'", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching whole words\n    return re.escape(pattern)"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4363, "num_retrieved": 15, "inference_time": 0.9565519839525223, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n            \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n        if verbose and result.stderr:\n            print(result.stderr.strip())\n            \n        # Return the return code\n        return result.returncode\n        \n    except subprocess.SubprocessError as e:\n        return e.returncode if hasattr(e, 'returncode') else -1\n    except Exception as e:\n        return -1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n            \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            env=env,\n            text=True,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n        if verbose and result.stderr:\n            print(result.stderr.strip())\n            \n        # Return the return code\n        return result.returncode\n        \n    except subprocess.SubprocessError as e:\n        return e.returncode", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n            \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n        if verbose and result.stderr:\n            print(result.stderr.strip())\n            \n        # Return the return code\n        return result.returncode\n        \n    except subprocess.SubprocessError as e:\n        return e.returncode if hasattr(e, 'returncode') else -1\n    except Exception as e:\n        return -1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n            \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n        if verbose and result.stderr:\n            print(result.stderr.strip())\n            \n        # Return the return code\n        return result.returncode\n        \n    except subprocess.SubprocessError as e:\n        return e.returncode if hasattr(e, 'returncode') else -1\n    except Exception as e:\n        return -1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands if isinstance(commands, list) else [commands]\n        \n        # Add arguments if provided\n        if args:\n            cmd.extend(args)\n            \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Handle verbose output\n        if verbose and result.stdout:\n            print(result.stdout.strip())\n        if verbose and result.stderr:\n            print(result.stderr.strip())\n            \n        # Return the return code\n        return result.returncode\n        \n    except subprocess.SubprocessError as e:\n        return e.returncode if hasattr(e, 'returncode') else -1\n    except Exception as e:\n        return -1"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 0.9565519839525223, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4041, "num_retrieved": 4, "inference_time": 0.9565519839525223, "generate_results": ["import rdflib\nfrom typing import Optional, Set, Union\n\ndef find_roots(\n    graph: \"rdflib.Graph\", \n    prop: \"rdflib.URIRef\", \n    roots: Optional[Set[\"rdflib.Node\"]] = None\n) -> Set[\"rdflib.Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., are not pointed to by prop)\n    # by checking if there's any triple (x, prop, y) where x is the subject\n    # and y is a node that has a parent (i.e., has a triple (y, prop, z))\n    \n    # First, collect all nodes that have a parent (i.e., are in the range of prop)\n    parents = set()\n    for s, p, o in graph:\n        if p == prop:\n            parents.add(o)\n    \n    # Now, find all nodes that are not in the parents set - these are the roots\n    # We need to check all nodes in the graph to see if they are not pointed to by prop\n    all_nodes = set()\n    for s, p, o in graph:\n        all_nodes.add(s)\n        all_nodes.add(o)\n    \n    # Roots are nodes that are not in the parents set\n    roots = all_nodes - parents\n    \n    # If there are no roots, return empty set\n    return roots", "import rdflib\nfrom typing import Optional, Set, Union\n\ndef find_roots(\n    graph: \"rdflib.Graph\", \n    prop: \"rdflib.URIRef\", \n    roots: Optional[Set[\"rdflib.Node\"]] = None\n) -> Set[\"rdflib.Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., are not pointed to by prop)\n    # by checking if there's any triple (x, prop, y) where x is the subject\n    # and y is the parent. If no such y exists for x, then x is a root.\n    \n    # We'll use a set to track all nodes that have a parent\n    has_parent = set()\n    \n    for s, p, o in graph:\n        if p == prop:\n            has_parent.add(s)\n    \n    # The roots are all nodes that are not in has_parent\n    # (i.e., they don't have a parent in the hierarchy)\n    roots.update(set(graph.subjects(prop)) - has_parent)\n    \n    # Recursively find roots by checking if any of the current roots have children\n    # that are not already in the roots set\n    # This is a simplified version - we just need to find nodes that are not pointed to\n    # by the given property\n    \n    # Actually, we need to find nodes that are not the object of any triple (x, prop, y)\n    # So we need to find all nodes that are not the object of any (x, prop, y)\n    \n    # Let's recompute: roots are nodes that are not the object of any triple (x, prop, y)\n    # So we need to find all nodes that are not in the set of objects of (x, prop, y)\n    \n    # But the above logic is flawed. Let's fix it:\n    \n    # We want to find nodes that are not the object of any triple (x, prop, y)\n    # So we need to find all nodes", "import rdflib\nfrom typing import Optional, Set, Union\n\ndef find_roots(\n    graph: \"rdflib.Graph\", \n    prop: \"rdflib.URIRef\", \n    roots: Optional[Set[\"rdflib.Node\"]] = None\n) -> Set[\"rdflib.Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., are not pointed to by prop)\n    # This means they are roots in the hierarchy\n    for s, p, o in graph:\n        if p == prop:\n            # If o is a subject that has no incoming edges with prop, it's a root\n            # But we need to check if o has any incoming edges with prop\n            # So we need to find all nodes that are not pointed to by prop\n            pass\n    \n    # Instead, we can iterate through all nodes and check if they have any incoming edges with prop\n    # If a node has no incoming edges with prop, it is a root\n    nodes = set()\n    for s, p, o in graph:\n        if p == prop:\n            nodes.add(o)\n    \n    # Find all nodes that are not in the set of nodes that have incoming edges with prop\n    # These are the roots\n    roots = set()\n    for node in graph.subjects():\n        if not any(o == node for s, p, o in graph if p == prop):\n            roots.add(node)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set, Union\n\ndef find_roots(\n    graph: \"rdflib.Graph\", \n    prop: \"rdflib.URIRef\", \n    roots: Optional[Set[\"rdflib.Node\"]] = None\n) -> Set[\"rdflib.Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no triple (subject, prop, ?))\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., exists a triple (subject, prop, parent))\n        parent = None\n        for s, p, o in graph:\n            if s == subject and p == prop:\n                parent = o\n                break\n        \n        # If no parent found, this is a root\n        if parent is None:\n            roots.add(subject)\n    \n    return roots", "import rdflib\nfrom typing import Optional, Set, Union\n\ndef find_roots(\n    graph: \"rdflib.Graph\", \n    prop: \"rdflib.URIRef\", \n    roots: Optional[Set[\"rdflib.Node\"]] = None\n) -> Set[\"rdflib.Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no triple (subject, prop, ?))\n    for subject in graph.subjects(prop, None):\n        # Check if this subject has a parent (i.e., exists a triple (subject, prop, parent))\n        parent = None\n        for s, p, o in graph:\n            if s == subject and p == prop:\n                parent = o\n                break\n        \n        # If no parent found, this is a root\n        if parent is None:\n            roots.add(subject)\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4484, "num_retrieved": 14, "inference_time": 0.9565519839525223, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4903, "num_retrieved": 35, "inference_time": 0.9565519839525223, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4131, "num_retrieved": 6, "inference_time": 0.9565519839525223, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4275, "num_retrieved": 13, "inference_time": 0.9565519839525223, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4236, "num_retrieved": 11, "inference_time": 1.2596674263477325, "generate_results": ["import logging\nimport os\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Determine schema filename (assumed to be config_filename + '.schema' or similar)\n            # In the context, schema is typically named after the config with .schema extension\n            # For simplicity, we assume schema filename is config_filename + '.schema'\n            schema_filename = config_filename + '.schema'\n            \n            # Parse configuration and validate against schema\n            config, config_logs = parse_configuration(config_filename, schema_filename, overrides, resolve_env)\n            \n            configs[config_filename] = config\n            logs.extend(config_logs)\n            \n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied when reading configuration file {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n        except Exception as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Unexpected error when processing configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n    \n    return configs, logs", "import logging\nimport os\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Determine schema filename (assumed to be config_filename + '.schema' or similar)\n            # In the context, schema is typically named similarly to config, e.g., config.yaml -> config.schema.yaml\n            # Since no explicit schema mapping is provided, we assume schema is in the same directory\n            # and named with .schema extension\n            schema_filename = config_filename + '.schema'\n            \n            # If schema file doesn't exist, we'll skip validation (but still load config)\n            if not os.path.exists(schema_filename):\n                # Skip validation for schema missing, but still load config\n                pass\n            \n            # Parse configuration and validate against schema\n            config, config_logs = parse_configuration(config_filename, schema_filename, overrides, resolve_env)\n            configs[config_filename] = config\n            logs.extend(config_logs)\n            \n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied when reading configuration file {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n        except Exception as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Error processing configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))", "import logging\nimport os\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Determine schema filename (assumed to be config_filename + '.schema' or similar)\n            # In the context, schema is typically named after the config with .schema extension\n            schema_filename = config_filename + '.schema'\n            \n            # Parse the configuration and validate against schema\n            config, config_logs = parse_configuration(config_filename, schema_filename, overrides, resolve_env)\n            \n            configs[config_filename] = config\n            logs.extend(config_logs)\n            \n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied when reading configuration file {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n        except Exception as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Unexpected error when processing configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n    \n    return configs, logs", "import logging\nimport os\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Determine schema filename (assumed to be config_filename + '.schema' or similar)\n            # In the context, schema is typically named after the config with .schema extension\n            schema_filename = config_filename + '.schema'\n            \n            # Parse configuration and validate against schema\n            config, config_logs = parse_configuration(config_filename, schema_filename, overrides, resolve_env)\n            \n            configs[config_filename] = config\n            logs.extend(config_logs)\n            \n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file not found: {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied when reading configuration file: {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration validation error in {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n        except Exception as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic.config',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Unexpected error when processing configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n\n    return configs, logs", "import logging\nimport os\nimport itertools\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            # Parse the configuration file\n            config = parse_configuration(\n                config_filename, \n                schema_filename=None,  # Schema is not provided in this function\n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n        except Exception as error:\n            # Capture any errors during parsing or validation\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname='',\n                lineno=0,\n                msg=f'Error processing configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=False\n            ))\n\n    return configs, logs"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4201, "num_retrieved": 11, "inference_time": 1.2596674263477325, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4309, "num_retrieved": 11, "inference_time": 1.2596674263477325, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not (0 <= ticks < 86400000000000):\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not (0 <= ticks < 86400000000000):\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not (0 <= ticks < 86400000000000):\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not (0 <= ticks < 86400000000000):\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not (0 <= ticks < 86400000000000):\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4227, "num_retrieved": 11, "inference_time": 1.2596674263477325, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Bolt 3.0 and above\n    handlers[Version(3, 0)] = Bolt4x0\n    handlers[Version(3, 1)] = Bolt4x1\n    handlers[Version(3, 2)] = Bolt4x2\n    handlers[Version(3, 3)] = Bolt4x3\n    handlers[Version(3, 4)] = Bolt4x4\n\n    # Bolt 4.0 and above\n    handlers[Version(4, 0)] = AsyncBolt4x0\n    handlers[Version(4, 1)] = AsyncBolt4x1\n    handlers[Version(4, 2)] = AsyncBolt4x2\n    handlers[Version(4, 3)] = AsyncBolt4x3\n    handlers[Version(4, 4)] = AsyncBolt4x4\n\n    if protocol_version is not None:\n        return {k: v for k, v in handlers.items() if k == protocol_version}\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Define supported protocol handlers based on the context\n    # Bolt 3.0 and above are supported in various versions\n    # We map the version tuples to their respective handler classes\n    \n    # Bolt 3.0\n    handlers[Version(3, 0)] = Bolt4x0\n    \n    # Bolt 3.1 (not explicitly defined in context, but inferred from pattern)\n    # Bolt 3.2 (not explicitly defined)\n    # Bolt 3.5 (not explicitly defined)\n    \n    # Bolt 4.1\n    handlers[Version(4, 1)] = Bolt4x1\n    \n    # Bolt 4.2\n    handlers[Version(4, 2)] = Bolt4x2\n    \n    # Bolt 4.4\n    handlers[Version(4, 4)] = Bolt4x4\n    \n    # Bolt 4.3 (not explicitly defined)\n    \n    # If protocol_version is specified, filter the dictionary\n    if protocol_version is not None:\n        version = Version(*protocol_version)\n        if version in handlers:\n            return {version: handlers[version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Bolt 3.0 and above\n    handlers[Version(3, 0)] = Bolt4x0\n    handlers[Version(3, 1)] = Bolt4x1\n    handlers[Version(3, 2)] = Bolt4x2\n    handlers[Version(3, 3)] = Bolt4x3\n    handlers[Version(3, 4)] = Bolt4x4\n\n    # Bolt 4.0 and above\n    handlers[Version(4, 0)] = AsyncBolt4x0\n    handlers[Version(4, 1)] = AsyncBolt4x1\n    handlers[Version(4, 2)] = AsyncBolt4x2\n    handlers[Version(4, 3)] = AsyncBolt4x3\n    handlers[Version(4, 4)] = AsyncBolt4x4\n\n    if protocol_version is not None:\n        return {k: v for k, v in handlers.items() if k == protocol_version}\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Bolt 3.0 and above\n    handlers[Version(3, 0)] = Bolt4x0\n    handlers[Version(3, 1)] = Bolt4x1\n    handlers[Version(3, 2)] = Bolt4x2\n    handlers[Version(3, 3)] = Bolt4x3\n    handlers[Version(3, 4)] = Bolt4x4\n\n    # Bolt 4.0 and above\n    handlers[Version(4, 0)] = AsyncBolt4x0\n    handlers[Version(4, 1)] = AsyncBolt4x1\n    handlers[Version(4, 2)] = AsyncBolt4x2\n    handlers[Version(4, 3)] = AsyncBolt4x3\n    handlers[Version(4, 4)] = AsyncBolt4x4\n\n    if protocol_version is not None:\n        return {k: v for k, v in handlers.items() if k == protocol_version}\n\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n\n    # Bolt 3.0 and above\n    if hasattr(cls, 'Bolt4x0') and hasattr(cls, 'Bolt4x1') and hasattr(cls, 'Bolt4x2') and hasattr(cls, 'Bolt4x3') and hasattr(cls, 'Bolt4x4'):\n        handlers[Version(4, 0)] = Bolt4x0\n        handlers[Version(4, 1)] = Bolt4x1\n        handlers[Version(4, 2)] = Bolt4x2\n        handlers[Version(4, 3)] = Bolt4x3\n        handlers[Version(4, 4)] = Bolt4x4\n\n    # Bolt 3.0 (legacy)\n    if hasattr(cls, 'Bolt3x0') and hasattr(cls, 'Bolt3x1') and hasattr(cls, 'Bolt3x2'):\n        handlers[Version(3, 0)] = Bolt3x0\n        handlers[Version(3, 1)] = Bolt3x1\n        handlers[Version(3, 2)] = Bolt3x2\n\n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n\n    return handlers"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4419, "num_retrieved": 15, "inference_time": 1.2596674263477325, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4118, "num_retrieved": 7, "inference_time": 1.2596674263477325, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4088, "num_retrieved": 6, "inference_time": 1.2596674263477325, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        if self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed, use recompute=True\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"cannot rescale histogram with zero scale\")\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        if self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed, use recompute=True\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"cannot rescale histogram with zero scale\")\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        if self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed, use recompute=True\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"cannot rescale histogram with zero scale\")\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"Compute or set scale (integral of the histogram).\n\n    If *other* is None, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to recompute).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        if self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed, use recompute=True\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"cannot rescale histogram with zero scale\")\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        if self._scale is None:\n            if not recompute:\n                raise lena.core.LenaValueError(\"scale not computed, use recompute=True\")\n            self._scale = self._compute_scale()\n        return self._scale\n    else:\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"cannot rescale histogram with zero scale\")\n        self._scale = other\n        return self._scale"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4067, "num_retrieved": 5, "inference_time": 1.2596674263477325, "generate_results": ["def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"other must be a numeric value\")\n    \n    # Get the coordinates and errors\n    coords = self.coords\n    if not coords:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Only rescale the last coordinate\n    last_coord_index = len(coords) - 1\n    last_coord = coords[last_coord_index]\n    \n    # Rescale the coordinate and all associated errors\n    if hasattr(last_coord, 'scale'):\n        last_coord.scale(other)\n    \n    # If the graph has errors, rescale them too\n    if hasattr(self, 'errors') and self.errors is not None:\n        for error in self.errors:\n            if hasattr(error, 'scale'):\n                error.scale(other)\n    \n    # Update the scale attribute\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaTypeError(\"other must be a numeric value\")\n    \n    # Get the coordinates and field names\n    coords = self._coords\n    field_names = self._field_names\n    \n    # Only rescale the last coordinate\n    last_coord_index = len(coords) - 1\n    \n    # Rescale the last coordinate and all associated errors\n    if hasattr(self, '_errors') and self._errors is not None:\n        # Rescale the last coordinate and its errors\n        for i, error in enumerate(self._errors):\n            if i == last_coord_index:\n                # Rescale the error value\n                error *= other\n            else:\n                # Only rescale the last coordinate's error\n                pass\n    \n    # Rescale the last coordinate\n    for i, coord in enumerate(coords):\n        if i == last_coord_index:\n            coord *= other\n    \n    # Update the scale\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"other must be a numeric value\")\n    \n    # Get the coordinates and errors\n    coords = self.coords\n    if not coords:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Only rescale the last coordinate\n    last_coord_index = len(coords) - 1\n    \n    # Get the last coordinate and its errors\n    last_coord = coords[last_coord_index]\n    if hasattr(last_coord, 'errors'):\n        # Rescale both the coordinate and its errors\n        for i, error in enumerate(last_coord.errors):\n            if error is not None:\n                last_coord.errors[i] *= other\n    \n    # Rescale the coordinate itself\n    last_coord *= other\n    \n    # Update the scale\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"other must be a numeric value\")\n    \n    # Get the coordinates and errors\n    coords = self.coords\n    if not coords:\n        raise lena.core.LenaValueError(\"graph has no coordinates\")\n    \n    # Only rescale the last coordinate\n    last_coord_index = len(coords) - 1\n    last_coord = coords[last_coord_index]\n    \n    # Rescale the coordinate and its errors\n    if hasattr(last_coord, 'scale'):\n        last_coord.scale(other)\n    \n    # If errors exist, rescale them too\n    if hasattr(last_coord, 'errors') and last_coord.errors is not None:\n        if hasattr(last_coord.errors, 'scale'):\n            last_coord.errors.scale(other)\n    \n    # Update the scale attribute\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"other must be a numeric value\")\n    \n    # Get the coordinates and field names\n    coords = self._coords\n    field_names = self._field_names\n    \n    # Only rescale the last coordinate\n    last_coord_index = len(coords) - 1\n    \n    # Get the last coordinate values\n    last_coord_values = coords[last_coord_index]\n    \n    # Scale the values\n    scaled_values = [val * other for val in last_coord_values]\n    \n    # Update the coordinates\n    coords[last_coord_index] = scaled_values\n    \n    # Scale the errors together with their coordinate\n    # Assuming errors are stored in the same array as values\n    # or in a separate structure - this implementation assumes\n    # errors are part of the coordinate values\n    # In a real implementation, errors would be accessed separately\n    \n    # Update the scale\n    self._scale = other\n    \n    return self._scale"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4011, "num_retrieved": 3, "inference_time": 1.2596674263477325, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    # Determine the dimension of the histogram\n    if hasattr(hist, 'edges'):\n        edges = hist.edges\n        dim = len(edges) if hasattr(edges[0], \"__iter__\") else 1\n    else:\n        raise TypeError(\"hist must be a histogram object with edges attribute\")\n    \n    # Validate field_names dimension\n    if len(field_names) != dim:\n        raise ValueError(f\"field_names must have {dim} elements, {len(field_names)} given\")\n    \n    # Determine coordinates for each bin\n    points = []\n    \n    # Get bin edges and bin values\n    if dim == 1:\n        edges = hist.edges\n        bin_values = hist.bins\n        for i, (left, right)", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    # Get the dimension of the histogram\n    if hasattr(hist, 'edges'):\n        edges = hist.edges\n        dim = len(edges) if hasattr(edges[0], \"__iter__\") else 1\n    else:\n        raise TypeError(\"hist must be a histogram object with edges attribute\")\n    \n    # Validate field_names dimension\n    if len(field_names) != dim:\n        raise ValueError(f\"field_names must have {dim} elements, {len(field_names)} given\")\n    \n    # Determine coordinates for each bin\n    points = []\n    \n    # Get bin edges and bin values\n    if dim == 1:\n        edges = hist.edges[0]\n        bin_values = hist.bins\n        for i, value in enumerate", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    # Determine the coordinate for each bin\n    coordinates = []\n    values = []\n    \n    # Get the edges and bins from the histogram\n    edges = hist.edges\n    bins = hist.bins\n    \n    # Determine the dimension of the histogram\n    dim = len(edges)\n    \n    # Create coordinate points for each bin\n    for i in range(len(bins)):\n        # Get the bin content\n        bin_content = bins[i]\n        \n        # Get the coordinate based on get_coordinate\n        if dim == 1:\n            edge = edges[0]\n            if get_coordinate == \"left\":\n                coord = edge[i]\n            elif get_coordinate == \"right\":\n                coord = edge[i + 1]\n            else:  #", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    # Get the dimension of the histogram\n    if hasattr(hist, 'edges'):\n        edges = hist.edges\n        dim = len(edges) if hasattr(edges[0], \"__iter__\") else 1\n    else:\n        raise TypeError(\"hist must be a histogram object with edges attribute\")\n    \n    # Validate field_names dimension\n    if len(field_names) != dim:\n        raise ValueError(f\"field_names must have the same length as the dimension of the histogram ({dim}), got {len(field_names)}\")\n    \n    # Determine coordinates for each bin\n    points = []\n    \n    # Get bin edges and bin values\n    if dim == 1:\n        edges = edges[0]\n        bin_values = hist.bins", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError('get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{}\" provided'.format(get_coordinate))\n    \n    # Determine the dimension of the histogram\n    if hasattr(hist, \"edges\"):\n        edges = hist.edges\n        dim = len(edges) if hasattr(edges[0], \"__iter__\") else 1\n    else:\n        raise TypeError(\"hist must be a histogram object with edges attribute\")\n    \n    # Validate field_names dimension\n    if len(field_names) != dim:\n        raise ValueError(f\"field_names must have {dim} elements, {len(field_names)} given\")\n    \n    # Determine coordinates for each bin\n    points = []\n    \n    # Get bin edges and bin values\n    if dim == 1:\n        edges = hist.edges\n        bin_values = hist.bins\n        for i, (left, right)"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4104, "num_retrieved": 6, "inference_time": 1.2596674263477325, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except Exception as e:\n            errors.append(f\"Error checking interface provision for {iface!r}: {e}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is None:\n                errors.append(f\"Missing required method {method_name!r} in candidate\")\n    except Exception as e:\n        errors.append(f\"Error checking method definitions: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # In a real implementation, we would compare signatures here\n                # For now, we just assume signature checking is done at a higher level\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except Exception as e:\n            errors.append(f\"Error checking interface provision for {iface!r}: {e}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is None:\n                errors.append(f\"Missing required method {method_name!r} in candidate\")\n    except Exception as e:\n        errors.append(f\"Error checking method definitions: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Compare method signature with interface specification\n                # This is a simplified check - actual signature comparison would require \n                # more detailed inspection of method parameters and return types\n                try:\n                    # Check if method has correct signature (simplified)\n                    # In practice, this", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface, \n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except Exception as e:\n            errors.append(f\"Error checking interface claim for {iface!r}: {e}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is None:\n                errors.append(f\"Missing required method {method_name!r} in candidate\")\n    except Exception as e:\n        errors.append(f\"Error checking method definitions: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Compare method signature with interface specification\n                # This is a simplified check - actual signature comparison would require\n                # more detailed inspection of method parameters, return types, etc.\n                # For now, we just verify the method exists and is callable\n                if not", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except Exception as e:\n            errors.append(f\"Error checking interface claim for {iface!r}: {e}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is None:\n                errors.append(f\"Missing required method {method_name!r} in candidate\")\n    except Exception as e:\n        errors.append(f\"Error checking methods for interface {iface!r}: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Check if method signature matches (simplified - actual signature checking would be more complex)\n                # This is a placeholder - actual implementation would need to compare method signatures\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        try:\n            if not iface.providedBy(candidate):\n                errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n        except Exception as e:\n            errors.append(f\"Error checking interface provision for {iface!r}: {e}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is None:\n                errors.append(f\"Missing required method {method_name!r} in candidate\")\n    except Exception as e:\n        errors.append(f\"Error checking method definitions: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Compare method signature with interface specification\n                # This is a simplified check - actual signature comparison would require more detail\n                # about method parameters and return types\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4103, "num_retrieved": 6, "inference_time": 1.2596674263477325, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface, \n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            if not hasattr(candidate, method_name):\n                errors.append(f\"Object {candidate} does not define method {method_name}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check method signatures\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Check if method signature matches (simplified - actual signature checking would be more complex)\n                # This is a placeholder - actual signature checking would require more detailed implementation\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    try:\n        for attr_name in iface:\n            if not hasattr(candidate, attr_name):\n                errors.append(f\"Object {candidate}", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error is present, \n        it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            if not hasattr(candidate, method_name):\n                errors.append(f\"Object {candidate} does not define method {method_name}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check that methods have correct signature\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Check method signature (simplified - actual signature checking would be more complex)\n                # This is a placeholder for actual signature validation\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    try:\n        for attr_name in iface:\n            if not hasattr(candidate, attr_name):\n                errors.append(f\"Object {candidate} does not define attribute {", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface, \n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            if not hasattr(candidate, method_name):\n                errors.append(f\"Object {candidate} does not define method {method_name}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check that methods have correct signature\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Check method signature - this is a simplified check\n                # In practice, this would involve comparing method signatures\n                # with the interface's method specifications\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    try:\n        for attr_name in iface:\n            if not hasattr(candidate, attr_name):\n                errors.append(f\"Object {candidate", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            if not hasattr(candidate, method_name):\n                errors.append(f\"Object {candidate} does not define method {method_name}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check that methods have correct signature\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Check method signature - this is a simplified check\n                # In a real implementation, we would compare actual signatures\n                # For now, we just verify the method exists and is callable\n                if not callable(method):\n                    errors.append(f\"Method {method_name} is not callable in {candidate}\")\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface, \n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    try:\n        for method_name in iface:\n            if not hasattr(candidate, method_name):\n                errors.append(f\"Object {candidate} does not define method {method_name}\")\n    except Exception as e:\n        errors.append(f\"Error checking methods: {e}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    try:\n        for method_name in iface:\n            method = getattr(candidate, method_name, None)\n            if method is not None:\n                # Check if method signature matches (simplified - actual signature checking would be more complex)\n                # This is a placeholder for actual signature verification\n                pass\n    except Exception as e:\n        errors.append(f\"Error checking method signatures: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    try:\n        for attr_name in iface:\n            if not hasattr(candidate, attr_name):\n                errors.append(f\"Object {candidate}"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4405, "num_retrieved": 16, "inference_time": 1.2596674263477325, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # This is a stub implementation based on the context\n    # In a real implementation, this would check if the candidate class\n    # correctly implements the given interface according to Zope Interface\n    # verification rules.\n    \n    # The actual implementation would typically involve checking the\n    # class's interface inheritance and ensuring it satisfies the interface\n    # requirements.\n    \n    # For now, we return True as a placeholder - in practice, this would\n    # involve more complex logic to verify interface compliance.\n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Implementation based on the context and standard zope.interface behavior\n    from zope.interface import implementedBy\n    from zope.interface import _empty\n    \n    # Get the implemented interfaces of the candidate class\n    implements = implementedBy(candidate)\n    \n    # Check if the interface is in the implemented interfaces\n    if iface not in implements:\n        return False\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check if the interface is present\n        # in the implementation, without strict C3 resolution order checks\n        return True\n    \n    # For strict verification, ensure the interface is properly implemented\n    # and that the class's interface resolution order is consistent\n    # with the interface's expected order\n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Implementation based on context and standard zope.interface behavior\n    from zope.interface import implementedBy\n    from zope.interface import _empty\n    \n    # Get the implemented interfaces of the candidate class\n    implements = implementedBy(candidate)\n    \n    # Check if the interface is in the implemented interfaces\n    if iface not in implements:\n        return False\n    \n    # If tentative is True, we allow for some flexibility in verification\n    if tentative:\n        # In tentative mode, we only check that the interface is in the implements\n        # list, without strict C3 resolution order checks\n        return True\n    \n    # For strict verification, ensure the interface is properly implemented\n    # and that the class's interface resolution order is consistent\n    # with the interface's expected resolution order\n    \n    # Check if the interface is directly implemented or inherited\n    # through the class's base classes\n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Implementation based on the context and standard zope.interface behavior\n    from zope.interface import implementedBy\n    from zope.interface import _empty\n    \n    # Get the implemented interfaces of the candidate class\n    implements = implementedBy(candidate)\n    \n    # Check if the candidate class implements the given interface\n    if iface not in implements:\n        return False\n    \n    # If tentative is True, we allow for the possibility that the interface\n    # might be provided, but not necessarily through the standard implementation\n    if tentative:\n        # In tentative mode, we only check if the interface is in the implementation\n        # chain, without strict verification\n        return True\n    \n    # Otherwise, we perform a strict verification\n    # The candidate must provide the interface directly or through inheritance\n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Implementation based on the context and standard zope.interface behavior\n    from zope.interface import implementedBy\n    from zope.interface import _empty\n    \n    # Get the implemented interfaces of the candidate class\n    implements = implementedBy(candidate)\n    \n    # Check if the candidate class implements the given interface\n    # The interface is considered implemented if it is in the implements tuple\n    if iface not in implements:\n        return False\n    \n    # If tentative is True, we allow for more lenient verification\n    # (e.g., for cases where the interface might be provided indirectly)\n    if tentative:\n        # In tentative mode, we only check if the interface is in the chain\n        # of implemented interfaces (not necessarily directly)\n        for base in implements.__bases__:\n            if iface is base:\n                return True\n        return False\n    \n    return True"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4486, "num_retrieved": 19, "inference_time": 1.2596674263477325, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any base class has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found in bases, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, determine based on bases\n    # In Python, if no metaclass is specified, the default metaclass is type\n    # For interface-based systems, we may need to consider specific metaclasses\n    # based on the base classes\n    \n    # Check if any base class is a metaclass itself\n    for base in bases:\n        if hasattr(base, '__class__') and hasattr(base.__class__, '__name__'):\n            # If a base is a metaclass, use it\n            if base.__class__.__name__ in ('MetaClass', 'InterfaceClass', 'ABCInterface'):\n                return base.__class__\n    \n    # Default to type if no explicit metaclass or metaclass-indicating base is found\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any of the bases have a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found in bases, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any base class has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found in bases, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    # If an explicit metaclass is provided, use it\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If no explicit metaclass is provided, determine based on bases\n    # In Python, if no metaclass is specified, the default metaclass is type\n    # For interface-based systems, we typically use the metaclass from the bases\n    # If all bases are InterfaceClass or similar, we might use a specific metaclass\n    # However, without explicit metaclass information, default to type\n    \n    # Check if any base is a metaclass (e.g., MetaClass from context)\n    for base in bases:\n        if hasattr(base, '__class__') and hasattr(base.__class__, '__name__'):\n            # If base is a metaclass, use it\n            if base.__class__.__name__ in ['MetaClass', 'InterfaceClass']:\n                return base\n    \n    # Default to type if no metaclass is found in bases\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4277, "num_retrieved": 14, "inference_time": 1.2596674263477325, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4324, "num_retrieved": 14, "inference_time": 1.2596674263477325, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    # This implementation is not directly available in the provided context\n    # as the LFU (Least Frequently Used) cache implementation is not fully shown\n    # and the popitem method for LFU is not defined in the retrieved context.\n    # Based on the context, we can infer that LFU caching is not implemented\n    # with a clear popitem method, so we cannot provide a correct implementation\n    # without additional information.\n    raise NotImplementedError(\"LFU cache popitem implementation not available in context\")", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    # This implementation is not directly available in the provided context\n    # as the LFU (Least Frequently Used) cache implementation is not fully shown\n    # and the popitem method for LFU is not defined in the retrieved context.\n    # Based on the context, we can infer that LFU caching is not implemented\n    # with a clear popitem method, so we cannot provide a correct implementation\n    # without additional information.\n    raise NotImplementedError(\"LFU cache popitem implementation not available in context\")", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    # This implementation is not directly available in the provided context\n    # as the LFU (Least Frequently Used) cache implementation is not fully shown\n    # and the popitem method for LFU is not defined in the retrieved context.\n    # Based on the context, we can infer that LFU caching is not implemented\n    # with a clear popitem method, so we cannot provide a correct implementation\n    # without additional information.\n    raise NotImplementedError(\"LFU cache popitem implementation not available in context\")", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    # This implementation is not directly available in the provided context\n    # for an LFU (Least Frequently Used) cache, as the context only shows\n    # LRU, MRU, FIFO, RR, and TTL caches, but not LFU.\n    # Since LFU is not implemented in the provided context, we cannot\n    # accurately implement popitem for LFU based on the given information.\n    # Therefore, we return a placeholder that would be appropriate for\n    # an LFU cache, though it's not fully specified in the context.\n    raise NotImplementedError(\"LFU cache popitem not implemented in context\")", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    # This implementation is not directly available in the provided context\n    # as the LFU (Least Frequently Used) cache implementation is not fully shown\n    # and the popitem method for LFU is not defined in the retrieved context.\n    # Based on the context, we can infer that LFU caching would require frequency\n    # tracking, which is not present in the provided code snippets.\n    raise NotImplementedError(\"LFU popitem implementation not available in context\")"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4408, "num_retrieved": 17, "inference_time": 1.2596674263477325, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4379, "num_retrieved": 16, "inference_time": 1.2580861449241638, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4297, "num_retrieved": 13, "inference_time": 1.2580861449241638, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Select a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get a random key from the cache\n    key = random.choice(list(self.__data.keys()))\n    value = self.__data.pop(key)\n    del self.__size[key]\n    self.__currsize -= self.__size[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Select a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get a random key from the cache\n    key = random.choice(list(self.__data.keys()))\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self.__data:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get a random key from the cache\n    key = random.choice(list(self.__data.keys()))\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4147, "num_retrieved": 7, "inference_time": 1.2580861449241638, "generate_results": ["def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the in-style parameter format\n    # This pattern should match in-style parameters (e.g., named or numbered)\n    # using the param_regex defined in the Style class\n    \n    param_regex = self._in_style.param_regex\n    # Ensure the pattern is properly escaped and formatted\n    import re\n    return re.compile(param_regex)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the parameter style\n    # This pattern matches in-style parameters using the style's param_regex\n    # The pattern should match escape sequences, named parameters, and numeric parameters\n    # as defined by the style's param_regex\n    \n    # Using the style's param_regex to define the pattern\n    # This is a placeholder implementation - actual implementation would depend on\n    # the specific parameter style (e.g., named, positional, etc.)\n    \n    # The pattern should match:\n    # - Escape sequences: escape_char + escape_regex\n    # - Named parameters: named parameter format (e.g., :name)\n    # - Numeric parameters: numeric parameter format (e.g., ? or %d)\n    \n    # Since the exact format depends on the style, we use the param_regex from the style\n    # to define the pattern\n    \n    import re\n    \n    # Build the pattern based on the style's param_regex\n    # This is a simplified version - actual implementation would need to handle\n    # the specific parameter style\n    \n    # The pattern should match the parameter style as defined in the style object\n    pattern = self.param_regex\n    \n    # Ensure the pattern is properly escaped and compiled\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the parameter style\n    # This pattern matches in-style parameters using the style's param_regex\n    # and escape_regex properties\n    pattern = self.param_regex\n    \n    # If escape_regex is provided, we need to account for escape sequences\n    # The pattern should match the escape sequence and parameter\n    # We use the escape_regex to define how escape sequences are matched\n    if self.escape_regex:\n        # Escape sequences are matched using the escape_regex\n        # We need to ensure the pattern accounts for escape sequences\n        # The pattern should match either a parameter or an escape sequence\n        pattern = f\"({self.escape_regex})|({pattern})\"\n    \n    # Compile the pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the parameter style\n    # This pattern matches in-style parameters using the style's param_regex\n    # and escape_regex properties\n    pattern = self.param_regex\n    \n    # If escape_regex is provided, we need to account for escape sequences\n    # The pattern should match escape sequences and parameter placeholders\n    if self.escape_regex:\n        # Escape the escape character in the pattern to prevent it from being matched\n        # as a literal character in the regex\n        escape_char = self.escape_char\n        escaped_escape_char = escape_char.replace('\\\\', '\\\\\\\\')\n        # Build a pattern that matches escape sequences and parameters\n        pattern = self.escape_regex + \"|\" + pattern\n        \n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the parameter style\n    # This pattern matches in-style parameters using the style's param_regex\n    # and escape_regex properties\n    pattern = self.param_regex\n    \n    # Ensure the pattern is properly escaped and handles escape sequences\n    # The pattern should match parameter placeholders and escape sequences\n    # as defined by the style\n    import re\n    return re.compile(pattern)"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4356, "num_retrieved": 14, "inference_time": 1.2580861449241638, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4044, "num_retrieved": 4, "inference_time": 1.2580861449241638, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4101, "num_retrieved": 6, "inference_time": 1.2580861449241638, "generate_results": ["def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4125, "num_retrieved": 7, "inference_time": 1.2580861449241638, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone doesn't support fold, just return the datetime\n    if not hasattr(self, 'fold') or self.fold is None:\n        return dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous\n    if hasattr(self, 'is_ambiguous') and self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # We use fold=0 for the earlier occurrence (first occurrence)\n        # and fold=1 for the later occurrence (second occurrence)\n        # Since we're in UTC, we need to determine which fold corresponds\n        # to the actual time in the local timezone\n        # We'll use fold=0 by default as it's the first occurrence\n        # and the datetime is unambiguous in UTC\n        return enfold(dt, fold=0).replace(tzinfo=self)\n    \n    # For non-ambiguous times, just return the datetime with the timezone\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone doesn't support fold, just return the original dt\n    if not hasattr(self, 'fold') or self.fold is None:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # We use fold=0 for the earlier occurrence (first occurrence)\n        # and fold=1 for the later occurrence (second occurrence)\n        # Since we're in UTC, we need to determine which fold corresponds\n        # to the original datetime\n        # We create two versions of the datetime with different folds\n        dt_fold_0 = dt.replace(tzinfo=self, fold=0)\n        dt_fold_1 = dt.replace(tzinfo=self, fold=1)\n        \n        # Compare the original dt with both fold versions\n        # If dt matches fold_0, then it's the first occurrence\n        # If dt matches fold_1, then it's the second occurrence\n        if dt == dt_fold_0:\n            return dt_fold_0\n        else:\n            return dt_fold_1\n    \n    # If not ambiguous, just return the original dt\n    return dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone doesn't support fold or is ambiguous detection,\n    # we just return the original datetime with the same timezone\n    if not hasattr(self, 'is_ambiguous') or not hasattr(self, 'fold'):\n        return dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous in this timezone\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # We use fold=0 by default (the first occurrence)\n        # But we need to check if the original dt has a fold attribute\n        # If it does, we use that fold value\n        if hasattr(dt, 'fold'):\n            # If the original datetime has a fold value, use it\n            # Otherwise, use fold=0\n            fold = dt.fold if dt.fold != 1 else 0\n        else:\n            fold = 0\n            \n        # Create a new datetime with the same time but in the target timezone\n        # using the fold value\n        new_dt = dt.replace(tzinfo=self, fold=fold)\n    else:\n        # For non-ambiguous times, just return the datetime with the timezone\n        new_dt = dt.replace(tzinfo=self)\n    \n    return new_dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone doesn't support fold, just return the original dt\n    if not hasattr(self, 'fold') or self.fold is None:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if hasattr(self, 'is_ambiguous') and self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # We use fold=0 for the earlier occurrence (first occurrence)\n        # and fold=1 for the later occurrence (second occurrence)\n        # Since we're in UTC, we need to determine which fold is correct\n        # based on the original dt's time\n        \n        # Create two versions of the datetime with different folds\n        dt_fold_0 = dt.replace(tzinfo=self, fold=0)\n        dt_fold_1 = dt.replace(tzinfo=self, fold=1)\n        \n        # Compare the original dt with both folded versions\n        # If dt is equal to dt_fold_0, it's the first occurrence\n        # If dt is equal to dt_fold_1, it's the second occurrence\n        if dt == dt_fold_0:\n            return dt_fold_0\n        else:\n            return dt_fold_1\n    \n    # If not ambiguous, just return the original dt\n    return dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the timezone doesn't support fold or is ambiguous detection,\n    # we just return the original datetime with the same timezone\n    if not hasattr(self, 'is_ambiguous') or not hasattr(self, 'fold'):\n        return dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous in this timezone\n    is_ambiguous = getattr(self, 'is_ambiguous', lambda x: False)(dt)\n    \n    # If it's ambiguous, we need to determine which fold to use\n    # We use fold=0 for the first occurrence (earlier in time)\n    # and fold=1 for the second occurrence (later in time)\n    if is_ambiguous:\n        # For ambiguous times, we return the fold=0 version by default\n        # (the earlier occurrence)\n        return enfold(dt, fold=0).replace(tzinfo=self)\n    \n    # If not ambiguous, just return the original datetime\n    return dt.replace(tzinfo=self)"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4048, "num_retrieved": 4, "inference_time": 1.2580861449241638, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4291, "num_retrieved": 12, "inference_time": 1.2580861449241638, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4316, "num_retrieved": 13, "inference_time": 1.2580861449241638, "generate_results": ["def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message (private payload)\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check if the request body contains a Diaspora-specific format\n    if request.body and b'<diaspora:' in request.body:\n        return \"diaspora\"\n    \n    # Check if the request method is a known Diaspora method\n    if request.method in ['POST', 'PUT'] and request.headers.get('Content-Type', '').startswith('application/diaspora+xml'):\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy format)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message (private format)\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    if request.body and request.body.startswith(b'<diaspora'):\n        return \"diaspora\"\n    \n    # Check if the request path or URL suggests a Diaspora request\n    if request.url and '/diaspora/' in request.url:\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy format)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message (private format)\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    if request.body and b'<diaspora:' in request.body:\n        return \"diaspora\"\n    \n    # Check if the request path or URL suggests a Diaspora request\n    if request.url and '/diaspora/' in request.url:\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy or standard)\n    if request.headers.get('Diaspora-Message') or request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message (private payload)\n    if request.headers.get('Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    if request.body and b'<diaspora:' in request.body:\n        return \"diaspora\"\n    \n    # Check if the request path or URL suggests a Diaspora request\n    if request.url and '/diaspora/' in request.url:\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy format)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message (private format)\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    if request.body and request.body.startswith(b'<diaspora'):\n        return \"diaspora\"\n    \n    # Check if the request path or method suggests a Diaspora request\n    if request.path.startswith('/diaspora/') or request.method in ['POST', 'PUT']:\n        return \"diaspora\"\n    \n    return None"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4380, "num_retrieved": 15, "inference_time": 1.2580861449241638, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request path starts with /matrix or contains matrix-related paths\n    if request.path.startswith('/matrix'):\n        return True\n    \n    # Check if the request has a specific Matrix-related header or parameter\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check for Matrix-specific query parameters\n    if request.query_params.get('matrix') or request.query_params.get('access_token'):\n        return True\n    \n    # Check if the request method is one typically used by Matrix (e.g., PUT, POST with specific paths)\n    if request.method in ['PUT', 'POST'] and (request.path.startswith('/_matrix') or request.path.startswith('/matrix')):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request path starts with /matrix or contains matrix-related paths\n    if request.path.startswith('/matrix'):\n        return True\n    \n    # Check if the request has a specific Matrix-related header or parameter\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check for Matrix-specific query parameters\n    if request.query_params.get('matrix') or request.query_params.get('access_token'):\n        return True\n    \n    # Check if the request method is one typically used by Matrix (e.g., PUT, POST with specific paths)\n    if request.method in ['PUT', 'POST'] and (request.path.startswith('/_matrix') or request.path.startswith('/matrix')):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request path starts with /matrix or contains matrix-related paths\n    if request.path.startswith('/matrix'):\n        return True\n    \n    # Check if the request contains matrix-specific headers or parameters\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check if the request method is related to matrix operations\n    if request.method in ['POST', 'PUT'] and 'matrix' in request.url.lower():\n        return True\n    \n    # Check if the request has a matrix-specific content type\n    content_type = request.headers.get('Content-Type', '')\n    if 'matrix' in content_type.lower():\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request path starts with /matrix or contains matrix-related paths\n    if request.path.startswith('/matrix'):\n        return True\n    \n    # Check if the request has a specific Matrix-related header or parameter\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check for Matrix-specific query parameters\n    if request.query_params.get('matrix') or request.query_params.get('access_token'):\n        return True\n    \n    # Check if the request method is one typically used by Matrix (e.g., PUT for transactions)\n    if request.method in ['PUT', 'POST'] and 'matrix' in request.path:\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request path indicates a Matrix endpoint\n    if request.path.startswith('/_matrix/'):\n        return True\n    \n    # Check if the request contains Matrix-specific headers or parameters\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check for Matrix-specific query parameters\n    query_params = request.url.split('?')[1] if '?' in request.url else ''\n    if 'access_token' in query_params or 'matrix' in query_params:\n        return True\n    \n    return False"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4454, "num_retrieved": 17, "inference_time": 1.2580861449241638, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import tz\n    \n    # Convert to UTC if not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.tzutc())\n    \n    # Format as RFC 3339 with timezone\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S UTC\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    import datetime\n    from dateutil import tz\n    \n    # Convert to UTC if not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.tzutc())\n    \n    # Format as RFC 3339 with timezone\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S UTC\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    import datetime\n    from dateutil import tz\n    \n    # Convert to UTC if not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.tzutc())\n    \n    # Format as RFC 3339 with timezone\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S UTC\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    from datetime import datetime\n    from dateutil import tz\n    \n    # Convert to UTC if it's not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.tzutc())\n    \n    # Format as \"Mon, 01 Jan 2020 12:00:00 GMT\"\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    import datetime\n    from dateutil import parser, tz\n    \n    # Ensure dt is a datetime object\n    if not isinstance(dt, datetime.datetime):\n        dt = parser.parse(dt)\n    \n    # Convert to UTC if not already\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=tz.UTC)\n    \n    # Format as RFC 3339 with timezone\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S UTC\")"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4332, "num_retrieved": 13, "inference_time": 1.2580861449241638, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines to handle code blocks\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with three or more backticks)\n        if line.strip().startswith('```'):\n            # Inside code block - skip processing for tags\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers and process the line\n        # Look for tags in the form #tag\n        # Use regex to find tags, but skip if inside code blocks\n        match = re.search(r'#([a-zA-Z0-9_\\-]+)', line)\n        while match:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # Replace the tag with replacer if provided\n            if replacer:\n                replacement = replacer(tag)\n                line = line.replace(f'#{tag}', replacement)\n            else:\n                line = line.replace(f'#{tag}', '')\n                \n            # Continue searching for more tags in the same line\n            match = re.search(r'#([a-zA-Z0-9_\\-]+)', line)\n        \n        result_lines.append(line)\n    \n    # Join lines back into a single string\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines to handle code blocks\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block - skip processing\n            result_lines.append(line)\n            continue\n            \n        # Check if line contains a tag (starts with #)\n        # We need to handle tags that might be inside code blocks\n        # Use regex to find tags, but skip if inside code blocks\n        # We'll use a simple approach: find # followed by word characters\n        import re\n        tag_matches = re.finditer(r'#([a-zA-Z_][a-zA-Z0-9_]*)(?=[\\s\\.\\,\\:\\;\\(\\)\\{\\}\\[\\]\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\-\\+\\=\\!\\?\\>\\<\\{\\}\\|\\[\\]\\(\\)\\{\\}\\", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    lines = text.splitlines()\n    result = []\n    \n    in_code_block = False\n    code_block_type = None\n    \n    for line in lines:\n        # Check for code block start (either triple backticks or four spaces)\n        if line.strip().startswith('```'):\n            in_code_block = True\n            code_block_type = line.strip().strip('`')\n            result.append(line)\n            continue\n            \n        if in_code_block:\n            # Inside code block, skip processing\n            if line.strip().startswith('```'):\n                in_code_block = False\n                code_block_type = None\n            result.append(line)\n            continue\n            \n        # Check for code block end\n        if line.strip().startswith('```'):\n            in_code_block = False\n            code_block_type = None\n            result.append(line)\n            continue\n            \n        # Process line outside code block\n        # Look for tags (e.g., #tag)\n        # We need to handle the case where tags might be inside quotes or other contexts\n        # We'll use a simple regex that matches #tag but not inside code blocks\n        # and also not inside quotes\n        line_result = line\n        tag_matches = re.finditer(r'#([a-zA-Z0-9_\\-]+)', line)\n        \n        for match in tag_matches:\n            tag = match.group(1)\n            if tag:\n                tags.add(tag)\n                if replacer:\n                    # Replace the tag with the result of the replacer function\n                    line_result = line_result.replace(f'#{tag}', replacer(tag))\n        \n        result.append(line_result)\n    \n    return tags, '\\n'.join(result)", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines to handle code blocks\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block, skip processing\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers and process the rest\n        # Look for tags in the form #tag\n        # We need to ignore tags inside code blocks\n        # Use regex to find tags, but skip if inside code blocks\n        # We'll process the line normally if not in code block\n        match = re.search(r'#(\\w+)', line)\n        while match:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag with the replacer result\n            if replacer:\n                replacement = replacer(tag)\n                line = line.replace(f'#{tag}', replacement)\n            else:\n                line = line.replace(f'#{tag}', '')\n                \n            match = re.search(r'#(\\w+)', line)\n        \n        result_lines.append(line)\n    \n    # Join lines back into a single string\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines to handle code blocks\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with three or more backticks)\n        if line.strip().startswith('```'):\n            # Inside code block - skip processing for tags\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers and process the line\n        # Look for tags in the form #tag\n        # We need to avoid matching inside code blocks\n        # Use regex to find #tag patterns\n        match = re.search(r'#([a-zA-Z0-9_\\-]+)', line)\n        while match:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # Replace the tag with the replacer function if provided\n            if replacer:\n                replacement = replacer(tag)\n                line = line.replace(f'#{tag}', replacement)\n            else:\n                line = line.replace(f'#{tag}', '')\n                \n            # Find the next tag\n            match = re.search(r'#([a-zA-Z0-9_\\-]+)', line)\n        \n        result_lines.append(line)\n    \n    # Join lines back into a single string\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4176, "num_retrieved": 8, "inference_time": 1.2580861449241638, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    from urllib.parse import urljoin, urlparse\n    \n    # Regular expression to match URLs in text\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    def replace_url(match):\n        url = match.group(1)\n        parsed = urlparse(url)\n        \n        # Handle relative URLs by joining with base URL (if needed)\n        # In practice, this would depend on context, but we'll just return the URL\n        # with basic attributes added\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{url}</a>'\n    \n    # Replace all URLs in the text with linkified versions\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    from urllib.parse import urljoin, urlparse\n    \n    def linkify_match(match):\n        url = match.group(1)\n        # Parse the URL to get scheme and netloc\n        parsed = urlparse(url)\n        scheme = parsed.scheme or 'http'\n        netloc = parsed.netloc\n        \n        # Ensure the URL has a valid scheme\n        if not netloc:\n            return match.group(0)\n            \n        # Create the full URL\n        full_url = f\"{scheme}://{netloc}{parsed.path}\"\n        if parsed.query:\n            full_url += f\"?{parsed.query}\"\n        if parsed.fragment:\n            full_url += f\"#{parsed.fragment}\"\n            \n        # Add attributes to the link\n        return f'<a href=\"{full_url}\" target=\"_blank\" rel=\"noopener noreferrer\">{match.group(0)}</a>'\n    \n    # Pattern to match URLs in text\n    pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    # Replace matches with linkified versions\n    return re.sub(pattern, linkify_match, text)", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    from urllib.parse import urljoin, urlparse\n    \n    # Regular expression to match URLs in text\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|[^\\s<>\"{}|\\\\^`[\\]]+\\.[^\\s<>\"{}|\\\\^`[\\]]+|[^\\s<>\"{}|\\\\^`[\\]]+\\.[^\\s<>\"{}|\\\\^`[\\]]+\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        parsed = urlparse(url)\n        # Ensure the URL is absolute\n        if not parsed.netloc:\n            # If no domain, try to make it absolute with a base URL\n            # This is a fallback - in real implementation, base URL should be provided\n            return url\n        \n        # Add target=\"_blank\" and rel=\"noopener noreferrer\" attributes\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{url}</a>'\n    \n    # Replace all URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    from urllib.parse import urljoin, urlparse\n    \n    # Regular expression to match URLs in text\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    def replace_url(match):\n        url = match.group(0)\n        parsed = urlparse(url)\n        \n        # Handle relative URLs\n        if not parsed.netloc:\n            # Assume it's relative and join with base URL (though base URL not provided)\n            # In practice, this would be context-dependent\n            pass\n        \n        # Add attributes like target=\"_blank\", rel=\"noopener noreferrer\"\n        # and ensure it's a valid link\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{url}</a>'\n    \n    # Replace all URLs found in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    import urllib.parse\n    \n    # Pattern to match URLs in text\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`[\\]]+|www\\.[^\\s<>\"{}|\\\\^`[\\]]+)'\n    \n    def replace_url(match):\n        url = match.group(1)\n        # Normalize URL\n        if url.startswith('www.'):\n            url = 'https://' + url\n        \n        # Parse URL to ensure it's valid\n        parsed = urllib.parse.urlparse(url)\n        if not parsed.netloc:\n            return match.group(0)\n            \n        # Ensure protocol is present\n        if not parsed.scheme:\n            url = 'https://' + url\n            \n        # Create link with proper attributes\n        return f'<a href=\"{url}\" rel=\"noopener noreferrer\" target=\"_blank\">{url}</a>'\n    \n    # Replace URLs in the text\n    result = re.sub(url_pattern, replace_url, text)\n    \n    return result"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4283, "num_retrieved": 12, "inference_time": 1.2580861449241638, "generate_results": ["def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get(\"content-type\")\n    except Exception as ex:\n        logger.warning(\"fetch_content_type | Failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"Fetch the HEAD of the remote url to determine the content type.\"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get(\"Content-Type\")\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - Failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"Fetch the HEAD of the remote url to determine the content type.\"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get(\"content-type\")\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - Failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n        return None\n    except requests.exceptions.RequestException:\n        return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type | Failed to fetch content type for %s: %s\", url, ex)\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4332, "num_retrieved": 13, "inference_time": 1.2580861449241638, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a hash symbol\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the hash symbol and check if the rest is valid\n    tag_name = tag[1:]\n    \n    # Tag name should not be empty\n    if not tag_name:\n        return False\n    \n    # Tag name should only contain alphanumeric characters, hyphens, and underscores\n    # and should not start or end with a hyphen or underscore\n    if not tag_name[0].isalnum() or not tag_name[-1].isalnum():\n        return False\n    \n    # Check for invalid characters\n    for char in tag_name:\n        if not char.isalnum() and char not in '-_':\n            return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a '#' character\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the '#' and check if the remaining part is valid\n    tag_name = tag[1:]\n    \n    # Tag name should not be empty\n    if not tag_name:\n        return False\n    \n    # Tag name should only contain alphanumeric characters, hyphens, and underscores\n    # and should not contain spaces or special characters\n    import re\n    pattern = r'^[a-zA-Z0-9_-]+$'\n    if not re.match(pattern, tag_name):\n        return False\n    \n    # Tag name should not be too long (common limit is 20 characters)\n    if len(tag_name) > 20:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a hash symbol\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the hash symbol for further validation\n    tag_name = tag[1:]\n    \n    # Tags should not be empty after removing the hash\n    if not tag_name:\n        return False\n    \n    # Tags should only contain alphanumeric characters, hyphens, and underscores\n    # and should not contain spaces or special characters\n    import re\n    pattern = r'^[a-zA-Z0-9_-]+$'\n    if not re.match(pattern, tag_name):\n        return False\n    \n    # Tags should not be too long (e.g., max 50 characters)\n    if len(tag) > 50:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a hash symbol\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the hash symbol and check if the rest is valid\n    tag_content = tag[1:]\n    \n    # Tag content should not be empty\n    if not tag_content:\n        return False\n    \n    # Tag content should only contain alphanumeric characters, hyphens, and underscores\n    # and should not contain spaces or special characters\n    import re\n    pattern = r'^[a-zA-Z0-9_-]+$'\n    if not re.match(pattern, tag_content):\n        return False\n    \n    # Tag length should be reasonable (e.g., not longer than 20 characters)\n    if len(tag_content) > 20:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a '#' character\n    if not tag.startswith('#'):\n        return False\n    \n    # Remove the '#' and check if the rest is valid\n    tag_name = tag[1:]\n    \n    # Tag name should not be empty\n    if not tag_name:\n        return False\n    \n    # Tag name should only contain alphanumeric characters, hyphens, and underscores\n    # and should not contain spaces or special characters\n    import re\n    pattern = r'^[a-zA-Z0-9_-]+$'\n    if not re.match(pattern, tag_name):\n        return False\n    \n    # Tag name should not be too long (common limit is 20 characters)\n    if len(tag_name) > 20:\n        return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4335, "num_retrieved": 13, "inference_time": 1.112322598695755, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4141, "num_retrieved": 7, "inference_time": 1.112322598695755, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4249, "num_retrieved": 11, "inference_time": 1.112322598695755, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Fetch the document from the provided URL\n    doc, status_code, error = fetch_document(url=url, path=path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Fetch the document from the provided URL\n    doc, status_code, error = fetch_document(url=url, path=path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Fetch the document from the provided URL\n    doc, status_code, error = fetch_document(url=url, path=path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Fetch the document from the provided URL\n    doc, status_code, error = fetch_document(url=url, path=path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Fetch the document from the provided URL\n    doc, status_code, error = fetch_document(url=url, path=path)\n    \n    if not doc:\n        return None\n    \n    try:\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n    \n    return doc"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4187, "num_retrieved": 8, "inference_time": 1.112322598695755, "generate_results": ["from Crypto.PublicKey import RSA\nfrom Crypto.Signature import pkcs1_15\nfrom Crypto.Hash import SHA256\nfrom lxml import etree\nimport base64\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Create a hash of the XML document (excluding signature elements)\n        # We'll use SHA-256 as the hash algorithm\n        xml_str = etree.tostring(root, encoding='unicode')\n        hash_obj = SHA256.new(xml_str.encode('utf-8'))\n        \n        # Load the public key\n        pub_key = RSA.import_key(public_key)\n        \n        # Verify the signature using PKCS#1 v1.5 padding\n        pkcs1_15.new(pub_key).verify(hash_obj, base64.b64decode(signature))\n        \n        return True\n    except (ValueError, TypeError, Exception):\n        return False", "from Crypto.PublicKey import RSA\nfrom lxml import etree\nfrom federation.exceptions import SignatureVerificationError\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Convert public key string to RSA public key object\n        public_key_obj = RSA.import_key(public_key)\n        \n        # Create a signature verifier using the public key\n        # The signature should be verified against the XML document\n        # We need to extract the XML content and verify the signature\n        xml_data = etree.tostring(root, encoding='unicode')\n        \n        # Verify the signature using the public key\n        # This assumes the signature is created using the RSA algorithm\n        # and the signature is over the XML content\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        \n        # Create a hash of the XML content\n        hash_obj = SHA256.new(xml_data.encode('utf-8'))\n        \n        # Verify the signature\n        verifier = pkcs1_15.new(public_key_obj)\n        verifier.verify(hash_obj, signature.encode('utf-8'))\n        \n        return True\n        \n    except (ValueError, TypeError, Exception) as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "from Crypto.PublicKey import RSA\nfrom lxml import etree\nfrom federation.exceptions import SignatureVerificationError\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Convert public key string to RSA public key object\n        pub_key = RSA.import_key(public_key)\n        \n        # Create a signature verifier using the public key\n        # The signature should be verified against the XML document\n        # We need to extract the actual data that was signed\n        # In Diaspora protocol, the signature is typically over the XML content\n        # We'll use the public key to verify the signature\n        \n        # Verify the signature against the document\n        # The signature is expected to be a string that was created using the private key\n        # and the XML content\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        \n        # Create a hash of the document content\n        hash_obj = SHA256.new(etree.tostring(root, encoding='unicode').encode('utf-8'))\n        \n        # Verify the signature\n        verifier = pkcs1_15.new(pub_key)\n        verifier.verify(hash_obj, signature.encode('utf-8'))\n        \n        return True\n        \n    except (ValueError, TypeError, Exception) as e:\n        # If verification fails, raise an error\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "from Crypto.PublicKey import RSA\nfrom lxml import etree\nfrom federation.exceptions import SignatureVerificationError\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Convert public key string to RSA public key object\n        public_key_obj = RSA.import_key(public_key)\n        \n        # Create a signature verifier using the public key\n        # The signature should be verified against the XML document\n        # We need to extract the XML content and verify the signature\n        xml_data = etree.tostring(root, encoding='unicode')\n        \n        # Verify the signature using the public key\n        # This assumes the signature is created using a standard RSA signature scheme\n        # and the XML content is the data being signed\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        \n        # Create a hash of the XML content\n        hash_obj = SHA256.new(xml_data.encode('utf-8'))\n        \n        # Verify the signature\n        verifier = pkcs1_15.new(public_key_obj)\n        verifier.verify(hash_obj, signature.encode('utf-8'))\n        \n        return True\n        \n    except (ValueError, TypeError, Exception) as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "from Crypto.PublicKey import RSA\nfrom lxml import etree\nfrom federation.exceptions import SignatureVerificationError\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Convert public key string to RSA public key object\n        public_key_obj = RSA.import_key(public_key)\n        \n        # Create a signature verifier using the public key\n        # The signature should be verified against the XML document\n        # We need to extract the XML content and verify the signature\n        xml_data = etree.tostring(root, encoding='unicode')\n        \n        # Verify the signature using the public key\n        # This assumes the signature is created using the RSA algorithm\n        # and the signature is over the XML content\n        from Crypto.Signature import pkcs1_15\n        from Crypto.Hash import SHA256\n        \n        # Create a hash of the XML content\n        hash_obj = SHA256.new(xml_data.encode('utf-8'))\n        \n        # Verify the signature\n        verifier = pkcs1_15.new(public_key_obj)\n        verifier.verify(hash_obj, signature.encode('utf-8'))\n        \n        return True\n        \n    except (ValueError, TypeError, Exception) as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4331, "num_retrieved": 13, "inference_time": 1.112322598695755, "generate_results": ["import json\nimport re\nfrom typing import Dict, Any\nfrom lxml import etree\nfrom federation.utils.text import validate_handle\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        # Extract required fields from JSON webfinger\n        result = {\n            \"subject\": data.get(\"subject\"),\n            \"aliases\": data.get(\"aliases\", []),\n            \"links\": []\n        }\n        \n        # Process links from JSON\n        for link in data.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and href:\n                result[\"links\"].append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        # Validate handle if subject exists\n        if result[\"subject\"]:\n            handle = result[\"subject\"].replace(\"acct:\", \"\")\n            if not validate_handle(handle):\n                # If handle is invalid, try to extract from other fields\n                pass\n                \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            # Parse XML document\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            handle = None\n            for elem in root.xpath(\"//link[@rel='http://webfinger.net/rel/profile-page']\"):\n                href = elem.get(\"href\")\n                if href:\n                    # Extract handle from href\n                    if \"@\" in href:\n                        handle = href.split(\"@\")[0]\n            \n            # Extract aliases\n            aliases = []\n            for elem in root.xpath(\"//link[@rel='http://webfinger.net/rel/profile-page']\"):\n                href = elem.get(\"href\")\n                if href:\n                    aliases.append(href)\n            \n            # Extract links\n            links = []\n            for elem in root.xpath(\"//link\"):\n                rel = elem.get(\"rel\")\n                type_ = elem.get(\"type\")\n                href = elem.get(\"href\")\n                if rel and href:\n                    links.append({\n                        \"rel\": rel,\n                        \"type\": type_,\n                        \"href", "import json\nimport re\nfrom typing import Dict, Any\nfrom lxml import etree\nfrom federation.utils.text import validate_handle\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        # Extract required fields from JSON webfinger\n        result = {\n            \"subject\": data.get(\"subject\"),\n            \"aliases\": data.get(\"aliases\", []),\n            \"links\": []\n        }\n        \n        # Process links from JSON\n        for link in data.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and href:\n                result[\"links\"].append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        # Validate handle if subject exists\n        if result[\"subject\"]:\n            handle = result[\"subject\"].replace(\"acct:\", \"\")\n            if not validate_handle(handle):\n                # If handle is invalid, try to extract from other fields\n                pass\n                \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            # Parse XML document\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            handle = None\n            for link in root.xpath(\"//link[@rel='http://webfinger.net/rel/profile-page']\"):\n                href = link.get(\"href\")\n                if href:\n                    # Extract handle from profile page URL\n                    handle = href.split(\"/\")[-2] if \"/\" in href else None\n                    if handle:\n                        subject = f\"acct:{handle}\"\n            \n            # Extract aliases\n            aliases = []\n            for link in root.xpath(\"//link[@rel='lrdd']\"):\n                template = link.get(\"template\")\n                if template:\n                    # Extract handle from template\n                    match = re.search(r\"q=([^&]+)\", template)\n                    if match:\n                        handle = match.group(1)\n                        aliases.append(f\"acct:{handle}\")\n            \n            # Extract links from XRD\n            links = []\n            for link in root.xpath(\"//link\"):", "import json\nimport re\nfrom typing import Dict, Any\nfrom lxml import etree\nfrom federation.utils.text import validate_handle\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        # Extract required fields from JSON webfinger\n        result = {\n            \"subject\": data.get(\"subject\"),\n            \"aliases\": data.get(\"aliases\", []),\n            \"links\": []\n        }\n        \n        # Process links from JSON\n        for link in data.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and href:\n                result[\"links\"].append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        # Validate handle if subject exists\n        if result[\"subject\"]:\n            handle = result[\"subject\"].replace(\"acct:\", \"\")\n            if not validate_handle(handle):\n                # If handle is invalid, try to extract from other fields\n                pass\n        \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            # Parse XML document\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            handle = None\n            guid = None\n            \n            # Look for subject in the XML\n            for elem in root.xpath(\"//link[@rel='http://webfinger.net/rel/profile-page']\"):\n                href = elem.get(\"href\")\n                if href:\n                    # Extract handle from profile page URL\n                    if '@' in href:\n                        handle = href.split('@')[0]\n            \n            # Extract aliases\n            aliases = []\n            for elem in root.xpath(\"//link[@rel='http://microformats.org/profile/hcard']\"):\n                href = elem.get(\"href\")\n                if href:\n                    aliases.append(href)\n            \n            # Extract links\n            links = []\n            for elem in root.xpath(\"//link\"):\n                rel = elem.get(\"rel\")\n                type_ = elem.get(\"type\")\n                href = elem.get(\"href\")\n                if rel and href:\n                    links.append", "import json\nimport re\nfrom typing import Dict, Any\nfrom lxml import etree\nfrom federation.utils.text import validate_handle\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        # Extract required fields from JSON webfinger\n        result = {\n            \"subject\": data.get(\"subject\"),\n            \"aliases\": data.get(\"aliases\", []),\n            \"links\": []\n        }\n        \n        # Process links from JSON\n        for link in data.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and href:\n                result[\"links\"].append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        # Validate handle if subject exists\n        if result[\"subject\"]:\n            handle = result[\"subject\"].replace(\"acct:\", \"\")\n            if not validate_handle(handle):\n                # If handle is invalid, try to extract from other fields\n                pass\n                \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD (old format)\n        try:\n            # Parse XML document\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            handle = None\n            guid = None\n            \n            # Look for subject in XRD\n            for elem in root.iter():\n                if elem.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n                    href = elem.get(\"href\")\n                    if href:\n                        # Extract handle from href\n                        if \"@\" in href:\n                            handle = href.split(\"@\")[0]\n                            # Try to extract guid from path\n                            if \"/users/\" in href:\n                                guid = href.split(\"/users/\")[1].split(\"/\")[0]\n                \n                if elem.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n                    href = elem.get(\"href\")\n                    if href:\n                        if \"/users/\" in href:\n                            guid = href.split(\"/users/\")[1].split(\"/\")[0]\n                \n                if elem.get(\"rel\") == \"http://jo", "import json\nimport re\nfrom typing import Dict, Any\nfrom lxml import etree\nfrom federation.utils.text import validate_handle\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        # Extract required fields from JSON webfinger\n        result = {\n            \"subject\": data.get(\"subject\"),\n            \"aliases\": data.get(\"aliases\", []),\n            \"links\": []\n        }\n        \n        # Process links from JSON\n        for link in data.get(\"links\", []):\n            rel = link.get(\"rel\")\n            type_ = link.get(\"type\")\n            href = link.get(\"href\")\n            if rel and href:\n                result[\"links\"].append({\n                    \"rel\": rel,\n                    \"type\": type_,\n                    \"href\": href\n                })\n        \n        # Validate handle if subject exists\n        if result[\"subject\"]:\n            handle = result[\"subject\"].replace(\"acct:\", \"\")\n            if not validate_handle(handle):\n                # If handle is invalid, try to extract from other fields\n                pass\n                \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            # Parse XML document\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject = None\n            handle = None\n            guid = None\n            \n            # Look for subject in XRD\n            for elem in root.iter():\n                if elem.get(\"rel\") == \"http://webfinger.net/rel/profile-page\":\n                    href = elem.get(\"href\")\n                    if href:\n                        # Extract handle from profile page URL\n                        handle_match = re.search(r\"([^/]+@[^/]+\\.tld)\", href)\n                        if handle_match:\n                            handle = handle_match.group(1)\n                            if handle:\n                                # Validate handle\n                                if not validate_handle(handle):\n                                    continue\n                                result = {\n                                    \"subject\": \"acct:%s\" % handle,\n                                    \"aliases\": [],\n                                    \"links\": []\n                                }\n                                \n                                # Extract links from XRD\n                                for link_elem in root.iter():\n                                    if link_elem.get(\"rel\"):\n                                        result[\"links\"].append"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4432, "num_retrieved": 17, "inference_time": 1.112322598695755, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Split handle to get username and domain\n        if \"@\" not in handle:\n            return None\n        \n        username, domain = handle.split(\"@\", 1)\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, status_code, exception = fetch_document(webfinger_url)\n        \n        # Return document if successful, otherwise None\n        if document and status_code == 200:\n            return document\n            \n        return None\n        \n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Split handle to get username and domain\n        if \"@\" not in handle:\n            return None\n        \n        username, domain = handle.split(\"@\", 1)\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, status_code, exception = fetch_document(webfinger_url)\n        \n        # Return document if successful, otherwise None\n        if document and status_code == 200:\n            return document\n            \n        return None\n        \n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Split handle to get username and domain\n        if \"@\" not in handle:\n            return None\n        \n        username, domain = handle.split(\"@\", 1)\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, status_code, exception = fetch_document(webfinger_url)\n        \n        # Return document if successful, otherwise None\n        if document and status_code == 200:\n            return document\n            \n        return None\n        \n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Split handle to get username and domain\n        if \"@\" not in handle:\n            return None\n        \n        username, domain = handle.split(\"@\", 1)\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, status_code, exception = fetch_document(webfinger_url)\n        \n        # Return document if successful, otherwise None\n        if document and status_code == 200:\n            return document\n            \n        return None\n        \n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Split handle to get username and domain\n        if \"@\" not in handle:\n            return None\n            \n        username, domain = handle.split(\"@\", 1)\n        \n        # Construct the webfinger URL\n        webfinger_url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, status_code, exception = fetch_document(webfinger_url)\n        \n        # Return document if successful, otherwise None\n        if document and status_code == 200:\n            return document\n            \n        return None\n        \n    except Exception:\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4430, "num_retrieved": 18, "inference_time": 1.112322598695755, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Try to retrieve the webfinger document using the standard webfinger endpoint\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document to extract the profile ID\n    # Diaspora webfinger typically has a \"self\" link with type \"application/diaspora+json\"\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            return {\"profile_id\": link[\"href\"]}\n    \n    # If no valid self link is found, return None\n    return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Try to retrieve the webfinger document using the standard webfinger endpoint\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document to extract the profile ID\n    # Diaspora webfinger typically has a \"self\" link with type \"application/diaspora+json\"\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            return {\"profile_id\": link[\"href\"]}\n    \n    # If no valid self link is found, return None\n    return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Try to retrieve the webfinger document using the handle\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document to extract the profile ID\n    # Diaspora webfinger typically has a \"self\" link with type \"application/diaspora+json\"\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            return {\"profile_id\": link[\"href\"]}\n    \n    logger.debug(\"retrieve_and_parse_diaspora_webfinger: found webfinger but it has no diaspora self href\")\n    return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Try to retrieve the webfinger document using the standard webfinger endpoint\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document to extract the profile ID\n    # Diaspora webfinger typically has a \"self\" link with type \"application/diaspora+json\"\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            return {\"profile_id\": link[\"href\"]}\n    \n    # If no valid self link is found, return None\n    return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Fetch the webfinger document using the handle\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document to extract the profile ID\n    # Diaspora webfinger typically has a \"self\" link with type \"application/diaspora+json\"\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            return {\"profile_id\": link[\"href\"]}\n    \n    logger.debug(\"retrieve_and_parse_diaspora_webfinger: found webfinger but it has no diaspora self href\")\n    return None"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4304, "num_retrieved": 14, "inference_time": 1.112322598695755, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger URL for the host\n    webfinger_url = f\"{host}/.well-known/webfinger?resource=acct:example@{host}\"\n    \n    # Try to retrieve the webfinger document\n    document = try_retrieve_webfinger_document(f\"example@{host}\")\n    \n    if not document:\n        # If webfinger fails, try to generate a host-meta document using the host\n        # We need to create a DiasporaHostMeta instance with the host\n        try:\n            hostmeta = DiasporaHostMeta(webfinger_host=host)\n            return hostmeta.render()\n        except Exception as e:\n            logger.error(f\"Failed to generate Diaspora host-meta for {host}: {e}\")\n            return None\n    \n    # Parse the retrieved document to extract host-meta information\n    # For now, we assume the document contains the necessary information\n    # and return a rendered XRD instance\n    return document", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger URL for the host\n    webfinger_url = f\"{host}/.well-known/webfinger?resource=acct:example@{host}\"\n    \n    # Try to retrieve the webfinger document\n    document = try_retrieve_webfinger_document(f\"example@{host}\")\n    \n    if not document:\n        # If webfinger fails, try to generate host-meta using the host\n        # We need to create a DiasporaHostMeta instance with webfinger_host\n        try:\n            # Use the host itself as webfinger_host\n            hostmeta = DiasporaHostMeta(webfinger_host=host)\n            return hostmeta.render()\n        except Exception as e:\n            logger.error(f\"Failed to generate Diaspora host-meta for {host}: {e}\")\n            return None\n    \n    # Parse the retrieved document to extract host-meta information\n    # For now, we assume the document contains the necessary information\n    # and return a generated host-meta document\n    return generate_host_meta(template=\"diaspora\", webfinger_host=host)", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the URL for the host-meta document\n    url = f\"{host}/.well-known/host-meta\"\n    \n    # Fetch the document using the fetch_document helper\n    document, code, exception = fetch_document(host=host, path=\"/.well-known/host-meta\")\n    \n    if exception:\n        logger.debug(\"retrieve_diaspora_host_meta: failed to fetch host-meta document: %s, %s\", code, exception)\n        return None\n    \n    if not document:\n        return None\n    \n    # Parse the document and return the XRD instance\n    # Assuming the document is already in XRD format and can be directly rendered\n    # We'll create a DiasporaHostMeta instance with the host\n    try:\n        hostmeta = DiasporaHostMeta(webfinger_host=host)\n        return hostmeta.render()\n    except Exception as e:\n        logger.debug(\"retrieve_diaspora_host_meta: error creating host-meta: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger URL for the host\n    webfinger_url = f\"{host}/.well-known/webfinger?resource=acct:example@{host}\"\n    \n    # Try to retrieve the webfinger document\n    document = try_retrieve_webfinger_document(f\"example@{host}\")\n    \n    if not document:\n        # If webfinger fails, try to generate host-meta using the host\n        # We need to create a DiasporaHostMeta instance with webfinger_host\n        try:\n            # Use the host itself as webfinger_host\n            hostmeta = DiasporaHostMeta(webfinger_host=host)\n            return hostmeta.render()\n        except Exception as e:\n            logger.error(f\"Failed to generate Diaspora host-meta: {e}\")\n            return None\n    \n    # Parse the document to extract necessary information\n    # For now, we assume the document contains the required data\n    # and return a generated host-meta document\n    try:\n        # Extract host information from the document\n        # This is a simplified implementation - actual parsing would depend on document structure\n        hostmeta = DiasporaHostMeta(webfinger_host=host)\n        return hostmeta.render()\n    except Exception as e:\n        logger.error(f\"Failed to generate Diaspora host-meta: {e}\")\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Construct the webfinger URL for the host\n    webfinger_url = f\"{host}/.well-known/webfinger?resource=acct:example@{host}\"\n    \n    # Try to retrieve the webfinger document\n    document = try_retrieve_webfinger_document(f\"example@{host}\")\n    \n    if not document:\n        # If webfinger fails, try to generate a host-meta document using the host\n        # We need to create a DiasporaHostMeta instance with the webfinger_host\n        # Since we don't have the full user handle or other details, we'll use the host\n        # as the webfinger_host and generate a basic host-meta\n        try:\n            hostmeta = DiasporaHostMeta(webfinger_host=host)\n            return hostmeta.render()\n        except Exception as e:\n            logger.warning(\"Failed to generate host-meta for %s: %s\", host, e)\n            return None\n    \n    # Parse the retrieved document to extract host-meta information\n    # For now, we'll assume the document contains the necessary information\n    # and return a rendered XRD document\n    return document"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4174, "num_retrieved": 8, "inference_time": 1.112322598695755, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4190, "num_retrieved": 8, "inference_time": 1.112322598695755, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val if not keys else None\n    \n    # Traverse through the remaining keys\n    for k in keys:\n        if current.get(k) is None:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val if not keys else {}\n    \n    # Iterate through the remaining keys to create the nested structure\n    for k in keys:\n        if current.get(k) is None:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value at the deepest level\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val if not keys else {}\n    \n    # Iterate through the remaining keys to build the nested structure\n    for k in keys:\n        if current.get(k) is None:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value at the last key\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val if not keys else {}\n    \n    # Iterate through the remaining keys to build the nested structure\n    for k in keys:\n        if current.get(k) is None:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value at the deepest level\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Start with the first key\n    current[key] = val if not keys else None\n    \n    # Traverse through the remaining keys\n    for k in keys:\n        if current.get(k) is None:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value\n    current[val] = val"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4303, "num_retrieved": 12, "inference_time": 1.112322598695755, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.cli.cli import COMPLEX_TYPES\n    \n    class IniType(ComplexType):\n        \"\"\"Complex type that resolves a list of file names from a string input.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if isinstance(value, str):\n                # Split the string by commas and strip whitespace\n                file_names = [fn.strip() for fn in value.split(',') if fn.strip()]\n                return file_names\n            elif isinstance(value, (list, tuple)):\n                return [fn.strip() for fn in value if fn.strip()]\n            else:\n                return []\n    \n    return IniType(\"file_names\", settings_dirs, spec_option.get(\"subcommand\", \"\"), spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.cli.cli import COMPLEX_TYPES\n    \n    class IniType(ComplexType):\n        \"\"\"Complex type that resolves a list of file names from a string input.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if isinstance(value, str):\n                # Split the string by commas and strip whitespace\n                file_names = [fn.strip() for fn in value.split(',') if fn.strip()]\n                return file_names\n            elif isinstance(value, (list, tuple)):\n                return [fn.strip() for fn in value if fn.strip()]\n            else:\n                return []\n    \n    return IniType(\"file_names\", settings_dirs, spec_option.get(\"subcommand\", \"\"), spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.cli.cli import COMPLEX_TYPES\n    \n    class IniType(ComplexType):\n        \"\"\"Complex type that resolves a list of file names from a string input.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if isinstance(value, str):\n                # Split the string by commas and strip whitespace\n                file_names = [fn.strip() for fn in value.split(',') if fn.strip()]\n                return file_names\n            elif isinstance(value, (list, tuple)):\n                return [fn.strip() for fn in value if fn.strip()]\n            else:\n                return []\n    \n    return IniType('list_of_file_names', settings_dirs, spec_option.get('subcommand', ''), spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.cli.cli import COMPLEX_TYPES\n    \n    class IniType(ComplexType):\n        \"\"\"Complex type that resolves a list of file names from a string input.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if isinstance(value, str):\n                # Split the string by commas and strip whitespace\n                file_names = [fn.strip() for fn in value.split(',') if fn.strip()]\n                return file_names\n            elif isinstance(value, (list, tuple)):\n                # Already a list, just return it\n                return [fn.strip() for fn in value if fn.strip()]\n            else:\n                # Return empty list for invalid input\n                return []\n    \n    return IniType('list_of_file_names', settings_dirs, spec_option.get('subcommand', ''), spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    from infrared.core.cli.cli import COMPLEX_TYPES\n    \n    class IniType(ComplexType):\n        \"\"\"Complex type that resolves a list of file names from a string input.\"\"\"\n        \n        def resolve(self, value):\n            \"\"\"Resolves the value of the complex type.\"\"\"\n            if isinstance(value, str):\n                # Split the string by commas and strip whitespace\n                file_names = [fn.strip() for fn in value.split(',') if fn.strip()]\n                return file_names\n            elif isinstance(value, (list, tuple)):\n                return [fn.strip() for fn in value if fn.strip()]\n            else:\n                return []\n    \n    # Create and return the complex type instance\n    return IniType(\n        arg_name=spec_option.get('name'),\n        settings_dirs=settings_dirs,\n        sub_command_name=spec_option.get('subcommand', ''),\n        spec_option=spec_option\n    )"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4132, "num_retrieved": 6, "inference_time": 1.112322598695755, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a singleton and stored as a class attribute\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home=cls.infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a singleton and should be created if not already\n    # This is a placeholder implementation based on context\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home=cls.infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a singleton and stored in CoreServices\n    # Based on context, AnsibleConfigManager is likely accessed through CoreServices\n    from infrared.core.services.ansible_config import AnsibleConfigManager\n    from infrared.core.services import CoreServices\n    \n    # Retrieve the existing instance or create a new one\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(CoreServices.infrared_home())\n    \n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a singleton and should be created if not already created\n    if not hasattr(cls, '_ansible_config_manager'):\n        infrared_home = os.environ.get('INFRARED_HOME', os.path.expanduser('~/.infrared'))\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming the manager is a singleton and should be created if not already created\n    if not hasattr(cls, '_ansible_config_manager'):\n        infrared_home = os.environ.get('INFRARED_HOME', os.path.expanduser('~/.infrared'))\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home)\n    return cls._ansible_config_manager"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4398, "num_retrieved": 16, "inference_time": 1.112322598695755, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    return CoreServices.workspace_manager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4340, "num_retrieved": 15, "inference_time": 1.112322598695755, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class and its internal state.\n    # In a real scenario, this would return an instance of the PluginManager.\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    return CoreServices.plugins_manager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4267, "num_retrieved": 10, "inference_time": 1.112322598695755, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if not spec_content:\n        raise IRValidatorException(\"Spec content is missing\")\n\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(\"Problem with loading spec file. %s\" % err)\n\n    # Define required fields that must be present in the spec\n    required_fields = [\n        'subparsers',\n        'description',\n        'version',\n        'plugin_type'\n    ]\n\n    for field in required_fields:\n        if field not in spec_dict:\n            raise IRValidatorException(f\"Mandatory field '{field}' is missing in spec file\")\n\n    # Additional validation for subparsers\n    if 'subparsers' not in spec_dict or not spec_dict['subparsers']:\n        raise IRValidatorException(\"Subparsers section is missing or empty in spec file\")\n\n    # Validate plugin_type\n    if 'plugin_type' not in spec_dict:\n        raise IRValidatorException(\"plugin_type field is missing in spec file\")\n\n    # Validate description\n    if 'description' not in spec_dict or not spec_dict['description']:\n        raise IRValidatorException(\"description field is missing or empty in spec file\")\n\n    # Validate version\n    if 'version' not in spec_dict or not spec_dict['version']:\n        raise IRValidatorException(\"version field is missing or empty in spec file\")\n\n    return spec_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if not spec_content:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(\"Problem with loading spec file. %s\" % err)\n\n    # Define required fields that must be present in the spec\n    required_fields = [\n        'subparsers',\n        'description',\n        'version',\n        'plugin_type'\n    ]\n\n    for field in required_fields:\n        if field not in spec_dict:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Additional validation for subparsers\n    if not spec_dict.get('subparsers'):\n        raise IRValidatorException(\"Missing 'subparsers' field in spec\")\n\n    # Validate that subparsers has at least one parser\n    subparsers = spec_dict['subparsers']\n    if not isinstance(subparsers, dict) or not subparsers:\n        raise IRValidatorException(\"Subparsers must be a non-empty dictionary\")\n\n    # Validate plugin_type\n    plugin_type = spec_dict.get('plugin_type')\n    if not plugin_type:\n        raise IRValidatorException(\"Missing 'plugin_type' field in spec\")\n\n    return spec_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if not spec_content:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(\"Problem with loading spec file. %s\" % err)\n\n    # Define required fields that must be present in the spec\n    required_fields = [\n        'subparsers',\n        'description',\n        'version',\n        'plugin_type'\n    ]\n\n    for field in required_fields:\n        if field not in spec_dict:\n            raise IRValidatorException(f\"Missing required field: {field} in spec file\")\n\n    # Additional validation for subparsers\n    if not spec_dict.get('subparsers'):\n        raise IRValidatorException(\"Missing 'subparsers' field in spec file\")\n\n    # Validate that subparsers has at least one parser\n    subparsers = spec_dict['subparsers']\n    if not isinstance(subparsers, dict) or not subparsers:\n        raise IRValidatorException(\"Subparsers must be a non-empty dictionary\")\n\n    # Validate plugin_type\n    plugin_type = spec_dict.get('plugin_type')\n    if not plugin_type:\n        raise IRValidatorException(\"Missing 'plugin_type' field in spec file\")\n\n    # Validate description\n    description = spec_dict.get('description')\n    if not description:\n        raise IRValidatorException(\"Missing 'description' field in spec file\")\n\n    # Validate version\n    version = spec_dict.get('version')\n    if not version:\n        raise IRValidatorException(\"Missing 'version' field in spec file\")\n\n    return spec_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(\"Problem with loading spec file. %s\" % err)\n\n    # Define required fields that must be present in the spec\n    required_fields = [\n        'subparsers',\n        'description',\n        'version',\n        'plugin_type'\n    ]\n\n    for field in required_fields:\n        if field not in spec_dict:\n            raise IRValidatorException(f\"Mandatory field '{field}' is missing in spec file\")\n\n    # Additional validation for subparsers\n    if not spec_dict.get('subparsers'):\n        raise IRValidatorException(\"Mandatory field 'subparsers' is missing in spec file\")\n\n    # Validate that subparsers has at least one parser\n    subparsers = spec_dict['subparsers']\n    if not isinstance(subparsers, dict) or not subparsers:\n        raise IRValidatorException(\"subparsers must be a non-empty dictionary\")\n\n    # Validate plugin_type\n    plugin_type = spec_dict.get('plugin_type')\n    if not plugin_type:\n        raise IRValidatorException(\"Mandatory field 'plugin_type' is missing in spec file\")\n\n    return spec_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise IRValidatorException(\"Problem with loading spec file. %s\" % err)\n\n    # Define required fields that must be present in the spec\n    required_fields = [\n        'subparsers',\n        'description',\n        'version',\n        'plugin_type'\n    ]\n\n    for field in required_fields:\n        if field not in spec_dict:\n            raise IRValidatorException(f\"Missing required field: {field} in spec file\")\n\n    # Additional validation for subparsers\n    if not spec_dict.get('subparsers'):\n        raise IRValidatorException(\"Missing 'subparsers' field in spec file\")\n\n    # Validate that subparsers has at least one parser\n    subparsers = spec_dict['subparsers']\n    if not isinstance(subparsers, dict) or not subparsers:\n        raise IRValidatorException(\"Subparsers must be a non-empty dictionary\")\n\n    # Validate plugin_type\n    plugin_type = spec_dict.get('plugin_type')\n    if not plugin_type:\n        raise IRValidatorException(\"Missing 'plugin_type' field in spec file\")\n\n    # Validate description\n    description = spec_dict.get('description')\n    if description is None:\n        raise IRValidatorException(\"Missing 'description' field in spec file\")\n\n    # Validate version\n    version = spec_dict.get('version')\n    if version is None:\n        raise IRValidatorException(\"Missing 'version' field in spec file\")\n\n    return spec_dict"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4320, "num_retrieved": 11, "inference_time": 1.112322598695755, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except yaml.YAMLError as e:\n        raise exceptions.IRValidatorException(f\"Error parsing YAML file: {e}\")\n\n    # Validate that the data is a dictionary (or list, depending on requirements)\n    # If required fields are missing, raise exception\n    # This is a placeholder - actual validation rules should be defined based on context\n    if not isinstance(data, (dict, list)):\n        raise exceptions.IRValidatorException(\"YAML file must contain a dictionary or list\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except yaml.YAMLError as e:\n        raise exceptions.IRValidatorException(f\"Error parsing YAML file: {e}\")\n\n    # Validate that the data is a dictionary (or list, depending on requirements)\n    # If required fields are missing, raise exception\n    # This is a placeholder - actual validation logic should be defined based on required fields\n    if not data:\n        raise exceptions.IRValidatorException(\"YAML file is empty\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except yaml.YAMLError as e:\n        raise exceptions.IRValidatorException(f\"Error parsing YAML file: {e}\")\n\n    # Validate that the data is a dictionary (or list, depending on requirements)\n    # If required fields are missing, raise exception\n    # This is a placeholder - actual validation rules should be defined based on context\n    if data is None:\n        raise exceptions.IRValidatorException(\"YAML file is empty or invalid\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRFileNotFoundException([yaml_file])\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except yaml.YAMLError as e:\n        raise exceptions.IRValidatorException(f\"Invalid YAML format in file {yaml_file}: {str(e)}\")\n\n    # Validate that the data is a dictionary (or list, depending on requirements)\n    # If required fields are not present, raise exception\n    # This is a placeholder - actual validation logic should be defined based on required fields\n    if not data:\n        raise exceptions.IRValidatorException(f\"YAML file {yaml_file} is empty\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    # Validate the file exists\n    if not os.path.isfile(os.path.abspath(yaml_file)):\n        raise exceptions.IRFileNotFoundException([yaml_file])\n\n    # Load the YAML file\n    try:\n        with open(os.path.abspath(yaml_file), 'r') as file:\n            data = yaml.safe_load(file)\n    except yaml.YAMLError as e:\n        raise exceptions.IRValidatorException(f\"Error parsing YAML file: {e}\")\n\n    # Validate that the data is a dictionary (or list, depending on requirements)\n    # If required fields are missing, raise exception\n    # This is a placeholder - actual validation rules should be defined based on context\n    if not data:\n        raise exceptions.IRValidatorException(\"YAML file is empty\")\n\n    return data"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4360, "num_retrieved": 13, "inference_time": 1.0163464546203613, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    # Process each included group\n    for group_name in include_groups:\n        # Assuming there's a mechanism to resolve group references\n        # This is a placeholder implementation based on typical behavior\n        if isinstance(group_name, str):\n            # In a real implementation, this would resolve the group\n            # by looking up in a registry or configuration\n            # For now, we just log or handle the inclusion\n            LOG.info(f\"Including group: {group_name}\")\n            # The actual implementation would depend on how groups are managed\n            # and how they are referenced in the spec files", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    # Process each included group\n    for group_name in include_groups:\n        # Assuming the group_name refers to a group defined in the spec\n        # This would typically involve looking up the group definition\n        # and merging it into the current parser_dict\n        # The actual implementation would depend on how groups are defined\n        # and how they are to be merged with the current parser_dict\n        pass", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    \n    # Expand shared groups if they are defined\n    if include_groups:\n        for group_dict in include_groups:\n            group_name = group_dict.get('name')\n            if group_name:\n                # Add the group to shared groups if it exists\n                if group_name not in shared_groups:\n                    shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    \n    # Expand shared groups if they are defined\n    if include_groups:\n        shared_groups.expand(include_groups)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    # Process each included group\n    for group_name in include_groups:\n        # Assuming there's a mechanism to resolve group references\n        # This is a placeholder implementation based on typical behavior\n        # in configuration parsing systems\n        if isinstance(group_name, str):\n            # In a real implementation, this would resolve the group\n            # by looking up group definitions in the spec or configuration\n            # For now, we just log or process the group name\n            LOG.info(f\"Including group: {group_name}\")\n            # The actual group resolution would depend on the context\n            # and how groups are defined in the spec files"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4291, "num_retrieved": 12, "inference_time": 1.0163464546203613, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_helper.spec_dict\n    \n    # Handle config section if present\n    if 'config' in spec_dict:\n        config = spec_dict['config']\n        defaults.update({\n            'plugin_type': config.get('plugin_type'),\n            'entry_point': config.get('entry_point'),\n            'roles_path': config.get('roles_path')\n        })\n    \n    # Handle subparsers and their options\n    subparsers = spec_dict.get('subparsers', {})\n    for subparser_name, subparser_dict in subparsers.items():\n        # Add default values from subparser configuration\n        if 'default' in subparser_dict:\n            defaults[subparser_name] = subparser_dict['default']\n    \n    # Add any other default values from spec\n    for key, value in spec_dict.items():\n        if key not in ['subparsers', 'config', 'description']:\n            defaults[key] = value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_helper.spec_dict\n    \n    # Handle config section if present\n    if 'config' in spec_dict:\n        config = spec_dict['config']\n        defaults.update({\n            'plugin_type': config.get('plugin_type'),\n            'entry_point': config.get('entry_point'),\n            'roles_path': config.get('roles_path')\n        })\n    \n    # Handle subparsers and their options\n    subparsers = spec_dict.get('subparsers', {})\n    for subparser_name, subparser_dict in subparsers.items():\n        # Add description if available\n        if 'description' in subparser_dict:\n            defaults['description'] = subparser_dict['description']\n        \n        # Add other options from subparser\n        if 'help' in subparser_dict:\n            defaults['help'] = subparser_dict['help']\n    \n    # Add any default values from the spec itself\n    if 'description' in spec_dict:\n        defaults['description'] = spec_dict['description']\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_helper.spec_dict\n    \n    # Process config section if present\n    if 'config' in spec_dict:\n        config = spec_dict['config']\n        defaults.update({\n            'plugin_type': config.get('plugin_type'),\n            'entry_point': config.get('entry_point'),\n            'roles_path': config.get('roles_path')\n        })\n    \n    # Process subparsers to get default values\n    subparsers = spec_dict.get('subparsers', {})\n    for subparser_name, subparser_dict in subparsers.items():\n        # Extract default values from subparser options\n        if 'default' in subparser_dict:\n            defaults[subparser_name] = subparser_dict['default']\n    \n    # Add any other default values from the spec\n    for key, value in spec_dict.items():\n        if key not in ['subparsers', 'config', 'description']:\n            defaults[key] = value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_helper.spec_dict\n    \n    # Handle config section if present\n    if 'config' in spec_dict:\n        config = spec_dict['config']\n        defaults.update({\n            'plugin_type': config.get('plugin_type'),\n            'entry_point': config.get('entry_point'),\n            'roles_path': config.get('roles_path')\n        })\n    \n    # Handle subparsers and their options\n    subparsers = spec_dict.get('subparsers', {})\n    for subparser_name, subparser_dict in subparsers.items():\n        # Add default values from subparser configuration\n        if 'default' in subparser_dict:\n            defaults[subparser_name] = subparser_dict['default']\n    \n    # Add any default values from the spec description or other metadata\n    if 'description' in spec_dict:\n        defaults['description'] = spec_dict['description']\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    spec_dict = self.spec_helper.spec_dict\n    \n    # Handle config section if present\n    if 'config' in spec_dict:\n        config = spec_dict['config']\n        defaults.update({\n            'plugin_type': config.get('plugin_type'),\n            'entry_point': config.get('entry_point'),\n            'roles_path': config.get('roles_path')\n        })\n    \n    # Handle subparsers and their options\n    subparsers = spec_dict.get('subparsers', {})\n    for subparser_name, subparser_dict in subparsers.items():\n        # Add description if available\n        if 'description' in subparser_dict:\n            defaults['description'] = subparser_dict['description']\n        \n        # Add other options from subparser\n        if 'help' in subparser_dict:\n            defaults['help'] = subparser_dict['help']\n    \n    # Add any default values from the spec itself\n    if 'description' in spec_dict:\n        defaults['description'] = spec_dict['description']\n    \n    return defaults"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4201, "num_retrieved": 9, "inference_time": 1.0163464546203613, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4224, "num_retrieved": 10, "inference_time": 1.0163464546203613, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Define deprecated arguments and their replacements\n    deprecated_args = {\n        'generate-answers-file': 'answers-file',\n        'dry-run': 'dry-run',\n        'output': 'output-file',\n        'verbose': 'verbosity',\n        'debug': 'debug'\n    }\n    \n    # Check for deprecated arguments in cli_args\n    deprecated_in_cli = []\n    for arg, value in cli_args.items():\n        if arg in deprecated_args:\n            deprecated_in_cli.append((arg, value))\n    \n    # Check for deprecated arguments in answer_file_args\n    deprecated_in_file = []\n    for arg, value in answer_file_args.items():\n        if arg in deprecated_args:\n            deprecated_in_file.append((arg, value))\n    \n    # Print deprecated arguments\n    if deprecated_in_cli or deprecated_in_file:\n        print(\"Deprecated arguments detected:\")\n        for arg, value in deprecated_in_cli:\n            print(f\"  --{arg} (deprecated, use --{deprecated_args[arg]} instead)\")\n        for arg, value in deprecated_in_file:\n            print(f\"  --{arg} (deprecated, use --{deprecated_args[arg]} instead)\")\n    \n    # Raise exception if any deprecated arguments are found\n    if deprecated_in_cli or deprecated_in_file:\n        raise exceptions.IRDeprecatedOptionsException(\n            \"The following arguments are deprecated: \" + \n            \", \".join([f\"--{arg}\" for arg in deprecated_args.keys()])\n        )", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Define deprecated arguments and their replacements\n    deprecated_args = {\n        'generate-answers-file': 'answers-file',\n        'dry-run': 'dry-run',\n        'output': 'output-file',\n        'verbose': 'verbosity',\n        'inventory': 'inventory-file'\n    }\n    \n    # Check for deprecated arguments in cli_args\n    deprecated_in_cli = []\n    for arg, value in cli_args.items():\n        if arg in deprecated_args:\n            deprecated_in_cli.append((arg, deprecated_args[arg]))\n    \n    # Check for deprecated arguments in answer_file_args\n    deprecated_in_file = []\n    for arg, value in answer_file_args.items():\n        if arg in deprecated_args:\n            deprecated_in_file.append((arg, deprecated_args[arg]))\n    \n    # Print deprecated arguments\n    if deprecated_in_cli or deprecated_in_file:\n        print(\"Warning: The following arguments are deprecated and will be removed in a future version:\")\n        \n        if deprecated_in_cli:\n            print(\"  In CLI arguments:\")\n            for arg, replacement in deprecated_in_cli:\n                print(f\"    {arg} -> {replacement}\")\n        \n        if deprecated_in_file:\n            print(\"  In answer file arguments:\")\n            for arg, replacement in deprecated_in_file:\n                print(f\"    {arg} -> {replacement}\")\n        \n        # If both CLI and file arguments contain deprecated ones, warn about potential conflicts\n        if deprecated_in_cli and deprecated_in_file:\n            print(\"  Note: Both CLI and file arguments contain deprecated options. Please update both.\")\n    \n    # Return True if any deprecations were found, False otherwise\n    return len(deprecated_in_cli) + len(deprecated_in_file) > 0", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Define deprecated arguments and their replacements\n    deprecated_args = {\n        'generate-answers-file': 'answers-file',\n        'dry-run': 'dry-run',\n        'output': 'output-file',\n        'verbose': 'verbosity',\n        'inventory': 'inventory-file'\n    }\n    \n    # Check for deprecated arguments in cli_args\n    deprecated_in_cli = []\n    for arg, value in cli_args.items():\n        if arg in deprecated_args:\n            deprecated_in_cli.append((arg, deprecated_args[arg]))\n    \n    # Check for deprecated arguments in answer_file_args\n    deprecated_in_file = []\n    for arg, value in answer_file_args.items():\n        if arg in deprecated_args:\n            deprecated_in_file.append((arg, deprecated_args[arg]))\n    \n    # Print deprecated arguments\n    if deprecated_in_cli or deprecated_in_file:\n        print(\"DEPRECATED ARGUMENTS DETECTED:\")\n        if deprecated_in_cli:\n            print(\"  In CLI arguments:\")\n            for arg, replacement in deprecated_in_cli:\n                print(f\"    {arg} (use {replacement} instead)\")\n        if deprecated_in_file:\n            print(\"  In answer file arguments:\")\n            for arg, replacement in deprecated_in_file:\n                print(f\"    {arg} (use {replacement} instead)\")\n    \n    # Raise exception if any deprecated arguments are found\n    if deprecated_in_cli or deprecated_in_file:\n        raise exceptions.IRDeprecatedOptionsException(\n            \"The following arguments are deprecated: \" + \n            \", \".join([f\"{arg} ({replacement})\" for arg, replacement in deprecated_in_cli + deprecated_in_file])\n        )", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Define deprecated arguments and their replacements\n    deprecated_args = {\n        'generate-answers-file': 'answers-file',\n        'dry-run': 'dry-run',\n        'output': 'output-file',\n        'verbose': 'verbosity',\n        'inventory': 'inventory-file'\n    }\n    \n    # Check for deprecated arguments in cli_args\n    deprecated_in_cli = []\n    for arg, value in cli_args.items():\n        if arg in deprecated_args:\n            deprecated_in_cli.append((arg, deprecated_args[arg]))\n    \n    # Check for deprecated arguments in answer_file_args\n    deprecated_in_file = []\n    for arg, value in answer_file_args.items():\n        if arg in deprecated_args:\n            deprecated_in_file.append((arg, deprecated_args[arg]))\n    \n    # Print warnings for deprecated arguments\n    if deprecated_in_cli:\n        print(\"WARNING: The following CLI arguments are deprecated and will be removed in a future version:\")\n        for arg, replacement in deprecated_in_cli:\n            print(f\"  --{arg} (use --{replacement} instead)\")\n    \n    if deprecated_in_file:\n        print(\"WARNING: The following arguments from answer file are deprecated and will be removed in a future version:\")\n        for arg, replacement in deprecated_in_file:\n            print(f\"  {arg} (use {replacement} instead)\")\n    \n    # Return True if any deprecations were found, False otherwise\n    return len(deprecated_in_cli) > 0 or len(deprecated_in_file) > 0", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Define deprecated arguments and their replacements\n    deprecated_args = {\n        'generate-answers-file': 'answers-file',\n        'dry-run': 'dry-run',\n        'output': 'output-file',\n        'verbose': 'verbosity',\n        'inventory': 'inventory-file'\n    }\n    \n    # Check for deprecated arguments in cli_args\n    deprecated_in_cli = []\n    for arg, value in cli_args.items():\n        if arg in deprecated_args:\n            deprecated_in_cli.append((arg, deprecated_args[arg]))\n    \n    # Check for deprecated arguments in answer_file_args\n    deprecated_in_file = []\n    for arg, value in answer_file_args.items():\n        if arg in deprecated_args:\n            deprecated_in_file.append((arg, deprecated_args[arg]))\n    \n    # Print deprecated arguments\n    if deprecated_in_cli or deprecated_in_file:\n        print(\"Warning: The following arguments are deprecated and will be removed in a future version:\")\n        if deprecated_in_cli:\n            print(\"  In CLI arguments:\")\n            for arg, replacement in deprecated_in_cli:\n                print(f\"    {arg} -> {replacement}\")\n        if deprecated_in_file:\n            print(\"  In answer file arguments:\")\n            for arg, replacement in deprecated_in_file:\n                print(f\"    {arg} -> {replacement}\")\n    \n    # Raise exception if any deprecated arguments are found\n    if deprecated_in_cli or deprecated_in_file:\n        raise exceptions.IRDeprecatedOptionsException(\n            \"Deprecated arguments found: \" + \", \".join(\n                [f\"{arg} -> {replacement}\" for arg, replacement in deprecated_in_cli + deprecated_in_file]\n            )\n        )"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4260, "num_retrieved": 10, "inference_time": 1.0163464546203613, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4263, "num_retrieved": 10, "inference_time": 1.0163464546203613, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4191, "num_retrieved": 9, "inference_time": 1.0163464546203613, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for arg in args:\n        if arg.startswith('--silent') or arg.startswith('-s'):\n            silent_args.append(arg)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for arg in args:\n        if arg.startswith('--silent') or arg.startswith('-s'):\n            silent_args.append(arg)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for arg in args:\n        if arg.startswith('--silent') or arg.startswith('-s'):\n            silent_args.append(arg)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for arg in args:\n        if arg.startswith('--silent') or arg.startswith('-s'):\n            silent_args.append(arg)\n    return silent_args", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    silent_args = []\n    for arg in args:\n        if arg.startswith('--silent') or arg.startswith('-s'):\n            silent_args.append(arg)\n    return silent_args"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4159, "num_retrieved": 9, "inference_time": 1.0163464546203613, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    \n    if absent:\n        raise Exception(\"Expected {} to be specified\".format(absent))\n    \n    return True", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    \n    if absent:\n        raise ValueError(\"Expected {} to be specified\".format(absent))\n    \n    return True", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    \n    if absent:\n        raise ValueError(\"Expected {} to be specified\".format(absent))\n    \n    return True", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    \n    if absent:\n        raise ValueError(\"Expected {} to be specified\".format(absent))\n    \n    return True", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    absent = []\n    for arg in required_args:\n        if arg not in args or args[arg] is None:\n            absent.append(arg)\n    \n    if absent:\n        raise ValueError(\"Expected {} to be specified\".format(absent))\n    \n    return True"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4306, "num_retrieved": 12, "inference_time": 1.0163464546203613, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if required_when:\n            # Evaluate the condition using the provided args\n            condition_met = False\n            condition_args = required_when.split(',')\n            for cond_arg in condition_args:\n                cond_arg_name = cond_arg.split('=')[0]\n                cond_arg_value = cond_arg.split('=')[1] if '=' in cond_arg else None\n                cond_arg_value = args.get(command_name, {}).get(cond_arg_name)\n                if cond_arg_value is not None and str(cond_arg_value).lower() == str(cond_arg_value).lower():\n                    condition_met = True\n                    break\n            if condition_met:\n                matched_args.append(option_name)\n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        condition_met = False\n        when_args = required_when.split(',')\n        \n        # Evaluate the condition based on args\n        for when_arg in when_args:\n            when_arg_name = when_arg.split('=')[0]\n            when_arg_value = when_arg.split('=')[1] if '=' in when_arg else None\n            \n            # Get the value from args\n            if when_arg_name in args.get(command_name, {}):\n                when_value = args[command_name][when_arg_name]\n                if when_value == when_arg_value:\n                    condition_met = True\n                    break\n        \n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        condition_met = False\n        when_args = required_when.split(',')\n        \n        # Evaluate the condition based on args\n        for when_arg in when_args:\n            when_arg_name = when_arg.split('=')[0]\n            when_arg_value = when_arg.split('=')[1] if '=' in when_arg else None\n            \n            # Get the value from args\n            if when_arg_name in args.get(command_name, {}):\n                when_value = args[command_name][when_arg_name]\n                if when_value == when_arg_value:\n                    condition_met = True\n                    break\n        \n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if required_when:\n            # Evaluate the condition using the provided args\n            condition_met = False\n            condition_args = required_when.split(',')\n            for cond_arg in condition_args:\n                cond_arg_name = cond_arg.split('=')[0]\n                cond_arg_value = cond_arg.split('=')[1] if '=' in cond_arg else None\n                cond_arg_value = args.get(command_name, {}).get(cond_arg_name)\n                if cond_arg_value is not None and str(cond_arg_value).lower() == str(cond_arg_value).lower():\n                    condition_met = True\n                    break\n            if condition_met:\n                matched_args.append(option_name)\n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        condition_met = False\n        when_args = required_when.split(',')\n        \n        # Evaluate the condition based on provided args\n        for when_arg in when_args:\n            when_arg_name = when_arg.split('=')[0]\n            when_arg_value = when_arg.split('=')[1]\n            \n            # Get the value of the when argument from args\n            when_value = args.get(command_name, {}).get(when_arg_name)\n            \n            # Check if the value matches the required_when condition\n            if when_value == when_arg_value:\n                condition_met = True\n                break\n                \n        if condition_met:\n            matched_args.append(option_name)\n            \n    return matched_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4292, "num_retrieved": 12, "inference_time": 1.0163464546203613, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if the string value exceeds any specified length\n            # Assuming length is specified in the context (e.g., in configuration)\n            # Since no specific length is provided in the context, we'll use a placeholder\n            # In a real implementation, length would be defined elsewhere\n            max_length = getattr(self, 'max_length', 100)  # Default max length\n            if len(arg_value) > max_length:\n                invalid_options.append((arg_name, arg_value, max_length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if string length exceeds any specified limit\n            # Assuming length is specified in the context, but not directly provided\n            # In absence of explicit length specification, we can't validate\n            # This is a placeholder - actual implementation would require length info\n            pass\n        # For now, we'll just log or raise if length exceeds a default threshold\n        # Since no specific length is provided in the context, we can't implement\n        # a concrete validation without additional parameters\n    \n    # If no specific length is defined in the context, we can't validate\n    # This function would need additional parameters (e.g., length_map) to work\n    # For now, return empty list as no invalid options found\n    return invalid_options", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if string length exceeds any specified limit\n            # Assuming length is specified in the context, but not directly provided\n            # In absence of explicit length specification, we'll assume it's not defined\n            # This is a placeholder - actual implementation would require length info\n            pass\n        # For now, we'll just return without validation as no length specification is provided\n    # If length specifications were available, we would compare arg_value length against them\n    return invalid_options", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if string length exceeds any specified limit\n            # Assuming length is specified in the context, but not directly provided\n            # In absence of explicit length specification, we'll assume it's not defined\n            # This is a placeholder - actual implementation would require length info\n            pass\n        # For now, we'll just return without validation as length info is not provided\n    # If length validation rules were defined elsewhere, they would be applied here\n    return invalid_options", "def validate_length_args(self, args):\n    \"\"\"Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if string length exceeds any specified limit\n            # Assuming length is specified in the context, but not provided here\n            # This is a placeholder - actual implementation would require knowing\n            # the expected length for each argument\n            pass\n        # For now, we'll just return without validation as the context\n        # does not specify the length limits for each argument\n    return invalid_options"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4293, "num_retrieved": 12, "inference_time": 1.0163464546203613, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    \n    # Iterate through all command names in args\n    for command_name, command_args in args.items():\n        # Get the option specifications for this command\n        for option_name, option_value in command_args.items():\n            # Check if this option has a choice specification\n            # This would typically come from a predefined spec or configuration\n            # For now, we assume that the option specification is available\n            # in a context or class attribute (e.g., self.option_specs)\n            # Since the actual specification is not provided in context,\n            # we'll simulate the validation based on a hypothetical option_spec\n            \n            # Example: if we had a way to get option specs, we'd do:\n            # option_spec = self.get_option_spec(command_name, option_name)\n            # if option_spec and 'choices' in option_spec:\n            #     if option_value not in option_spec['choices']:\n            #         invalid_options.append((command_name, option_name, option_value, option_spec['choices']))\n            \n            # Placeholder: Since no actual option specs are provided in context,\n            # we cannot implement the full validation without additional data.\n            # This is a stub that would need to be completed with actual spec data.\n            pass\n    \n    # If there are invalid options, raise an exception\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    \n    # Iterate through all command names in args\n    for command_name, command_args in args.items():\n        # Get the option specifications for this command\n        for option_name, option_value in command_args.items():\n            # Check if this option has a choice specification\n            # This would typically come from a predefined spec or configuration\n            # For now, we assume that choice information is available in a context\n            # or from a separate source (e.g., command definitions)\n            \n            # Since the actual choice validation logic depends on the specific\n            # command and option definitions, we'll simulate a validation\n            # based on a hypothetical available_choices mapping\n            \n            # Example: if we had a mapping of command:option -> available_choices\n            # we would check if option_value is in available_choices\n            \n            # Placeholder: In a real implementation, this would be replaced\n            # with actual choice validation logic from command definitions\n            pass\n    \n    # If we found invalid options, raise an exception\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    \n    # Assuming the available choices are stored in a dictionary within args\n    # or in a predefined structure, we iterate over the command-specific arguments\n    # and validate each choice against available options.\n    \n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Check if this option has a defined choice list\n            if isinstance(option_value, str) and option_value:\n                # This is a simplified implementation - in real context,\n                # the available choices would be defined in a schema or spec\n                # For example, if available_choices were stored in a dict like:\n                # choices = {'command': ['list', 'add', 'remove'], ...}\n                pass\n    \n    # In the actual implementation, we would need to reference the\n    # available choices from a schema or spec dictionary.\n    # Since the context doesn't provide the full schema, we assume\n    # the validation is done via a predefined choices dictionary.\n    \n    # Example structure of available choices:\n    # choices = {\n    #     'command': ['list', 'add', 'remove', 'update'],\n    #     'plugin_type': ['ssh', 'ansible', 'docker']\n    # }\n    \n    # For now, we'll just return if no invalid options found\n    if not invalid_options:\n        return True\n    \n    # Raise exception if invalid choices found\n    raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    \n    # Assuming the available choices are stored in a dictionary within args\n    # or in a predefined structure, we iterate over all arguments to check choices\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Check if this option has a defined choice list\n            if isinstance(option_value, str) and option_value:\n                # This is a simplified implementation - actual logic would depend on\n                # how choices are defined in the system\n                # For example, if choices are stored in a separate dictionary\n                # or in the command specification\n                pass\n    \n    # Use the IRInvalidChoiceException to raise error if invalid choices found\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    \n    # Assuming the command structure and option specifications are available\n    # through some internal mechanism (e.g., self.option_specs or similar)\n    # We'll iterate over the available options and validate each choice\n    \n    for command_name, command_options in self.option_specs.items():\n        for option_name, option_spec in command_options.items():\n            if 'choices' in option_spec:\n                option_value = args.get(command_name, {}).get(option_name)\n                if option_value is not None:\n                    available_choices = option_spec['choices']\n                    if option_value not in available_choices:\n                        invalid_options.append((option_name, option_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4260, "num_retrieved": 11, "inference_time": 1.0163464546203613, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Extract min and max values from args if they exist\n    min_val = args.get('min', None)\n    max_val = args.get('max', None)\n    value = args.get('value', None)\n    \n    # If any of the required arguments are missing, raise an error\n    if min_val is None or max_val is None or value is None:\n        raise UtilError(\"Expected 'min', 'max', and 'value' to be specified\")\n    \n    # Convert to integers for comparison\n    try:\n        min_val = int(min_val)\n        max_val = int(max_val)\n        value = int(value)\n    except (ValueError, TypeError):\n        raise UtilError(\"Invalid value type for min, max, or value. All must be integers.\")\n    \n    # Check if value is within the range [min, max]\n    if value < min_val or value > max_val:\n        raise UtilError(f\"Value {value} is not within the range [{min_val}, {max_val}]\")\n    \n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Extract min and max values from args if they exist\n    min_val = args.get('min', None)\n    max_val = args.get('max', None)\n    value = args.get('value', None)\n    \n    # If any required argument is missing, raise an error\n    if min_val is None or max_val is None or value is None:\n        raise UtilError(\"Expected 'min', 'max', and 'value' to be specified\")\n    \n    # Convert to integers for comparison\n    try:\n        min_val = int(min_val)\n        max_val = int(max_val)\n        value = int(value)\n    except (ValueError, TypeError):\n        raise UtilError(\"min, max, and value must be integers\")\n    \n    # Check if value is within the range [min, max]\n    if value < min_val or value > max_val:\n        raise UtilError(f\"Value {value} is not within the range [{min_val}, {max_val}]\")\n    \n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Extract min and max values from args if they exist\n    min_val = args.get('min', None)\n    max_val = args.get('max', None)\n    value = args.get('value', None)\n    \n    # If any required argument is missing, raise an error\n    if min_val is None or max_val is None or value is None:\n        raise UtilError(\"Missing required arguments: min, max, or value\")\n    \n    # Convert to integers for comparison\n    try:\n        min_val = int(min_val)\n        max_val = int(max_val)\n        value = int(value)\n    except (ValueError, TypeError):\n        raise UtilError(\"Invalid value type for min, max, or value. All must be integers.\")\n    \n    # Check if value is within the range [min, max]\n    if value < min_val or value > max_val:\n        raise UtilError(f\"Value {value} is not within the range [{min_val}, {max_val}]\")\n    \n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name in args:\n        value = args[arg_name]\n        if not isinstance(value, (int, float)):\n            continue\n            \n        min_val = self.module.params.get(f\"{arg_name}_min\")\n        max_val = self.module.params.get(f\"{arg_name}_max\")\n        \n        if min_val is not None and max_val is not None:\n            min_val = float(min_val)\n            max_val = float(max_val)\n            \n            if not (min_val <= value <= max_val):\n                raise UtilError(f\"Value '{value}' for argument '{arg_name}' is not within the range [{min_val}, {max_val}]\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Extract min and max values from args if they exist\n    min_val = args.get('min', None)\n    max_val = args.get('max', None)\n    value = args.get('value', None)\n    \n    # If any of the required arguments are missing, raise an error\n    if min_val is None or max_val is None or value is None:\n        raise UtilError(\"Expected 'min', 'max', and 'value' to be specified\")\n    \n    # Convert to integers for comparison\n    try:\n        min_val = int(min_val)\n        max_val = int(max_val)\n        value = int(value)\n    except (ValueError, TypeError):\n        raise UtilError(\"Invalid value type for min, max, or value. All must be integers.\")\n    \n    # Check if value is within the range [min, max]\n    if value < min_val or value > max_val:\n        raise UtilError(f\"Value {value} is not within the range [{min_val}, {max_val}]\")\n    \n    return True"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4340, "num_retrieved": 13, "inference_time": 1.0163464546203613, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\n    \n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Create the complex type instance with the provided parameters\n    complex_type = ComplexType(\n        arg_name=option_name,\n        settings_dirs=self.settings_dirs,\n        sub_command_name=subcommand,\n        spec_option=spec_option\n    )\n    return complex_type"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4105, "num_retrieved": 6, "inference_time": 1.0163464546203613, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Define common control arguments that should not be in the spec yml\n    control_keys = [\n        'debug', 'dry-run', 'output', 'verbose', 'generate-answers-file',\n        'extra-vars', 'ansible-args'\n    ]\n    \n    for key, value in args.items():\n        if key in control_keys:\n            control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            nested_args[key] = value\n    \n    # Handle custom arguments specifically (these are passed as ansible variables)\n    # Custom arguments are typically passed in a format like --additional-args option1=value1;option2=value2\n    # We need to parse these and extract the actual ansible variables\n    if 'additional-args' in args:\n        additional_args = args['additional-args']\n        if additional_args:\n            # Parse the additional args using the AdditionalOptionsType logic\n            additional_options_type = AdditionalOptionsType()\n            parsed_args = additional_options_type.resolve(additional_args)\n            # Convert parsed args back to a flat dict of key=value pairs\n            custom_args = {}\n            for i, arg in enumerate(parsed_args):\n                if arg.startswith('--'):\n                    # Remove the -- prefix\n                    key = arg[2:]\n                    if '=' in key:\n                        key, value = key.split('=', 1)\n                        custom_args[key] = value\n                    else:\n                        # If no =, treat as a flag (not a variable)\n                        pass\n            # Add custom args to nested_args\n            nested_args.update(custom_args)\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Define common control arguments that should not be in spec yml\n    control_keys = [\n        'debug', 'dry-run', 'output', 'verbose', 'generate-answers-file',\n        'extra-vars', 'ansible-args'\n    ]\n    \n    for key, value in args.items():\n        if key in control_keys:\n            control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            nested_args[key] = value\n    \n    # Handle custom arguments specifically (if they are passed as --custom-args or similar)\n    # In this implementation, we assume that any argument not in control_keys is nested\n    # Custom arguments are not explicitly defined in the context, so we treat them as nested\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Define common control arguments that should not be in the spec yml\n    control_keys = [\n        'debug', 'dry-run', 'generate-answers-file', 'output', 'verbose',\n        'extra-vars', 'ansible-args'\n    ]\n    \n    for key, value in args.items():\n        if key in control_keys:\n            control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            nested_args[key] = value\n    \n    # Handle custom arguments specifically (these are passed as ansible variables)\n    # Custom arguments are typically passed as --custom-arg=value or similar\n    # They are not part of the nested structure but are used as ansible variables\n    # We need to identify if any argument starts with 'custom-' or similar\n    custom_args = {}\n    for key, value in args.items():\n        if key.startswith('custom-'):\n            custom_args[key] = value\n    \n    return control_args, nested_args, custom_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Define common control arguments that should not be in the spec yml\n    control_keys = [\n        'debug', 'dry-run', 'generate-answers-file', 'output', 'verbose',\n        'ansible-args', 'extra-vars'\n    ]\n    \n    for key, value in args.items():\n        if key in control_keys:\n            control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            nested_args[key] = value\n    \n    # Handle custom arguments specifically (these are passed as ansible variables)\n    # Custom arguments are typically passed as additional args that are not part of the nested structure\n    # In this implementation, we assume that any argument not in control_keys or nested_args is custom\n    # However, based on the context, custom_args are explicitly defined as custom ansible variables\n    # We need to identify if there are any arguments that are meant to be custom ansible variables\n    \n    # Looking at the context, custom_args are passed through the specification and are merged into vars_dict\n    # The context shows that custom_args are passed as part of the parsed_args and are used to update vars_dict\n    # Since the function is to split into control and nested, and custom is not explicitly defined in the split,\n    # we assume that custom arguments are a subset of nested arguments or are handled separately\n    \n    # Based on the spec_handler context, custom_args are passed as part of parsed_args and are used to update vars_dict\n    # Since the function is to return control_args and nested_args, and custom_args are not explicitly defined,\n    # we assume that custom arguments are a subset of nested arguments or are not directly split here\n    \n    # Therefore, we return control_args and nested_args as defined above\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Define common control arguments that should not be in spec yml\n    control_keys = [\n        'debug', 'dry-run', 'generate-answers-file', 'output', 'verbose',\n        'extra-vars', 'ansible-args'\n    ]\n    \n    for key, value in args.items():\n        if key in control_keys:\n            control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            nested_args[key] = value\n    \n    # Handle custom arguments (those that start with 'custom-' prefix)\n    # These are treated as custom ansible variables\n    custom_args = {}\n    for key, value in args.items():\n        if key.startswith('custom-'):\n            custom_args[key.replace('custom-', '')] = value\n    \n    return control_args, nested_args, custom_args"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4184, "num_retrieved": 8, "inference_time": 1.0163464546203613, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for var in extra_vars:\n        if isinstance(var, dict):\n            vars_dict.update(var)\n        else:\n            # Assume var is a string in format \"key=value\"\n            try:\n                key, value = var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If split fails, skip invalid entries\n                pass\n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            vars_dict.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If split fails, skip invalid format\n                pass\n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            vars_dict.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If split fails, skip invalid format\n                pass\n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            vars_dict.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If split fails, skip invalid format\n                pass\n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            vars_dict.update(extra_var)\n        else:\n            # Assume extra_var is a string in format \"key=value\"\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If split fails, skip invalid format\n                pass\n    return vars_dict"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4132, "num_retrieved": 6, "inference_time": 1.0163464546203613, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n\n    # Prepare the command\n    cmd = ['ansible-playbook', playbook_path]\n\n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n\n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars=\"{}\"'.format(yaml.safe_dump(extra_vars, default_flow_style=False))])\n        cmd.extend(vars_str.split())\n\n    # Add ansible_args if provided\n    if ansible_args:\n        args_list = []\n        for key, value in ansible_args.items():\n            if isinstance(value, list):\n                args_list.extend(['--' + key] + [str(v) for v in value])\n            else:\n                args_list.extend(['--' + key, str(value)])\n        cmd.extend(args_list)\n\n    # Execute the playbook\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code on error", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n\n    # Prepare the command\n    cmd = ['ansible-playbook', playbook_path]\n\n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n\n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars=\"{}\"'.format(yaml.safe_dump(extra_vars, default_flow_style=False))])\n        cmd.extend(vars_str.split())\n\n    # Add ansible_args if provided\n    if ansible_args:\n        args_list = []\n        for key, value in ansible_args.items():\n            if isinstance(value, list):\n                args_list.extend(['--' + key] + [str(v) for v in value])\n            else:\n                args_list.extend(['--' + key, str(value)])\n        cmd.extend(args_list)\n\n    # Execute the playbook\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code on error", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n\n    # Prepare the ansible-playbook command\n    cmd = ['ansible-playbook', playbook_path]\n\n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n\n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars=\"{}\"'.format(yaml.safe_dump(extra_vars, default_flow_style=False))])\n        cmd.extend(vars_str.split())\n\n    # Add ansible arguments if provided\n    if ansible_args:\n        args_list = []\n        for key, value in ansible_args.items():\n            if isinstance(value, list):\n                args_list.extend(['--' + key] + [str(v) for v in value])\n            else:\n                args_list.extend(['--' + key, str(value)])\n        cmd.extend(args_list)\n\n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code for any exception", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n\n    # Prepare the command\n    cmd = ['ansible-playbook', playbook_path]\n\n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n\n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars=\"{}\"'.format(yaml.safe_dump(extra_vars, default_flow_style=False))])\n        cmd.extend(vars_str.split())\n\n    # Add ansible arguments if provided\n    if ansible_args:\n        args_list = []\n        for key, value in ansible_args.items():\n            if isinstance(value, list):\n                args_list.extend(['--' + key] + [str(v) for v in value])\n            else:\n                args_list.extend(['--' + key, str(value)])\n        cmd.extend(args_list)\n\n    # Execute the playbook\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            timeout=300  # 5 minute timeout\n        )\n        return result.returncode\n    except subprocess.TimeoutExpired:\n        return 1\n    except Exception as e:\n        return 1", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    import subprocess\n    import os\n\n    # Prepare the command\n    cmd = ['ansible-playbook', playbook_path]\n\n    # Add verbosity if specified\n    if verbose is not None:\n        cmd.extend(['-v'] * verbose)\n\n    # Add extra vars if provided\n    if extra_vars:\n        vars_str = ' '.join(['--extra-vars=\"{}\"'.format(yaml.safe_dump(extra_vars, default_flow_style=False))])\n        cmd.extend(vars_str.split())\n\n    # Add ansible_args if provided\n    if ansible_args:\n        args_list = []\n        for key, value in ansible_args.items():\n            if isinstance(value, str):\n                args_list.extend(['--' + key, value])\n            else:\n                args_list.extend(['--' + key, str(value)])\n        cmd.extend(args_list)\n\n    # Execute the playbook\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=ir_workspace.path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1  # Return non-zero exit code on error"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4053, "num_retrieved": 4, "inference_time": 1.2841903765996296, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for passing to ansible_playbook\n    # Extracting ansible-specific arguments from cli_args\n    ansible_args = {}\n    for arg in cli_args:\n        if arg.startswith('--'):\n            # Handle arguments like --verbose, --extra-vars, etc.\n            if '=' in arg:\n                key, value = arg.split('=', 1)\n                ansible_args[key] = value\n            else:\n                ansible_args[arg] = None\n    \n    # Prepare extra-vars for Ansible\n    extra_vars = vars_dict\n    \n    # Execute the Ansible playbook using the provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=ansible_args.get('verbose', None),\n        extra_vars=extra_vars,\n        ansible_args=ansible_args.get('ansible-args', None)\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for passing to ansible_playbook\n    ansible_args = {}\n    if cli_args:\n        ansible_args = {'ansible-args': ' '.join(cli_args)}\n    \n    # Execute the Ansible playbook with the provided variables and workspace\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        extra_vars=vars_dict,\n        ansible_args=ansible_args.get('ansible-args')\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for passing to ansible_playbook\n    # Extract relevant arguments from cli_args (e.g., verbose, ansible-args)\n    ansible_args = {}\n    for arg in cli_args:\n        if arg.startswith('--verbose'):\n            ansible_args['verbose'] = True\n        elif arg.startswith('--ansible-args'):\n            ansible_args['ansible-args'] = arg.split('=', 1)[1] if '=' in arg else None\n    \n    # Execute the Ansible playbook with the provided vars_dict and arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=ansible_args.get('verbose'),\n        extra_vars=vars_dict,\n        ansible_args=ansible_args.get('ansible-args')\n    )\n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for passing to ansible_playbook\n    # Extract relevant arguments from cli_args (e.g., verbose, ansible-args)\n    ansible_args = {}\n    for arg in cli_args:\n        if arg.startswith('--verbose'):\n            ansible_args['verbose'] = True\n        elif arg.startswith('--ansible-args'):\n            ansible_args['ansible-args'] = arg.split('=', 1)[1] if '=' in arg else None\n    \n    # Execute the Ansible playbook with the provided vars_dict and arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=ansible_args.get('verbose'),\n        extra_vars=vars_dict,\n        ansible_args=ansible_args.get('ansible-args')\n    )\n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Convert cli_args to a dictionary for passing to ansible_playbook\n    # Extract ansible-specific arguments from cli_args\n    ansible_args = {}\n    for arg in cli_args:\n        if arg.startswith('--'):\n            # Handle ansible arguments like --verbose, --extra-vars, etc.\n            if '=' in arg:\n                key, value = arg.split('=', 1)\n                ansible_args[key] = value\n            else:\n                ansible_args[arg] = None\n    \n    # Prepare extra-vars for Ansible\n    extra_vars = vars_dict\n    \n    # Execute the Ansible playbook using the provided workspace and plugin\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=ansible_args.get('verbose', None),\n        extra_vars=extra_vars,\n        ansible_args=ansible_args.get('ansible-args', None))\n    \n    return result"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4169, "num_retrieved": 8, "inference_time": 1.2841903765996296, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get all options for the specified command\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Map option names to their expected types\n    type_map = {}\n    for option in options:\n        if option.type:\n            type_map[option.name] = option.type\n    \n    # Convert values to appropriate types\n    for key, value in values_dict.items():\n        if key in type_map:\n            try:\n                if type_map[key] == 'int':\n                    values_dict[key] = int(value)\n                elif type_map[key] == 'bool':\n                    values_dict[key] = value.lower() in ('true', '1', 'yes', 'on')\n                elif type_map[key] == 'float':\n                    values_dict[key] = float(value)\n                elif type_map[key] == 'list':\n                    values_dict[key] = value.split(',') if value else []\n                elif type_map[key] == 'str':\n                    values_dict[key] = str(value)\n            except (ValueError, TypeError):\n                # Keep original value if conversion fails\n                pass", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get all options for the specified command\n    options = self.get_parser_option_specs(parser_name)\n    \n    for option in options:\n        arg_name = option['name']\n        if arg_name in values_dict:\n            value = values_dict[arg_name]\n            # Convert based on option type if specified\n            if option.get('type') == 'int':\n                values_dict[arg_name] = int(value)\n            elif option.get('type') == 'bool':\n                values_dict[arg_name] = value.lower() in ['true', '1', 'yes', 'on']\n            elif option.get('type') == 'float':\n                values_dict[arg_name] = float(value)\n            elif option.get('type') == 'list':\n                values_dict[arg_name] = value.split(',') if value else []\n            elif option.get('type') == 'str':\n                values_dict[arg_name] = str(value)\n            # Default to string conversion if no specific type\n            else:\n                values_dict[arg_name] = str(value)", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get all options for the specified command\n    options = self.get_parser_option_specs(parser_name)\n    \n    for option in options:\n        arg_name = option['name']\n        if arg_name in values_dict:\n            value = values_dict[arg_name]\n            # Convert based on option type if specified\n            if option.get('type') == 'int':\n                values_dict[arg_name] = int(value)\n            elif option.get('type') == 'bool':\n                values_dict[arg_name] = value.lower() in ('true', '1', 'yes', 'on')\n            elif option.get('type') == 'float':\n                values_dict[arg_name] = float(value)\n            elif option.get('type') == 'list':\n                values_dict[arg_name] = value.split(',') if value else []\n            elif option.get('type') == 'str':\n                values_dict[arg_name] = str(value)\n            # Default to string if no specific type is defined\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get all options for the specified command\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Map option names to their type specifications\n    option_types = {}\n    for option in options:\n        if 'type' in option and option['type'] is not None:\n            option_types[option['name']] = option['type']\n    \n    # Convert values to appropriate types\n    for key, value in values_dict.items():\n        if key in option_types:\n            type_spec = option_types[key]\n            if type_spec == 'int':\n                values_dict[key] = int(value)\n            elif type_spec == 'bool':\n                values_dict[key] = value.lower() in ('true', '1', 'yes', 'on')\n            elif type_spec == 'float':\n                values_dict[key] = float(value)\n            elif type_spec == 'list':\n                values_dict[key] = value.split(',') if value else []\n            elif type_spec == 'str':\n                values_dict[key] = str(value)\n    \n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get all options for the specified command\n    options = self.get_parser_option_specs(parser_name)\n    \n    for option in options:\n        arg_name = option['name']\n        if arg_name in values_dict:\n            value = values_dict[arg_name]\n            # Convert based on option type if specified\n            if option.get('type') == 'int':\n                values_dict[arg_name] = int(value)\n            elif option.get('type') == 'bool':\n                values_dict[arg_name] = value.lower() in ['true', '1', 'yes', 'on']\n            elif option.get('type') == 'float':\n                values_dict[arg_name] = float(value)\n            elif option.get('type') == 'list':\n                values_dict[arg_name] = value.split(',') if value else []\n            elif option.get('type') == 'str':\n                values_dict[arg_name] = str(value)\n            # Default to string if no type specified\n    return values_dict"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4188, "num_retrieved": 8, "inference_time": 1.2841903765996296, "generate_results": ["import os\nimport yaml\nfrom collections import OrderedDict\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_file = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_file):\n        return {}\n    \n    with open(spec_file, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    result = {}\n    \n    # Extract plugin_type from config if present\n    if 'config' in spec:\n        config = spec['config']\n        result['plugin_type'] = config.get('plugin_type', '')\n        result['entry_point'] = config.get('entry_point', '')\n        result['roles_path'] = config.get('roles_path', '')\n    \n    # Extract description\n    if 'description' in spec:\n        result['description'] = spec['description']\n    \n    # Extract subparsers (only first key if exists)\n    if 'subparsers' in spec:\n        subparsers = spec['subparsers']\n        if subparsers:\n            first_key = next(iter(subparsers.keys()))\n            result['subparsers'] = first_key\n    \n    return result", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    result = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                result[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            result[key] = value\n    \n    return result", "import os\nimport yaml\nfrom collections import OrderedDict\n\ndef get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_file = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_file):\n        return {}\n\n    with open(spec_file, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n\n    # Flatten the spec dictionary\n    result = {}\n    \n    # Extract plugin_type from config if present\n    if 'config' in spec:\n        config = spec['config']\n        result['plugin_type'] = config.get('plugin_type', '')\n        result['entry_point'] = config.get('entry_point', '')\n        result['roles_path'] = config.get('roles_path', '')\n    \n    # Extract description\n    if 'description' in spec:\n        result['description'] = spec['description']\n    \n    # Extract subparsers (only the first key if present)\n    if 'subparsers' in spec:\n        subparsers = spec['subparsers']\n        if subparsers:\n            first_key = next(iter(subparsers.keys()))\n            result['subparsers'] = first_key\n    \n    return result", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    result = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                result[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            result[key] = value\n    \n    return result", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec = yaml.safe_load(stream) or {}\n    \n    # Flatten the spec dictionary\n    result = {}\n    for key, value in spec.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in value.items():\n                result[f\"{key}.{sub_key}\"] = sub_value\n        else:\n            result[key] = value\n    \n    return result"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4335, "num_retrieved": 12, "inference_time": 1.2841903765996296, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INFRARED_CONFIG_PATH'):\n        os.environ['INFRARED_CONFIG_PATH'] = self._config_file", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INFRARED_CONFIG_PATH'):\n        os.environ['INFRARED_CONFIG_PATH'] = self._config_file", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INIR_CONFIG_PATH'):\n        os.environ['INIR_CONFIG_PATH'] = self._config_file", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INFRARED_CONFIG_PATH'):\n        os.environ['INFRARED_CONFIG_PATH'] = self._config_file", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('INFRARED_CONFIG_PATH'):\n        os.environ['INFRARED_CONFIG_PATH'] = self._config_file"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4400, "num_retrieved": 15, "inference_time": 1.2841903765996296, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would involve parsing the spec_dict\n    # and adding appropriate CLI options to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would involve parsing the spec_dict\n    # and adding appropriate CLI options to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would involve parsing the spec_dict\n    # and adding appropriate CLI options to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would involve parsing the spec_dict\n    # and adding appropriate CLI arguments to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This implementation assumes the spec object has a spec_dict that defines\n    # the CLI options to be added to the subparser\n    spec_dict = self.spec_helper.spec_dict\n    \n    # Process subparsers if defined\n    subparsers = spec_dict.get('subparsers', {})\n    if subparsers:\n        for subparser_name, subparser_dict in subparsers.items():\n            cmd_parser = root_subparsers.add_parser(\n                subparser_name,\n                help=subparser_dict.get('help', '')\n            )\n            # Extend the subparser with its own options\n            self._extend_parser(cmd_parser, subparser_dict)\n    \n    # Add main parser options from the spec\n    main_options = spec_dict.get('main', {})\n    for opt_name, opt_value in main_options.items():\n        if opt_name == 'help':\n            continue\n        # Handle different option types\n        action = opt_value.get('action', 'store')\n        default = opt_value.get('default')\n        help_text = opt_value.get('help', '')\n        \n        if action == 'store_true':\n            cmd_parser.add_argument(\n                '--' + opt_name,\n                action='store_true',\n                help=help_text,\n                default=default\n            )\n        elif action == 'store':\n            cmd_parser.add_argument(\n                '--' + opt_name,\n                help=help_text,\n                default=default\n            )\n        elif action == 'count':\n            cmd_parser.add_argument(\n                '--' + opt_name,\n                action='count',\n                help=help_text,\n                default=default\n            )\n        elif action == 'append':\n            cmd_parser.add_argument(\n                '--' + opt_name,\n                action='append',\n                help=help_text,\n                default=default\n            )\n        else:\n            # Handle other actions or types as needed\n            cmd_parser.add_argument(\n                '--' + opt_name,\n                help=help_text,\n                default=default\n            )"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 4096, "actual_tokens": 4203, "num_retrieved": 12, "inference_time": 1.2841903765996296, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")"]}
