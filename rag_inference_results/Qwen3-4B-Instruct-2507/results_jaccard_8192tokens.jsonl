{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9603, "num_retrieved": 51, "inference_time": 1.2111813724040985, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: the nanoseconds of the time\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    # Extract hours, minutes, seconds, and nanoseconds from the total nanoseconds\n    # First convert nanoseconds to seconds and nanoseconds\n    seconds = nanoseconds // 1_000_000_000\n    nanoseconds_remaining = nanoseconds % 1_000_000_000\n    \n    # Extract hours, minutes, and seconds from the total seconds\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a time object with the extracted components\n    # Note: The tz parameter is used to set the timezone info\n    # For Time and LocalTime, we need to handle the timezone appropriately\n    # Since Time and LocalTime are different, we need to determine which one to create\n    \n    # If tz is provided, we create a Time with timezone info\n    # Otherwise, we create a LocalTime (which has no timezone)\n    if tz is not None:\n        # Create a time with timezone info\n        from datetime import time, timezone\n        # Convert to a datetime.time object with timezone\n        t = time(hours, minutes, seconds, nanoseconds_remaining)\n        # Return a Time object with timezone info\n        return Time(hours, minutes, seconds, nanoseconds_remaining, tz)\n    else:\n        # Create a LocalTime (without timezone)\n        return Time(hours, minutes, seconds, nanoseconds_remaining)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds component of the time.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    # Extract hours, minutes, seconds, and nanoseconds from the total nanoseconds\n    # Time is represented as hours, minutes, seconds, nanoseconds\n    # We need to convert nanoseconds to hours, minutes, seconds\n    \n    # First, convert nanoseconds to seconds\n    seconds = nanoseconds // 1_000_000_000\n    \n    # Extract hours, minutes, and remaining seconds\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Get the remaining nanoseconds\n    remaining_nanoseconds = nanoseconds % 1_000_000_000\n    \n    # Create a time object with the extracted components\n    # Note: The actual implementation would depend on the internal structure of Time class\n    # Based on context, Time likely has hour, minute, second, nanosecond, and tzinfo attributes\n    \n    # Since we don't have direct access to the Time class constructor, we assume it's defined\n    # in the context and can be constructed with these components\n    return Time(hours, minutes, seconds, remaining_nanoseconds, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds of the time.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    # Extract hours, minutes, seconds, and nanoseconds from the total nanoseconds\n    # Time is represented as hours, minutes, seconds, nanoseconds\n    # We need to convert nanoseconds to hours, minutes, seconds\n    \n    # First, convert nanoseconds to seconds\n    seconds = nanoseconds // 1_000_000_000\n    \n    # Get the remaining nanoseconds\n    remaining_nanoseconds = nanoseconds % 1_000_000_000\n    \n    # Convert seconds to hours, minutes, and seconds\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a Time object with the extracted components\n    # Note: The Time class constructor expects hour, minute, second, nanosecond, and optional tzinfo\n    return Time(hours, minutes, seconds, remaining_nanoseconds, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds of the time.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    # Extract hours, minutes, seconds, and nanoseconds from the total nanoseconds\n    # First convert nanoseconds to seconds and nanoseconds\n    seconds = nanoseconds // 1_000_000_000\n    nanoseconds_remaining = nanoseconds % 1_000_000_000\n    \n    # Extract hours, minutes, and seconds from the total seconds\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a time object with the extracted components\n    # Note: The tz parameter is used to set timezone info if provided\n    # In the context of Time/LocalTime, tz is typically not used for LocalTime\n    # but may be used for Time with timezone\n    return Time(hours, minutes, seconds, nanoseconds_remaining, tz)", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: The nanoseconds of the time.\n    :param tz: Optional timezone information.\n    :return: Time\n    \"\"\"\n    # Extract hours, minutes, seconds, and nanoseconds from the total nanoseconds\n    # Time is represented as hours, minutes, seconds, nanoseconds\n    # We need to convert nanoseconds to hours, minutes, seconds\n    \n    # First, convert nanoseconds to seconds\n    seconds = nanoseconds // 1_000_000_000\n    \n    # Extract hours, minutes, and seconds\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Get remaining nanoseconds\n    remaining_nanoseconds = nanoseconds % 1_000_000_000\n    \n    # Create a time object with the extracted components\n    # Note: The original context shows Time.from_native uses datetime.time\n    # and the constructor takes hour, minute, second, nanosecond, tzinfo\n    return Time(hours, minutes, seconds, remaining_nanoseconds, tz)"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9477, "num_retrieved": 48, "inference_time": 1.2111813724040985, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return Structure(b\"t\", value.days, value.seconds, value.microseconds)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for timedelta values.\n\n    :param value:\n    :type value: timedelta\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_timedelta(value)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9774, "num_retrieved": 56, "inference_time": 1.2111813724040985, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"t\", value.hour, value.minute, value.second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"t\", value.hour, value.minute, value.second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"t\", value.hour, value.minute, value.second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"t\", value.hour, value.minute, value.second, nanosecond)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    nanosecond = value.nanosecond\n    return Structure(b\"t\", value.hour, value.minute, value.second, nanosecond)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9699, "num_retrieved": 54, "inference_time": 1.2111813724040985, "generate_results": ["def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    # Extract coordinates from the point\n    x, y = value.x, value.y\n    \n    # Return a structure with the point data\n    return Structure(b\"P\", x, y)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    # Extract coordinates from the point\n    x, y = value.x, value.y\n    \n    # Return a structure with the point data\n    return Structure(b\"P\", x, y)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    # Extract coordinates from the point\n    x, y = value.coordinates\n    \n    # Return a structure with the point data\n    return Structure(b\"P\", x, y)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    # Extract coordinates from the point\n    x, y = value.coordinates\n    \n    # Return a structure with the point type identifier and coordinates\n    return Structure(b\"P\", x, y)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for `Point` values.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    # Extract coordinates from the point\n    x, y = value.x, value.y\n    \n    # Return a structure with the point data\n    return Structure(b\"P\", x, y)"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10845, "num_retrieved": 91, "inference_time": 1.2111813724040985, "generate_results": ["def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self.keys())"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8598, "num_retrieved": 19, "inference_time": 1.2111813724040985, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we see Bolt3 and Bolt4x1/Bolt4x0 classes\n    # Bolt3 supports versions 3.5, 4.0, 4.1, 4.2, 4.3, 4.4\n    # Bolt4x1 supports versions 4.1-4.4\n    \n    # Add Bolt3 handler for versions 3.5 and above\n    if protocol_version is None or protocol_version >= (3, 5):\n        handlers[(3, 5)] = Bolt3\n    \n    # Add Bolt4x1 handler for versions 4.1 and above\n    if protocol_version is None or protocol_version >= (4, 1):\n        handlers[(4, 1)] = Bolt4x1\n    \n    # If a specific protocol version is requested, filter the dictionary\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we see Bolt3 and Bolt4x1/Bolt4x0 classes\n    # These are supported by Neo4j versions 3.5, 4.0, 4.1, 4.2, 4.3, 4.4\n    \n    # Bolt 3 (supported by Neo4j 3.5, 4.0, 4.1, 4.2, 4.3, 4.4)\n    handlers[(3, 5)] = Bolt3\n    \n    # Bolt 4.1 (supported by Neo4j 4.1 - 4.4)\n    handlers[(4, 1)] = Bolt4x1\n    \n    # Bolt 4.0 (not explicitly defined in context, but likely supported)\n    # Since no explicit class is defined for 4.0, we skip it\n    \n    # If protocol_version is specified, return only that version if supported\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we have:\n    # - Bolt3 for versions 3.5, 4.0, 4.1, 4.2, 4.3, 4.4\n    # - Bolt4x1 for versions 4.1-4.4\n    \n    # Bolt 3 support (versions 3.5, 4.0, 4.1, 4.2, 4.3, 4.4)\n    handlers[(3, 5)] = Bolt3\n    handlers[(4, 0)] = Bolt3\n    handlers[(4, 1)] = Bolt3\n    handlers[(4, 2)] = Bolt3\n    handlers[(4, 3)] = Bolt3\n    handlers[(4, 4)] = Bolt3\n    \n    # Bolt 4.1 support (versions 4.1-4.4)\n    handlers[(4, 1)] = AsyncBolt4x1  # Note: This would be for async, but we need to be consistent\n    handlers[(4, 2)] = AsyncBolt4x1\n    handlers[(4, 3)] = AsyncBolt4x1\n    handlers[(4, 4)] = AsyncBolt4x1\n    \n    # For the sync version, we have:\n    # Bolt4x1 for versions 4.1-4.4\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] =", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we see Bolt3 and Bolt4x1/Bolt4x0 classes\n    # These are supported by Neo4j versions 3.5, 4.0, 4.1, 4.2, 4.3, and 4.4\n    \n    # Bolt 3 (versions 3.5, 4.0, 4.1, 4.2, 4.3, 4.4)\n    handlers[(3, 5)] = Bolt3\n    \n    # Bolt 4.1 (versions 4.1 - 4.4)\n    handlers[(4, 1)] = Bolt4x1\n    handlers[(4, 2)] = Bolt4x1  # Assuming same handler for 4.2\n    handlers[(4, 3)] = Bolt4x1  # Assuming same handler for 4.3\n    handlers[(4, 4)] = Bolt4x1  # Assuming same handler for 4.4\n    \n    # If protocol_version is specified, filter the dictionary\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n    \n    handlers = {}\n    \n    # Define supported protocol handlers based on the context\n    # Bolt 3 handlers\n    if hasattr(cls, 'Bolt3') and cls.Bolt3 is not None:\n        handlers[(3, 0)] = cls.Bolt3\n    if hasattr(cls, 'Bolt4x0') and cls.Bolt4x0 is not None:\n        handlers[(4, 0)] = cls.Bolt4x0\n    if hasattr(cls, 'Bolt4x1') and cls.Bolt4x1 is not None:\n        handlers[(4, 1)] = cls.Bolt4x1\n    if hasattr(cls, 'AsyncBolt3') and cls.AsyncBolt3 is not None:\n        handlers[(3, 0)] = cls.AsyncBolt3\n    if hasattr(cls, 'AsyncBolt4x0') and cls.AsyncBolt4x0 is not None:\n        handlers[(4, 0)] = cls.AsyncBolt4x0\n    if hasattr(cls, 'AsyncBolt4x1') and cls.AsyncBolt4x1 is not None:\n        handlers[(4, 1)] = cls.AsyncBolt4x1\n    \n    # Filter by protocol_version if specified\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8434, "num_retrieved": 13, "inference_time": 1.2111813724040985, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            # The transaction function will be called with the transaction object\n            # and any additional arguments\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            # The transaction function will be called with the transaction object\n            # and any additional arguments\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            # The transaction function will be called with the transaction object\n            # and any additional arguments\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            # The transaction function will be called with the transaction object\n            # and any additional arguments\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"Decorator for transaction functions that allows extra control over how the transaction is carried out.\"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Pass metadata and timeout to the transaction function\n            # The transaction function will be called with the transaction object\n            # and any additional arguments\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10307, "num_retrieved": 77, "inference_time": 1.2111813724040985, "generate_results": ["def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self.index(key)\n    except ValueError:\n        return -1"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8893, "num_retrieved": 28, "inference_time": 1.2111813724040985, "generate_results": ["def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.value(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [record.values(*keys) for record in self]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8491, "num_retrieved": 17, "inference_time": 1.2111813724040985, "generate_results": ["def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    result = {}\n    record = self\n    record_keys = list(record.keys())\n    \n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_keys):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            result[record_keys[key]] = record[key]\n        else:\n            result[key] = record.get(key)\n    \n    # Add missing keys with None values\n    for key in record_keys:\n        if key not in result:\n            result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    result = {}\n    record = self\n    \n    # Handle both index and key lookups\n    for key in keys:\n        try:\n            # Try to interpret as index (integer)\n            index = int(key)\n            if index < 0:\n                # Negative index - convert to positive\n                index = len(record) + index\n            if index >= len(record):\n                raise IndexError(f\"Index {index} is out of bounds\")\n            result[key] = record[index]\n        except (ValueError, TypeError):\n            # Treat as key name\n            if key in record:\n                result[key] = record[key]\n            else:\n                result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    result = {}\n    record = self\n    \n    # Handle both index and key lookups\n    for key in keys:\n        try:\n            # Try to interpret as index (integer)\n            index = int(key)\n            if index < 0:\n                # Negative index - convert to positive\n                index = len(record) + index\n            if index >= len(record):\n                raise IndexError(f\"Index {index} is out of bounds\")\n            result[key] = record[index]\n        except (ValueError, TypeError):\n            # Try to interpret as key (string)\n            if key not in record:\n                result[key] = None\n            else:\n                result[key] = record[key]\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    result = {}\n    record = self\n    record_keys = record.keys()\n    \n    for key in keys:\n        if isinstance(key, int):\n            try:\n                result[key] = record[key]\n            except IndexError:\n                raise IndexError(f\"Index {key} is out of bounds\")\n        else:\n            result[key] = record[key]\n    \n    return result", "def data(self, *keys):\n    \"\"\"Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(self)\n    \n    result = {}\n    record = self\n    \n    for key in keys:\n        try:\n            if isinstance(key, int):\n                value = record.values()[key]\n                result[key] = value\n            else:\n                value = record.value(key)\n                result[key] = value\n        except IndexError:\n            raise IndexError(f\"Index out of bounds: {key}\")\n    \n    return result"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9345, "num_retrieved": 43, "inference_time": 1.2111813724040985, "generate_results": ["def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes\n    bytes_data = self.data[-2:]\n    \n    # Convert to big-endian unsigned 16-bit integer\n    return (bytes_data[0] << 8) | bytes_data[1]", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    value = self.data[-2] << 8 | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    value = self.data[-2] << 8 | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n    16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes and convert to big-endian unsigned int\n    value = self.data[-2] << 8 | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop a 16-bit unsigned integer\")\n    \n    # Extract the last two bytes\n    bytes_data = self.data[-2:]\n    \n    # Convert to big-endian unsigned 16-bit integer\n    return (bytes_data[0] << 8) | bytes_data[1]"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8740, "num_retrieved": 23, "inference_time": 1.2111813724040985, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(\"DISCARD\", fields=(n, qid), dehydration_hooks=dehydration_hooks, response=None)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(\"DISCARD\", fields=(n, qid), dehydration_hooks=dehydration_hooks, response=None)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(\"DISCARD\", fields=(n, qid), dehydration_hooks=dehydration_hooks, response=None)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self.outbox.append_message(\"DISCARD\", (n, qid), dehydration_hooks)\n    self.responses.append(None)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    self._append(\"DISCARD\", fields=(n, qid), dehydration_hooks=dehydration_hooks, response=None)"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8675, "num_retrieved": 21, "inference_time": 1.2111813724040985, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+.\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    routing_context = self.routing_context or {}\n    log.debug(\"[#%04X]  C: BEGIN %r %r %r %r %r\", self.local_port,\n              mode, bookmarks, metadata, timeout, db)\n    \n    if bookmarks is None:\n        bookmarks = []\n    else:\n        bookmarks = list(bookmarks)\n    \n    self._append(b\"\\x02\", (mode, bookmarks, metadata, timeout, db),\n                 response=Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)\n    \n    return self.responses[-1]", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+.\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    routing_context = self.routing_context or {}\n    log.debug(\"[#%04X]  C: BEGIN %r %r %r %r\", self.local_port,\n              mode, bookmarks, metadata, db)\n    \n    if bookmarks is None:\n        bookmarks = []\n    else:\n        bookmarks = list(bookmarks)\n    \n    self._append(b\"\\x02\", (mode, bookmarks, metadata, db, timeout),\n                 response=Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)\n    \n    return self.responses[-1]", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+.\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    routing_context = self.routing_context or {}\n    log.debug(\"[#%04X]  C: BEGIN %r %r %r %r\", self.local_port,\n              mode, bookmarks, metadata, db)\n    \n    if bookmarks is None:\n        bookmarks = []\n    else:\n        bookmarks = list(bookmarks)\n    \n    self._append(b\"\\x02\", (mode, bookmarks, metadata, db),\n                 response=Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)\n    \n    return self.responses[-1]", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+.\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    routing_context = self.routing_context or {}\n    log.debug(\"[#%04X]  C: BEGIN %r %r %r %r\", self.local_port,\n              mode, bookmarks, metadata, db)\n    \n    if bookmarks is None:\n        bookmarks = []\n    else:\n        bookmarks = list(bookmarks)\n    \n    self._append(b\"\\x02\", (mode, bookmarks, metadata, db, timeout),\n                 response=Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)\n    \n    return self.responses[-1]", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\" Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+.\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    routing_context = self.routing_context or {}\n    log.debug(\"[#%04X]  C: BEGIN %r %r %r %r\", self.local_port,\n              mode, bookmarks, metadata, db)\n    \n    if bookmarks is None:\n        bookmarks = []\n    else:\n        bookmarks = list(bookmarks)\n    \n    self._append(b\"\\x02\", (mode, bookmarks, metadata, db),\n                 response=Response(self, \"begin\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)\n    \n    return self.responses[-1]"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9995, "num_retrieved": 64, "inference_time": 1.2111813724040985, "generate_results": ["def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is odd or even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"Round to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is odd or even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is odd or even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Convert to integer part and fractional part\n    integer_part = int(n)\n    fractional_part = n - integer_part\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is odd or even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10215, "num_retrieved": 70, "inference_time": 1.2111813724040985, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Define the metaclass for the new class\n    class PointMeta(type):\n        def __call__(cls, *args, **kwargs):\n            # Ensure the instance has the required fields\n            if len(args) != len(fields):\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(args)}\")\n            # Create a new instance with the provided fields\n            instance = super().__call__(*args, **kwargs)\n            # Set the srid based on the fields if available\n            if len(args) > 0:\n                srid = srid_map.get(fields[0], None)\n                if srid is not None:\n                    instance.srid = srid\n            return instance\n    \n    # Create the new class with the specified name\n    class PointSubclass(metaclass=PointMeta):\n        def __init__(self, *coordinates):\n            # Initialize with the provided coordinates\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(coordinates)}\")\n            super().__init__(*coordinates)\n        \n        def __repr__(self):\n            return f\"{name}({', '.join(map(str, self))})\"\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates, **kwargs):\n            # Ensure the correct number of coordinates is provided\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(coordinates)}\")\n            \n            # Store the coordinates and SRID\n            instance = super().__new__(cls)\n            instance._coordinates = list(coordinates)\n            instance._fields = fields\n            instance._srid_map = srid_map\n            \n            # Set SRID if provided\n            if 'srid' in kwargs:\n                instance.srid = kwargs['srid']\n            \n            return instance\n        \n        def __repr__(self):\n            return f\"{self.__class__.__name__}({self._coordinates}, srid={self.srid})\"\n        \n        def __len__(self):\n            return len(self._coordinates)\n        \n        def __getitem__(self, index):\n            return self._coordinates[index]\n    \n    # Add the class to the global namespace\n    globals()[name] = PointSubclass\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates, **kwargs):\n            # Ensure the correct number of coordinates is provided\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(coordinates)}\")\n            \n            # Store the coordinates and SRID if provided\n            instance = super().__new__(cls)\n            instance._coordinates = list(coordinates)\n            instance._fields = fields\n            \n            # Set SRID if available\n            if 'srid' in kwargs:\n                instance.srid = kwargs['srid']\n            \n            return instance\n        \n        def __repr__(self):\n            return f\"{self.__class__.__name__}({self._coordinates}, srid={self.srid if hasattr(self, 'srid') else None})\"\n    \n    # Register the class with srid_map for hydration\n    srid_map[name] = (PointSubclass, len(fields))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates, **kwargs):\n            # Ensure the correct number of coordinates is provided\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Point {name} requires {len(fields)} coordinates\")\n            \n            # Store the SRID if provided\n            srid = kwargs.get('srid')\n            if srid is not None:\n                if srid not in srid_map:\n                    raise ValueError(f\"SRID {srid} not found in srid_map\")\n            \n            # Create the instance\n            instance = super().__new__(cls)\n            return instance\n        \n        def __init__(self, *coordinates, **kwargs):\n            super().__init__(*coordinates, **kwargs)\n            # Set SRID if provided\n            srid = kwargs.get('srid')\n            if srid is not None:\n                self.srid = srid\n    \n    # Add the class to the global namespace\n    globals()[name] = PointSubclass\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates, **kwargs):\n            # Ensure the correct number of coordinates is provided\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(coordinates)}\")\n            \n            # Store the coordinates and SRID if provided\n            instance = super().__new__(cls)\n            instance._coordinates = list(coordinates)\n            instance._fields = fields\n            \n            # Set SRID if available in srid_map\n            if 'srid' in kwargs:\n                instance.srid = kwargs['srid']\n            else:\n                # Try to get SRID from srid_map based on coordinate system\n                srid = srid_map.get(fields[0], None)\n                if srid is not None:\n                    instance.srid = srid\n            \n            return instance\n        \n        def __repr__(self):\n            return f\"{self.__class__.__name__}({self._coordinates}, srid={self.srid})\"\n    \n    # Set the class name\n    PointSubclass.__name__ = name\n    \n    # Add a class attribute to store the field names\n    PointSubclass.fields = fields\n    \n    # Add a class attribute to store the SRID mapping\n    PointSubclass.srid_map = srid_map\n    \n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9713, "num_retrieved": 55, "inference_time": 1.2111813724040985, "generate_results": ["def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator", "def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator", "def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator", "def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecated_message = message\n        return func\n    return decorator", "def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    :param message: The deprecation message to display when the function is called.\n    \"\"\"\n    def decorator(func):\n        func._deprecated = True\n        func._deprecation_message = message\n        return func\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9526, "num_retrieved": 52, "inference_time": 1.3162915408611298, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code using R's options() function\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be configured via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code execution\n    return f\"options(stringsAsFactors = FALSE, warn = 1)\\n{code}\""]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9064, "num_retrieved": 36, "inference_time": 1.3162915408611298, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency == 1:\n        # Run serially\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n' if varargs else '',\n            text=True,\n            capture_output=True,\n            encoding='utf-8',\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    \n    # For parallel execution, use xargs with the specified concurrency\n    # This is a simplified version - in practice, this would use a proper xargs implementation\n    # or spawn processes with proper file argument distribution\n    \n    # Use the target_concurrency to determine how many processes to spawn\n    # This is a simplified implementation that assumes xargs is available\n    # and uses the target_concurrency as the number of jobs\n    \n    # In a real implementation, we would use subprocess.Popen or similar\n    # to run the command with multiple arguments in parallel\n    \n    # For now, we'll use a simple approach that runs the command with xargs\n    # and the specified concurrency\n    \n    # This is a placeholder implementation - in a real scenario, this would\n    # use the actual xargs command or a proper parallel execution mechanism\n    \n    # If we don't have xargs available, fall back to serial execution\n    try:\n        # Use the actual xargs command with the specified concurrency\n        # This assumes xargs is available in the environment\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n' if varargs else '',\n            text=True,\n            capture_output=True,\n            encoding='utf-8',\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    except FileNotFoundError:\n        # Fallback to serial execution if xargs is not available\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n' if varargs else '',", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency == 1:\n        # Run serially\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n' if varargs else '',\n            capture_output=True,\n            text=True,\n            encoding='utf-8',\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    \n    # For parallel execution, use xargs with the specified concurrency\n    # This is a simplified version - in practice, xargs would be used to distribute work\n    # across multiple processes\n    import multiprocessing as mp\n    \n    # Create a pool of workers\n    with mp.Pool(processes=target_concurrency) as pool:\n        # Prepare the arguments for each worker\n        args_list = [(cmd, [arg]) for arg in varargs]\n        \n        # Execute the command for each argument\n        results = pool.starmap(\n            lambda c, arg: subprocess.run(\n                c,\n                input=arg,\n                capture_output=True,\n                text=True,\n                encoding='utf-8',\n                **kwargs\n            ),\n            args_list\n        )\n    \n    # Aggregate the results\n    final_returncode = 0\n    final_output = b''\n    \n    for result in results:\n        final_returncode = max(final_returncode, result.returncode)\n        final_output += result.stdout.encode('utf-8')\n    \n    return final_returncode, final_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency == 1:\n        # Run serially\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n' if varargs else '',\n            text=True,\n            capture_output=True,\n            encoding='utf-8',\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    \n    # For parallel execution, use xargs with the specified concurrency\n    # This is a simplified version - in practice, xargs would be used with proper partitioning\n    import subprocess\n    import os\n    \n    # Create a temporary file to hold the arguments\n    temp_file = None\n    try:\n        temp_file = os.mktemp()\n        with open(temp_file, 'w') as f:\n            f.write('\\n'.join(varargs) + '\\n')\n        \n        # Use xargs with the specified concurrency\n        result = subprocess.run(\n            cmd,\n            input='',\n            text=True,\n            capture_output=True,\n            encoding='utf-8',\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    finally:\n        if temp_file:\n            os.unlink(temp_file)", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency == 1:\n        # Run serially\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n' if varargs else '',\n            capture_output=True,\n            text=True,\n            encoding='utf-8',\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    \n    # For parallel execution, use xargs with the specified concurrency\n    # This is a simplified version - in practice, xargs would handle the partitioning\n    # and execution of commands across multiple processes\n    import subprocess\n    import os\n    \n    # Create a temporary file to hold the arguments\n    temp_file = None\n    try:\n        temp_file = os.mktemp()\n        with open(temp_file, 'w') as f:\n            f.write('\\n'.join(varargs) + '\\n')\n        \n        # Use xargs to run the command with the specified concurrency\n        # This assumes that xargs is available in the environment\n        result = subprocess.run(\n            ['xargs', '-P', str(target_concurrency), '-I', '{}'] + list(cmd),\n            input='\\n'.join(varargs) + '\\n',\n            capture_output=True,\n            text=True,\n            encoding='utf-8',\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    except FileNotFoundError:\n        # Fallback to serial execution if xargs is not available\n        result = subprocess.run(\n            cmd,\n            input='\\n'.join(varargs) + '\\n' if varargs else '',\n            capture_output=True,\n            text=True,\n            encoding='utf-8',\n            **kwargs\n        )\n        return result.returncode, result.stdout.encode('utf-8')\n    finally:\n        if temp_file:\n            os.remove(temp_file)", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    if target_concurrency == 1:\n        # Run serially\n        result = subprocess.run(cmd, input='\\n'.join(varargs), text=True, capture_output=True)\n        return result.returncode, result.stdout.encode('utf-8')\n    \n    # For parallel execution, use xargs with the specified concurrency\n    # This is a simplified version - in practice, xargs would handle the parallelization\n    # using the system's xargs utility or similar\n    import subprocess\n    import shlex\n    \n    # Construct the xargs command with the specified concurrency\n    xargs_cmd = ['xargs', '-P', str(target_concurrency)]\n    \n    # Add the command and arguments\n    xargs_cmd.extend(cmd)\n    \n    # If color is enabled, pass it through\n    if color:\n        xargs_cmd.extend(['--color=always'])\n    \n    # Execute the command with the varargs\n    result = subprocess.run(\n        xargs_cmd,\n        input='\\n'.join(varargs),\n        text=True,\n        capture_output=True,\n        timeout=300  # reasonable timeout\n    )\n    \n    return result.returncode, result.stdout.encode('utf-8')"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10055, "num_retrieved": 69, "inference_time": 1.3162915408611298, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    return random.sample(seq, k=len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    return random.sample(seq, k=len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    # Use a deterministic random seed to ensure the same shuffle each time\n    random.seed(0)\n    return random.sample(seq, len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    return random.sample(seq, k=len(seq))", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    import random\n    return random.sample(seq, k=len(seq))"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9927, "num_retrieved": 64, "inference_time": 1.3162915408611298, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    parts = s.split('.')\n    return tuple(int(part) for part in parts)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9032, "num_retrieved": 37, "inference_time": 1.3162915408611298, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure that the command path is properly normalized\n    # and that shebangs are handled correctly by using the system's executable finder\n    # This is particularly important for deep-path shebangs which might not be resolved\n    # correctly in Windows environments.\n    \n    # Normalize the command by ensuring that paths are properly formatted\n    # and that the first element is the executable path\n    if len(cmd) == 0:\n        return cmd\n    \n    # If the first element is a shebang, we need to handle it specially\n    # For Windows, we need to ensure that the shebang is properly resolved\n    # and that the executable is found in the system path\n    if cmd[0].startswith('#!'):\n        # For shebangs, we need to find the actual executable\n        # This is handled by the parse_shebang module\n        # We replace the shebang with the actual executable path\n        import pre_commit.parse_shebang\n        executable = pre_commit.parse_shebang.find_executable(cmd[0][2:])\n        if executable:\n            # Replace the shebang with the actual executable\n            new_cmd = (executable,) + cmd[1:]\n            return new_cmd\n        else:\n            # If no executable is found, return the original command\n            return cmd\n    \n    # For non-shebang commands, we just return the original command\n    # with any necessary path normalization\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure that paths are properly normalized\n    # and that shebangs are handled correctly by using the system's\n    # executable finder instead of relying on the shell\n    if sys.platform == 'win32':\n        # Normalize paths to use forward slashes for consistency\n        # and ensure that executables are properly resolved\n        normalized_cmd = []\n        for arg in cmd:\n            # Replace backslashes with forward slashes for path consistency\n            normalized_arg = arg.replace('\\\\', '/')\n            normalized_cmd.append(normalized_arg)\n        return tuple(normalized_cmd)\n    \n    # For non-Windows systems, return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure that paths are properly normalized\n    # and that shebangs are handled correctly by using the system's\n    # executable finder instead of relying on the shell\n    if sys.platform == 'win32':\n        # Normalize paths to use forward slashes for consistency\n        # and ensure that executables are properly resolved\n        normalized_cmd = []\n        for arg in cmd:\n            # Replace backslashes with forward slashes for path consistency\n            normalized_arg = arg.replace('\\\\', '/')\n            normalized_cmd.append(normalized_arg)\n        return tuple(normalized_cmd)\n    \n    # For non-Windows systems, return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure that paths are properly normalized\n    # and that shebangs are handled correctly by using the system's\n    # executable finder instead of relying on the shell\n    if sys.platform == 'win32':\n        # Convert any path components to use forward slashes for consistency\n        # and ensure that the command is properly formatted for Windows\n        normalized = []\n        for part in cmd:\n            # Replace backslashes with forward slashes for path consistency\n            normalized.append(part.replace('\\\\', '/'))\n        return tuple(normalized)\n    \n    # For non-Windows platforms, return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # On Windows, we need to ensure that the command is properly normalized\n    # to handle shebangs and path issues\n    if sys.platform == 'win32':\n        # Convert any paths to use forward slashes for consistency\n        # and ensure that the command is properly formatted for Windows\n        result = []\n        for arg in cmd:\n            # Normalize path separators to forward slashes\n            normalized_arg = arg.replace(os.sep, os.altsep)\n            result.append(normalized_arg)\n        return tuple(result)\n    return cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9438, "num_retrieved": 51, "inference_time": 1.3162915408611298, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Use lock if provided to handle concurrent access\n            if lock is not None:\n                with lock:\n                    if cache_key in cache_instance:\n                        return cache_instance[cache_key]\n                    else:\n                        result = func(*args, **kwargs)\n                        cache_instance[cache_key] = result\n                        return result\n            else:\n                if cache_key in cache_instance:\n                    return cache_instance[cache_key]\n                else:\n                    result = func(*args, **kwargs)\n                    cache_instance[cache_key] = result\n                    return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Try to get the result from the cache\n            try:\n                result = cache_instance[cache_key]\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n            \n            return result\n        \n        # Set up the wrapper with the original function's attributes\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_value = key(*args, **kwargs)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[key_value]\n                return result\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_value] = result\n                return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            import functools\n            wrapper = functools.wraps(func)(wrapper)\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[cache_key]\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n            \n            return result\n        \n        # Add a lock if provided for thread safety\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache if isinstance(cache, Cache) else cache()\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_value = key(*args, **kwargs)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[key_value]\n                return result\n            except KeyError:\n                # If not in cache, compute the result and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_value] = result\n                return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            import functools\n            wrapper = functools.wraps(func)(wrapper)\n            wrapper.__lock__ = lock\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9139, "num_retrieved": 40, "inference_time": 1.3162915408611298, "generate_results": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    def decorator(func):\n        if typed:\n            key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n            \n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        \n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    \n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    from functools import update_wrapper\n    from cachetools import TTLCache\n    \n    cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n    \n    if typed:\n        key = lambda *args, **kwargs: (hashkey(*args, **kwargs), tuple(type(v) for v in args), tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return update_wrapper(wrapper, func)\n    \n    return decorator"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9029, "num_retrieved": 36, "inference_time": 1.3162915408611298, "generate_results": ["import functools\nfrom cachetools import MRUCache\nfrom cachetools.keys import hashkey\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize)\n        if typed:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass\n            return v\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import MRUCache\nfrom cachetools.keys import hashkey\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize)\n        if typed:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass\n            return v\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import MRUCache\nfrom cachetools.keys import hashkey\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize=maxsize)\n        if typed:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass\n            return v\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import MRUCache\nfrom cachetools.keys import hashkey\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize)\n        if typed:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[k] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import MRUCache\nfrom cachetools.keys import hashkey\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU) algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = MRUCache(maxsize=maxsize)\n        if typed:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass\n            return v\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9029, "num_retrieved": 36, "inference_time": 1.3162915408611298, "generate_results": ["import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = lambda *args, **kwargs: hashkey(*args, **kwargs)\n    else:\n        cache = LRUCache(maxsize)\n        key_func = lambda *args, **kwargs: hashkey(*args, **kwargs)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = lambda *args, **kwargs: (args, tuple(sorted(kwargs.items())))\n    else:\n        cache = LRUCache(maxsize)\n        key_func = lambda *args, **kwargs: args + tuple(sorted(kwargs.items()))\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n    else:\n        cache = LRUCache(maxsize)\n        key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass\n            return v\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n    else:\n        cache = LRUCache(maxsize)\n        key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass\n            return v\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = lambda *args, **kwargs: hashkey(*args, **kwargs)\n    else:\n        cache = LRUCache(maxsize)\n        key_func = lambda *args, **kwargs: hashkey(*args, **kwargs)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9029, "num_retrieved": 36, "inference_time": 1.3162915408611298, "generate_results": ["import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return (hashkey(*args, **kwargs), \n                       tuple(type(v) for v in args), \n                       tuple(type(v) for _, v in sorted(kwargs.items())))\n        else:\n            def key_func(*args, **kwargs):\n                return hashkey(*args, **kwargs)\n        \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return functools.update_wrapper(wrapper, func)\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return (hashkey(*args, **kwargs), \n                       tuple(type(v) for v in args),\n                       tuple(type(v) for _, v in sorted(kwargs.items())))\n        else:\n            def key_func(*args, **kwargs):\n                return hashkey(*args, **kwargs)\n        \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return functools.update_wrapper(wrapper, func)\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return (hashkey(*args, **kwargs), \n                       tuple(type(v) for v in args),\n                       tuple(type(v) for _, v in sorted(kwargs.items())))\n        else:\n            def key_func(*args, **kwargs):\n                return hashkey(*args, **kwargs)\n        \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return functools.update_wrapper(wrapper, func)\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return (hashkey(*args, **kwargs), \n                       tuple(type(v) for v in args), \n                       tuple(type(v) for _, v in sorted(kwargs.items())))\n        else:\n            def key_func(*args, **kwargs):\n                return hashkey(*args, **kwargs)\n        \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return functools.update_wrapper(wrapper, func)\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize=maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return (hashkey(*args, **kwargs), \n                       tuple(type(v) for v in args), \n                       tuple(type(v) for _, v in sorted(kwargs.items())))\n        else:\n            def key_func(*args, **kwargs):\n                return hashkey(*args, **kwargs)\n        \n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        \n        return functools.update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8936, "num_retrieved": 45, "inference_time": 1.3162915408611298, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the (key, value) pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except KeyError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        value = self.pop(key)\n        return (key, value)"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9415, "num_retrieved": 50, "inference_time": 1.3162915408611298, "generate_results": ["def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    try:\n        return self[key]\n    except KeyError:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9416, "num_retrieved": 50, "inference_time": 1.3162915408611298, "generate_results": ["def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9218, "num_retrieved": 43, "inference_time": 1.3162915408611298, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that uses the cache\n        def wrapper(self, *args, **kwargs):\n            # Generate the key using the provided key function\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            with lock:\n                try:\n                    cache[k] = v\n                except ValueError:\n                    return v  # value too large\n            return v\n        \n        # Preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        wrapper.__self__ = None\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        def wrapper(self, *args, **kwargs):\n            # Create a key for the method call\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original method and store the result\n            v = func(self, *args, **kwargs)\n            with lock:\n                try:\n                    cache[k] = v\n                except ValueError:\n                    return v  # value too large\n            return v\n        \n        # Preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        wrapper.__module__ = func.__module__\n        wrapper.__qualname__ = func.__qualname__\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        cache_key = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the method call\n            k = cache_key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            \n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                with lock:\n                    cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n            \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = 0, 0\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    pass\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(method):\n        key = key if callable(key) else hashkey\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the method call\n            k = key(self, *args, **kwargs)\n            \n            with lock:\n                try:\n                    return cache[k]\n                except KeyError:\n                    pass\n            \n            # Call the original method and store the result\n            result = method(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            with lock:\n                try:\n                    cache[k] = result\n                except ValueError:\n                    return result  # value too large\n            \n            return result\n        \n        # Preserve the original method's metadata\n        wrapper.__name__ = method.__name__\n        wrapper.__doc__ = method.__doc__\n        wrapper.__module__ = method.__module__\n        wrapper.__self__ = method.__self__\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        key_func = key if callable(key) else hashkey\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the method call\n            k = key_func(self, *args, **kwargs)\n            \n            with lock:\n                try:\n                    # Try to get the value from cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Cache miss, call the original function\n                    v = func(self, *args, **kwargs)\n                    # Store the result in cache\n                    try:\n                        cache.setdefault(k, v)\n                    except ValueError:\n                        return v  # value too large\n                    \n            return v\n        \n        # Add cache info and clear methods to wrapper\n        def cache_info():\n            with lock:\n                hits, misses = 0, 0\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    pass  # stats reset in wrapper\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 7109, "num_retrieved": 23, "inference_time": 1.3162915408611298, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback information\n    try:\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        tb_lines = []\n        for frame in tb:\n            tb_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Format the stack trace\n        stack_trace = \"\\n\".join(tb_lines) if tb_lines else \"\"\n        \n        # Combine everything\n        result = f\"{exc_type}: {exc_msg}\"\n        if stack_trace:\n            result += f\"\\nTraceback (most recent call last):\\n{stack_trace}\"\n            \n        return result\n        \n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            trace_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            trace_lines.append(f\"{frame.filename}:{frame.lineno}:{frame.name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback\n    tb = traceback.extract_tb(e.__traceback__, max_level)\n    \n    # Format the exception\n    exc_type = e.__class__.__name__\n    exc_msg = e.args[0] if e.args else \"No message\"\n    \n    # Build the trace string\n    trace_lines = []\n    for frame in tb:\n        filename = frame.filename\n        line_no = frame.lineno\n        name = frame.name\n        trace_lines.append(f\"{filename}:{line_no}:{name}\")\n    \n    # Limit path level if needed\n    path_lines = []\n    for frame in tb:\n        filename = frame.filename\n        if filename.startswith('/'):\n            path_lines.append(filename)\n    \n    # Format the result\n    result = f\"{exc_type}: {exc_msg}\"\n    if trace_lines:\n        result += \"\\nTraceback (most recent call last):\\n\"\n        for line in trace_lines:\n            result += f\"  {line}\\n\"\n    \n    return result", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            trace_lines.append(f\"{filename}:{lineno}:{name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        return f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8878, "num_retrieved": 33, "inference_time": 1.3162915408611298, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode if not overwrite, or write mode if overwrite\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode or write mode depending on overwrite flag\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    import os\n    \n    # Convert text buffer to bytes using specified encoding\n    bytes_buffer = text_buffer.encode(encoding)\n    \n    # Open file in append mode or write mode depending on overwrite flag\n    mode = 'a' if not overwrite else 'w'\n    \n    try:\n        with open(file_name, mode, encoding=encoding) as f:\n            f.write(text_buffer)\n        return len(bytes_buffer)\n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8749, "num_retrieved": 28, "inference_time": 1.08592090010643, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as f:\n            return f.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, str(e))\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8961, "num_retrieved": 36, "inference_time": 1.08592090010643, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    \n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n    :param file_name: File name.\n    :type file_name: str\n    :return: Return true (exist), false (do not exist, or invalid file name)\n    :rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    \n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    \n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    \n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    \n    try:\n        import os\n        return os.path.exists(file_name)\n    except Exception:\n        return False"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 5115, "num_retrieved": 16, "inference_time": 1.08592090010643, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters or local data if needed\n    # Note: ContextFilter uses gevent.local, so we don't need to explicitly reset it\n    # as it's per-thread and managed by the local instance\n    \n    # Optionally, reset any platform-specific state if needed\n    # This is a placeholder - actual implementation depends on what needs to be reset\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state found in context, so no additional reset)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state is referenced in context, so minimal reset)", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters or local data if needed\n    # Note: ContextFilter uses gevent.local, so we don't need to explicitly reset it\n    # as it's managed by the gevent context\n    \n    # Reinitialize any platform-specific logging if needed\n    # This is a placeholder - actual implementation depends on application needs\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration\n    logging.getLogger().setLevel(logging.NOTSET)\n    \n    # Clear any existing handlers\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    \n    # Reset any context filters\n    ContextFilter.LOC.__dict__.clear()\n    \n    # Reset any platform-specific state if needed\n    # (No specific state is referenced in the context, so this is minimal)"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9722, "num_retrieved": 58, "inference_time": 1.08592090010643, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation that should be overridden\n    # by the specific test class to return the actual implementation class\n    # without the 'Py' or 'Fallback' suffix.\n    # The actual implementation would depend on the specific context\n    # and how the C optimizations are being handled.\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This implementation assumes that the target class name is available\n    # in the current namespace, and we need to remove the 'Py' or 'Fallback' suffix\n    # The actual implementation would depend on the specific context\n    # where this method is used, such as the class being tested\n    \n    # Extract the class name from the current context\n    # This is a placeholder - in a real implementation, this would be\n    # determined by the specific class being tested\n    name = self.__class__.__name__\n    \n    # Remove 'Py' or 'Fallback' suffix if present\n    if name.endswith('Py'):\n        return self.__class__.__name__[:-2]\n    elif name.endswith('Fallback'):\n        return self.__class__.__name__[:-9]\n    \n    return self.__class__", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation that should be overridden\n    # by the specific test class to return the actual implementation class\n    # without the 'Py' or 'Fallback' suffix.\n    # The actual implementation would depend on the specific context\n    # and how the C optimizations are being applied.\n    raise NotImplementedError", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This implementation assumes that the class name with 'Py' or 'Fallback'\n    # suffix is available in the globals, and we need to remove those suffixes\n    name = self.__class__.__name__\n    if name.endswith('Py'):\n        return self.__class__.__module__ + '.' + name[:-2]\n    elif name.endswith('Fallback'):\n        return self.__class__.__module__ + '.' + name[:-8]\n    return self.__class__", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This implementation assumes that the class name with 'Py' or 'Fallback'\n    # suffix is available in the globals, and we need to remove those suffixes\n    name = self.__class__.__name__\n    if name.endswith('Py'):\n        return self.__class__.__bases__[0]\n    elif name.endswith('Fallback'):\n        return self.__class__.__bases__[0]\n    return self.__class__"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8748, "num_retrieved": 25, "inference_time": 1.08592090010643, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and the final merged ordering\n    result = []\n    position_map = {}\n    \n    # Process each ordering\n    for ordering in orderings:\n        # For each element in the ordering, find its position in the result\n        # and insert it if it's not already present\n        for i, item in enumerate(ordering):\n            if item not in position_map:\n                position_map[item] = len(result)\n                result.append(item)\n    \n    # Now we need to ensure that the orderings are consistent\n    # We'll create a list of all elements and their positions\n    # Then we'll merge them in the order they appear in the first ordering\n    # and then insert the elements from other orderings in their correct positions\n    \n    # Create a list of all unique elements\n    unique_elements = list(set(result))\n    \n    # Create a mapping from element to its position in the result\n    element_to_pos = {elem: i for i, elem in enumerate(result)}\n    \n    # For each ordering, ensure that the suffix starting with each element\n    # is preserved in the result\n    for ordering in orderings:\n        # Find the position of the first element in the ordering\n        first_element = ordering[0]\n        first_pos = element_to_pos.get(first_element, -1)\n        \n        # If the first element is not in the result, add it\n        if first_pos == -1:\n            result.insert(0, first_element)\n            element_to_pos[first_element] = 0\n        \n        # Now ensure that the rest of the ordering is preserved\n        # starting from the first element\n        for i, item in enumerate(ordering[1", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, item in enumerate(ordering):\n            if item not in positions:\n                positions[item] = {}\n            positions[item][i] = j\n    \n    # Sort orderings by the first element to ensure consistent merging\n    # We'll use the first ordering as a base and extend it with others\n    result = []\n    \n    # Find all unique elements across all orderings\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # For each element, determine its position in the final ordering\n    # We need to preserve the order within each ordering\n    # and ensure that if an element appears in multiple orderings,\n    # the suffix starting with that element must be preserved\n    \n    # Create a list of all elements in the order they appear in the first ordering\n    # Then insert elements from other orderings in their correct positions\n    base_ordering = orderings[0]\n    result.extend(base_ordering)\n    \n    # For each subsequent ordering, insert elements that are not already in the result\n    # in the correct position based on their appearance in the base ordering\n    for ordering in orderings[1:]:\n        for item in ordering:\n            # Find the position of this item in the current result\n            pos = -1\n            for i, existing_item in enumerate(result):\n                if existing_item == item:\n                    pos = i\n                    break\n            \n            # If the item is not in the result, insert it at the correct position\n            if pos == -1:\n                # Find the position in the ordering where this item appears", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, item in enumerate(ordering):\n            if item not in positions:\n                positions[item] = {}\n            positions[item][i] = j\n    \n    # Sort orderings by the first element to establish a base order\n    # Then merge based on the constraints\n    result = []\n    used = set()\n    \n    # Process each ordering to build the result\n    # We need to ensure that if an element appears in multiple orderings,\n    # the suffix starting with that element must be preserved in all orderings\n    \n    # Create a list of all unique elements with their positions\n    elements = []\n    for ordering in orderings:\n        for item in ordering:\n            if item not in elements:\n                elements.append(item)\n    \n    # Sort elements by their first appearance in any ordering\n    # This gives us a base order\n    element_order = []\n    for item in elements:\n        # Find the first ordering where this item appears\n        first_ordering = None\n        for i, ordering in enumerate(orderings):\n            if item in ordering:\n                if first_ordering is None or ordering.index(item) < orderings[first_ordering].index(item):\n                    first_ordering = i\n        element_order.append(item)\n    \n    # Now build the result by inserting elements in the order they appear\n    # in the first ordering, then filling in the gaps with other orderings\n    result = []\n    for item in element_order:\n        # Find the position of this item in each ordering\n        positions_in_orderings = []\n        for ordering in orderings:\n            try:\n                pos = ordering.index(item", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n    \n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n    \n    For example:\n    \n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, item in enumerate(ordering):\n            if item not in positions:\n                positions[item] = {}\n            positions[item][i] = j\n    \n    # Find the first ordering as the base\n    base_ordering = orderings[0]\n    result = []\n    \n    # For each element in the base ordering, find its position in other orderings\n    # and ensure that the suffix starting with that element is preserved\n    for item in base_ordering:\n        # Find all orderings that contain this item\n        orderings_with_item = [ordering for ordering in orderings if item in ordering]\n        \n        # If the item appears in multiple orderings, find the earliest position\n        # where it appears in each ordering\n        item_positions = {}\n        for ordering in orderings_with_item:\n            try:\n                pos = ordering.index(item)\n                item_positions[ordering] = pos\n            except ValueError:\n                continue\n        \n        # If the item appears in multiple orderings, we need to preserve the suffix\n        # that starts with this item in all orderings\n        if item_positions:\n            # Find the ordering with the earliest position for this item\n            earliest_pos = min(item_positions[ordering] for ordering in item_positions)\n            \n            # Add the item to the result\n            result.append(item)\n            \n            # Add the suffix of each ordering that starts with this item\n            for ordering in orderings_with_item:\n                try:\n                    suffix_start = ordering.index(item)\n                    suffix = ordering[suffix_start:]\n                    # Only add suffix if it's not already in the result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and the final merged ordering\n    result = []\n    positions = {}\n    \n    # For each ordering, determine the position of each element\n    for ordering in orderings:\n        for i, item in enumerate(ordering):\n            if item not in positions:\n                positions[item] = []\n            positions[item].append(i)\n    \n    # Sort the orderings by the first element that appears in them\n    # This ensures that we process orderings in a way that respects the constraints\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else None)\n    \n    # Build the result by merging orderings\n    # We need to ensure that if an element appears in multiple orderings,\n    # the suffix starting with that element must be in all orderings\n    # We'll use a greedy approach to build the result\n    \n    # Create a set of all elements that appear in any ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # For each element, find the earliest position it appears in any ordering\n    # and ensure that the suffix starting with that element is preserved\n    element_positions = {}\n    for element in all_elements:\n        positions_in_orderings = [pos for pos in positions[element] if pos < len(orderings)]\n        if positions_in_orderings:\n            element_positions[element] = min(positions_in_orderings)\n    \n    # Sort elements by their first appearance in any ordering\n    sorted_elements = sorted(all_elements, key=lambda x: element_positions.get(x, float('inf')))\n    \n    # Build the result by inserting elements in the order of their first appearance"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9646, "num_retrieved": 57, "inference_time": 1.08592090010643, "generate_results": ["def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        return Declaration()\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9966, "num_retrieved": 67, "inference_time": 1.08592090010643, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro order to ensure minimal bases\n    # This is a simplified version - in practice, this would need to\n    # consider the actual MRO and dependencies\n    return tuple(unique_classes)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro order to ensure minimal bases\n    # This is a simplified version - in practice, this would need to\n    # consider the actual MRO and dependencies\n    return tuple(unique_classes)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Convert to a set to remove duplicates, then back to a tuple\n    unique_classes = tuple(set(classes))\n    \n    # Sort by mro order to maintain consistency\n    # This is a simplified version - actual implementation would need\n    # to consider inheritance and MRO ordering\n    return tuple(sorted(unique_classes, key=lambda cls: getattr(cls, '__mro__', (cls,))[-1]))", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates by converting to a set and back to a tuple\n    unique_classes = list(set(classes))\n    \n    # Sort the unique classes to ensure consistent ordering\n    # This is a minimal representation that preserves the interface hierarchy\n    unique_classes.sort()\n    \n    return tuple(unique_classes)", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    if not classes:\n        return ()\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by mro order to ensure minimal bases\n    # This is a simplified version - in practice, this would need to consider\n    # the actual MRO and dependencies\n    return tuple(unique_classes)"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 11202, "num_retrieved": 108, "inference_time": 1.08592090010643, "generate_results": ["def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        return [(name, self.direct(name)) for name in self.names()]\n    else:\n        return [(name, self.direct(name)) for name in self.names(all=True)]", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attribute names and descriptions\n        names = []\n        for name in self.names():\n            description = self.direct(name)\n            if description is not None:\n                names.append((name, description))\n        return names\n    else:\n        # Return all attribute names and descriptions including those from base interfaces\n        names = []\n        for name in self.names(all=True):\n            description = self.direct(name)\n            if description is not None:\n                names.append((name, description))\n        return names", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attribute names and descriptions\n        names = []\n        for name in self.names():\n            description = self.direct(name)\n            if description is not None:\n                names.append((name, description))\n        return names\n    else:\n        # Return all attribute names and descriptions including those from base interfaces\n        names = []\n        for name in self.names(all=True):\n            description = self.direct(name)\n            if description is not None:\n                names.append((name, description))\n        return names", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        return [(name, self.direct(name)) for name in self.names()]\n    else:\n        return [(name, self.direct(name)) for name in self.names(all=True)]", "def namesAndDescriptions(self, all=False):\n    \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n    if not all:\n        # Return only directly defined attribute names and descriptions\n        names = []\n        for name in self.names():\n            description = self.direct(name)\n            if description is not None:\n                names.append((name, description))\n        return names\n    else:\n        # Return all attribute names and descriptions including those from base interfaces\n        names = []\n        for name in self.names(all=True):\n            description = self.direct(name)\n            if description is not None:\n                names.append((name, description))\n        return names"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10679, "num_retrieved": 91, "inference_time": 1.08592090010643, "generate_results": ["def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include all names defined by the interface directly and indirectly\n        names = []\n        for interface in self.__bases__:\n            names.extend(interface.names(all=True))\n        return list(set(names))", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include all names from direct and indirect interfaces\n        names = []\n        for name in self.__dict__:\n            names.append(name)\n        return names", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include all names from direct and indirect interfaces\n        names = []\n        for name in self.__dict__:\n            names.append(name)\n        return names", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include all names from direct and indirect interfaces\n        names = []\n        for name in self.__dict__:\n            names.append(name)\n        return names", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Include all names from direct and indirect interfaces\n        names = []\n        for name in self.__dict__:\n            names.append(name)\n        return names"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9484, "num_retrieved": 51, "inference_time": 1.08592090010643, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration or similar interface specification\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    for item in sequence:\n        if isinstance(item, (tuple, list)):\n            output.extend(_normalizeargs(item, output))\n        elif hasattr(item, 'interfaces'):\n            # Assume it's a Declaration object or similar\n            output.extend(item.interfaces)\n        else:\n            output.append(item)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8774, "num_retrieved": 27, "inference_time": 1.08592090010643, "generate_results": ["def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    try:\n        from zope.interface._compat import _should_attempt_c_optimizations\n        return _should_attempt_c_optimizations()\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8975, "num_retrieved": 33, "inference_time": 1.08592090010643, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy\n    import sys\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python == '1' or pure_python == 'true':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy\n    import sys\n    if 'pypy' in sys.version.lower():\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python == '1' or pure_python == 'true':\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    return True"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10542, "num_retrieved": 88, "inference_time": 1.08592090010643, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9612, "num_retrieved": 55, "inference_time": 1.08592090010643, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not bool(os.environ.get('PURE_PYTHON'))", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not bool(os.environ.get('PURE_PYTHON'))", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not bool(os.environ.get('PURE_PYTHON'))", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not bool(os.environ.get('PURE_PYTHON'))", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not bool(os.environ.get('PURE_PYTHON'))"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8783, "num_retrieved": 28, "inference_time": 1.08592090010643, "generate_results": ["def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    else:\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    else:\n        self.bins = self.make_bins()", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, 'initial_value'):\n        self.bins = self.initial_value\n    elif hasattr(self, 'make_bins'):\n        self.bins = self.make_bins()"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8462, "num_retrieved": 16, "inference_time": 1.08592090010643, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # Extract coordinates and values from the graph\n    coords = self.coords\n    values = self.values\n    \n    # Prepare the output rows\n    rows = []\n    \n    # If header is provided, add it as the first row\n    if header is not None:\n        rows.append(header)\n    \n    # Process each point in the graph\n    for coord, value in zip(coords, values):\n        # Convert coordinates to strings and join with separator\n        coord_str = separator.join(str(c) for c in coord)\n        \n        # Convert value to string representation\n        if isinstance(value, (list, tuple)):\n            value_str = separator.join(str(v) for v in value)\n        else:\n            value_str = str(value)\n            \n        # Combine coordinate and value parts\n        row = coord_str + separator + value_str\n        rows.append(row)\n    \n    return rows", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # Extract coordinates and values from the graph\n    coords = self.coords\n    values = self.values\n    \n    # Prepare the output rows\n    rows = []\n    \n    # If header is provided, add it as the first row\n    if header is not None:\n        rows.append(header)\n    \n    # Process each point in the graph\n    for coord, value in zip(coords, values):\n        # Convert coordinate to string with separator\n        coord_str = separator.join(str(c) for c in coord)\n        \n        # Convert value to string (if it's a tuple, join with separator)\n        if isinstance(value, (list, tuple)):\n            value_str = separator.join(str(v) for v in value)\n        else:\n            value_str = str(value)\n            \n        # Combine coordinate and value\n        row = coord_str + separator + value_str\n        rows.append(row)\n    \n    return rows", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # Extract coordinates and values from the graph\n    coords = []\n    values = []\n    \n    # Assuming self is a graph with points defined by coordinates and values\n    # This is a simplified implementation based on the context\n    for point in self.points:\n        coord = point.coordinate\n        value = point.value\n        \n        # Convert coordinate to string representation\n        coord_str = separator.join(str(c) for c in coord)\n        \n        # Convert value to string representation\n        if isinstance(value, (list, tuple)):\n            value_str = separator.join(str(v) for v in value)\n        else:\n            value_str = str(value)\n            \n        coords.append(coord_str)\n        values.append(value_str)\n    \n    # Create the CSV output\n    rows = []\n    \n    # Add header if provided\n    if header is not None:\n        rows.append(header)\n    \n    # Add data rows\n    for coord_str, value_str in zip(coords, values):\n        rows.append(coord_str + separator + value_str)\n    \n    return rows", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # Extract coordinates and values from the graph\n    coords = self.coords\n    values = self.values\n    \n    # Prepare the output rows\n    rows = []\n    \n    # If header is provided, add it as the first row\n    if header is not None:\n        rows.append(header)\n    \n    # Process each point in the graph\n    for coord, value in zip(coords, values):\n        # Convert coordinates to strings and join with separator\n        coord_str = separator.join(str(c) for c in coord)\n        \n        # Convert value to string representation\n        if isinstance(value, (list, tuple)):\n            value_str = separator.join(str(v) for v in value)\n        else:\n            value_str = str(value)\n            \n        # Combine coordinate and value parts\n        row = coord_str + separator + value_str\n        rows.append(row)\n    \n    return rows", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not None, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # Extract coordinates and values from the graph\n    coords = self.coords\n    values = self.values\n    \n    # Prepare the output rows\n    rows = []\n    \n    # If header is provided, add it to the output\n    if header is not None:\n        rows.append(header)\n    \n    # For each point in the graph, create a row\n    for coord, value in zip(coords, values):\n        # Convert coordinates to strings and join with separator\n        coord_str = separator.join(str(c) for c in coord)\n        \n        # Convert value to string representation\n        if isinstance(value, (list, tuple)):\n            value_str = separator.join(str(v) for v in value)\n        else:\n            value_str = str(value)\n            \n        # Combine coordinate and value parts\n        row = coord_str + separator + value_str\n        rows.append(row)\n    \n    return rows"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9814, "num_retrieved": 67, "inference_time": 1.585360437631607, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph._parsed_error_names\n    error_ind = -1\n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            break\n    return error_ind", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph._parsed_error_names\n    error_ind = -1\n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            break\n    return error_ind", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph._parsed_error_names\n    error_ind = -1\n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            break\n    return error_ind", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph._parsed_error_names\n    error_ind = -1\n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            break\n    return error_ind", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self.root_graph._parsed_error_names\n    error_ind = 0\n    for err in errors:\n        if err[1] == coord_name:\n            error_ind = err[3]\n            break\n    return error_ind"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8325, "num_retrieved": 11, "inference_time": 1.585360437631607, "generate_results": ["def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error information from the graph\n    errors = self._parsed_error_names\n    \n    if not errors:\n        return\n        \n    # Initialize error section in context\n    if \"error\" not in context:\n        context[\"error\"] = {}\n    \n    # Process each error field\n    for err in errors:\n        err_name = err[1]  # error field name (x, y, z, etc.)\n        err_type = err[2]  # error type (e.g., low, high)\n        err_index = err[3]  # index of the error in coords\n        \n        # Map error name to coordinate (x, y, z)\n        coord_name = err_name\n        if coord_name.startswith(\"error_\"):\n            # Extract the base coordinate name (e.g., \"error_x_low\" -> \"x_low\")\n            coord_name = coord_name.replace(\"error_\", \"\")\n        \n        # Create the error path in context\n        error_path = [\"error\"]\n        if coord_name.startswith(\"x\"):\n            error_path.append(\"x\")\n        elif coord_name.startswith(\"y\"):\n            error_path.append(\"y\")\n        elif coord_name.startswith(\"z\"):\n            error_path.append(\"z\")\n        else:\n            # Default to x if unknown\n            error_path.append(\"x\")\n            \n        # Add error suffix (low/high)\n        if err_type == \"low\":\n            error_path.append(\"low\")\n        elif err_type == \"high\":\n            error_path.append(\"high\")\n        else:\n            error_path.append(\"low\")  # default to low\n        \n        # Create the full path\n        error_key = \"_\".", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Get the error information from the graph\n    errors = self._parsed_error_names\n    \n    # If there are no errors, return early\n    if not errors:\n        return\n    \n    # Initialize the error section in context\n    error_section = context.get(\"error\", {})\n    \n    # Process each error\n    for err in errors:\n        err_name = err[1]  # The coordinate name (x, y, z)\n        err_type = err[2]  # The error type (e.g., low, high)\n        err_index = err[3]  # The index of the error in coords\n        \n        # Map coordinate names to x, y, z\n        if err_name == \"x\":\n            error_key = \"x_\" + err_type\n        elif err_name == \"y\":\n            error_key = \"y_\" + err_type\n        elif err_name == \"z\":\n            error_key = \"z_\" + err_type\n        else:\n            # Default to using the coordinate name\n            error_key = err_name + \"_\" + err_type\n            \n        # Append the error index to the error section\n        error_section[error_key] = {\"index\": err_index}\n    \n    # Update the context with the error section\n    context[\"error\"] = error_section", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Get the error information from the graph\n    errors = self._parsed_error_names\n    \n    # Initialize the error dictionary\n    error_dict = {}\n    \n    # Process each error field\n    for err in errors:\n        err_name = err[1]  # The coordinate name (x, y, z, etc.)\n        err_index = err[3]  # The index of the error in coords\n        \n        # Map coordinate names to x, y, z\n        if err_name == \"x\":\n            error_key = \"x\"\n        elif err_name == \"y\":\n            error_key = \"y\"\n        elif err_name == \"z\":\n            error_key = \"z\"\n        else:\n            # For any other coordinate, use the name\n            error_key = err_name\n            \n        # Create the error entry\n        if error_key not in error_dict:\n            error_dict[error_key] = {}\n            \n        # Add the error index\n        if err[2]:  # If it's a low error (e.g., error_low)\n            error_key += \"_low\"\n        else:  # If it's a high error (e.g., error_high)\n            error_key += \"_high\"\n            \n        error_dict[error_key] = {\"index\": err_index}\n    \n    # Update the context with the error information\n    if \"error\" not in context:\n        context[\"error\"] = {}\n        \n    # Merge the error dictionary into context.error\n    for key, value in error_dict.items():\n        context[\"error", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error information from the graph\n    errors = self._parsed_error_names\n    \n    # If there are no errors, return early\n    if not errors:\n        return\n    \n    # Initialize error section in context\n    error_section = context.get(\"error\", {})\n    \n    # Process each error field\n    for err in errors:\n        err_name = err[1]  # The coordinate name (x, y, z, etc.)\n        err_type = err[2]  # Whether it's an error (True) or not\n        err_index = err[3]  # The index of the error in coords\n        \n        # Map coordinate name to x, y, z\n        if err_name == \"x\":\n            error_key = \"x\"\n        elif err_name == \"y\":\n            error_key = \"y\"\n        elif err_name == \"z\":\n            error_key = \"z\"\n        else:\n            # For any other coordinate, use the name as is\n            error_key = err_name\n            \n        # Create error subcontext\n        if err_type:\n            # For error fields, create a key with the error type\n            # e.g., \"x_low\", \"x_high\", etc.\n            error_type = err[0]  # e.g., \"low\", \"high\"\n            error_key_full = f\"{error_key}_{error_type}\"\n            \n            # Append the index to the error section\n            error_section[error_key_full] = {\"index\": err_index}\n    \n    # Update the context with the error section\n    context[\"error\"] = error_section", "def _update_context(self, context):\n    \"\"\"Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Get the error information from the graph\n    errors = self._parsed_error_names\n    \n    # If there are no errors, return early\n    if not errors:\n        return\n    \n    # Initialize the error section in context\n    error_section = context.get(\"error\", {})\n    \n    # Process each error\n    for err in errors:\n        # Extract error name and coordinate\n        error_name = err[1]  # e.g., \"x\", \"y\", \"z\"\n        error_suffix = err[2]  # e.g., \"low\", \"high\"\n        \n        # Map coordinate names to x, y, z\n        if error_name == \"x\":\n            coord_name = \"x\"\n        elif error_name == \"y\":\n            coord_name = \"y\"\n        elif error_name == \"z\":\n            coord_name = \"z\"\n        else:\n            continue\n            \n        # Create the error key (e.g., \"x_low\", \"x_high\", \"y_low\", etc.)\n        error_key = f\"{coord_name}_{error_suffix}\"\n        \n        # Get the index of the error in the coordinates\n        error_index = err[3]\n        \n        # Append the error index to the error section\n        error_section[error_key] = {\"index\": error_index}\n    \n    # Update the context with the error section\n    context[\"error\"] = error_section"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8787, "num_retrieved": 28, "inference_time": 1.585360437631607, "generate_results": ["def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Unify the bins and edges to handle multi-dimensional cases\n    # For each dimension, compute the integral over the bin edges\n    if isinstance(edges, (list, tuple)) and len(edges) == 1:\n        # 1D case\n        edges = edges[0]\n        bin_widths = np.diff(edges)\n        return np.sum(bins * bin_widths)\n    \n    # Multi-dimensional case\n    # Compute the volume of each bin and multiply by bin content\n    total_integral = 0.0\n    for i in range(len(edges)):\n        # For each dimension, compute the width of the bin\n        bin_widths = []\n        for j in range(len(edges)):\n            if j == i:\n                bin_widths.append(np.diff(edges[j])[0])\n            else:\n                bin_widths.append(1.0)  # For other dimensions, we assume unit width\n        \n        # Compute the volume of the bin\n        bin_volume = np.prod(bin_widths)\n        total_integral += bin_volume * bins[i]\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Unify the bins and edges to handle multi-dimensional cases\n    # For 1D: bins is a sequence, edges is a sequence of length n+1\n    # For multi-dimensional: bins is a nested structure, edges is a nested sequence\n    \n    # Extract the dimensions\n    if isinstance(edges, (list, tuple)) and len(edges) == 1:\n        # 1D case\n        edges = edges[0]\n        bin_edges = np.array(edges)\n        bin_widths = np.diff(bin_edges)\n        return np.sum(np.array(bins) * bin_widths)\n    \n    # Multi-dimensional case\n    # Unify the edges to a consistent format\n    unified_edges = hf.unify_1_md(bins, edges)\n    \n    # Compute the integral by summing over all bins\n    # Each bin's contribution is its value times the volume of the bin\n    total_integral = 0.0\n    \n    # For each bin, compute the volume (product of bin widths)\n    for i in range(len(unified_edges)):\n        bin_volume = 1.0\n        for dim in range(len(unified_edges[0])):\n            bin_width = unified_edges[dim][i+1] - unified_edges[dim][i]\n            bin_volume *= bin_width\n        \n        # Add the contribution of this bin\n        bin_value = unified_edges[1][i]  # This is not correct - need to fix\n        # Actually, we need to access the bins properly\n        pass\n    \n    # Correct implementation for multi-dimensional case\n    # We need to compute the integral as sum over bins of (bin_value * bin_volume)\n    # where bin_volume is the product of the differences in edges\n    \n    # Recompute properly\n    if isinstance(bins, (list, tuple)) and len(bins) == 1:\n        # 1D case\n        edges = edges[0]\n        bin_edges = np.array(edges)\n        bin_widths = np.diff(bin_edges)\n        return np.sum(np.array(bins[0]) * bin_widths)\n    \n    # Multi-dimensional case\n    # Use the unified format from the context", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Unify the bins and edges into a multidimensional structure\n    # For a histogram, the integral is the sum of bin values multiplied by bin width\n    total_integral = 0.0\n    \n    # Handle multidimensional case by iterating over each dimension\n    # For each bin, compute the volume (product of bin widths) and multiply by bin value\n    if isinstance(edges, (list, tuple)) and len(edges) > 1:\n        # Multi-dimensional case\n        from itertools import product\n        \n        # Get the bin edges for each dimension\n        edge_arrays = edges\n        \n        # Compute the integral by summing over all bins\n        for bin_indices in product(*[range(len(edge_array) - 1) for edge_array in edge_arrays]):\n            # Get the bin value\n            bin_value = bins[bin_indices]\n            \n            # Compute the volume of the bin\n            bin_volume = 1.0\n            for i, edge_array in enumerate(edge_arrays):\n                bin_volume *= (edge_array[bin_indices[i] + 1] - edge_array[bin_indices[i]])\n            \n            total_integral += bin_value * bin_volume\n            \n    else:\n        # One-dimensional case\n        edge_array = edges\n        for i in range(len(edge_array) - 1):\n            bin_value = bins[i]\n            bin_width = edge_array[i + 1] - edge_array[i]\n            total_integral += bin_value * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Unify the bins and edges into a multidimensional structure\n    # For 1D case, bins and edges are both 1D arrays\n    # For higher dimensions, we need to handle each dimension separately\n    \n    # If bins is a scalar, return it directly (though this is unlikely)\n    if isinstance(bins, (int, float)):\n        return bins\n    \n    # For 1D case\n    if len(edges) == 1:\n        # edges is a single array of bin edges\n        # bins is a 1D array of bin values\n        # Compute integral as sum of bin values * bin width\n        bin_widths = np.diff(edges)\n        return np.sum(bins * bin_widths)\n    \n    # For higher dimensions, we need to compute the integral over all dimensions\n    # We'll use the fact that the integral is the sum over all bins of bin_value * volume_of_bin\n    # The volume of each bin is the product of the bin widths in each dimension\n    \n    # Convert edges to a list of arrays (each dimension has its own edges)\n    # For each dimension, compute the bin widths\n    dim = len(edges)\n    bin_widths = []\n    for i in range(dim):\n        bin_widths.append(np.diff(edges[i]))\n    \n    # Compute the volume of each bin\n    # For each bin, the volume is the product of the bin widths in each dimension\n    # We need to compute the integral as sum of bin_value * volume_of_bin\n    \n    # If bins is a nested structure (e.g., 2D histogram), we need to handle it properly\n    # For a 2D histogram, bins is a 2D array where bins[i][j] is the value of bin (i,j)\n    # The volume of bin (i,j) is bin_widths[0][i] * bin_widths[1][j]\n    \n    # For multidimensional case, we need to compute the integral over all dimensions\n    # We'll use the fact that the integral is the sum over all bins of bin_value * volume_of_bin\n    \n    # If bins is a 1D", "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    import numpy as np\n    \n    # Unify the bins and edges to handle multidimensional case\n    # For each dimension, compute the volume of the bin and multiply by the bin content\n    if isinstance(edges, (list, tuple)) and len(edges) == 1:\n        # 1D case\n        edges = edges[0]\n        bin_widths = np.diff(edges)\n        return np.sum(bins * bin_widths)\n    \n    # Multi-dimensional case\n    # Compute the volume of each bin and multiply by the bin content\n    # The volume is the product of the differences between consecutive edges\n    bin_volumes = []\n    for dim in range(len(edges)):\n        dim_edges = edges[dim]\n        dim_widths = np.diff(dim_edges)\n        bin_volumes.append(dim_widths)\n    \n    # Compute the volume for each bin\n    volumes = np.array([np.prod([dim_widths[i] for i, dim_widths in enumerate(bin_volumes)]) for _ in range(len(bins))])\n    \n    # Sum over all bins\n    return np.sum(bins * volumes)"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8594, "num_retrieved": 22, "inference_time": 1.585360437631607, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, sequence.Sequence):\n        # Check if the sequence contains a FillRequest element\n        seq_type = \"\"\n        if isinstance(seq, fill_request_seq.FillRequestSeq):\n            seq_type = \"fill_request\"\n        elif isinstance(seq, source.Source):\n            seq_type = \"source\"\n        elif isinstance(seq, fill_compute_seq.FillComputeSeq):\n            seq_type = \"fill_compute\"\n        else:\n            # Try to determine type by checking elements\n            try:\n                seq, seq_type = _get_seq_with_type(seq)\n            except:\n                return False\n                \n        return seq_type == \"fill_request\"\n    \n    # Check if seq is a sequence that might contain a FillRequest element\n    if hasattr(seq, '__iter__'):\n        for el in seq:\n            if isinstance(el, fill_request_seq.FillRequest):\n                return True\n            if isinstance(el, fill_request_seq.FillRequestSeq):\n                return True\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return False\n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq is a sequence and contains a FillRequest element\n    if isinstance(seq, sequence.Sequence):\n        for el in seq:\n            if isinstance(el, fill_request_seq.FillRequest):\n                return True\n    elif isinstance(seq, tuple):\n        for item in seq:\n            if isinstance(item, fill_request_seq.FillRequest):\n                return True\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return False\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    if isinstance(seq, sequence.Sequence):\n        # Check if the sequence contains a FillRequest element\n        try:\n            from .check_sequence_type import is_fill_request_el\n            from . import lena_sequence\n            \n            # Flatten the sequence to check its elements\n            flat_seq = flatten(seq)\n            for el in flat_seq:\n                if isinstance(el, lena_sequence.LenaSequence):\n                    if is_fill_request_el(el):\n                        return True\n            return False\n        except Exception:\n            return False\n    \n    # Check if seq is a tuple or iterable\n    if isinstance(seq, (tuple, list)):\n        for item in seq:\n            if is_fill_request_seq(item):\n                return True\n        return False\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return False\n    \n    if isinstance(seq, source.Source):\n        return False\n    \n    if isinstance(seq, tuple):\n        for el in seq:\n            if is_fill_request_seq(el):\n                return True\n    \n    if isinstance(seq, lena_sequence.LenaSequence):\n        # Check if any element in the sequence is a FillRequest element\n        seq_type = ct.is_fill_request_el(seq)\n        if seq_type:\n            return True\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, sequence.Sequence):\n        # Check if the sequence contains a FillRequest element\n        seq_obj = seq\n        for el in seq_obj:\n            if isinstance(el, fill_request_seq.FillRequest):\n                return True\n    \n    # Check if it's a Source sequence (should not be)\n    if isinstance(seq, source.Source):\n        return False\n    \n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9913, "num_retrieved": 73, "inference_time": 1.585360437631607, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10225, "num_retrieved": 82, "inference_time": 1.585360437631607, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(obj.run)"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9835, "num_retrieved": 70, "inference_time": 1.585360437631607, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'compute'.\"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8359, "num_retrieved": 14, "inference_time": 1.585360437631607, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1 or level > 0:\n            # If level is -1, go deep; otherwise, only check one level\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                if level == 0:\n                    # At level 0, only include if values differ\n                    if d1[key] != d2[key]:\n                        result[key] = d1[key]\n                else:\n                    # Recursively process nested dictionaries\n                    nested_diff = difference(d1[key], d2[key], level - 1)\n                    if nested_diff:\n                        result[key] = nested_diff\n            else:\n                # Values are not dictionaries, so check if they differ\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1 or level > 0:\n            # If level is -1, go deep; if level > 0, check only one level deep\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                if level == 0:\n                    # At level 0, just check if values are different\n                    if d1[key] != d2[key]:\n                        result[key] = d1[key]\n                else:\n                    # Recursively process nested dictionaries\n                    nested_diff = difference(d1[key], d2[key], level - 1)\n                    if nested_diff:\n                        result[key] = nested_diff\n            else:\n                # Non-dict values: include if different\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1 or level > 0:\n            # If level is -1, go deep; if level > 0, only check one level\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                if level == 0:\n                    # At level 0, only check if values are different\n                    if d1[key] != d2[key]:\n                        result[key] = d1[key]\n                else:\n                    # Recursively process nested dictionaries\n                    nested_diff = difference(d1[key], d2[key], level - 1)\n                    if nested_diff:\n                        result[key] = nested_diff\n            else:\n                # Values are not dictionaries, so check if they differ\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1 or level > 0:\n            # If level is -1, go deep; otherwise, only check one level\n            if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                if level == 0:\n                    # At level 0, only check if values are different\n                    if d1[key] != d2[key]:\n                        result[key] = d1[key]\n                else:\n                    # Recursively process nested dictionaries\n                    nested_diff = difference(d1[key], d2[key], level - 1)\n                    if nested_diff:\n                        result[key] = nested_diff\n            else:\n                # Values are not dictionaries, so check if they differ\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    result = {}\n    \n    for key in d1:\n        if key not in d2:\n            result[key] = d1[key]\n        elif level == -1 or level > 0:\n            # If level is -1, go infinitely deep\n            # If level is 0, only check for presence\n            if level == 0:\n                # Only include if key is in both but values differ\n                if d1[key] != d2[key]:\n                    result[key] = d1[key]\n            else:\n                # Recursively check if values are dictionaries\n                if isinstance(d1[key], dict) and isinstance(d2[key], dict):\n                    # Recursively process nested dictionaries\n                    nested_diff = difference(d1[key], d2[key], level - 1)\n                    if nested_diff:\n                        result[key] = nested_diff\n                else:\n                    # Values differ\n                    if d1[key] != d2[key]:\n                        result[key] = d1[key]\n    \n    return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9015, "num_retrieved": 36, "inference_time": 1.585360437631607, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the histogram edges\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    # For each dimension, check if coord[i] is within [edges[i][0], edges[i][1])\n    for i, coord_val in enumerate(coord):\n        if not (edges[i][0] <= coord_val < edges[i][1]):\n            return  # Ignore coordinates outside the histogram edges\n    \n    # If coordinates are within edges, update the bin content\n    # Find the bin index for the given coordinates\n    bin_index = []\n    for i, coord_val in enumerate(coord):\n        # Find the bin index for coord_val in dimension i\n        # This is a simplified approach - in practice, you might need to use a more sophisticated binning algorithm\n        # For now, we assume the binning is linear and we can compute the index directly\n        bin_index.append(int((coord_val - edges[i][0]) / (edges[i][1] - edges[i][0]) * (len(edges[i]) - 1)))\n    \n    # Update the bin content at the computed index\n    # This assumes the histogram bins are stored in a nested structure\n    # and that we can access them via the bin_index\n    bin_content = self.bins\n    for i, idx in enumerate(bin_index):\n        if isinstance(bin_content, list):\n            bin_content = bin_content[idx]\n        else:\n            # If bin_content is not a list, we need to handle it differently\n            # This is a simplified implementation\n            break\n    \n    # Add the weight to the bin content\n    if isinstance(bin_content, (int, float)):\n        bin_content += weight\n    else:\n        # If bin_content is a list or other structure, we need to handle it appropriately\n        # This is a simplified implementation\n        bin_content.append(weight)\n    \n    # Update the histogram bins\n    self.bins = bin_content", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Check if coordinates are within the histogram edges\n    edges = self.edges\n    if not hasattr(coord, '__iter__'):\n        coord = (coord,)\n    \n    # Validate that all coordinates are within the defined edges\n    for i, c in enumerate(coord):\n        if not isinstance(edges[i], (list, tuple)):\n            raise lena.core.LenaValueError(f\"edges at dimension {i} must be a list or tuple\")\n        \n        lower, upper = edges[i][0], edges[i][-1]\n        if c < lower or c >= upper:\n            return  # Ignore coordinates outside the edges\n    \n    # Get the bin index for the given coordinates\n    bin_indices = []\n    for i, c in enumerate(coord):\n        # Find the bin index for coordinate c in dimension i\n        bin_idx = 0\n        for j, edge in enumerate(edges[i]):\n            if c >= edge:\n                bin_idx = j\n                break\n        bin_indices.append(bin_idx)\n    \n    # Get the bin content at the calculated indices\n    bin_content = self.bins\n    current_bin = bin_content\n    for idx in bin_indices:\n        if isinstance(current_bin, (list, tuple)):\n            current_bin = current_bin[idx]\n    \n    # Add the weight to the bin content\n    if current_bin is None:\n        current_bin = 0\n    current_bin += weight\n    \n    # Update the bin content\n    if isinstance(bin_content, (list, tuple)):\n        # Update the bin content in the nested structure\n        temp = bin_content\n        for idx in bin_indices[:-1]:\n            temp = temp[idx]\n        temp[bin_indices[-1]] = current_bin\n    else:\n        # For single bin case\n        bin_content = current_bin", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges of the histogram\n    edges = self.edges\n    \n    # Check if coordinates are within the histogram edges\n    # For each dimension, check if coord[i] is within [edges[i][0], edges[i][1])\n    for i, coord_val in enumerate(coord):\n        if not (edges[i][0] <= coord_val < edges[i][1]):\n            return  # Ignore coordinates outside the histogram edges\n    \n    # If coordinates are within edges, update the bin content\n    # Find the bin index for the given coordinates\n    bin_index = []\n    for i, coord_val in enumerate(coord):\n        # Find the bin index for coord_val in dimension i\n        # This is a simplified approach - in practice, you might need to use\n        # a more sophisticated binning algorithm\n        bin_index.append(0)  # Placeholder - actual implementation depends on binning logic\n    \n    # Update the bin content at the calculated index\n    # This is a simplified implementation - actual implementation would depend\n    # on the specific binning structure and how coordinates map to bins\n    bin_content = self.bins\n    if isinstance(bin_content, list):\n        # Update the bin content at the calculated index\n        # This is a simplified approach - actual implementation would depend\n        # on the specific binning structure\n        pass", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the edges of the histogram\n    edges = self.edges\n    \n    # Check if the coordinate is within the histogram edges\n    # For each dimension, check if coord[i] is within [edges[i][0], edges[i][1])\n    for i, edge in enumerate(edges):\n        if not (edge[0] <= coord[i] < edge[1]):\n            return  # Coordinate is outside the histogram edges\n    \n    # If we reach here, the coordinate is within bounds\n    # Update the bin content at the given coordinate\n    # Use get_bin_on_index to get the bin content at the given coordinate\n    bin_content = lena.structures.get_bin_on_index(coord, self.bins)\n    \n    # Add the weight to the bin content\n    if bin_content is None:\n        self.bins[coord] = weight\n    else:\n        self.bins[coord] += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Check if coordinates are within the histogram edges\n    edges = self.edges\n    if not edges:\n        return\n    \n    # Get the bin index for the given coordinates\n    try:\n        bin_index = []\n        for i, edge in enumerate(edges):\n            if not hasattr(edge, '__iter__'):\n                # 1D case\n                if coord[i] < edge[0] or coord[i] >= edge[1]:\n                    return  # Outside edges, ignore\n                bin_index.append(int((coord[i] - edge[0]) / (edge[1] - edge[0])))\n            else:\n                # Multi-dimensional case\n                if coord[i] < edge[0][0] or coord[i] >= edge[1][0]:\n                    return  # Outside edges, ignore\n                bin_index.append(int((coord[i] - edge[0][0]) / (edge[1][0] - edge[0][0])))\n    except (IndexError, TypeError):\n        return\n    \n    # Get the bin content at the calculated index\n    bin_content = self.bins\n    if isinstance(bin_content, (list, tuple)):\n        # Navigate to the bin using the index\n        for i in bin_index:\n            if i >= len(bin_content):\n                return\n            bin_content = bin_content[i]\n    \n    # Add the weight to the bin content\n    if isinstance(bin_content, (int, float)):\n        bin_content += weight\n    else:\n        # If bin content is a list or other structure, append weight\n        bin_content.append(weight)"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8489, "num_retrieved": 17, "inference_time": 1.585360437631607, "generate_results": ["import re\nfrom typing import Dict, Any, List, Union\n\ndef _validate_labels(labels: Dict[Any, Any]) -> None:\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, underscore, starting with letter or underscore)\n    key_regex = r'^[a-z][a-z0-9_]*$'\n    # Label values must be strings (or bytes-like objects)\n    value_regex = r'^[a-zA-Z0-9._-]+$'\n    \n    errors = []\n    \n    # Validate label keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n        elif not re.match(key_regex, key):\n            errors.append({str(key): f\"Label key '{key}' does not match the regex {key_regex}\"})\n    \n    # Validate label values\n    for key, value in labels.items():\n        if not isinstance(value, (str, bytes)):\n            if isinstance(value, list):\n                for item in value:\n                    if not isinstance(item, (str, bytes)):\n                        errors.append({str(item): \"expected string or bytes-like object\"})\n            else:\n                errors.append({str(value): \"expected string or bytes-like object\"})\n        elif not re.match(value_regex, value):\n            errors.append({str(value): f\"Label value '{value}' does not match the regex {value_regex}\"})\n    \n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List, Union\n    \n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, hyphen, underscore, start with letter or digit)\n    key_regex = r'^[a-z][a-z0-9._-]*$'\n    # Label values must be strings (or bytes-like objects)\n    value_regex = r'^[a-zA-Z0-9._-]+$'\n    \n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n        elif not re.match(key_regex, key):\n            errors.append({str(key): f\"Label key '{key}' does not match the regex {key_regex}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, (str, list)):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n        \n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        else:\n            if not isinstance(value,", "import re\nfrom typing import Dict, Any, List, Union\nfrom validation import ValidationError  # Assuming ValidationError is defined in the module\n\ndef _validate_labels(labels: Dict[Any, Any]) -> None:\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    errors = []\n\n    # Define regular expressions for label keys and values\n    # Label keys must be valid strings (alphanumeric, hyphens, dots, and underscores)\n    # Label values must be strings or bytes-like objects\n    key_regex = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*$'\n    value_regex = r'^[a-zA-Z0-9_\\-\\.]+$'\n\n    for key, value in labels.items():\n        # Validate label key\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n        if not re.match(key_regex, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex {key_regex}\"})\n\n        # Validate label value\n        if value is None:\n            continue\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n        else:\n            if not re.match(value_regex, value):\n                errors.append({value: f\"Label value '{value}' does not match the regex {value_regex}\"})\n\n    if errors:\n        raise ValidationError(messages=errors)", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List, Union\n    \n    # Regular expressions for label keys and values\n    LABEL_KEY_REGEX = r'^[a-z][a-z0-9_-]*$'\n    LABEL_VALUE_REGEX = r'^[a-z0-9][a-z0-9_.-]*[a-z0-9]$'\n    \n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n        if not re.match(LABEL_KEY_REGEX, key):\n            errors.append({str(key): f\"Label key '{key}' does not match the regex {LABEL_KEY_REGEX}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, (str, list)):\n            errors.append({str(value): \"Label value must be a string or list\"})\n            continue\n            \n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({f\"[{item}]\": \"expected string or bytes-like object\"})\n        else:\n            if not re.match(LABEL_VALUE_REGEX, value):\n                errors.append({str", "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    import re\n    from typing import Dict, Any, List, Union\n    \n    # Define regular expressions for label keys and values\n    # Label keys must be valid identifiers (alphanumeric, hyphen, underscore, starting with letter or digit)\n    key_regex = r'^[a-z][a-z0-9._-]*[a-z0-9]$'\n    # Label values must be strings or bytes-like objects\n    value_regex = r'^[a-zA-Z0-9._-]+$'\n    \n    errors = []\n    \n    # Validate keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"expected string or bytes-like object\"})\n        elif not re.match(key_regex, key):\n            errors.append({str(key): f\"Label key '{key}' does not match the regex {key_regex}\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        if not isinstance(value, (str, list)):\n            errors.append({str(value): \"expected string or bytes-like object\"})\n        elif isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"expected string or bytes-like object\"})\n                elif not re.match"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9185, "num_retrieved": 38, "inference_time": 1.585360437631607, "generate_results": ["import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must:\n    # - start with a letter or number\n    # - contain only letters, numbers, hyphens, underscores, and dots\n    # - be 1-253 characters long\n    # - not start or end with a hyphen or underscore\n    # - not contain consecutive hyphens or underscores\n    \n    pattern = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must follow these rules:\n    # - Start with a letter or number\n    # - Can contain letters, numbers, hyphens, and underscores\n    # - Cannot start or end with a hyphen or underscore\n    # - Cannot have consecutive hyphens or underscores\n    pattern = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must:\n    # - start with a letter or number\n    # - contain only letters, numbers, hyphens, and underscores\n    # - be 1-253 characters long\n    # - not start or end with a hyphen or underscore\n    pattern = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must:\n    # - start with a letter or number\n    # - contain only letters, numbers, hyphens, and underscores\n    # - be 1-253 characters long\n    # - not start or end with a hyphen or underscore\n    pattern = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$')\n    return pattern", "import re\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Krake resource names must:\n    # - start with a letter or number\n    # - contain only letters, numbers, hyphens, underscores, and dots\n    # - be 1-253 characters long\n    # - not start or end with a hyphen or underscore\n    # - not contain consecutive hyphens or underscores\n    \n    pattern = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9]$')\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8948, "num_retrieved": 32, "inference_time": 1.585360437631607, "generate_results": ["def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is expected to validate a value against a regular expression\n    # However, the context does not provide the specific regular expression pattern\n    # or the implementation details for validation.\n    # Based on the context, we can infer that this function should raise ValidationError\n    # if the value does not match the expected pattern.\n    \n    # Since no specific pattern or pattern lookup mechanism is provided in the context,\n    # we cannot implement the actual validation logic.\n    # The function would typically use a regex pattern and check if value matches it.\n    \n    # Placeholder implementation - in a real scenario, this would use a specific regex pattern\n    # and raise ValidationError if the value does not match.\n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string.\")\n    \n    # Example pattern - this is just a placeholder and not derived from context\n    # In actual implementation, this would be replaced with the actual pattern\n    import re\n    pattern = r\"^[a-zA-Z0-9._%+-]+$\"\n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected pattern.\")", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is not fully defined in the context.\n    # Based on the context, it appears to be related to validation logic,\n    # but no specific regular expression or implementation details are provided.\n    # A minimal implementation that raises ValidationError for invalid values\n    # is provided here as a placeholder.\n    \n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string.\")\n    \n    # Placeholder: if value does not match a regex pattern, raise ValidationError\n    # Since no specific pattern is provided in context, this is a generic check\n    if not value or not value.strip():\n        raise ValidationError(\"Value cannot be empty or whitespace only.\")\n    \n    # If specific regex pattern was intended, it should be defined here\n    # For now, we assume basic validation\n    return True", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is expected to validate a value against a regular expression\n    # However, no regular expression pattern is provided in the context\n    # Therefore, we cannot implement the actual validation logic without additional context\n    \n    # Since the context does not specify the pattern or provide a way to get it,\n    # we assume that the validation is handled by a pattern that is externally defined\n    # and thus we raise a ValidationError if the value does not match the pattern\n    \n    # Placeholder implementation - in a real scenario, this would use a pattern\n    # For example: if value doesn't match r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$':\n    #     raise ValidationError(\"Invalid email format\")\n    \n    # Since no pattern is provided, we simply return without validation\n    # This is a minimal implementation based on the available context\n    pass", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is referenced in the context but not fully implemented\n    # Based on the context, it appears to be related to validation using regex\n    # Since no specific regex pattern or implementation details are provided,\n    # we assume a default validation pattern that checks for non-empty strings\n    # and basic format (this is a placeholder implementation)\n    \n    if not value or not isinstance(value, str):\n        raise ValidationError(\"Value must be a non-empty string.\")\n    \n    # Placeholder regex pattern - in a real implementation, this would be\n    # defined based on specific requirements\n    import re\n    pattern = r\"^[a-zA-Z0-9._-]+$\"  # Simple pattern for alphanumeric, dots, underscores, hyphens\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the required pattern.\")", "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # This function is not fully defined in the context\n    # Based on the context, it appears to be related to validation logic\n    # However, no specific regular expression or validation rules are provided\n    # Therefore, we implement a placeholder that would raise ValidationError\n    # if the value is not a string or if it fails a basic pattern check\n    \n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string\")\n    \n    # Placeholder pattern - in a real implementation, this would be specific to the context\n    # For example, if the pattern was defined elsewhere, it would be used here\n    if not value.strip():\n        raise ValidationError(\"Value cannot be empty\")\n    \n    # If specific pattern rules were defined in the context, they would be applied here\n    # Since they are not provided, we return without further validation\n    return True"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9003, "num_retrieved": 34, "inference_time": 1.585360437631607, "generate_results": ["def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    if not key:\n        raise ValidationError(\"Key cannot be empty.\")\n    \n    # Assuming the regular expression is defined in the context, \n    # but it's not provided in the retrieved context.\n    # This is a placeholder implementation based on typical key validation patterns.\n    # In a real scenario, the actual regex pattern would be defined elsewhere.\n    \n    # Example pattern: alphanumeric with hyphens, underscores, and dots\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Key '{key}' does not conform to the required pattern.\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    if not key:\n        raise ValidationError(\"Key cannot be empty.\")\n    \n    # Assuming the regular expression is defined in a pattern attribute or similar\n    # Since the exact pattern is not provided in context, this is a placeholder\n    # that would normally use a pattern defined elsewhere (e.g., in a class or config)\n    # For example, if there's a pattern defined in a class with a 'pattern' attribute\n    # that matches valid keys, this function would use that pattern.\n    \n    # Placeholder implementation - in real scenario, this would use the actual pattern\n    # from the context (e.g., from a key pattern defined in a class)\n    if not isinstance(key, str):\n        raise ValidationError(\"Key must be a string.\")\n    \n    # Example pattern validation (this should be replaced with actual pattern)\n    # For instance, if keys must match a regex like [a-z][a-z0-9_]*, then:\n    import re\n    pattern = re.compile(r'^[a-z][a-z0-9_]*$')\n    if not pattern.match(key):\n        raise ValidationError(f\"Key '{key}' does not conform to the required pattern.\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    if not key:\n        raise ValidationError(\"Key cannot be empty.\")\n    \n    # Assuming the regular expression is defined in the context, \n    # but it's not provided in the retrieved context.\n    # This is a placeholder implementation based on typical key validation.\n    # In a real scenario, the actual regex pattern would be defined elsewhere.\n    \n    # Example pattern: alphanumeric with hyphens, underscores, and dots\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Key '{key}' does not conform to the required pattern.\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    if not key:\n        raise ValidationError(\"Key cannot be empty.\")\n    \n    # Assuming the regular expression is defined in the context, \n    # but it's not provided in the retrieved context.\n    # This is a placeholder implementation based on typical key validation patterns.\n    # In a real implementation, the actual regex pattern would be defined elsewhere.\n    \n    # Example pattern: alphanumeric with hyphens, underscores, and dots\n    import re\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Key '{key}' does not conform to the required pattern.\")", "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    if not key:\n        raise ValidationError(\"Key cannot be empty.\")\n    \n    # Assuming the regular expression is defined in the context, \n    # but it's not provided in the retrieved context.\n    # This is a placeholder implementation based on typical key validation patterns.\n    # In a real scenario, the actual regex pattern would be defined elsewhere.\n    \n    # Example pattern: alphanumeric with hyphens, underscores, and dots\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Key '{key}' does not conform to the required pattern.\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8424, "num_retrieved": 13, "inference_time": 1.585360437631607, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        # Copy identifying fields from manifest\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            # Copy metadata fields, but only name is required\n            try:\n                result[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n        except KeyError:\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n            \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        # Copy identifying fields from manifest\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            # Copy metadata fields, but only name is required\n            try:\n                result[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n        except KeyError:\n            pass\n    \n    # Process all key-value pairs in manifest_dict\n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        # Copy identifying fields from manifest\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            # Copy metadata fields, but only name is required\n            try:\n                result[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n            # Other metadata fields are not copied as they are not part of observer schema\n        except KeyError:\n            pass\n    \n    # Process all key-value pairs in manifest_dict\n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        # Copy identifying fields from manifest\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            # Copy metadata fields, but only name is required\n            try:\n                result[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n        except KeyError:\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n            \n    return result", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    result = {}\n    \n    if first_level:\n        # Copy identifying fields from manifest\n        try:\n            result[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            result[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            result[\"metadata\"] = {}\n            # Copy metadata fields, but only name is required\n            try:\n                result[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n        except KeyError:\n            pass\n    \n    for key, value in manifest_dict.items():\n        if key in [\"apiVersion\", \"kind\", \"metadata\"]:\n            continue\n            \n        if isinstance(value, dict):\n            result[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            result[key] = generate_default_observer_schema_list(value)\n        else:\n            result[key] = None\n    \n    return result"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8517, "num_retrieved": 15, "inference_time": 1.585360437631607, "generate_results": ["def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API\n\n    This function goes through all observed fields and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for idx, observed in enumerate(observer_schema):\n        if idx >= len(response):\n            # If there's an observed element but no corresponding response, we need to add it\n            last_applied_manifest.append(observed)\n        else:\n            # If the observed element is present in response, update the last_applied_manifest\n            # with the response value for the corresponding index\n            if isinstance(observed, dict):\n                # Recursively update dictionary fields\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx] if idx < len(last_applied_manifest) else None,\n                    observed,\n                    response[idx] if idx < len(response) else None\n                )\n            elif isinstance(observed, list):\n                # Recursively update list fields\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx] if idx < len(last_applied_manifest) else [],\n                    observed,\n                    response[idx] if idx < len(response) else []\n                )\n            else:\n                # For non-dict, non-list fields, just assign the response value\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n                else:\n                    last_applied_manifest.append(response[idx])", "def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API\n\n    This function goes through all observed fields and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for idx, observed in enumerate(observer_schema):\n        if idx >= len(response):\n            # If there's an observed element but no corresponding response, we need to add it\n            last_applied_manifest.append(observed)\n        else:\n            # If the observed element is present in the response, update the last_applied_manifest\n            # with the response value if it's different from the current one\n            if isinstance(observed, dict):\n                # Recursively update the dictionary part\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx] if idx < len(last_applied_manifest) else None,\n                    observed,\n                    response[idx] if idx < len(response) else None\n                )\n            elif isinstance(observed, list):\n                # Recursively update the list part\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx] if idx < len(last_applied_manifest) else None,\n                    observed,\n                    response[idx] if idx < len(response) else None\n                )\n            else:\n                # For scalar values, just ensure the value is set in last_applied_manifest\n                if idx < len(last_applied_manifest):\n                    # If the current element exists, update it\n                    if last_applied_manifest[idx] is None:\n                        last_applied_manifest[idx] = observed\n                else:\n                    # If the current element doesn't exist, add it\n                    last_applied_manifest.append(observed)", "def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API\n\n    This function goes through all observed fields and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for idx, observed in enumerate(observer_schema):\n        if idx >= len(response):\n            # If there's an observed element but no corresponding response, we need to add it\n            # This could happen if the resource was not found in the API response\n            continue\n            \n        # Check if the current observed element is already in last_applied_manifest\n        try:\n            get_kubernetes_resource_idx(last_applied_manifest, observed)\n        except IndexError:\n            # If not found, add the observed resource to last_applied_manifest\n            last_applied_manifest.append(response[idx])\n            \n        # Recursively process nested dictionaries and lists\n        if isinstance(observed, dict):\n            # For dictionary elements, recursively update the nested structure\n            for key, value in observed.items():\n                if isinstance(value, dict):\n                    # Update nested dictionary\n                    pass\n                elif isinstance(value, list):\n                    # Update nested list\n                    pass\n                else:\n                    # Handle scalar values\n                    pass\n        elif isinstance(observed, list):\n            # For list elements, recursively update the list structure\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )", "def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API\n\n    This function goes through all observed fields and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for idx, observed in enumerate(observer_schema):\n        if idx >= len(response):\n            # If there's an observed element but no corresponding response, we need to add it\n            last_applied_manifest.append(observed)\n        else:\n            # If the observed element is present in the response, update the last_applied_manifest\n            # with the response value if it's different from the current one\n            if isinstance(observed, dict):\n                # Recursively update the dictionary part\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx] if idx < len(last_applied_manifest) else None,\n                    observed,\n                    response[idx] if idx < len(response) else None\n                )\n            elif isinstance(observed, list):\n                # Recursively update the list part\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx] if idx < len(last_applied_manifest) else None,\n                    observed,\n                    response[idx] if idx < len(response) else None\n                )\n            else:\n                # For scalar values, just ensure the value is present\n                if idx < len(last_applied_manifest):\n                    # Update the value if it's different from the response\n                    if last_applied_manifest[idx] != response[idx]:\n                        last_applied_manifest[idx] = response[idx]\n                else:\n                    # If not present, add it\n                    last_applied_manifest.append(observed)", "def update_last_applied_manifest_list_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_dict_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API\n\n    This function goes through all observed fields and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for idx, observed in enumerate(observer_schema):\n        if idx >= len(response):\n            # If there's an observed element but no corresponding response, we need to add it\n            last_applied_manifest.append(observed)\n        else:\n            # If the observed element is present in the response, update the last_applied_manifest\n            # with the response value if it's different from the current one\n            if isinstance(observed, dict):\n                # Recursively update the dictionary part\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx] if idx < len(last_applied_manifest) else None,\n                    observed,\n                    response[idx] if idx < len(response) else None\n                )\n            elif isinstance(observed, list):\n                # Recursively update the list part\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx] if idx < len(last_applied_manifest) else None,\n                    observed,\n                    response[idx] if idx < len(response) else None\n                )\n            else:\n                # For scalar values, just ensure the value is present\n                if idx >= len(last_applied_manifest):\n                    last_applied_manifest.append(observed)\n                else:\n                    # If the value is not present in the last_applied_manifest, add it\n                    if last_applied_manifest[idx] is None:\n                        last_applied_manifest[idx] = observed"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8479, "num_retrieved": 14, "inference_time": 1.585360437631607, "generate_results": ["def update_last_applied_manifest_dict_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, just assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, just assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, just assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(last_applied_manifest, observer_schema, response):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this function is called recursively to update a partial ``last_applied_manifest`` from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            # For non-dict, non-list values, just assign the response value\n            last_applied_manifest[key] = response[key]", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = response[key]"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8740, "num_retrieved": 21, "inference_time": 1.2190972715616226, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Start with an empty schema\n    observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Check if a custom observer schema is already defined for this resource\n        if resource.get(\"observer_schema\"):\n            # If custom schema exists, skip it\n            continue\n            \n        # Create a default observer schema entry for the resource\n        # The default schema includes the resource kind and name\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"name\": resource[\"name\"],\n            \"api_version\": resource.get(\"api_version\", \"\"),\n            \"metadata\": {\n                \"namespace\": resource.get(\"namespace\", \"\")\n            }\n        }\n        \n        # Add the default schema to the list\n        observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Start with an empty schema\n    observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Check if a custom observer schema is already defined for this resource\n        if resource.get(\"observer_schema\"):\n            # If custom schema exists, skip this resource\n            continue\n            \n        # Create a default observer schema entry for the resource\n        # The default schema includes the resource kind and name\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"name\": resource[\"name\"],\n            \"api_version\": resource.get(\"api_version\", \"\"),\n            \"metadata\": {\n                \"namespace\": resource.get(\"metadata\", {}).get(\"namespace\"),\n                \"labels\": resource.get(\"metadata\", {}).get(\"labels\", {})\n            }\n        }\n        \n        # Add the default schema to the observer schema list\n        observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Initialize the default observer schema\n    observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Check if a custom observer schema is already defined for this resource\n        if resource.get(\"observer_schema\"):\n            # If custom schema exists, skip it and use the existing one\n            observer_schema.append(resource)\n            continue\n            \n        # Generate default observer schema for the resource\n        # Default schema includes basic metadata and resource type\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"] if \"metadata\" in resource and \"name\" in resource[\"metadata\"] else \"\",\n                \"namespace\": resource[\"metadata\"][\"namespace\"] if \"metadata\" in resource and \"namespace\" in resource[\"metadata\"] else \"\"\n            },\n            \"spec\": resource.get(\"spec\", {}),\n            \"status\": resource.get(\"status\", {})\n        }\n        \n        # Add the default schema to the observer schema list\n        observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Initialize the default observer schema\n    observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Check if a custom observer schema is already defined for this resource\n        if resource.get(\"observer_schema\"):\n            # If custom schema exists, skip it and use the existing one\n            observer_schema.append(resource)\n            continue\n            \n        # Generate default observer schema for the resource\n        # The default schema includes the resource's kind, apiVersion, and metadata\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"].get(\"name\"),\n                \"namespace\": resource[\"metadata\"].get(\"namespace\")\n            }\n        }\n        \n        # Add the default schema to the observer schema list\n        observer_schema.append(default_schema)\n    \n    return observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Initialize the default observer schema\n    observer_schema = []\n    \n    # Iterate over each resource in the application's manifest\n    for resource in app.spec.manifest:\n        # Check if a custom observer schema is already defined for this resource\n        if resource.get(\"observer_schema\"):\n            # If custom schema exists, skip it\n            continue\n            \n        # Create a default observer schema entry for the resource\n        default_schema = {\n            \"kind\": resource[\"kind\"],\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"].get(\"name\"),\n                \"namespace\": resource[\"metadata\"].get(\"namespace\")\n            },\n            \"spec\": resource.get(\"spec\", {})\n        }\n        \n        # Add the default schema to the observer schema list\n        observer_schema.append(default_schema)\n    \n    return observer_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8613, "num_retrieved": 19, "inference_time": 1.2190972715616226, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query.\n    param_conversions = []\n    if self._out_style.is_named:\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    else:\n        in_counter = itertools.count()\n        out_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter, out_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n\n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query.\n    param_conversions = []\n    if self._out_style.is_named:\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    else:\n        in_counter = itertools.count()\n        out_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter, out_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n\n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query.\n    param_conversions = []\n    if self._out_style.is_named:\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    else:\n        in_counter = itertools.count()\n        out_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter, out_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n\n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query.\n    param_conversions = []\n    if self._out_style.is_named:\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    else:\n        in_counter = itertools.count()\n        out_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter, out_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n\n    return out_sql, out_params", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if is_sequence(params):\n        pass\n    elif isinstance(params, Mapping):\n        params = self._mapping_as_sequence(params)\n    else:\n        raise TypeError(\"params:{!r} is not a sequence or mapping.\".format(params))\n\n    # Convert query.\n    param_conversions = []\n    if self._out_style.is_named:\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions), sql)\n        out_params = self.__convert_params(params, param_conversions)\n    else:\n        in_counter = itertools.count()\n        out_counter = itertools.count()\n        out_sql = self._in_regex.sub(partial(self.__regex_replace, params, param_conversions, in_counter, out_counter), sql)\n        out_params = self.__convert_params(params, param_conversions)\n\n    return out_sql, out_params"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8614, "num_retrieved": 19, "inference_time": 1.2190972715616226, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n            contains the set of in-style parameters. It maps each parameter\n            (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n            is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n            If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n            *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n            parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    iter_params = iter(many_params)\n    first_params = next(iter_params)\n\n    if is_sequence(first_params):\n        pass\n    elif isinstance(first_params, Mapping):\n        first_params = self._mapping_as_sequence(first_params)\n    else:\n        raise TypeError(\"many_params[0]:{!r} is not a sequence or mapping.\".format(first_params))\n\n    # Convert query.\n    param_conversions = []\n    out_sql = self._in_regex.sub(partial(self.__regex_replace, first_params, param_conversions), sql)\n\n    # Convert parameters.\n    out_params = self.__convert_many_params(itertools.chain((first_params,), iter_params), param_conversions)\n\n    return out_sql, out_params"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8792, "num_retrieved": 33, "inference_time": 1.2190972715616226, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        if path is None:\n            # Validate the root filesystem\n            validator.obj_fs = self.root_fs\n        else:\n            # Validate object at given path\n            validator.obj_fs = open_fs(path, create=False)\n        \n        # Validate the object structure\n        if validator.validate(path):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        if path is None:\n            # Validate the root filesystem\n            validator.obj_fs = self.root_fs\n        else:\n            # Validate object at given path\n            validator.obj_fs = open_fs(path, create=False)\n        \n        # Validate the object structure\n        if validator.validate(path):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        if path is None:\n            # Validate the root filesystem\n            validator.obj_fs = self.root_fs\n        else:\n            # Validate object at given path\n            validator.obj_fs = open_fs(path, create=False)\n        \n        # Validate the object structure\n        if validator.validate(path):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        if path is None:\n            # Validate the root filesystem\n            validator.obj_fs = self.root_fs\n        else:\n            # Validate object at given path\n            validator.obj_fs = open_fs(path, create=False)\n        \n        # Validate the object structure\n        if validator.validate(path):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True, show_errors=True, check_digests=True, lax_digests=self.lax_digests)\n    try:\n        if path is None:\n            # Validate the root filesystem\n            validator.obj_fs = self.root_fs\n        else:\n            # Validate object at given path\n            validator.obj_fs = open_fs(path, create=False)\n        \n        # Validate the object structure\n        if validator.validate(path):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9042, "num_retrieved": 45, "inference_time": 1.2190972715616226, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    return self.__str__()", "def status_str(self, prefix=''):\n    \"\"\"\n    Return string of validator status, with optional prefix.\n    \"\"\"\n    return self.__str__()"]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8934, "num_retrieved": 38, "inference_time": 1.2190972715616226, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8651, "num_retrieved": 27, "inference_time": 1.2190972715616226, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8325, "num_retrieved": 13, "inference_time": 1.2190972715616226, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        inv_validator.validate(inventory, extract_spec_version=True)\n    else:\n        inv_validator.validate(inventory)\n    \n    return inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        else:\n            inv_validator.validate(inventory)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        try:\n            spec_version = inventory.get('type', '').split('_')[-1]\n            if spec_version and spec_version.startswith('ocfl_object_'):\n                spec_version = spec_version[len('ocfl_object_'):]\n                if spec_version.startswith('1.'):\n                    spec_version = spec_version[1:]\n                inv_validator.spec_version = spec_version\n        except (AttributeError, KeyError, ValueError):\n            pass\n    \n    inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        try:\n            spec_version = inventory.get('type', '').split('_')[-1]\n            if spec_version and spec_version.startswith('ocfl_object_'):\n                spec_version = spec_version[len('ocfl_object_'):]\n                if spec_version.isdigit():\n                    spec_version = '.'.join(spec_version.split('.')[0:2])\n            else:\n                spec_version = self.spec_version\n        except (AttributeError, ValueError):\n            spec_version = self.spec_version\n        inv_validator.validate(inventory, extract_spec_version=True)\n    else:\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n    \n    return inventory, inv_validator", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    inv_validator = InventoryValidator(log=self.log, where='root',\n                                       lax_digests=self.lax_digests,\n                                       spec_version=self.spec_version)\n    \n    if extract_spec_version:\n        inv_validator.validate(inventory, extract_spec_version=True)\n    else:\n        inv_validator.validate(inventory)\n    \n    return inv_validator"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8444, "num_retrieved": 18, "inference_time": 1.2190972715616226, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest_algorithm in digests_used[filepath]:\n            for digest in digests_used[filepath][digest_algorithm]:\n                if digest not in manifest_files[filepath]:\n                    self.log.error('E092c', where=filepath, digest_algorithm=digest_algorithm, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest_algorithm in digests_used[filepath]:\n            for digest in digests_used[filepath][digest_algorithm]:\n                if digest not in manifest_files[filepath]:\n                    self.log.error('E092c', where=filepath, digest_algorithm=digest_algorithm, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest_algorithm in digests_used[filepath]:\n            for digest in digests_used[filepath][digest_algorithm]:\n                if digest not in manifest_files[filepath]:\n                    self.log.error('E092c', where=filepath, digest_algorithm=digest_algorithm, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest_algorithm in digests_used[filepath]:\n            for digest in digests_used[filepath][digest_algorithm]:\n                if digest not in manifest_files[filepath]:\n                    self.log.error('E092c', where=filepath, digest_algorithm=digest_algorithm, digest=digest)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest_algorithm in digests_used[filepath]:\n            for digest in digests_used[filepath][digest_algorithm]:\n                if digest not in manifest_files[filepath]:\n                    self.log.error('E092c', where=filepath, digest_algorithm=digest_algorithm, digest=digest)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8305, "num_retrieved": 12, "inference_time": 1.2190972715616226, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the prior version's spec version is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=prior.spec_version, \n                       prev_version_dir='current', prev_spec_version=self.spec_version)\n    # Check that the manifest in prior version does not contain any files not in current inventory\n    if 'manifest' in prior.inventory:\n        for digest in prior.inventory['manifest']:\n            for filepath in prior.inventory['manifest'][digest]:\n                if filepath not in self.inventory['manifest']:\n                    self.log.error('E023b', where='prior', missing_filepaths=filepath)\n    # Check that the fixity in prior version does not contain any digests not in current inventory\n    if 'fixity' in prior.inventory:\n        for digest_algorithm in prior.inventory['fixity']:\n            for digest in prior.inventory['fixity'][digest_algorithm]:\n                for filepath in prior.inventory['fixity'][digest_algorithm][digest]:\n                    if filepath not in self.inventory['manifest']:\n                        self.log.error('E093b', where='prior', digest_algorithm=digest_algorithm, \n                                       digest=digest, content_path=filepath)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the prior version's spec version is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=prior.spec_version, \n                       prev_version_dir='current', prev_spec_version=self.spec_version)\n    # Check that the manifest in prior version does not contain files not in current inventory\n    if 'manifest' in prior.inventory:\n        for digest in prior.inventory['manifest']:\n            for filepath in prior.inventory['manifest'][digest]:\n                if filepath not in self.inventory['manifest']:\n                    self.log.error('E023b', where='prior', missing_filepaths=filepath)\n    # Check that the fixity in prior version does not contain digests not in current inventory\n    if 'fixity' in prior.inventory:\n        for digest_algorithm in prior.inventory['fixity']:\n            for digest in prior.inventory['fixity'][digest_algorithm]:\n                for filepath in prior.inventory['fixity'][digest_algorithm][digest]:\n                    if filepath not in self.inventory['manifest']:\n                        self.log.error('E093b', where='prior', digest_algorithm=digest_algorithm, \n                                       digest=digest, content_path=filepath)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the prior version's spec version is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=prior.spec_version, \n                       prev_version_dir='current', prev_spec_version=self.spec_version)\n    # Check that the manifest in prior version does not contain any files not present in current inventory\n    if 'manifest' in prior.inventory:\n        for digest in prior.inventory['manifest']:\n            for filepath in prior.inventory['manifest'][digest]:\n                if filepath not in self.inventory['manifest']:\n                    self.log.error('E023b', where='prior', missing_filepaths=filepath)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the prior version's spec version is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=prior.spec_version, \n                       prev_version_dir='current', prev_spec_version=self.spec_version)\n    # Check that the manifest in prior version does not contain files not present in current inventory\n    if 'manifest' in prior.inventory:\n        for digest in prior.inventory['manifest']:\n            for filepath in prior.inventory['manifest'][digest]:\n                if filepath not in self.inventory['manifest']:\n                    self.log.error('E023b', where='prior', missing_filepaths=filepath)\n    # Check that the fixity in prior version does not contain digests not present in current inventory\n    if 'fixity' in prior.inventory:\n        for digest_algorithm in prior.inventory['fixity']:\n            for digest in prior.inventory['fixity'][digest_algorithm]:\n                for filepath in prior.inventory['fixity'][digest_algorithm][digest]:\n                    if filepath not in self.inventory['manifest']:\n                        self.log.error('E093b', where='prior', digest_algorithm=digest_algorithm, \n                                       digest=digest, content_path=filepath)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the prior version uses a spec version that is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where='prior', this_spec_version=prior.spec_version, \n                       prev_version_dir='current', prev_spec_version=self.spec_version)\n    # Check that the manifest in the prior version does not contain any files not present in the current inventory\n    if 'manifest' in prior.inventory:\n        for digest in prior.inventory['manifest']:\n            for filepath in prior.inventory['manifest'][digest]:\n                if filepath not in self.inventory['manifest']:\n                    self.log.error('E023b', where='prior', missing_filepaths=filepath)\n    # Check that the fixity in the prior version does not contain any digests not present in the current inventory\n    if 'fixity' in prior.inventory:\n        for digest_algorithm in prior.inventory['fixity']:\n            for digest in prior.inventory['fixity'][digest_algorithm]:\n                for filepath in prior.inventory['fixity'][digest_algorithm][digest]:\n                    if filepath not in self.inventory['fixity']:\n                        self.log.error('E093b', where='prior', digest_algorithm=digest_algorithm, \n                                       digest=digest, content_path=filepath)"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8351, "num_retrieved": 14, "inference_time": 1.2190972715616226, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        raise ValueError(f\"Version {version} not found in inventory\")\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the actual content files from the manifest for this digest\n        manifest_files = inventory['manifest'][digest]\n        # Map logical paths to the actual content files on disk\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add all manifest files for this digest to the set of content files\n            for content_file in manifest_files:\n                logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        raise ValueError(f\"Version {version} not found in inventory\")\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the actual content files from the manifest for this digest\n        manifest_files = inventory['manifest'][digest]\n        # Map logical paths to the actual content files on disk\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add all manifest files for this digest to the set of content files\n            for content_file in manifest_files:\n                logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        raise ValueError(f\"Version {version} not found in inventory\")\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the actual content files from the manifest for this digest\n        manifest_files = inventory['manifest'][digest]\n        # Map logical paths to the actual content files on disk\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            # Add all manifest files for this digest to the set of content files\n            for content_file in manifest_files:\n                logical_path_map[logical_path].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        raise ValueError(f\"Version {version} not found in inventory\")\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the actual content files from the manifest for this digest\n        manifest_files = inventory['manifest'][digest]\n        # Map logical paths to the actual content files on disk\n        for logical_path in logical_files:\n            if logical_path not in logical_path_map:\n                logical_path_map[logical_path] = set()\n            # Add all manifest files for this digest to the set\n            for content_file in manifest_files:\n                logical_path_map[logical_path].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    if 'versions' not in inventory or version not in inventory['versions']:\n        raise ValueError(f\"Version {version} not found in inventory\")\n    \n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        # Get the actual content files from the manifest for this digest\n        manifest_files = inventory['manifest'][digest]\n        # Map logical paths to the actual content files on disk\n        for logical_file in logical_files:\n            if logical_file not in logical_path_map:\n                logical_path_map[logical_file] = set()\n            # Add all manifest files for this digest to the set of content files\n            for content_file in manifest_files:\n                logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8371, "num_retrieved": 15, "inference_time": 1.2190972715616226, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9707, "num_retrieved": 59, "inference_time": 1.2190972715616226, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_folder(path):\n        return file_utils.files_list(path)\n    elif file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9087, "num_retrieved": 36, "inference_time": 1.2190972715616226, "generate_results": ["def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list and group them by XML basename\n    for file in files:\n        # Extract the basename of the current file\n        file_basename = os.path.basename(file)\n        \n        # Get the prefix from the file basename (everything before the extension)\n        file_prefix = os.path.splitext(file_basename)[0]\n        \n        # Check if the file belongs to the same XML basename\n        if file_prefix.startswith(xml_basename):\n            # If not already in the dictionary, initialize with empty list\n            if xml_basename not in grouped_files:\n                grouped_files[xml_basename] = []\n            \n            # Add the file to the corresponding XML basename group\n            grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list and group them by XML basename\n    for file in files:\n        # Get the basename of the current file\n        file_basename = os.path.basename(file)\n        \n        # Extract the prefix from the file name (before extension)\n        file_prefix = os.path.splitext(file_basename)[0]\n        \n        # Check if the file belongs to the same XML basename\n        if file_prefix.startswith(xml_basename):\n            # If not already in the dictionary, initialize with empty list\n            if xml_basename not in grouped_files:\n                grouped_files[xml_basename] = []\n            \n            # Add the file to the corresponding XML basename group\n            grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list\n    for file in files:\n        # Get the basename of the current file\n        file_basename = os.path.basename(file)\n        \n        # Extract the prefix from the file name (before extension)\n        file_prefix = os.path.splitext(file_basename)[0]\n        \n        # Check if the file belongs to the same XML basename\n        if file_prefix.startswith(xml_basename):\n            # If not already in the dictionary, initialize with empty list\n            if xml_basename not in grouped_files:\n                grouped_files[xml_basename] = []\n            \n            # Add the file to the corresponding XML basename group\n            grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list\n    for file in files:\n        # Extract the basename of each file\n        file_basename = os.path.basename(file)\n        \n        # Check if the file is an XML file\n        if file_basename.endswith(\".xml\"):\n            # Extract the basename of the XML file\n            xml_file_basename = os.path.splitext(file_basename)[0]\n            \n            # Group files by XML basename\n            if xml_file_basename == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(xml_filename, files):\n    \"\"\"\n    Group files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    # Extract the basename of the XML file (without extension)\n    xml_basename = os.path.splitext(xml_filename)[0]\n    \n    # Create a dictionary to group files by XML basename\n    grouped_files = {}\n    \n    # Iterate through the files list\n    for file in files:\n        # Extract the basename of each file\n        file_basename = os.path.basename(file)\n        \n        # Check if the file is an XML file\n        if file_basename.endswith(\".xml\"):\n            # Extract the basename of the XML file\n            xml_file_basename = os.path.splitext(file_basename)[0]\n            \n            # Group files by the XML basename\n            if xml_file_basename == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n    \n    return grouped_files"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9139, "num_retrieved": 33, "inference_time": 1.2190972715616226, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    if ext == '.xml':\n        return file == prefix\n    elif ext == '.pdf':\n        return file == prefix or any(file == f'{prefix}-{lang}' for lang in pdf_langs)\n    return False", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(f'{prefix}-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(f'{prefix}-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(f'{prefix}-'))", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix or (ext == '.pdf' and file.startswith(f'{prefix}-'))"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9087, "num_retrieved": 33, "inference_time": 1.2190972715616226, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [file for file in files if os.path.basename(file).startswith(prefix)]"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9467, "num_retrieved": 48, "inference_time": 1.5838333368301392, "generate_results": ["def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to group files by XML basename\n    packages_data = defaultdict(dict)\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Process each file\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Check if it's a file (not a directory)\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract XML basename (without extension)\n        xml_basename = filename[:-4]  # Remove .xml extension\n        \n        # Get the full path of the XML file\n        xml_path = file_path\n        \n        # Look for related files (renditions, assets) with the same basename\n        # Check for files that match the XML basename pattern\n        for item in files:\n            item_path = os.path.join(folder, item)\n            if not os.path.isfile(item_path):\n                continue\n                \n            item_ext = os.path.splitext(item)[1].lower()\n            \n            # Skip XML files (already processed)\n            if item_ext == '.xml':\n                continue\n                \n            # Check if the item has the same basename as the XML file\n            item_basename = os.path.splitext(item)[0]\n            if item_basename == xml_basename:\n                # Add the file to the corresponding package\n                if item_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.tif', '.tiff']:\n                    # Add to renditions or assets based on extension\n                    if item_ext in ['.pdf']:\n                        packages_data[xml_basename]['renditions'] = packages_data[xml_basename].get('renditions', []) + [item_path]\n                    else:\n                        packages_data[xml_basename]['assets'] = packages_data[xml_basename].get('assets', []) + [item_path]\n        \n        # Add the XML file to the package\n        packages_data[xml_basename]['xml'] = xml_path\n    \n    # Convert defaultdict to regular dict\n    return dict(packages_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to group files by XML basename\n    packages_data = defaultdict(dict)\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Process each file\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Extract file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() == '.xml':\n            # Get XML basename (without extension)\n            xml_basename = os.path.splitext(filename)[0]\n            \n            # Store the XML file path\n            packages_data[xml_basename]['xml'] = file_path\n            \n            # Look for related files (renditions, assets) with similar naming pattern\n            # Check for rendition files (e.g., xml_name.pdf)\n            rendition_pattern = f\"{xml_basename}.pdf\"\n            asset_pattern = f\"{xml_basename}.jpg\"\n            \n            # Check for rendition files\n            rendition_files = [f for f in files if f.startswith(xml_basename) and f.endswith('.pdf')]\n            for rendition_file in rendition_files:\n                rendition_path = os.path.join(folder, rendition_file)\n                packages_data[xml_basename]['renditions'] = packages_data[xml_basename].get('renditions', []) + [rendition_path]\n            \n            # Check for asset files (e.g., xml_name-gf01.jpg)\n            asset_files = [f for f in files if f.startswith(xml_basename) and f.endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n            for asset_file in asset_files:\n                asset_path = os.path.join(folder, asset_file)\n                packages_data[xml_basename]['assets'] = packages_data[xml_basename].get('assets', []) + [asset_path]\n    \n    # Remove empty entries\n    packages_data = {k: v for k, v in packages_data.items() if v}\n    \n    return dict(packages_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to group files by XML basename\n    packages_data = defaultdict(dict)\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Process each file\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Extract file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() == '.xml':\n            # Get XML basename (without extension)\n            xml_basename = os.path.splitext(filename)[0]\n            \n            # Store the XML file path\n            packages_data[xml_basename]['xml'] = file_path\n            \n            # Look for related files (renditions and assets) with matching basename\n            for file in files:\n                file_path = os.path.join(folder, file)\n                if not os.path.isfile(file_path):\n                    continue\n                    \n                file_ext = os.path.splitext(file)[1].lower()\n                \n                # Check if it's a rendition or asset file\n                if file_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.tif', '.tiff', '.gif']:\n                    # Check if the filename matches the XML basename\n                    if os.path.splitext(file)[0] == xml_basename:\n                        file_type = 'rendition' if file_ext in ['.pdf'] else 'asset'\n                        if file_type not in packages_data[xml_basename]:\n                            packages_data[xml_basename][file_type] = []\n                        packages_data[xml_basename][file_type].append(file_path)\n    \n    # Convert defaultdict to regular dict\n    return dict(packages_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to group files by XML basename\n    packages_data = defaultdict(dict)\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Process each file\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Extract file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() == '.xml':\n            # Get XML basename (without extension)\n            xml_basename = os.path.splitext(filename)[0]\n            \n            # Store the XML file path\n            packages_data[xml_basename]['xml'] = file_path\n            \n            # Look for related files (renditions, assets) with similar naming pattern\n            # Assume renditions and assets follow the pattern: <xml_basename>-<suffix>.<ext>\n            # For example: 1414-431X-bjmbr-54-10-e11439.pdf, 1414-431X-bjmbr-54-10-e11439-gf01.jpg\n            \n            # Check for rendition files (typically .pdf)\n            for rendition_ext in ['.pdf']:\n                rendition_pattern = f\"{xml_basename}{rendition_ext}\"\n                rendition_files = [f for f in files if f.startswith(xml_basename) and f.endswith(rendition_ext)]\n                packages_data[xml_basename]['renditions'] = [os.path.join(folder, f) for f in rendition_files]\n            \n            # Check for asset files (typically .jpg, .png, .tif)\n            for asset_ext in ['.jpg', '.jpeg', '.png', '.tif', '.tiff']:\n                asset_pattern = f\"{xml_basename}{asset_ext}\"\n                asset_files = [f for f in files if f.startswith(xml_basename) and f.endswith(asset_ext)]\n                packages_data[xml_basename]['assets'] = [os.path.join(folder, f) for f in asset_files]\n    \n    # Convert defaultdict to regular dict", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to group files by XML basename\n    packages_data = defaultdict(dict)\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Process each file\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Extract file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() == '.xml':\n            # Get the basename without extension\n            xml_basename = os.path.splitext(filename)[0]\n            \n            # Store the XML file path\n            packages_data[xml_basename]['xml'] = file_path\n            \n            # Look for related files (renditions, assets) with similar naming pattern\n            # Check for rendition files (e.g., xml_name.pdf)\n            rendition_pattern = f\"{xml_basename}.pdf\"\n            asset_pattern = f\"{xml_basename}.jpg\"\n            \n            # Check for rendition files\n            rendition_files = [f for f in files if f.startswith(xml_basename) and f.endswith('.pdf')]\n            for rendition_file in rendition_files:\n                rendition_path = os.path.join(folder, rendition_file)\n                packages_data[xml_basename]['renditions'] = packages_data[xml_basename].get('renditions', []) + [rendition_path]\n            \n            # Check for asset files (e.g., xml_name-gf01.jpg)\n            asset_files = [f for f in files if f.startswith(xml_basename) and f.endswith(('.jpg', '.jpeg', '.png', '.gif'))]\n            for asset_file in asset_files:\n                asset_path = os.path.join(folder, asset_file)\n                packages_data[xml_basename]['assets'] = packages_data[xml_basename].get('assets', []) + [asset_path]\n    \n    # Remove empty entries\n    packages_data = {k: v for k, v in packages_data.items() if v}\n    \n    return dict(packages_data)"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9018, "num_retrieved": 31, "inference_time": 1.5838333368301392, "generate_results": ["def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}", "def _eval_file(prefix, file_path, file_folder):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n        for lang in pdf_langs:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': os.path.join(file_folder, file_path)}\n    return {'type': 'asset', 'path': os.path.join(file_folder, file_path)}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10619, "num_retrieved": 86, "inference_time": 1.5838333368301392, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adds a rendition to the package with the specified language and file path.\n    \n    Parameters\n    ----------\n    lang : str\n        The language code (e.g., 'en', 'pt') for the rendition.\n    file_path : str\n        The path to the rendition file.\n    \n    Returns\n    -------\n    dict\n        A dictionary containing the rendition information with keys 'original' and the language-specific name.\n    \"\"\"\n    # Create rendition dictionary with original and language-specific names\n    rendition = {\n        \"original\": os.path.basename(file_path),\n        lang: f\"{os.path.splitext(file_path)[0]}-{lang}.{os.path.splitext(file_path)[1]}\"\n    }\n    return rendition", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adds a rendition to the package with the specified language and file path.\n    \n    Parameters\n    ----------\n    lang : str\n        The language code (e.g., 'en', 'pt') for the rendition.\n    file_path : str\n        The path to the file to be added as a rendition.\n    \n    Returns\n    -------\n    dict\n        A dictionary containing the rendition information with keys 'original' and the language code.\n    \"\"\"\n    rendition_info = {\n        \"original\": os.path.basename(file_path),\n        lang: f\"{os.path.splitext(os.path.basename(file_path))[0]}-{lang}.{os.path.splitext(file_path)[1]}\"\n    }\n    return rendition_info", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adds a rendition to the package with the specified language and file path.\n    \n    Parameters\n    ----------\n    lang : str\n        The language code (e.g., 'en', 'pt') for the rendition.\n    file_path : str\n        The path to the rendition file.\n    \n    Returns\n    -------\n    dict\n        A dictionary containing the rendition information with keys 'original' and the language-specific name.\n    \"\"\"\n    # Create the rendition dictionary with original and language-specific names\n    rendition = {\n        \"original\": os.path.basename(file_path),\n        lang: f\"{os.path.splitext(file_path)[0]}-{lang}.{os.path.splitext(file_path)[1]}\"\n    }\n    return rendition", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adds a rendition to the package with the specified language and file path.\n    \n    Parameters\n    ----------\n    lang : str\n        The language code (e.g., 'en', 'pt') for the rendition.\n    file_path : str\n        The path to the rendition file.\n    \n    Returns\n    -------\n    dict\n        A dictionary containing the rendition information with keys 'original' and the language-specific name.\n    \"\"\"\n    # Create the rendition dictionary\n    rendition = {\n        \"original\": os.path.basename(file_path),\n        lang: f\"{os.path.splitext(file_path)[0]}-{lang}.{os.path.splitext(file_path)[1]}\"\n    }\n    return rendition", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adds a rendition to the package with the specified language and file path.\n    \n    Parameters\n    ----------\n    lang : str\n        The language code (e.g., 'en', 'pt') for the rendition.\n    file_path : str\n        The path to the rendition file.\n    \n    Returns\n    -------\n    dict\n        A dictionary with the rendition information in the format:\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\"\n        }\n    \"\"\"\n    rendition_info = {\n        \"original\": os.path.basename(file_path),\n        lang: f\"{os.path.splitext(file_path)[0]}-{lang}.{os.path.splitext(file_path)[1]}\"\n    }\n    return rendition_info"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10462, "num_retrieved": 80, "inference_time": 1.5838333368301392, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"\n    Adds an asset to the package with a canonical name based on the basename and file extension.\n    \n    Parameters\n    ----------\n    basename : str\n        The base name of the asset (without extension), e.g., \"artigo02-gf03\"\n    file_path : str\n        The path to the file on disk\n        \n    Returns\n    -------\n    None\n    \"\"\"\n    # Extract the extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create the canonical asset name using the basename and extension\n    canonical_name = f\"{basename}{ext}\"\n    \n    # Add the asset with its canonical name and file path\n    self.assets[canonical_name] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    Adds an asset to the package with a canonical name based on the basename and file extension.\n    \n    Parameters\n    ----------\n    basename : str\n        The base name of the asset (e.g., \"artigo02-gf03\")\n    file_path : str\n        The path to the file on disk\n        \n    Returns\n    -------\n    dict\n        A dictionary with the canonical name and the file path\n    \"\"\"\n    # Extract the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create the canonical name by combining basename with the extension\n    canonical_name = f\"{basename}{ext}\"\n    \n    # Store the asset with its canonical name and original path\n    self.assets[canonical_name] = file_path\n    \n    return {\n        \"name\": canonical_name,\n        \"uri\": f\"{self.package_name}.{canonical_name}\",\n        \"path\": file_path\n    }", "def add_asset(self, basename, file_path):\n    \"\"\"\n    Adds an asset to the package with a canonical name based on the basename and file extension.\n    \n    Parameters\n    ----------\n    basename : str\n        The base name of the asset (e.g., \"artigo02-gf03\")\n    file_path : str\n        The path to the file on disk\n        \n    Returns\n    -------\n    dict\n        A dictionary with the canonical name and the file path\n    \"\"\"\n    # Extract the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create the canonical name by combining basename with extension\n    canonical_name = f\"{basename}{ext}\"\n    \n    # Store the asset with its canonical name and original path\n    self.assets[canonical_name] = file_path\n    \n    return {\n        \"name\": canonical_name,\n        \"uri\": f\"{self.package_name}.{canonical_name}\",\n        \"path\": file_path\n    }", "def add_asset(self, basename, file_path):\n    \"\"\"\n    Adds an asset to the package with a canonical name based on the basename and file extension.\n    \n    Parameters\n    ----------\n    basename : str\n        The base name of the asset (e.g., \"artigo02-gf03\")\n    file_path : str\n        The path to the file on disk\n        \n    Returns\n    -------\n    dict\n        A dictionary with the canonical name and the file path\n    \"\"\"\n    # Extract the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create the canonical name by combining basename with extension\n    canonical_name = f\"{basename}{ext}\"\n    \n    # Store the asset with its canonical name and original path\n    self.assets[canonical_name] = file_path\n    \n    return {\n        \"name\": canonical_name,\n        \"uri\": f\"{self.package_name}.{canonical_name}\",\n        \"path\": file_path\n    }", "def add_asset(self, basename, file_path):\n    \"\"\"\n    Adds an asset to the package with a canonical name based on the basename and file extension.\n    \n    Parameters\n    ----------\n    basename : str\n        The base name of the asset (e.g., \"artigo02-gf03\")\n    file_path : str\n        The path to the file on disk\n    \n    Returns\n    -------\n    dict\n        A dictionary with the canonical name and the file path\n    \"\"\"\n    # Extract the file extension from the file path\n    _, ext = os.path.splitext(file_path)\n    \n    # Create the canonical name by combining basename with extension\n    canonical_name = f\"{basename}{ext}\"\n    \n    # Store the asset with its canonical name and original path\n    self.assets[canonical_name] = file_path\n    \n    return {\n        \"name\": canonical_name,\n        \"uri\": f\"{self.package_name}.{canonical_name}\",\n        \"path\": file_path\n    }"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9395, "num_retrieved": 47, "inference_time": 1.5838333368301392, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    file_list = []\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n    \n    # Group files by their XML basename\n    xml_basenames = {}\n    for filename in file_list:\n        if filename.endswith(\".xml\"):\n            basename = os.path.splitext(filename)[0]\n            if basename not in xml_basenames:\n                xml_basenames[basename] = []\n            xml_basenames[basename].append(filename)\n        else:\n            # For non-XML files, we'll associate them with the XML they belong to\n            # by checking if their basename matches any XML basename\n            xml_basename = os.path.splitext(filename)[0]\n            if xml_basename in xml_basenames:\n                xml_basenames[xml_basename].append(filename)\n    \n    # Create the result dictionary\n    result = {}\n    for xml_basename, files in xml_basenames.items():\n        result[xml_basename] = files\n    \n    return result", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    file_list = []\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n    \n    # Group files by their XML basename\n    xml_basenames = {}\n    for filename in file_list:\n        if filename.endswith(\".xml\"):\n            basename = os.path.splitext(filename)[0]\n            if basename not in xml_basenames:\n                xml_basenames[basename] = []\n            xml_basenames[basename].append(filename)\n        else:\n            # For non-XML files, we'll add them to the corresponding XML basename\n            # by checking if they are part of an XML file's path\n            base_dir = os.path.dirname(filename)\n            if base_dir and base_dir.endswith(\".xml\"):\n                # This is a bit of a hack - we assume that non-XML files are part of the XML\n                # that they are associated with\n                pass\n    \n    # Create the result dictionary\n    result = {}\n    for basename, filenames in xml_basenames.items():\n        # Extract the XML file from the list\n        xml_file = None\n        for f in filenames:\n            if f.endswith(\".xml\"):\n                xml_file = f\n                break\n        \n        if xml_file:\n            # Get the rest of the files that are associated with this XML\n            other_files = [f for f in filenames if f != xml_file]\n            \n            # Group other files by their basename\n            file_groups = {}\n            for f in other_files:\n                file_part = os.path.basename(f)\n                if file_part not in file_groups:\n                    file_groups[file_part] = []\n                file_groups[file_part].append(f)\n            \n            result[basename] = {\n                'xml': xml_file,\n                'files': file_groups\n            }\n    \n    return result", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    file_list = []\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n    \n    # Group files by their XML basename\n    xml_basenames = {}\n    for filename in file_list:\n        if filename.endswith(\".xml\"):\n            basename = os.path.splitext(filename)[0]\n            if basename not in xml_basenames:\n                xml_basenames[basename] = []\n            xml_basenames[basename].append(filename)\n        else:\n            # For non-XML files, we'll add them to the corresponding XML basename\n            # by checking if they are part of an XML file's path structure\n            # This is a heuristic approach to group assets and renditions\n            for xml_basename in xml_basenames:\n                if xml_basename in filename:\n                    xml_basenames[xml_basename].append(filename)\n                    break\n    \n    # Create the result dictionary\n    result = {}\n    for xml_basename in xml_basenames:\n        result[xml_basename] = {\n            'xml_files': xml_basenames[xml_basename],\n            'files': xml_basenames[xml_basename]\n        }\n    \n    return result", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    file_list = []\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n    \n    # Group files by their XML basename\n    xml_basenames = {}\n    for filename in file_list:\n        file_ext = os.path.splitext(filename)[-1]\n        if file_ext == '.xml':\n            basename = os.path.splitext(filename)[0]\n            if basename not in xml_basenames:\n                xml_basenames[basename] = []\n            xml_basenames[basename].append(filename)\n        else:\n            # For non-XML files, we'll group them under the XML basename if possible\n            # Extract the prefix from the XML filename (without extension)\n            xml_prefix = None\n            for xml_filename in xml_basenames.keys():\n                if filename.startswith(xml_filename):\n                    xml_prefix = xml_filename\n                    break\n            if xml_prefix:\n                if xml_prefix not in xml_basenames:\n                    xml_basenames[xml_prefix] = []\n                xml_basenames[xml_prefix].append(filename)\n    \n    # Create the result dictionary\n    result = {}\n    for basename, files in xml_basenames.items():\n        result[basename] = {\n            'xml_files': files,\n            'other_files': []\n        }\n        \n        # Separate XML and non-XML files\n        xml_files = [f for f in files if f.endswith('.xml')]\n        non_xml_files = [f for f in files if not f.endswith('.xml')]\n        \n        result[basename]['xml_files'] = xml_files\n        result[basename]['other_files'] = non_xml_files\n    \n    return result", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    file_list = []\n    with ZipFile(zip_path, 'r') as zf:\n        file_list = zf.namelist()\n    \n    # Group files by their XML basename\n    xml_basenames = {}\n    for filename in file_list:\n        basename, ext = os.path.splitext(filename)\n        if ext == '.xml':\n            # Use the basename of the XML file as key\n            xml_basenames[basename] = {\n                'xml': filename,\n                'files': []\n            }\n            # Add all files that share the same XML basename\n            for file in file_list:\n                file_basename, file_ext = os.path.splitext(file)\n                if file_basename == basename:\n                    xml_basenames[basename]['files'].append(file)\n    \n    return xml_basenames"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9426, "num_retrieved": 47, "inference_time": 1.5838333368301392, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        files = zf.namelist()\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        files = zf.namelist()\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        files = zf.namelist()\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        files = zf.namelist()\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zf:\n        files = zf.namelist()\n    return files"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10075, "num_retrieved": 67, "inference_time": 1.5838333368301392, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9784, "num_retrieved": 58, "inference_time": 1.5838333368301392, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10136, "num_retrieved": 69, "inference_time": 1.5838333368301392, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by a space or other common separator\n    # The issue element typically contains volume, number, and supplement information\n    # Example: \"12 3 sup1\" or \"12(3)sup1\"\n    import re\n    \n    # Match patterns like \"12 3 sup1\" or \"12(3)sup1\"\n    match = re.search(r'(\\d+)\\s*([0-9]+)(?:\\s*sup[0-9]+)?', issue)\n    if match:\n        volume = int(match.group(1))\n        number = int(match.group(2))\n        suppl = 0\n    else:\n        # Try to extract just the number part\n        match = re.search(r'(\\d+)(?:\\s*sup[0-9]+)?', issue)\n        if match:\n            number = int(match.group(1))\n            suppl = 0\n        else:\n            number = 0\n            suppl = 0\n    \n    # Extract supplement if present\n    suppl_match = re.search(r'sup([0-9]+)', issue, re.IGNORECASE)\n    if suppl_match:\n        suppl = int(suppl_match.group(1))\n    \n    return number, suppl", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by a space or other common separator\n    # The issue element typically contains volume, number, and supplement information\n    # We assume the format is \"volume number[suppl]\" or similar\n    parts = issue.strip().split()\n    \n    # Try to extract number and supplement\n    number = 0\n    suppl = 0\n    \n    # Look for number (usually a digit)\n    for part in parts:\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # Look for supplement (usually a digit after 'suppl' or in a suffix)\n    for part in parts:\n        if part.lower().startswith('suppl') or part.lower().startswith('s'):\n            # Extract the digit after 'suppl' or 's'\n            suppl_part = part.replace('suppl', '').replace('s', '').strip()\n            if suppl_part.isdigit():\n                suppl = int(suppl_part)\n            break\n    \n    return number, suppl", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by a space or other common separators\n    # The issue string typically contains volume, number, and supplement information\n    parts = issue.strip().split()\n    \n    # Try to extract number and supplement\n    number = 0\n    suppl = 0\n    \n    # Look for number (usually a digit or a digit followed by 's' or 'suppl')\n    for part in parts:\n        # Check if part contains a number\n        if part.isdigit():\n            number = int(part)\n            break\n        # Check if part contains a number followed by 's' or 'suppl'\n        elif part.endswith('s') or part.endswith('suppl'):\n            # Extract the number before 's' or 'suppl'\n            num_part = part[:-1]\n            if num_part.isdigit():\n                number = int(num_part)\n                break\n    \n    # Look for supplement (usually a digit or 'suppl')\n    for part in parts:\n        if part.isdigit():\n            # If we already have a number, check if this is a supplement\n            if number == 0:\n                number = int(part)\n            else:\n                suppl = int(part)\n            break\n        elif part.endswith('suppl') or part.endswith('s'):\n            # Extract the number before 'suppl' or 's'\n            num_part = part[:-1]\n            if num_part.isdigit():\n                suppl = int(num_part)\n                break\n    \n    return number, suppl", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by a space or other common separator\n    # The issue element typically contains volume, number, and supplement information\n    # We assume the format is \"volume number[suppl]\" or similar\n    parts = issue.strip().split()\n    \n    # Try to extract number and supplement\n    number = 0\n    suppl = 0\n    \n    # Look for number (usually the second part)\n    for part in parts:\n        # Check if part contains a number\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # Look for supplement (usually after a 's' or 'suppl' indicator)\n    for part in parts:\n        if part.lower().endswith('s') or part.lower().startswith('suppl'):\n            # Extract the number after 's' or 'suppl'\n            if part.lower().startswith('suppl'):\n                suppl = 1\n            else:\n                # Try to extract number after 's'\n                if len(part) > 1 and part[-1] == 's':\n                    # Extract the number before 's'\n                    num_part = part[:-1]\n                    if num_part.isdigit():\n                        suppl = int(num_part)\n    \n    # If no number found, try to extract from the full string\n    if number == 0:\n        # Try to find a number in the string\n        match = re.search(r'\\d+', issue)\n        if match:\n            number = int(match.group())\n    \n    # If no supplement found, try to extract from the string\n    if suppl == 0:\n        # Look for 'suppl' or 's' in the string\n        if 'suppl' in issue.lower():\n            suppl = 1\n        elif 's' in issue.lower():\n            # Try to extract number before 's'\n            match = re.search(r'(\\d+)s', issue)\n            if match:\n                suppl = int(match.group(1))\n    \n    return number, suppl", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, 0\n    \n    # Split the issue string by a space or other common separator\n    # The issue element typically contains volume, number, and supplement information\n    # We assume the format is \"volume number[suppl]\" or similar\n    parts = issue.strip().split()\n    \n    # Try to extract number and supplement\n    number = 0\n    suppl = 0\n    \n    # Look for number (usually a digit or a digit followed by 's' or 'suppl')\n    for part in parts:\n        # Check if part contains a number\n        if part.isdigit():\n            number = int(part)\n            break\n        # Check if part contains a number followed by 's' or 'suppl'\n        elif part.endswith('s') or part.endswith('suppl'):\n            # Extract the number before 's' or 'suppl'\n            num_part = part[:-1]\n            if num_part.isdigit():\n                number = int(num_part)\n            break\n    \n    # If no number found, try to extract supplement\n    # Look for 'suppl' or 's' at the end\n    for part in parts:\n        if part.lower() == 'suppl' or part.lower() == 's':\n            suppl = 1\n            break\n    \n    return number, suppl"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 1.5838333368301392, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 1.5838333368301392, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # These simplifications are typically handled at the expression level\n            # by evaluating the structure of the expression tree\n            # Since we don't have direct access to the internal structure,\n            # we assume the expression already represents a simplified form\n            # or that the simplification is handled by the BooleanAlgebra class\n            result.append(expr)\n        else:\n            result.append(expr)\n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # Check if expression is of form A & (~A | B) or A | (~A & B)\n        \n        # Simplify using the absorption laws\n        if isinstance(expr, Expression):\n            # Simplify using the absorption laws\n            # This is a simplified implementation that applies the rules directly\n            # In a full implementation, we would need to parse the expression tree\n            # and apply the rules recursively\n            \n            # For now, we just return the expression as is\n            # A full implementation would require parsing and tree traversal\n            pass\n    \n    # Return a new list of expressions after applying absorption rules\n    # This is a placeholder - actual implementation would require deep parsing\n    return args", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # These simplifications are typically handled by the Expression class\n            # We assume the Expression class has methods to handle these simplifications\n            simplified = expr.simplify()\n            if simplified is not None:\n                result.append(simplified)\n            else:\n                result.append(expr)\n        else:\n            result.append(expr)\n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we just return the original expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # This is a simplified implementation assuming expressions are in canonical form\n        # In a full implementation, we would need to parse the expression tree\n        \n        # For now, we'll just return the original expressions\n        # A full implementation would require recursive traversal of expression trees\n        result.append(expr)\n    \n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9089, "num_retrieved": 37, "inference_time": 1.5838333368301392, "generate_results": ["def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, hook):\n            setattr(self, hook, [])\n        if not isinstance(getattr(self, hook), list):\n            setattr(self, hook, [])\n        getattr(self, hook).append(fn)\n        return fn\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, hook):\n            setattr(self, hook, [])\n        if not isinstance(getattr(self, hook), list):\n            setattr(self, hook, [])\n        getattr(self, hook).append(fn)\n        return fn\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, hook):\n            setattr(self, hook, [])\n        if not isinstance(getattr(self, hook), list):\n            setattr(self, hook, [])\n        getattr(self, hook).append(fn)\n        return fn\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, hook):\n            setattr(self, hook, [])\n        if not isinstance(getattr(self, hook), list):\n            setattr(self, hook, [])\n        getattr(self, hook).append(fn)\n        return fn\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(fn):\n        if not hasattr(self, hook):\n            setattr(self, hook, [])\n        if not isinstance(getattr(self, hook), list):\n            setattr(self, hook, [])\n        getattr(self, hook).append(fn)\n        return fn\n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8907, "num_retrieved": 32, "inference_time": 1.5838333368301392, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"static\": {\n                \"name\": user\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port\n        }\n    }"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8832, "num_retrieved": 29, "inference_time": 1.5838333368301392, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Check if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Check if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Check if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # First treat UTC as wall time and get the transition we're in.\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Determine if the datetime is ambiguous and in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9020, "num_retrieved": 35, "inference_time": 1.5838333368301392, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through each attribute and convert fractional values to integers\n    # For days, we convert the fractional part to hours and add to hours\n    if hasattr(self, 'days'):\n        days = int(self.days)\n        fractional_days = self.days - days\n        hours = int(fractional_days * 24)\n        if hours > 0:\n            result.days = days + 1\n            result.hours = hours\n        else:\n            result.days = days\n            result.hours = 0\n    \n    # For other attributes, ensure they are integers\n    for attr in ['years', 'months', 'weeks', 'days', 'hours', 'minutes', 'seconds', 'microseconds']:\n        if hasattr(self, attr):\n            value = getattr(self, attr)\n            if isinstance(value, float):\n                value = int(value)\n            elif isinstance(value, (int, float)):\n                value = int(value)\n            setattr(result, attr, value)\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all relative attributes and convert fractional values to integers\n    # For each attribute, if it has a fractional part, convert it to integer by rounding\n    # and adjust the value accordingly\n    \n    # Handle days first - convert fractional days to integer days and hours\n    if hasattr(self, 'days'):\n        days = self.days\n        if not isinstance(days, int):\n            days_int = int(round(days))\n            # Convert fractional part to hours\n            fractional_days = days - days_int\n            hours = int(round(fractional_days * 24))\n            result.days = days_int\n            result.hours = result.hours + hours\n        else:\n            result.days = days\n    \n    # Handle hours - convert fractional hours to integer hours\n    if hasattr(self, 'hours'):\n        hours = self.hours\n        if not isinstance(hours, int):\n            hours_int = int(round(hours))\n            result.hours = hours_int\n    \n    # Handle minutes - convert fractional minutes to integer minutes\n    if hasattr(self, 'minutes'):\n        minutes = self.minutes\n        if not isinstance(minutes, int):\n            minutes_int = int(round(minutes))\n            result.minutes = minutes_int\n    \n    # Handle seconds - convert fractional seconds to integer seconds\n    if hasattr(self, 'seconds'):\n        seconds = self.seconds\n        if not isinstance(seconds, int):\n            seconds_int = int(round(seconds))\n            result.seconds = seconds_int\n    \n    # Handle weeks - convert fractional weeks to integer weeks\n    if hasattr(self, 'weeks'):\n        weeks = self.weeks\n        if not isinstance(weeks, int):\n            weeks_int = int(round(weeks))\n            result.weeks = weeks_int\n    \n    # Handle months - convert fractional months to integer months\n    if hasattr(self, 'months'):\n        months = self.months\n        if not isinstance(months, int):\n            months_int = int(round(months))\n            result.months = months_int\n    \n    # Handle years - convert fractional years to integer years\n    if", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all relative attributes and convert fractional values to integers\n    # For each attribute, if it has a fractional component, convert it to integer by rounding\n    # and adjust the value accordingly\n    \n    # Handle days first - if days is fractional, convert to integer days and add the remainder to hours\n    if hasattr(self, 'days') and self.days is not None:\n        days = int(self.days)\n        remainder = (self.days - days) * 24  # Convert fractional days to hours\n        if remainder > 0:\n            # Add the remainder to hours\n            if hasattr(self, 'hours') and self.hours is not None:\n                result.hours = self.hours + int(remainder)\n            else:\n                result.hours = int(remainder)\n    \n    # Handle other attributes (hours, minutes, seconds, weeks, months, years, etc.)\n    # For each attribute, convert fractional values to integers\n    for attr in ['hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        if hasattr(self, attr) and self.__dict__.get(attr) is not None:\n            value = self.__dict__[attr]\n            if isinstance(value, (int, float)):\n                if isinstance(value, float):\n                    value = int(round(value))\n                result.__dict__[attr] = value\n    \n    # Handle the case where we have fractional days that need to be converted to hours\n    # This is already handled above\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all relative attributes and convert fractional values to integers\n    # For days, we convert fractional part to hours and add to hours\n    if hasattr(self, 'days'):\n        days = self.days\n        if not isinstance(days, int):\n            days_int = int(days)\n            days_frac = days - days_int\n            # Convert fractional days to hours\n            hours_frac = days_frac * 24\n            # Add fractional hours to hours attribute\n            if hasattr(self, 'hours'):\n                hours = self.hours + hours_frac\n            else:\n                hours = hours_frac\n            # Update days to integer part\n            result.days = days_int\n            # Update hours with fractional part converted\n            result.hours = int(hours)\n        else:\n            result.days = days\n    \n    # Handle other attributes similarly\n    for attr in ['months', 'weeks', 'hours', 'minutes', 'seconds', 'years']:\n        if hasattr(self, attr):\n            value = getattr(self, attr)\n            if not isinstance(value, int):\n                value_int = int(value)\n                value_frac = value - value_int\n                # Convert fractional part to appropriate unit\n                if attr == 'hours':\n                    # Convert fractional hours to minutes\n                    minutes_frac = value_frac * 60\n                    if hasattr(self, 'minutes'):\n                        minutes = self.minutes + minutes_frac\n                    else:\n                        minutes = minutes_frac\n                    result.hours = int(minutes)\n                elif attr == 'minutes':\n                    # Convert fractional minutes to seconds\n                    seconds_frac = value_frac * 60\n                    if hasattr(self, 'seconds'):\n                        seconds = self.seconds + seconds_frac\n                    else:\n                        seconds = seconds_frac\n                    result.minutes = int(seconds)\n                elif attr == 'seconds':\n                    # Convert fractional seconds to microseconds\n                    microseconds_frac = value_frac * 1000000\n                    result.seconds = int(microseconds_frac)\n                else:\n                    result.__dict__[attr] = value", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    result = self.__class__()\n    \n    # Convert all fractional values to integers\n    for attr, value in self.__dict__.items():\n        if isinstance(value, (int, float)):\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if value != int(value):\n                    # For fractional days, convert to hours and add to hours\n                    if attr == 'days':\n                        days = int(value)\n                        hours = int((value - days) * 24)\n                        result.days += days\n                        result.hours += hours\n                    elif attr == 'hours':\n                        result.hours = int(value)\n                else:\n                    result.__dict__[attr] = int(value)\n            else:\n                result.__dict__[attr] = value\n    \n    # Ensure all values are integers\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks']:\n        if attr in result.__dict__:\n            result.__dict__[attr] = int(result.__dict__[attr])\n    \n    return result"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8862, "num_retrieved": 29, "inference_time": 1.5838333368301392, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring using UTF-8 encoding\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return the original result (already bytes or unicode)\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring using UTF-8 encoding\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return the original result (already bytes or unicode)\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring using UTF-8 encoding\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return the original result (already bytes or unicode)\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring using UTF-8 encoding\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return the original result (already bytes or unicode)\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        # In Python 2, convert unicode to bytestring using UTF-8 encoding\n        def wrapper(self):\n            result = namefunc(self)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        # In Python 3, return the original result (already bytes or unicode)\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8881, "num_retrieved": 35, "inference_time": 1.6032642275094986, "generate_results": ["def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fallback to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\", \"full-revisionid\": None, \"dirty\": False, \"error\": None, \"date\": None}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fallback to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\", \"full-revisionid\": None, \"dirty\": False, \"error\": None, \"date\": None}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fallback to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\", \"full-revisionid\": None, \"dirty\": False, \"error\": None, \"date\": None}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fallback to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\", \"full-revisionid\": None, \"dirty\": False, \"error\": None, \"date\": None}", "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    try:\n        return versions_from_file(\"_version.py\")\n    except Exception:\n        # Fallback to default version if unable to read _version.py\n        return {\"version\": \"0.0.0\", \"full-revisionid\": None, \"dirty\": False, \"error\": None, \"date\": None}"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8864, "num_retrieved": 35, "inference_time": 1.6032642275094986, "generate_results": ["def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"\n    Render the given version pieces into the requested style.\n    \"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8828, "num_retrieved": 33, "inference_time": 1.6032642275094986, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if \"+\" in pieces[\"closest-tag\"]:\n            return \".\"\n        else:\n            return \"+\"\n    else:\n        return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if \"+\" in pieces[\"closest-tag\"]:\n            return \".\"\n        else:\n            return \"+\"\n    else:\n        return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if \"+\" in pieces[\"closest-tag\"]:\n            return \".\"\n        else:\n            return \"+\"\n    else:\n        return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if \"+\" in pieces[\"closest-tag\"]:\n            return \".\"\n        else:\n            return \"+\"\n    else:\n        return \"+\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if \"+\" in pieces[\"closest-tag\"]:\n            return \".\"\n        else:\n            return \"+\"\n    else:\n        return \"+\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8657, "num_retrieved": 27, "inference_time": 1.6032642275094986, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(f\"Command failed with return code {e.returncode}\", file=sys.stderr)\n            if e.stderr:\n                print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(f\"Error running command: {str(e)}\", file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(f\"Command failed with return code {e.returncode}\", file=sys.stderr)\n            if e.stdout:\n                print(e.stdout, file=sys.stderr)\n            if e.stderr:\n                print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(f\"Unexpected error: {e}\", file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE if not hide_stderr else None,\n            stderr=subprocess.STDOUT if not hide_stderr else None,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose or if command succeeded\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if result.stderr:\n                print(result.stderr)\n        \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stdout or e.stderr or \"Command failed\")\n        raise\n    except Exception as e:\n        if verbose:\n            print(f\"Error running command: {e}\")\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Handle output based on verbose flag\n        if verbose:\n            print(f\"Command: {' '.join(cmd)}\")\n            if result.stdout:\n                print(f\"STDOUT: {result.stdout}\")\n            if result.stderr:\n                print(f\"STDERR: {result.stderr}\")\n        \n        # Hide stderr if requested\n        if hide_stderr and result.stderr:\n            result.stderr = \"\"\n            \n        return result\n        \n    except subprocess.SubprocessError as e:\n        print(f\"Error running command: {e}\", file=sys.stderr)\n        return None", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else subprocess.DEVNULL,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        raise"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8924, "num_retrieved": 37, "inference_time": 1.6032642275094986, "generate_results": ["def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"  # default value, can be overridden by configuration\n    cfg.style = \"pep440\"  # default style\n    cfg.versionfile_source = \"_version.py\"  # default version file source\n    cfg.versionfile_build = None  # default build file\n    cfg.tag_prefix = \"\"  # default tag prefix\n    cfg.parentdir_prefix = \"\"  # default parent directory prefix\n    cfg.verbose = False  # default verbose mode\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"  # default value, can be overridden by configuration\n    cfg.style = \"pep440\"  # default style\n    cfg.versionfile_source = \"_version.py\"  # default version file source\n    cfg.versionfile_build = None  # default build file\n    cfg.tag_prefix = \"\"  # default tag prefix\n    cfg.parentdir_prefix = \"\"  # default parent directory prefix\n    cfg.verbose = False  # default verbose mode\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"  # default value, can be overridden by configuration\n    cfg.style = \"pep440\"  # default style\n    cfg.versionfile_source = \"_version.py\"  # default version file source\n    cfg.versionfile_build = None  # default build file\n    cfg.tag_prefix = \"\"  # default tag prefix\n    cfg.parentdir_prefix = \"\"  # default parent directory prefix\n    cfg.verbose = False  # default verbose mode\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"  # default value, can be overridden by actual configuration\n    cfg.style = \"pep440\"  # default style\n    cfg.versionfile_source = \"_version.py\"  # default version file source\n    cfg.versionfile_build = None  # default build file\n    cfg.tag_prefix = \"\"  # default tag prefix\n    cfg.parentdir_prefix = \"\"  # default parent directory prefix\n    cfg.verbose = False  # default verbose mode\n    return cfg", "def get_config():\n    \"\"\"\n    Create, populate and return the VersioneerConfig() object.\n    \"\"\"\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"  # default value, can be overridden by configuration\n    cfg.style = \"pep440\"  # default style\n    cfg.versionfile_source = \"_version.py\"  # default version file source\n    cfg.versionfile_build = None  # default build file\n    cfg.tag_prefix = \"\"  # default tag prefix\n    cfg.parentdir_prefix = \"\"  # default parent directory prefix\n    cfg.verbose = False  # default verbose mode\n    return cfg"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8510, "num_retrieved": 23, "inference_time": 1.6032642275094986, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        f.vcs = vcs\n        f.method = method\n        return f\n    \n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8483, "num_retrieved": 19, "inference_time": 1.6032642275094986, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Validate the object root hierarchy\n    try:\n        # Initialize validator\n        validator = Validator(show_warnings=show_warnings, show_errors=True)\n        validator.initialize()\n        \n        # Get all entries in the root\n        for entry in self.obj_fs.scandir(''):\n            if entry.is_dir:\n                # Check if it's a version directory or a known directory\n                if entry.name in ('extensions', 'content'):\n                    continue\n                elif re.match(r'^v\\d+$', entry.name):\n                    # This is a version directory - validate it\n                    validator.log.info(\"Validating version directory: %s\", entry.name)\n                else:\n                    # Unexpected directory\n                    validator.log.error('E001b', dir=entry.name)\n            elif entry.is_file:\n                # Check if it's a known file\n                if entry.name in ('inventory.json', 'inventory.json.sha512'):\n                    # Valid inventory file\n                    pass\n                elif entry.name.startswith('0=ocfl_object_'):\n                    # Valid namaste file\n                    pass\n                else:\n                    # Unexpected file\n                    validator.log.error('E001a', file=entry.name)\n        \n        # Validate objects if requested\n        if validate_objects:\n            # Get all version directories\n            version_dirs = [d.name for d in self.obj_fs.scandir('') if d.is_dir and re.match(r'^v\\d+$', d.name)]\n            \n            # Validate each version directory\n            for vdir in version_dirs:\n                try:\n                    # Validate inventory in version directory\n                    inv_file = fs.path.join(vdir, 'inventory.json')\n                    if self.obj_fs.exists(inv_file):\n                        validator.validate_inventory(inv_file, where=vdir)\n                    else:\n                        validator.log.error('E063', where=vdir)\n                except ValidatorAbortException:\n                    pass\n        \n        # Count valid objects\n        if validator.log.num_errors == 0:\n            good_objects += 1\n            \n        num_objects += 1\n        \n    except Exception as e:\n        validator.log.error('E002', error=str(e))\n    \n    return num_objects, good", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the root directory to find all OCFL objects\n    for entry in self.obj_fs.scandir(''):\n        if entry.is_dir:\n            # Check if this directory is an OCFL object\n            # OCFL objects typically have a namaste file or inventory.json\n            namaste_file = '0=ocfl_object_1.0'  # Default namaste file name\n            inventory_file = 'inventory.json'\n            \n            # Check for presence of inventory.json\n            if not self.obj_fs.exists(inventory_file):\n                continue\n                \n            # Validate the object if it has inventory\n            try:\n                # Create a validator instance for this object\n                validator = Validator(show_warnings=show_warnings, show_errors=True)\n                validator.obj_fs = self.obj_fs\n                validator.validate(entry.name)\n                \n                # Check if validation passed\n                if validator.log.num_errors == 0:\n                    good_objects += 1\n                num_objects += 1\n                    \n            except ValidatorAbortException:\n                # Validation was aborted, skip this object\n                num_objects += 1\n                continue\n                \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Validate the object root hierarchy\n    try:\n        # Initialize validator\n        validator = Validator(show_warnings=show_warnings, show_errors=True)\n        validator.initialize()\n        \n        # Get all entries in the root\n        for entry in self.obj_fs.scandir(''):\n            if entry.is_dir:\n                # Check if this is a version directory or a known object directory\n                if entry.name.startswith('v'):\n                    # Skip version directories for now\n                    continue\n                elif entry.name == 'extensions':\n                    # Validate extensions directory\n                    self.validate_extensions_dir()\n                elif entry.name == 'inventory.json':\n                    # This is part of the object root, validate it\n                    pass\n                elif entry.name == 'inventory.json.sha512':\n                    # This is the sidecar, validate it\n                    pass\n                else:\n                    # Any other directory is invalid\n                    self.log.error('E001b', dir=entry.name)\n            elif entry.is_file:\n                # Check if this is a namaste file\n                if entry.name.startswith('0='):\n                    # This is a namaste file, validate it\n                    pass\n                else:\n                    # Any other file is invalid\n                    self.log.error('E001a', file=entry.name)\n        \n        # Validate objects if requested\n        if validate_objects:\n            # Find all namaste files and validate each object\n            namastes = find_namastes(0, pyfs=self.obj_fs)\n            num_objects = len(namastes)\n            \n            for namaste in namastes:\n                try:\n                    # Validate the object using the namaste file\n                    validator.spec_version = namaste.version\n                    validator.validate(namaste.filename)\n                    good_objects += 1\n                except ValidatorAbortException:\n                    continue\n        \n        # Return the results\n        return num_objects, good_objects\n        \n    except Exception as e:\n        self.log.error('E002', error=str(e))\n        return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the root directory to find all OCFL objects\n    for entry in self.obj_fs.scandir(''):\n        if entry.is_dir:\n            # Check if this directory is an OCFL object\n            # OCFL objects typically have a namaste file or inventory.json\n            inventory_path = fs.path.join(entry.name, 'inventory.json')\n            namaste_path = fs.path.join(entry.name, '0=ocfl_object_1.0')\n            \n            # Check if inventory.json exists\n            if self.obj_fs.exists(inventory_path):\n                # Create a validator instance for this object\n                validator = Validator(show_warnings=show_warnings, show_errors=True)\n                validator.obj_fs = self.obj_fs\n                validator.check_digests = check_digests\n                \n                # Validate the object\n                try:\n                    # Set up the validator with the object directory\n                    validator.validate(entry.name)\n                    \n                    # If no errors, the object is valid\n                    if validator.log.num_errors == 0:\n                        good_objects += 1\n                except ValidatorAbortException:\n                    pass\n                \n                num_objects += 1\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the root directory to find all OCFL objects\n    for entry in self.obj_fs.scandir(''):\n        if entry.is_dir:\n            # Check if this directory is an OCFL object (has a namaste file or inventory.json)\n            namaste_found = False\n            inventory_found = False\n            \n            # Look for namaste files or inventory.json\n            for subentry in self.obj_fs.scandir(entry.name):\n                if subentry.is_file:\n                    filename = subentry.name\n                    if filename == 'inventory.json':\n                        inventory_found = True\n                    elif filename.startswith('0='):\n                        namaste_found = True\n            \n            # If we found either a namaste file or inventory.json, it's an OCFL object\n            if namaste_found or inventory_found:\n                num_objects += 1\n                \n                # Validate the object if requested\n                if validate_objects:\n                    try:\n                        # Create a validator instance for this object\n                        validator = Validator(show_warnings=show_warnings, show_errors=True, check_digests=check_digests)\n                        validator.obj_fs = self.obj_fs\n                        validator.validate(entry.name)\n                        \n                        # If no errors were found, the object is valid\n                        if validator.log.num_errors == 0:\n                            good_objects += 1\n                    except ValidatorAbortException:\n                        # Object validation failed, but we still count it as checked\n                        pass\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8713, "num_retrieved": 29, "inference_time": 1.6032642275094986, "generate_results": ["def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        raise ObjectException(\"Identifier is not set!\")\n    \n    # Set up the root inventory with basic metadata\n    inventory = self.start_inventory()\n    \n    # Create the root directory structure\n    if not self.obj_fs.exists(''):\n        self.obj_fs.makedirs('')\n    \n    # Write the initial inventory and sidecar\n    self.write_inventory_and_sidecar(inventory)\n    \n    # Write the object declaration\n    self.write_object_declaration()\n    \n    # Initialize the root inventory with the first version\n    vdir = 'v1'\n    if not self.obj_fs.exists(vdir):\n        self.obj_fs.makedir(vdir)\n    \n    # Set up the initial version with empty manifest\n    inventory['versions'][vdir] = {\n        'manifest': {}\n    }\n    \n    # Log initialization success\n    self.log.info(\"Initialized OCFL storage root for %s\", self.id)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        raise ObjectException(\"Identifier is not set!\")\n    \n    # Ensure the object filesystem is properly set up\n    if self.obj_fs is None:\n        # Create a new filesystem for the object root\n        self.obj_fs = open_fs(self.path or '.', create=True)\n    \n    # Create the necessary directories and files for an OCFL storage root\n    # This includes the root inventory, object declaration, and any required directories\n    \n    # Create the root inventory directory\n    if not self.obj_fs.exists(''):\n        self.obj_fs.makedirs('')\n    \n    # Create the object declaration file\n    obj_declaration = self.object_declaration_object()\n    obj_declaration_file = fs.path.join('', '0=ocfl_object_' + self.spec_version)\n    with self.obj_fs.open(obj_declaration_file, 'w') as fh:\n        fh.write(str(obj_declaration))\n    \n    # Create the root inventory file\n    inventory = self.start_inventory()\n    inventory_file = fs.path.join('', 'inventory.json')\n    with self.obj_fs.open(inventory_file, 'w') as fh:\n        json.dump(inventory, fh, sort_keys=True, indent=2)\n    \n    # Create the inventory sidecar file\n    digest = file_digest(inventory_file, self.digest_algorithm, pyfs=self.obj_fs)\n    sidecar_file = fs.path.join('', 'inventory.json.' + self.digest_algorithm)\n    with self.obj_fs.open(sidecar_file, 'w') as fh:\n        fh.write(digest + ' ' + inventory_file + '\\n')\n    \n    # Initialize the extensions directory if needed\n    if not self.obj_fs.exists('extensions'):\n        self.obj_fs.makedirs('extensions')\n    \n    # Set up the registered extensions\n    for ext in self.registered_extensions:\n        ext_dir = fs.path.join('extensions', ext)\n        if not self.obj_fs.exists(ext_dir):\n            self.obj_fs.makedirs(ext_dir)\n    \n    # Log initialization completion\n    self.log.info(\"Initialized OCFL storage root for object %s\", self.id)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        raise ObjectException(\"Identifier is not set!\")\n    \n    # Ensure the object filesystem is properly set up\n    if self.obj_fs is None:\n        # Create a new filesystem for the object root\n        self.obj_fs = open_fs(self.path or '.', create=True)\n    \n    # Create the necessary directories and files for the OCFL storage root\n    # This includes creating the root inventory and object declaration\n    self.write_inventory_and_sidecar(None, write_inventory=False)\n    \n    # Create the object declaration file\n    self.write_object_declaration()\n    \n    # Initialize the root inventory with default values\n    inventory = self.start_inventory()\n    self.write_inventory_and_sidecar(inventory)\n    \n    # Set up the initial version directory (v1)\n    vdir = 'v1'\n    if not self.obj_fs.exists(vdir):\n        self.obj_fs.makedir(vdir)\n    \n    # Initialize the root inventory with default values\n    self.log.info(\"Initialized OCFL storage root for %s\", self.id)", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object's root filesystem if not already done\n    if self.obj_fs is None and self.path is None:\n        # Create a new filesystem for the root object\n        self.obj_fs = open_fs(self.path, create=True) if self.path else open_fs('.', create=True)\n    \n    # Ensure the root directory exists\n    if self.obj_fs is not None:\n        if not self.obj_fs.exists(''):\n            self.obj_fs.makedirs('')\n    \n    # Initialize the root inventory and sidecar\n    inventory = self.start_inventory()\n    self.write_inventory_and_sidecar(inventory)\n    \n    # Write the object declaration\n    self.write_object_declaration()\n    \n    # Initialize the root directory with necessary files\n    self.obj_fs.makedirs('extensions')\n    \n    # Set up default version directory\n    self._initialize_version_directories()\n    \n    # Log initialization completion\n    self.log.info(\"OCFL storage root initialized successfully\")", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        raise ObjectException(\"Identifier is not set!\")\n    \n    # Create the root inventory with basic metadata\n    inventory = self.start_inventory()\n    \n    # Set up the root directory structure\n    self.obj_fs.makedir('v1')\n    \n    # Write the initial inventory and sidecar for version v1\n    self.write_inventory_and_sidecar(inventory, vdir='v1', write_inventory=True)\n    \n    # Write the object declaration\n    self.write_object_declaration()\n    \n    # Initialize the root inventory with the initial version\n    self.log.info(\"Initialized OCFL storage root for %s\", self.id)"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8518, "num_retrieved": 21, "inference_time": 1.6032642275094986, "generate_results": ["def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Convert version to integer for arithmetic\n    try:\n        v = int(version)\n    except ValueError:\n        raise ValueError(f\"Invalid version number: {version}\")\n    \n    # Increment the version number\n    next_v = v + 1\n    \n    # Convert back to string\n    return str(next_v)", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\"\"\"\n    # Convert version to integer and increment\n    version_num = int(version)\n    next_num = version_num + 1\n    \n    # Convert back to string\n    return str(next_num)", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Convert version to integer for arithmetic\n    try:\n        v = int(version)\n    except ValueError:\n        raise ValueError(f\"Invalid version format: {version}\")\n    \n    # Increment the version number\n    next_v = v + 1\n    \n    # Convert back to string\n    return str(next_v)", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\"\"\"\n    # Convert version to integer for arithmetic\n    try:\n        v = int(version)\n    except ValueError:\n        raise ValueError(f\"Invalid version number: {version}\")\n    \n    # Increment the version number\n    next_v = v + 1\n    \n    # Convert back to string\n    return str(next_v)", "def next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\"\"\"\n    # Convert version to integer and increment\n    version_num = int(version) + 1\n    # Convert back to string\n    return str(version_num)"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8298, "num_retrieved": 11, "inference_time": 1.6032642275094986, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    version_digest_differences = {}\n    \n    for vdir in version_dirs:\n        # Validate that each version has an inventory\n        inventory = self.get_inventory(vdir)\n        if not inventory:\n            self.error(\"E045\", version=vdir)\n            return\n            \n        # If this is the first version (v1), set it as root\n        if root_inventory is None:\n            root_inventory = inventory\n            continue\n            \n        # Compare manifest digests between current and root inventory\n        current_manifest = inventory.get('manifest', {})\n        root_manifest = root_inventory.get('manifest', {})\n        \n        # Find digests that exist in current but not in root\n        for digest, paths in current_manifest.items():\n            if digest not in root_manifest:\n                # This digest is new in this version\n                version_digest_differences[vdir] = version_digest_differences.get(vdir, []) + [digest]\n                \n        # Find digests that exist in root but not in current (should not happen in valid sequence)\n        for digest in root_manifest:\n            if digest not in current_manifest:\n                self.error(\"E046\", version=vdir, digest=digest)\n                return\n    \n    return version_digest_differences", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = self.inventory\n    if not root_inventory:\n        self.error(\"E036a\")\n        return\n    \n    # Track digests that differ from root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if version directory exists in inventory\n        if vdir not in root_inventory['versions']:\n            self.error(\"E065\", version=vdir)\n            return\n            \n        # Get the version inventory data\n        version_data = root_inventory['versions'][vdir]\n        \n        # Check if version has a valid state block\n        if 'state' not in version_data:\n            self.error(\"E064\", version=vdir)\n            return\n            \n        # Check if version has a manifest block\n        if 'manifest' not in version_data:\n            self.error(\"E063\", version=vdir)\n            return\n            \n        # Compare digests in current version with root inventory\n        # Get root manifest digests\n        root_manifest = root_inventory.get('manifest', {})\n        current_manifest = version_data.get('manifest', {})\n        \n        # Add any digests in current version not in root\n        for digest, paths in current_manifest.items():\n            if digest not in root_manifest:\n                different_digests.add(digest)\n    \n    # Return the set of digests that differ from root\n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    version_digest_differences = {}\n    \n    for vdir in version_dirs:\n        # Validate that each version has an inventory\n        inventory = self.get_inventory(vdir)\n        if not inventory:\n            self.error(\"E050\", version=vdir)\n            return\n            \n        # If this is the first version, set it as root\n        if root_inventory is None:\n            root_inventory = inventory\n            continue\n            \n        # Compare the manifest digests with the root inventory\n        manifest = inventory.get('manifest', {})\n        root_manifest = root_inventory.get('manifest', {})\n        \n        # Track digests that are in this version but not in root\n        new_digests = set()\n        for digest, paths in manifest.items():\n            if digest not in root_manifest:\n                new_digests.add(digest)\n        \n        # Track digests that are in root but not in this version\n        missing_digests = set()\n        for digest, paths in root_manifest.items():\n            if digest not in manifest:\n                missing_digests.add(digest)\n        \n        # Record differences\n        if new_digests:\n            version_digest_differences[vdir] = new_digests\n        \n        # Validate that the version inventory is consistent with the previous ones\n        # (this is a simplified check - actual validation would depend on more context)\n        if missing_digests:\n            self.warning(\"W051\", version=vdir, missing_digests=','.join(missing_digests))\n    \n    return version_digest_differences", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    version_digest_differences = {}\n    \n    for vdir in version_dirs:\n        # Validate that each version has an inventory\n        inventory = self.inventory['versions'][vdir]\n        if not inventory:\n            self.error(\"E055\", version=vdir)\n            continue\n            \n        # Check if this is the root version\n        if vdir == 'v1':\n            root_inventory = inventory\n            continue\n            \n        # Compare content digests with root inventory\n        current_manifest = inventory.get('manifest', {})\n        root_manifest = root_inventory.get('manifest', {})\n        \n        # Track digests that are in current version but not in root\n        new_digests = []\n        for digest, paths in current_manifest.items():\n            if digest not in root_manifest:\n                new_digests.append(digest)\n        \n        # Track digests that are in root but not in current version\n        missing_digests = []\n        for digest, paths in root_manifest.items():\n            if digest not in current_manifest:\n                missing_digests.append(digest)\n        \n        # Record differences\n        if new_digests:\n            version_digest_differences[vdir] = {\n                'added': new_digests,\n                'removed': []\n            }\n        if missing_digests:\n            version_digest_differences[vdir] = {\n                'added': [],\n                'removed': missing_digests\n            }\n        \n        # Validate that the version inventory has a valid structure\n        if 'state' not in inventory:\n            self.error(\"E056\", version=vdir)\n        elif not isinstance(inventory['state'], dict):\n            self.error(\"E057\", version=vdir)\n            \n        # Validate that the version has a valid digest algorithm\n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E058\", version=vdir)\n            \n    return version_digest_differences", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n    \"\"\"\n    root_inventory = None\n    version_digest_differences = {}\n    \n    for vdir in version_dirs:\n        # Validate that each version has an inventory\n        inventory = self.get_inventory(vdir)\n        if not inventory:\n            self.error(\"E045\", version=vdir)\n            return\n            \n        # If this is the first version, set it as root\n        if root_inventory is None:\n            root_inventory = inventory\n            continue\n            \n        # Compare manifest digests with root inventory\n        manifest = inventory.get('manifest', {})\n        root_manifest = root_inventory.get('manifest', {})\n        \n        # Track digests that are in this version but not in root\n        new_digests = []\n        for digest, paths in manifest.items():\n            if digest not in root_manifest:\n                new_digests.append(digest)\n        \n        # Track digests that are in root but not in this version\n        missing_digests = []\n        for digest, paths in root_manifest.items():\n            if digest not in manifest:\n                missing_digests.append(digest)\n        \n        # Record differences\n        if new_digests:\n            version_digest_differences[vdir] = new_digests\n        \n        # Validate that the version inventory is consistent with the previous ones\n        # (this is a simplified check - actual validation would depend on more context)\n        \n    return version_digest_differences"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8458, "num_retrieved": 18, "inference_time": 1.6032642275094986, "generate_results": ["def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the given path\n        from ocfl import open_fs\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes('0', pyfs=fs)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* file found, likely a file\n        \n        # Check the content of the first 0=* file\n        first_namaste = namastes[0]\n        filename = first_namaste.filename\n        \n        # Match against 0=ocfl_object_ patterns\n        match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', filename)\n        \n        if match:\n            version = match.group(1)\n            if version == '1.0':\n                return 'root'  # Root storage root\n            else:\n                return 'object'  # OCFL object with version\n        else:\n            # Doesn't match 0=ocfl_object_ pattern\n            return 'file'  # Likely a file (e.g., inventory)\n            \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the given path\n        from ocfl import open_fs\n        fs = open_fs(path)\n    except Exception as e:\n        return f\"error: {str(e)}\"\n    \n    # Look for 0=* Namaste files\n    namastes = find_namastes(0, pyfs=fs)\n    \n    if len(namastes) == 0:\n        return \"error: no 0=* Namaste file found\"\n    \n    # Check if there's exactly one 0=* file\n    if len(namastes) > 1:\n        return \"error: multiple 0=* Namaste files found\"\n    \n    # Get the first (and only) 0=* file\n    namaste = namastes[0]\n    filename = namaste.filename\n    \n    # Check if it's a 0=ocfl_object_* file\n    if filename.startswith('0=ocfl_object_'):\n        # Check if it's in the root directory\n        if path == '/':\n            return 'root'\n        else:\n            return 'object'\n    \n    # Check if it's a file (not a directory)\n    if not filename.startswith('0='):\n        return \"error: file does not start with 0=\"\n    \n    # If we get here, it's a file but not a valid 0=ocfl_object_ file\n    return 'file'", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Attempt to open the filesystem at the given path\n        from ocfl import open_fs\n        fs = open_fs(path)\n    except Exception as e:\n        return f\"error: {str(e)}\"\n    \n    # Look for any 0=* Namaste files\n    namastes = find_namastes(0, pyfs=fs)\n    \n    if len(namastes) == 0:\n        return \"error: no 0=* Namaste file found\"\n    \n    # Check if there's exactly one 0=ocfl_object_1.0 file (storage root)\n    zero_eqs = [namaste.filename for namaste in namastes if namaste.filename.startswith('0=')]\n    if len(zero_eqs) == 0:\n        return \"error: no 0=* file found\"\n    \n    # Check if the 0=* file is exactly 0=ocfl_object_1.0 (storage root)\n    for namaste in namastes:\n        if namaste.filename == '0=ocfl_object_1.0':\n            return 'root'\n    \n    # Check if the 0=* file is of the form 0=ocfl_object_X.Y (OCFL object)\n    for namaste in namastes:\n        if namaste.filename.startswith('0=ocfl_object_'):\n            match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', namaste.filename)\n            if match:\n                version = match.group(1)\n                if version == '1.0':\n                    return 'root'\n                else:\n                    return 'object'\n    \n    # If we get here, we have a file that doesn't match expected patterns\n    return 'file'", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        from ocfl import open_fs\n        fs = open_fs(path)\n    except Exception as e:\n        return f\"error: {str(e)}\"\n    \n    # Look for 0=* Namaste files\n    try:\n        namastes = find_namastes(0, pyfs=fs)\n    except Exception as e:\n        return f\"error: {str(e)}\"\n    \n    if len(namastes) == 0:\n        return \"file\"  # No 0=* file found, likely a file\n    \n    # Check if there's a 0=ocfl_object_1.0 or 0=ocfl_object_x.y file\n    zero_eqs = [namaste.filename for namaste in namastes if namaste.filename.startswith('0=')]\n    \n    if len(zero_eqs) == 0:\n        return \"file\"\n    \n    # Check if any 0= file matches the object pattern\n    for namaste in namastes:\n        if namaste.filename.startswith('0=ocfl_object_'):\n            # This is an object declaration\n            match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', namaste.filename)\n            if match:\n                version = match.group(1)\n                if version == '1.0':\n                    return 'root'  # Root storage root\n                else:\n                    return 'object'  # OCFL object directory\n        elif namaste.filename == '0=ocfl_object_1.0':\n            return 'root'  # Root storage root\n    \n    # If we get here, it's likely a file with a 0= file that doesn't match the expected pattern\n    return 'file'", "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        from ocfl import open_fs\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes('0', pyfs=fs)\n        \n        if len(namastes) == 0:\n            return \"no_0_equals\"\n        \n        # Check if there's a 0=ocfl_object_1.0 or 0=ocfl_object_x.y file\n        zero_eqs = [namaste.filename for namaste in namastes if namaste.filename.startswith('0=')]\n        \n        if len(zero_eqs) == 0:\n            return \"no_0_equals\"\n        \n        # Check if it's a storage root (has a 0= declaration)\n        if len(namastes) == 1 and namastes[0].filename == '0=ocfl_object_1.0':\n            return 'root'\n        \n        # Check if it's an object (has a 0=ocfl_object_x.y file)\n        for zero_eq in zero_eqs:\n            match = re.match(r'0=ocfl_object_(\\d+\\.\\d+)', zero_eq)\n            if match:\n                version = match.group(1)\n                if version == '1.0':\n                    return 'object'\n                else:\n                    return 'object'\n        \n        # If we have multiple 0= files, it's likely an invalid state\n        if len(zero_eqs) > 1:\n            return \"multiple_0_equals\"\n        \n        # If we have a 0= file but it's not a valid ocfl_object format\n        for zero_eq in zero_eqs:\n            if not zero_eq.startswith('0=ocfl_object_'):\n                return \"invalid_0_equals\"\n        \n        return \"unknown\"\n        \n    except Exception as e:\n        return f\"error: {str(e)}\""]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 13751, "num_retrieved": 176, "inference_time": 1.6032642275094986, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if they are provided\n    if bug_ids:\n        params['bug_id'] = bug_ids\n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if they are provided\n    if bug_ids:\n        params['bug_id'] = bug_ids\n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if they are provided\n    if bug_ids:\n        params['bug_id'] = bug_ids\n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if they are provided\n    if bug_ids:\n        params['bug_id'] = bug_ids\n    return params", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # Add bug_ids to the params if they are provided\n    if bug_ids:\n        params['bug_id'] = bug_ids\n    return params"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8456, "num_retrieved": 16, "inference_time": 1.6032642275094986, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9014, "num_retrieved": 36, "inference_time": 1.6032642275094986, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repo', help='Borg repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--remote-path', help='Remote path for the repository')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--show-rc', action='store_true', help='Show return code')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--keep-weekly', type=int, help='Number of weekly archives to keep')\n    parser.add_argument('--keep-monthly', type=int, help='Number of monthly archives to keep')\n    parser.add_argument('--keep-daily', type=int, help='Number of daily archives to keep')\n    parser.add_argument('--keep-yearly', type=int, help='Number of yearly archives to keep')\n    parser.add_argument('--keep-snapshots', type=int, help='Number of snapshots to keep')\n    parser.add_argument('--keep-archives', type=int, help='Number of archives to keep')\n    parser.add_argument('--keep-keep', type=int, help='Number of keep archives to keep')\n    parser.add_argument('--keep-keep-weekly', type=int, help='Number of keep weekly archives to keep')\n    parser.add_argument('--keep-keep-monthly', type=int, help='Number of keep monthly archives to keep')\n    parser.add_argument('--", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repo', help='Borg repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--remote-path', help='Remote path for the repository')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--show-rc', action='store_true', help='Show return code')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--keep-weekly', type=int, help='Number of weekly archives to keep')\n    parser.add_argument('--keep-monthly', type=int, help='Number of monthly archives to keep')\n    parser.add_argument('--keep-daily', type=int, help='Number of daily archives to keep')\n    parser.add_argument('--keep-yearly', type=int, help='Number of yearly archives to keep')\n    parser.add_argument('--keep-keep', type=int, help='Number of keep archives to keep')\n    parser.add_argument('--keep-keep', type=int, help='Number of keep archives to keep')\n    parser.add_argument('--keep-keep', type=int, help='Number of keep archives to keep')\n    parser.add_argument('--keep-keep', type=int, help='Number of keep archives to keep')\n    parser.add_argument('--keep-keep', type=int, help='Number of keep archives to keep')\n    parser.add_argument('--keep-keep', type", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repo', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--remote-path', help='Remote path')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--show-rc', action='store_true', help='Show return code')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--keep-weekly', type=int, help='Number of weekly archives to keep')\n    parser.add_argument('--keep-monthly', type=int, help='Number of monthly archives to keep')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present pattern')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--source-directories', help='Source directories')\n    parser.add_argument('--exclude-patterns', help='Exclude patterns')\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--consistency', help='Consistency checks')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repo', help='Borg repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--remote-path', help='Remote path for repository')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--show-rc', action='store_true', help='Show return code')\n    parser.add_argument('--keep-weekly', type=int, help='Number of weekly archives to keep')\n    parser.add_argument('--keep-monthly', type=int, help='Number of monthly archives to keep')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present pattern')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--paths', help='Paths to list')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument('--repo', help='Borg repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--remote-path', help='Remote path for repository')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--show-rc', action='store_true', help='Show return code')\n    parser.add_argument('--patterns-from', help='File containing patterns')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--prefix', help='Prefix for glob patterns')\n    parser.add_argument('--keep-weekly', type=int, help='Number of weekly archives to keep')\n    parser.add_argument('--keep-monthly', type=int, help='Number of monthly archives to keep')\n    parser.add_argument('--keep-daily', type=int, help='Number of daily archives to keep')\n    parser.add_argument('--keep-yearly', type=int, help='Number of yearly archives to keep')\n    parser.add_argument('--keep-snapshots', type=int, help='Number of snapshots to keep')\n    parser.add_argument('--keep-archives', type=int, help='Number of archives to keep')\n    parser.add_argument('--keep-keep', type=int, help='Number of keep archives to keep')\n    parser.add_argument('--keep-keep-keep', type=int, help='Number of keep keep archives to keep')\n    parser.add_argument('--keep-keep-keep-keep', type=int, help='Number of keep keep keep archives to keep')\n    parser.add_argument('--keep-keep-keep-keep-keep', type=int, help='"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9164, "num_retrieved": 42, "inference_time": 1.6032642275094986, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all action names from the parser\n    actions = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in actions:\n        if action.option_strings:\n            # Add each option string (e.g., --option)\n            flags.extend(action.option_strings)\n    \n    # Join all flags with spaces and return\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all action names from the parser\n    actions = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in actions:\n        if action.option_strings:\n            # Add each option string (e.g., --foo, -f)\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    \n    # Join all flags with spaces\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all action names from the parser\n    actions = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in actions:\n        if isinstance(action, argparse._StoreAction):\n            # For store actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreTrueAction):\n            # For boolean flags, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._AppendAction):\n            # For append actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    \n    # Return the flags as a space-separated string\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all action names from the parser\n    actions = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in actions:\n        if isinstance(action, argparse._StoreAction):\n            # For store actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreTrueAction):\n            # For store true actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    \n    # Remove duplicates by converting to set and back to list\n    unique_flags = list(set(flags))\n    \n    # Join with spaces\n    return ' '.join(unique_flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    import argparse\n    \n    # Get all action names from the parser\n    actions = parser._actions\n    \n    # Build a list of flags\n    flags = []\n    \n    for action in actions:\n        if isinstance(action, argparse._StoreAction):\n            # For store actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreTrueAction):\n            # For store true actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n        elif isinstance(action, argparse._StoreConstAction):\n            # For store const actions, add the flag\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    \n    # Remove duplicates and sort\n    unique_flags = sorted(set(flags))\n    \n    # Join with spaces\n    return ' '.join(unique_flags)"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8921, "num_retrieved": 33, "inference_time": 1.6032642275094986, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name')\n    \n    # Add common arguments\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--info', action='store_true', help='Show info output')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add common options\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    parser.add_argument('--remote-path', help='Remote path')\n    \n    # Add subparsers for different commands\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-if-present', help='Exclude if present pattern')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump files')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    list_parser.add_argument('--prefix', help='Prefix for archive names')\n    \n    info_parser = subparsers.add_parser('info', help='Show archive information')\n    info_parser.add_argument('--prefix', help='Prefix for archive names')\n    info_parser.add_argument('--archive', help='Specific archive to show info for')\n    \n    prune_parser = subparsers.add_parser('prune', help='Prune old archives')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Number of weekly archives to keep')\n    prune_parser.add_argument('--keep-monthly', type=int, help='Number of monthly archives to keep')\n    \n    # Add global arguments\n    parser.add_argument('--version", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    global_parser.add_argument('--info', action='store_true', help='Show info output')\n    global_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add subparser for create\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--exclude', help='Exclude patterns')\n    create_parser.add_argument('--exclude-if-present', help='Exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    \n    # Add subparser for list\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Archive name')\n    list_parser.add_argument('--prefix', help='Prefix for archives')\n    \n    # Add subparser for info\n    info_parser = subparsers.add_parser('info', help='Show archive information')\n    info_parser.add_argument('--archive', help='Archive name')\n    info_parser.add_argument('--prefix', help='Prefix for archives')\n    \n    # Add subparser for prune\n    prune_parser = subparsers.add_parser('prune', help='Prune archives')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Keep weekly archives')\n    prune_parser.add_argument('--keep-monthly', type=int, help='Keep monthly archives')\n    \n    # Add subparser for transfer\n    transfer_parser = subparsers.add_parser('transfer', help='Transfer archives')\n    transfer_parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    transfer_parser.add_argument('--archive', help='Archive name')\n    transfer_parser.add_argument('--dry-run', action='store_true', help='", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--debug', action='store_true')\n    global_parser.add_argument('--info', action='store_true')\n    global_parser.add_argument('--json', action='store_true')\n    \n    # Add subparsers for different commands\n    create_parser = subparsers.add_parser('create')\n    create_parser.add_argument('--archive', help='archive name')\n    create_parser.add_argument('--exclude-from', help='exclude patterns from file')\n    create_parser.add_argument('--exclude-caches', action='store_true')\n    create_parser.add_argument('--exclude-if-present', help='exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true')\n    create_parser.add_argument('--exclude-nodump', action='store_true')\n    \n    list_parser = subparsers.add_parser('list')\n    list_parser.add_argument('--archive', help='archive name')\n    list_parser.add_argument('--json', action='store_true')\n    \n    info_parser = subparsers.add_parser('info')\n    info_parser.add_argument('--archive', help='archive name')\n    info_parser.add_argument('--prefix', help='prefix for archives')\n    info_parser.add_argument('--json', action='store_true')\n    \n    prune_parser = subparsers.add_parser('prune')\n    prune_parser.add_argument('--keep-weekly', type=int, help='keep weekly archives')\n    prune_parser.add_argument('--keep-monthly', type=int, help='keep monthly archives')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create result dictionary\n    result = {}\n    \n    # Global arguments\n    result['global'] = args\n    \n    # Subparser arguments\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add common options\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--lock-wait', type=int, help='Lock wait time')\n    parser.add_argument('--remote-path', help='Remote path')\n    \n    # Add global arguments\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create a dictionary to store the parsed arguments by subparser name\n    result = {}\n    \n    # Store global arguments\n    result['global'] = args\n    \n    # If there's a subparser name, store the arguments for that subparser\n    if hasattr(args, 'subparser_name'):\n        result[args.subparser_name] = args\n    \n    return result", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    parser.add_argument('--debug', action='store_true', help='Enable debug mode')\n    parser.add_argument('--info', action='store_true', help='Show info')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    parser.add_argument('--remote-path', help='Remote path')\n    \n    # Add common options\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name or pattern')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--prefix', help='Prefix for archive names')\n    \n    # Add other common options\n    parser.add_argument('--keep-weekly', type=int, help='Number of weekly archives to keep')\n    parser.add_argument('--keep-monthly', type=int, help='Number of monthly archives to keep')\n    parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    parser.add_argument('--exclude-if-present', help='Exclude if present')\n    parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    \n    # Add global arguments\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    \n    # Parse the arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Create a dictionary to store the parsed arguments by subparser name\n    result = {}\n    \n    # Store global arguments\n    result['global'] = args\n    \n    # If there's a subparser name, store it in the result\n    if hasattr(args, 'subparser_name'):\n        result[args"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8601, "num_retrieved": 21, "inference_time": 1.588523030281067, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    \n    # Initialize result dictionary\n    parsed_arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order they appear in the subparsers dict\n    for subparser_name, parser in subparsers.items():\n        # Create a namespace to hold parsed arguments for this subparser\n        parsed_args = parser.parse_args(remaining_arguments, namespace=argparse.Namespace())\n        \n        # If arguments were parsed, store them in the result dictionary\n        if parsed_args:\n            parsed_arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining list\n            remaining_arguments = [arg for arg in remaining_arguments if arg not in parser._get_args(parsed_args)]\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Create a namespace to hold parsed arguments\n        parsed_args = parser.parse_args(remaining, namespace=argparse.Namespace())\n        \n        # If arguments were parsed, add them to the result\n        if parsed_args:\n            arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining list\n            # We need to find the indices of the parsed arguments and remove them\n            # This is a simplified approach assuming the arguments are in order\n            # and that the parser will consume them in order\n            remaining = [arg for arg in remaining if not any(\n                arg.startswith('--' + k) or arg.startswith('-' + k) \n                for k in dir(parsed_args) if not k.startswith('_')\n            )]\n    \n    # If no arguments were parsed by any subparser, return empty dict and original list\n    if not arguments:\n        return ({}, unparsed_arguments)\n    \n    return (arguments, remaining)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining = list(unparsed_arguments)\n    \n    # Try to parse arguments with each subparser in order\n    for subparser_name, parser in subparsers.items():\n        # Parse the remaining arguments with this subparser\n        parsed_args = parser.parse_args(remaining, namespace=argparse.Namespace())\n        if parsed_args:\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining = []\n            for arg in unparsed_arguments:\n                if arg in remaining:\n                    remaining.remove(arg)\n            # Rebuild remaining list with arguments not consumed\n            remaining = [arg for arg in unparsed_arguments if arg not in [str(getattr(parsed_args, name)) for name in dir(parsed_args) if not name.startswith('_')]]\n    \n    # If no arguments were parsed by any subparser, return empty dict and original list\n    if not arguments:\n        return ({}, unparsed_arguments)\n    \n    return (arguments, remaining)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    \n    # Initialize result dictionary\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dict\n    for subparser_name, parser in subparsers.items():\n        # Try to parse the remaining arguments with this subparser\n        try:\n            # Create a namespace from parsing the arguments\n            parsed_args = parser.parse_args(remaining_arguments)\n            parsed_arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining list\n            remaining_arguments = []\n            # Check if there are any arguments left to parse\n            if remaining_arguments:\n                # Re-assign the remaining arguments to be parsed by the next subparser\n                # This is a simplified approach; in practice, you'd need to handle the parsing more carefully\n                # For now, we'll just assume that the first subparser that matches gets the arguments\n                break\n        except SystemExit:\n            # If the parser raises SystemExit, it means no arguments matched\n            # Continue to the next subparser\n            continue\n    \n    # If no subparser matched any arguments, return empty parsed arguments and the original arguments\n    if not parsed_arguments:\n        parsed_arguments = {}\n    \n    return (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    \n    # Initialize result dictionary\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they are defined\n    for subparser_name, subparser in subparsers.items():\n        # Create a parser instance for the current subparser\n        parser = subparser\n        \n        # Parse the remaining arguments using the current subparser\n        try:\n            # Parse arguments with the current subparser\n            parsed_args = parser.parse_args(remaining_arguments, namespace=argparse.Namespace())\n            \n            # Store the parsed arguments in the result dictionary\n            parsed_arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining list\n            # Find the indices of the parsed arguments and remove them\n            remaining_arguments = [arg for arg in remaining_arguments if not any(\n                arg.startswith('--' + key) or arg.startswith('-' + key) for key in parsed_args.__dict__.keys()\n            )]\n            \n            # This is a simplified approach - in reality, we need to parse the arguments properly\n            # The above logic is not correct for handling argument parsing\n            \n            # Instead, we need to properly parse the arguments using the subparser\n            # We'll use a different approach\n            \n            # Reset remaining arguments and parse properly\n            break\n        except SystemExit:\n            # If the parser raises SystemExit, it means no arguments were parsed\n            pass\n    \n    # Correct implementation: parse arguments in order of subparsers\n    parsed_arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a namespace to hold the parsed arguments\n        parsed_args = subparser.parse_args(remaining_arguments)\n        \n        # Store the parsed arguments\n        parsed_arguments[subparser_name] = parsed_args\n        \n        # Remove the arguments that were parsed\n        # This is tricky because argparse doesn't provide direct access to which arguments were parsed"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8984, "num_retrieved": 35, "inference_time": 1.588523030281067, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands\n    \n    top_level_parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool')\n    \n    # Add common global flags\n    top_level_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    # Add subparsers for different commands\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available commands')\n    \n    # Add commands from borgmatic.commands module\n    for command_name in dir(borgmatic.commands):\n        if command_name.startswith('_'):\n            continue\n        command_module = getattr(borgmatic.commands, command_name)\n        if hasattr(command_module, 'make_parser'):\n            subparser = subparsers.add_parser(command_name, help=command_module.__doc__)\n            # Add common arguments to all commands\n            subparser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n            subparser.add_argument('--debug', action='store_true', help='Enable debug output')\n            subparser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    \n    return (top_level_parser, subparsers)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands\n    \n    top_level_parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool')\n    \n    # Add common global flags\n    top_level_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    top_level_parser.add_argument('--debug', action='store_true', help='Enable debug output')\n    top_level_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    \n    # Create subparsers for different commands\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available commands')\n    \n    # Add subparser for create command\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--exclude', help='Exclude patterns')\n    create_parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    create_parser.add_argument('--exclude-if-present', help='Exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    create_parser.add_argument('--patterns-from', help='File containing patterns')\n    \n    # Add subparser for list command\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    list_parser.add_argument('--info', action='store_true', help='Show info')\n    list_parser.add_argument('--debug', action='store_true', help='Show debug info')\n    list_parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    \n    # Add subparser for check command\n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    check_parser.add_argument('--repository', action='store_true', help='Check repository')\n    check_parser.add_argument('--archives', action='store_true', help='Check archives')\n    check_parser.add_argument('--data', action='store_true', help='Check data')\n    check_parser.add_argument('--last', type=int,", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.create as create\n    import borgmatic.commands.dump as dump\n    import borgmatic.commands.list as list\n    import borgmatic.commands.rlist as rlist\n    import borgmatic.commands.info as info\n    import borgmatic.commands.check as check\n    import borgmatic.commands.transfer as transfer\n    import borgmatic.commands.compact as compact\n    import borgmatic.commands.prune as prune\n    import borgmatic.commands.borgmatic as borgmatic\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - a tool for managing Borg repositories')\n    \n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create subparser for create\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--source', help='Source directory')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--exclude', help='Exclude patterns')\n    create_parser.add_argument('--exclude-from', help='Exclude patterns from file')\n    create_parser.add_argument('--exclude-if-present', help='Exclude if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude nodump')\n    create_parser.add_argument('--patterns-from', help='Patterns from file')\n    create_parser.add_argument('--compression', help='Compression level')\n    create_parser.add_argument('--compression-level', type=int, help='Compression level')\n    create_parser.add_argument('--compression-algorithm', help='Compression algorithm')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude caches')\n    create_parser.add_argument('--no-diff', action='store_true', help='Do not use diff')\n    create_parser.add_argument('--no-verify', action='store_true', help='Do not verify')\n    create_parser.add_argument('--no-verify-archives', action='store_true', help='Do not verify archives')\n    create_parser.add_argument('--no-verify-archives', action='store_true', help='Do not verify archives')\n    \n    # Create subparser for dump\n    dump_parser = subparsers.add_parser('dump', help='Dump", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands\n    \n    top_level_parser = argparse.ArgumentParser(description='Borgmatic - a Borg repository management tool')\n    \n    # Add common global arguments\n    top_level_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    \n    # Add subparsers for different commands\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available commands')\n    \n    # Add parser for create command\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--exclude', help='Exclude patterns')\n    create_parser.add_argument('--exclude-from', help='File containing exclude patterns')\n    create_parser.add_argument('--exclude-if-present', help='Exclude if present')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files that are not to be dumped')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--patterns-from', help='File containing patterns')\n    create_parser.add_argument('--source', help='Source directory')\n    \n    # Add parser for list command\n    list_parser = subparsers.add_parser('list', help='List archives or paths')\n    list_parser.add_argument('--archive', help='Archive name')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    \n    # Add parser for check command\n    check_parser = subparsers.add_parser('check', help='Check repository consistency')\n    check_parser.add_argument('--checks', nargs='+', choices=['repository', 'archives', 'data'], help='Checks to perform')\n    check_parser.add_argument('--last', type=int, help='Check the last time')\n    check_parser.add_argument('--prefix', help='Prefix for glob archives')\n    \n    # Add parser for info command\n    info_parser = subparsers.add_parser('info', help='Display archive information')\n    info_parser.add_argument('--archive', help='Archive name')\n    info_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    info_parser.add_argument('--prefix', help='", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands\n    \n    top_level_parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool')\n    \n    # Add common global arguments\n    top_level_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    # Add subparsers for different commands\n    subparsers = top_level_parser.add_subparsers(dest='action', help='Available commands')\n    \n    # Add commands from borgmatic.commands module\n    for command_name in dir(borgmatic.commands):\n        if command_name.startswith('_'):\n            continue\n        command_class = getattr(borgmatic.commands, command_name)\n        if hasattr(command_class, 'make_parser'):\n            parser = command_class.make_parser(top_level_parser)\n            subparsers.add_parser(command_name, help=parser.description, parents=[parser])\n    \n    return top_level_parser, subparsers"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8499, "num_retrieved": 17, "inference_time": 1.588523030281067, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8820, "num_retrieved": 29, "inference_time": 1.588523030281067, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('action', help='Action to perform')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--log-json', action='store_true', help='Log output in JSON format')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--show', action='store_true', help='Show configuration')\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('action', help='Action to perform')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--log-json', action='store_true', help='Log output in JSON format')\n    parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    parser.add_argument('--show', action='store_true', help='Show configuration')\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    # Add common arguments\n    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], default='INFO')\n    parser.add_argument('--log-json', action='store_true')\n    parser.add_argument('--dry-run', action='store_true')\n    \n    # Add action-specific arguments\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (actual implementation would depend on specific actions)\n    config_parser = subparsers.add_parser('config', help='Configure borgmatic')\n    config_parser.add_argument('--config-file', help='Path to configuration file')\n    \n    list_parser = subparsers.add_parser('list', help='List archives in a repository')\n    list_parser.add_argument('--prefix', help='Filter archives by prefix')\n    list_parser.add_argument('--match-archives', help='Match archives using glob pattern')\n    \n    prune_parser = subparsers.add_parser('prune', help='Prune archives')\n    prune_parser.add_argument('--keep', type=int, help='Keep the last N archives')\n    \n    # Add more subparsers as needed\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument('action', help='Action to perform')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--log-json', action='store_true', help='Log output in JSON format')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--show', action='store_true', help='Show configuration')\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('action', help='Action to perform')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--log-json', action='store_true', help='Log output in JSON format')\n    parser.add_argument('--lock-wait', help='Lock wait time')\n    parser.add_argument('--show', action='store_true', help='Show configuration')\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    \n    return parser.parse_args(arguments)"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9230, "num_retrieved": 43, "inference_time": 1.588523030281067, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9276, "num_retrieved": 45, "inference_time": 1.588523030281067, "generate_results": ["def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ \"$installed_script\" != \"\" ]] && [[ \"$installed_script\" != \"$(cat \"$this_filename\" 2> /dev/null)\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee \"$this_filename\"',\n                        \"$this_filename\"\n                    )}\"\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"not __fish_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''_init_completion -n \"_action_parser_condition\" -n \"_exact_option_condition\" -a '{action_name}' -d '{shlex.quote(action_parser.description)}''''\n            for", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            \"$this_filename\",\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(\"$@\")\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"! _borgmatic_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"! _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''_init_completion -n \"_action_parser_condition\" -n \"_exact_option_condition\" -a '{action_name}' -d", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            \"$this_filename\",\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=(\"$@\")\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${all_args[@]}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"! _borgmatic_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"! _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''_init_completion -n \"_action_parser_condition\" -n \"_exact_option_condition\" -a '{action_name}' -d", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            \"$this_filename\",\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"! _fish_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"! _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''complete -o default -F _borgmatic_{action_name} -n \"$", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                if [[ -f \"$this_filename\" ]]; then\n                    local this_script=$(cat \"$this_filename\" 2>/dev/null)\n                    local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                    if [[ \"$this_script\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            \"$this_filename\",\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n\n            _borgmatic_current_arg() {{\n                local all_args=($(compgen -W \"$(commandline -poc)\" --))\n                if [[ ${#all_args[@]} -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in \"${@}\"; do\n                    if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"not __bash_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"not _borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''complete -o default -F _borgmatic_{action_name} -n \"$"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8626, "num_retrieved": 22, "inference_time": 1.588523030281067, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--debug', action='store_true', help='enable debug logging')\n    global_parser.add_argument('--info', action='store_true', help='enable info logging')\n    \n    # Add subparsers for different actions\n    # Example subparsers (these would be defined in actual implementation)\n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    list_parser.add_argument('--archive', help='list files in specific archive')\n    list_parser.add_argument('--paths', nargs='+', help='list files in specific paths')\n    \n    rlist_parser = subparsers.add_parser('rlist', help='list archives in remote repository')\n    rlist_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    rlist_parser.add_argument('--prefix', help='filter archives by prefix')\n    rlist_parser.add_argument('--match-archives', help='match archives with pattern')\n    \n    info_parser = subparsers.add_parser('info', help='display information about archives')\n    info_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    info_parser.add_argument('--prefix', help='filter archives by prefix')\n    info_parser.add_argument('--match-archives', help='match archives with pattern')\n    \n    transfer_parser = subparsers.add_parser('transfer', help='transfer archives')\n    transfer_parser.add_argument('--dry-run', action='store_true', help='dry run mode')\n    transfer_parser.add_argument('--source-repository', help='source repository path')\n    transfer_parser.add_argument('--match-archives', help='match archives with pattern')\n    \n    # Parse arguments\n    args = parser.parse_args(unparsed_arguments)\n    \n    # Convert to dict mapping from subparser name to argparse.Namespace", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output log in JSON format')\n    global_parser.add_argument('--debug', action='store_true', help='enable debug logging')\n    global_parser.add_argument('--info', action='store_true', help='enable info logging')\n    \n    # Add subparsers for different actions\n    # Example subparsers (these would be defined in the actual implementation)\n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    list_parser.add_argument('--prefix', help='filter archives by prefix')\n    list_parser.add_argument('--match-archives', help='match archives with a pattern')\n    list_parser.add_argument('--archive', help='list files in a specific archive')\n    list_parser.add_argument('--paths', nargs='*', help='list files in specific paths')\n    \n    rlist_parser = subparsers.add_parser('rlist', help='list archives in a remote repository')\n    rlist_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    rlist_parser.add_argument('--prefix', help='filter archives by prefix')\n    rlist_parser.add_argument('--match-archives', help='match archives with a pattern')\n    \n    info_parser = subparsers.add_parser('info', help='display information about archives')\n    info_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    info_parser.add_argument('--prefix', help='filter archives by prefix')\n    info_parser.add_argument('--match-archives', help='match archives with a pattern')\n    \n    transfer_parser = subparsers.add_parser('transfer', help='transfer archives')\n    transfer_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    transfer_parser.add_argument('--source-repository', help='source repository path')\n    transfer_parser.add_argument('--archive', help='", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--debug', action='store_true', help='enable debug logging')\n    global_parser.add_argument('--info', action='store_true', help='output info messages')\n    \n    # Add subparsers for different actions\n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('archive', nargs='?', help='archive name to list')\n    list_parser.add_argument('--paths', nargs='*', help='paths to list within an archive')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    rlist_parser = subparsers.add_parser('rlist', help='list archives in a remote repository')\n    rlist_parser.add_argument('--prefix', help='prefix to match archives')\n    rlist_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    info_parser = subparsers.add_parser('info', help='display information about archives')\n    info_parser.add_argument('archive', nargs='?', help='archive name to display information about')\n    info_parser.add_argument('--prefix', help='prefix to match archives')\n    info_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    rinfo_parser = subparsers.add_parser('rinfo', help='display information about a remote repository')\n    rinfo_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    transfer_parser = subparsers.add_parser('transfer', help='transfer archives')\n    transfer_parser.add_argument('--source-repository', help='source repository path')\n    transfer_parser.add_argument('--archive', help='archive name to transfer')\n    transfer_parser.add_argument('--match-archives', help='match archives using a pattern')\n    transfer_parser.add_argument('--dry-run', action='store_true', help='perform", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-commands')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--debug', action='store_true', help='enable debug logging')\n    global_parser.add_argument('--info', action='store_true', help='enable info logging')\n    \n    # Add subparsers for different actions\n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    list_parser.add_argument('--prefix', help='filter archives by prefix')\n    list_parser.add_argument('--match-archives', help='match archives with a pattern')\n    list_parser.add_argument('--archive', help='specific archive to list')\n    list_parser.add_argument('--paths', nargs='*', help='paths to list within an archive')\n    \n    rlist_parser = subparsers.add_parser('rlist', help='list archives in a remote repository')\n    rlist_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    rlist_parser.add_argument('--prefix', help='filter archives by prefix')\n    rlist_parser.add_argument('--match-archives', help='match archives with a pattern')\n    rlist_parser.add_argument('--archive', help='specific archive to list')\n    \n    info_parser = subparsers.add_parser('info', help='display information about archives')\n    info_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    info_parser.add_argument('--prefix', help='filter archives by prefix')\n    info_parser.add_argument('--match-archives', help='match archives with a pattern')\n    info_parser.add_argument('--archive', help='specific archive to display information for')\n    \n    rinfo_parser = subparsers.add_parser('rinfo', help='display information about a remote repository')\n    rinfo_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    \n    transfer", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='subparser_name', help='sub-command help')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--log-json', action='store_true', help='output logs in JSON format')\n    global_parser.add_argument('--debug', action='store_true', help='enable debug logging')\n    global_parser.add_argument('--info', action='store_true', help='enable info logging')\n    \n    # Add subparsers for different actions\n    list_parser = subparsers.add_parser('list', help='list archives')\n    list_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    list_parser.add_argument('--prefix', help='filter archives by prefix')\n    list_parser.add_argument('--match-archives', help='match archives using shell pattern')\n    list_parser.add_argument('--glob-archives', help='match archives using glob pattern')\n    list_parser.add_argument('archive', nargs='?', help='archive name to list')\n    \n    rlist_parser = subparsers.add_parser('rlist', help='list archives in remote repository')\n    rlist_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    rlist_parser.add_argument('--prefix', help='filter archives by prefix')\n    rlist_parser.add_argument('--match-archives', help='match archives using shell pattern')\n    rlist_parser.add_argument('--glob-archives', help='match archives using glob pattern')\n    \n    info_parser = subparsers.add_parser('info', help='display information about archives')\n    info_parser.add_argument('--json', action='store_true', help='output in JSON format')\n    info_parser.add_argument('--prefix', help='filter archives by prefix')\n    info_parser.add_argument('--match-archives', help='match archives using shell pattern')\n    info_parser.add_argument('--glob-archives', help='match archives using glob pattern')\n    \n    rinfo_parser = subparsers.add_parser('rinfo', help='display information about remote repository')\n    rinfo_parser.add_argument('--json', action='store_true', help='output in JSON format')"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8523, "num_retrieved": 18, "inference_time": 1.588523030281067, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: argparse.Namespace())\n    \n    # Convert unparsed arguments to list if it's not already\n    args_list = list(unparsed_arguments) if isinstance(unparsed_arguments, (list, tuple)) else []\n    \n    # Iterate through subparsers to try to parse arguments\n    for subparser_name, parser in subparsers.items():\n        # Parse arguments using the subparser\n        try:\n            # Create a namespace to hold parsed arguments\n            parsed_args = parser.parse_args(args_list, namespace=parsed_arguments[subparser_name])\n            \n            # If arguments were successfully parsed, update the parsed_arguments dict\n            if parsed_args:\n                # Update the namespace with parsed values\n                for attr, value in vars(parsed_args).items():\n                    setattr(parsed_arguments[subparser_name], attr, value)\n            \n            # Remove arguments that were parsed by this subparser\n            args_list = [arg for arg in args_list if not parser.match(arg)]\n            \n        except SystemExit:\n            # If parsing fails, continue to next subparser\n            continue\n    \n    # Convert the parsed arguments dictionary to a regular dict\n    result_dict = dict(parsed_arguments)\n    \n    # Return the parsed arguments and remaining arguments\n    return (result_dict, args_list)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: argparse.Namespace())\n    \n    # Convert unparsed arguments to list for easier manipulation\n    args_list = list(unparsed_arguments)\n    \n    # Process each argument in order\n    for arg in args_list:\n        # Try to parse the argument with each subparser in order\n        for subparser_name, parser in subparsers.items():\n            try:\n                # Attempt to parse the argument with the current subparser\n                # We need to parse the argument as if it were part of the subparser's command line\n                # This requires building a new argument parser instance or using the existing one\n                # We'll use the existing parser to parse the argument\n                \n                # Create a new namespace to hold the parsed arguments\n                namespace = parser.parse_args([arg])\n                \n                # If we successfully parsed the argument, we can assign it to the subparser\n                # and break out of the loop to avoid assigning to multiple subparsers\n                parsed_arguments[subparser_name].__dict__.update(namespace.__dict__)\n                break\n            except SystemExit:\n                # If the parser raises SystemExit (e.g., due to invalid arguments), continue\n                continue\n            except Exception:\n                # If any other exception occurs, continue to the next subparser\n                continue\n    \n    # Extract the remaining arguments that were not parsed by any subparser\n    remaining_arguments = [arg for arg in args_list if not any(\n        arg in parsed_arguments[subparser_name] for subparser_name in parsed_arguments\n    )]\n    \n    # Return the parsed arguments dictionary and the remaining arguments\n    return (dict(parsed_arguments), remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: argparse.Namespace())\n    \n    # Convert unparsed arguments to list if it's not already\n    args_list = list(unparsed_arguments) if isinstance(unparsed_arguments, (list, tuple)) else []\n    \n    # Process each argument in order\n    for arg in args_list:\n        # Try to parse the argument with each subparser in order\n        for subparser_name, parser in subparsers.items():\n            try:\n                # Attempt to parse the argument with the current subparser\n                # We need to check if the argument matches any of the subparser's expected arguments\n                # This is a simplified approach - in practice, we'd use the parser's parse_args method\n                # with a list of arguments\n                \n                # Create a new namespace to hold the parsed arguments\n                namespace = parser.parse_args([arg])\n                \n                # If we successfully parsed the argument, store it in the parsed_arguments dict\n                # and break to move to the next argument\n                if namespace:\n                    parsed_arguments[subparser_name].__dict__.update(namespace.__dict__)\n                    break\n            except SystemExit:\n                # If the parser raises SystemExit, it means the argument is not valid for this subparser\n                # Continue to the next subparser\n                pass\n            except Exception:\n                # Any other exception means the argument is not valid for this subparser\n                # Continue to the next subparser\n                pass\n    \n    # Extract the remaining arguments that were not parsed by any subparser\n    remaining_args = [arg for arg in args_list if not any(\n        arg in parsed_arguments[subparser_name] for subparser_name in parsed_arguments\n    )]\n    \n    return (dict(parsed_arguments), remaining_args)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: argparse.Namespace())\n    \n    # Convert unparsed_arguments to a list if it's not already\n    args_list = list(unparsed_arguments) if isinstance(unparsed_arguments, (list, tuple)) else []\n    \n    # Process each argument in order\n    remaining_args = args_list[:]\n    \n    # Iterate through subparsers to allow each to parse arguments\n    for subparser_name, parser in subparsers.items():\n        # Create an argument parser for the current subparser\n        # We need to parse arguments that match the subparser's expected arguments\n        try:\n            # Parse the arguments using the subparser\n            parsed_args = parser.parse_args(remaining_args, namespace=parsed_arguments[subparser_name])\n            \n            # If arguments were parsed successfully, remove them from remaining_args\n            if parsed_args is not None:\n                # Find the indices of the parsed arguments and remove them\n                parsed_args_list = []\n                for arg in args_list:\n                    if arg in remaining_args:\n                        parsed_args_list.append(arg)\n                        remaining_args.remove(arg)\n                \n                # Update the parsed arguments for this subparser\n                for attr_name, attr_value in vars(parsed_args).items():\n                    setattr(parsed_arguments[subparser_name], attr_name, attr_value)\n                    \n        except SystemExit:\n            # If parsing fails due to invalid arguments, continue to next subparser\n            pass\n    \n    # Convert the parsed arguments dictionary to a regular dict\n    parsed_args_dict = dict(parsed_arguments)\n    \n    # Return the parsed arguments and remaining arguments\n    return (parsed_args_dict, remaining_args)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    import argparse\n    from collections import defaultdict\n    \n    # Create a dictionary to store parsed arguments for each subparser\n    parsed_arguments = defaultdict(lambda: argparse.Namespace())\n    \n    # Convert unparsed arguments to list for easier manipulation\n    args_list = list(unparsed_arguments)\n    \n    # Process each argument in order\n    for arg in args_list:\n        # Try to parse the argument with each subparser in order\n        for subparser_name, parser in subparsers.items():\n            # Try to parse the argument with the current subparser\n            try:\n                # Create a parser instance for the current subparser\n                # We need to parse the argument against the subparser's arguments\n                # We'll use the subparser's argument parser to parse the argument\n                # We need to handle the case where the argument is not recognized\n                # by the subparser\n                \n                # Create a namespace to hold the parsed arguments\n                namespace = parser.parse_args([arg])\n                \n                # If we successfully parsed the argument, store it in the parsed_arguments\n                # dictionary and remove it from the args_list\n                parsed_arguments[subparser_name].__dict__.update(namespace.__dict__)\n                args_list.remove(arg)\n                break\n            except SystemExit:\n                # If the parser raises a SystemExit, it means the argument is not valid\n                # for this subparser, so we continue to the next subparser\n                continue\n            except Exception:\n                # If any other exception occurs, we continue to the next subparser\n                continue\n    \n    # Return the parsed arguments dictionary and the remaining arguments\n    return (dict(parsed_arguments), args_list)"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9039, "num_retrieved": 37, "inference_time": 1.588523030281067, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog='''Examples:\n  borgmatic --config /path/to/config\n  borgmatic create --archive my-archive --repository /path/to/repo\n  borgmatic list --repository /path/to/repo --archive my-archive\n  borgmatic check --repository /path/to/repo --archive my-archive\n  borgmatic transfer --source-repository /path/to/source --repository /path/to/dest\n'''\n    )\n    \n    # Global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument(\n        '--log-json',\n        action='store_true',\n        help='Output logs in JSON format'\n    )\n    global_parser.add_argument(\n        '--lock-wait',\n        type=int,\n        help='Wait for lock for specified number of seconds'\n    )\n    global_parser.add_argument(\n        '--verbosity',\n        choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set verbosity level'\n    )\n    \n    # Subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # create action\n    create_parser = subparsers.add_parser('create', help='Create a new archive')\n    create_parser.add_argument(\n        '--archive',\n        help='Name of the archive to create'\n    )\n    create_parser.add_argument(\n        '--repository',\n        help='Path to the repository'\n    )\n    create_parser.add_argument(\n        '--exclude-from',\n        nargs='+',\n        help='File containing exclude patterns'\n    )\n    create_parser.add_argument(\n        '--exclude-caches',\n        action='store_true',\n        help='Exclude caches from backup'\n    )\n    create_parser.add_argument(\n        '--exclude-if-present',\n        nargs='+',\n        help='Exclude files if present'\n    )\n    create_parser.add_argument(\n        '--keep-exclude-tags',\n        action='store_true',\n        help='Keep exclude tags'\n    )\n    create_parser.add_argument(\n        '--exclude-nodump',\n        action='store_true',\n        help='Exclude files that should not be dumped'\n    )\n    create_parser.add_argument", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool to manage Borg repositories',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument(\n        '--log-json', \n        action='store_true', \n        help='Output logs in JSON format'\n    )\n    global_parser.add_argument(\n        '--lock-wait', \n        type=int, \n        help='Wait for lock for specified number of seconds'\n    )\n    global_parser.add_argument(\n        '--verbosity', \n        choices=['disabled', 'error', 'answer', 'some', 'lots'], \n        help='Set verbosity level'\n    )\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    # Example: create, list, rinfo, check, etc.\n    # These would be defined with their own argument groups and parsers\n    \n    # Placeholder for actual subparsers - in a real implementation, these would be defined\n    # with specific arguments based on the action\n    \n    return (parser, subparsers, parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - a Borg backup tool with configuration and automation')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--lock-wait', type=int, help='Wait time for lock acquisition in seconds')\n    global_parser.add_argument('--verbosity', choices=['disabled', 'error', 'answer', 'some', 'lots'], default='some', help='Set verbosity level')\n    \n    # Add subparsers\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be defined in the actual implementation)\n    # config_parser = subparsers.add_parser('config', help='Manage configuration')\n    # list_parser = subparsers.add_parser('list', help='List archives')\n    # create_parser = subparsers.add_parser('create', help='Create a backup')\n    # check_parser = subparsers.add_parser('check', help='Check repository integrity')\n    # transfer_parser = subparsers.add_parser('transfer', help='Transfer archives')\n    \n    # Return the top-level parser and subparsers as a tuple\n    return parser, subparsers, parser", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A tool for managing Borg repositories')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--lock-wait', type=int, help='Wait for lock for specified number of seconds')\n    global_parser.add_argument('--debug', action='store_true', help='Enable debug output')\n    global_parser.add_argument('--info', action='store_true', help='Enable info output')\n    \n    # Create subparsers\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add subparsers for each command\n    borgmatic.commands.arguments.add_subparsers(subparsers)\n    \n    return parser, subparsers, parser", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(description='Borgmatic - A Borg backup tool with configuration and automation')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--lock-wait', type=int, help='Wait for lock for specified number of seconds')\n    global_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be defined in the actual implementation)\n    # The actual implementation would include specific parsers for each command\n    \n    # Return the top-level parser, subparsers, and the combined parser\n    return (parser, subparsers, parser)"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8748, "num_retrieved": 23, "inference_time": 1.588523030281067, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        # If max_results is not positive, return empty generator\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        # If max_results is not positive, return empty generator\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        # If max_results is not positive, return empty generator\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        # If max_results is not positive, return empty generator\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if max_results <= 0:\n        # If max_results is not positive, return empty generator\n        return\n    \n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9359, "num_retrieved": 44, "inference_time": 1.588523030281067, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit\n    if 'K' in size:\n        # Extract number part\n        num_part = size.replace('K', '').strip()\n        return int(num_part) * 1024\n    elif 'M' in size:\n        # Extract number part\n        num_part = size.replace('M', '').strip()\n        return int(num_part) * 1024 * 1024\n    elif 'G' in size:\n        # Extract number part\n        num_part = size.replace('G', '').strip()\n        return int(num_part) * 1024 * 1024 * 1024\n    else:\n        # No unit, assume it's bytes\n        return int(size)", "def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit\n    if 'K' in size:\n        # Extract the number part\n        num_part = size.replace('K', '').strip()\n        return int(num_part) * 1024\n    elif 'M' in size:\n        # Extract the number part\n        num_part = size.replace('M', '').strip()\n        return int(num_part) * 1024 * 1024\n    elif 'G' in size:\n        # Extract the number part\n        num_part = size.replace('G', '').strip()\n        return int(num_part) * 1024 * 1024 * 1024\n    else:\n        # No unit, assume it's in bytes\n        return int(size)", "def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit\n    if 'K' in size or 'k' in size:\n        # Extract the number part\n        num_part = size.replace('K', '').replace('k', '')\n        return int(num_part) * 1024\n    elif 'M' in size or 'm' in size:\n        # Extract the number part\n        num_part = size.replace('M', '').replace('m', '')\n        return int(num_part) * 1024 * 1024\n    elif 'G' in size or 'g' in size:\n        # Extract the number part\n        num_part = size.replace('G', '').replace('g', '')\n        return int(num_part) * 1024 * 1024 * 1024\n    else:\n        # No unit, assume it's bytes\n        return int(size)", "def size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit\n    if 'K' in size:\n        # Extract the number part\n        num_part = size.replace('K', '').strip()\n        return int(num_part) * 1024\n    elif 'M' in size:\n        # Extract the number part\n        num_part = size.replace('M', '').strip()\n        return int(num_part) * 1024 * 1024\n    elif 'G' in size:\n        # Extract the number part\n        num_part = size.replace('G', '').strip()\n        return int(num_part) * 1024 * 1024 * 1024\n    else:\n        # No unit, assume it's in bytes\n        return int(size)"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8809, "num_retrieved": 30, "inference_time": 1.588523030281067, "generate_results": ["def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n    \n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n    \n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n    \n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n    \n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n    \n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8720, "num_retrieved": 25, "inference_time": 1.588523030281067, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10653, "num_retrieved": 80, "inference_time": 1.588523030281067, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', 'n/a', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', 'n/a', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', 'n/a', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', 'n/a', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    return val.lower() in ['none', 'null', 'nil', 'n/a', '']"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9230, "num_retrieved": 43, "inference_time": 1.588523030281067, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(sorted(set(flags)))"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8632, "num_retrieved": 21, "inference_time": 1.588523030281067, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(f\"Recursion detected: {path_name} has already been processed\")\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(f\"Recursion detected: {path_name} has already been processed\")\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(f\"Recursion detected: {path_name} has already been processed\")\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(f\"Recursion detected: {path_name} has already been processed\")\n        return True\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    abs_path = os.path.abspath(path_name)\n    if abs_path in processed:\n        if verbose:\n            print(f\"Recursion detected: {path_name} has already been processed\")\n        return True\n    processed.add(abs_path)\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9295, "num_retrieved": 47, "inference_time": 1.1433023363351822, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of (previous, current, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create tuple of three points: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple: (previous, current, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple: previous, current, next\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 1167, "num_retrieved": 5, "inference_time": 1.1433023363351822, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        remainder = number % len(alphabet)\n        result = alphabet[remainder] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        result = alphabet[number % len(alphabet)] + result\n        number //= len(alphabet)\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    while number > 0:\n        number, remainder = divmod(number, len(alphabet))\n        result = alphabet[remainder] + result\n    \n    if padding is not None and len(result) < padding:\n        result = result.zfill(padding)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9447, "num_retrieved": 48, "inference_time": 1.1433023363351822, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10980, "num_retrieved": 105, "inference_time": 1.1433023363351822, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10146, "num_retrieved": 85, "inference_time": 1.1433023363351822, "generate_results": ["def subclasses(cls):\n    \"\"\"Return all subclasses of a class, recursively\"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"Return all subclasses of a class, recursively\"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"Return all subclasses of a class, recursively\"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"Return all subclasses of a class, recursively\"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"Return all subclasses of a class, recursively\"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 1161, "num_retrieved": 5, "inference_time": 1.1433023363351822, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9353, "num_retrieved": 45, "inference_time": 1.1433023363351822, "generate_results": ["import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the gzip content\n    with gzip.open(response.content, 'rb') as gz_file:\n        # Extract to destination path\n        with shutil.unpack_archive(gz_file, destination_path) as archive:\n            pass  # Extracting is handled by unpack_archive\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the gzip content\n    with gzip.open(response.content, 'rb') as gz_file:\n        # Extract to destination path\n        with shutil.unpack_archive(gz_file, destination_path) as archive:\n            pass  # Extracting is handled by unpack_archive\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the gzip content\n    with gzip.open(response.content, 'rb') as gz_file:\n        # Extract to destination path\n        with shutil.unpack_archive(gz_file, destination_path) as archive:\n            pass  # Extracting is handled by unpack_archive\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the gzip content\n    with gzip.open(response.content, 'rb') as gz_file:\n        # Extract to destination path\n        with shutil.unpack_archive(gz_file, destination_path) as archive:\n            pass  # Extracting is handled by unpack_archive\n    \n    return destination_path", "import requests\nimport gzip\nimport shutil\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url, allow_redirects=True)\n    response.raise_for_status()\n    \n    # Decompress the gzip content\n    with gzip.open(response.content, 'rb') as gz_file:\n        # Extract to destination path\n        with shutil.unpack_archive(gz_file, destination_path) as archive:\n            pass  # Extracting is handled by shutil.unpack_archive\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9772, "num_retrieved": 57, "inference_time": 1.1433023363351822, "generate_results": ["import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "def os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.name == \"posix\" and \"darwin\" in os.uname().sysname.lower()", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "def os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.name == \"posix\" and \"darwin\" in platform.system().lower()", "def os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return os.name == \"posix\" and \"darwin\" in platform.system().lower()"]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8860, "num_retrieved": 28, "inference_time": 1.1433023363351822, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Escape special regex characters in the pattern\n        regex_key = re.escape(regex_key)\n        # Replace the original key with the regex pattern\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add the regex pattern to the result with original value\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add the regex pattern to the result with the original value\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Escape special regex characters in the pattern\n        regex_key = re.escape(key)\n        # Replace * with .* to match any sequence of characters\n        regex_key = regex_key.replace(r'\\*', r'(.*)')\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add the regex pattern to the result with original value\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9967, "num_retrieved": 70, "inference_time": 1.1433023363351822, "generate_results": ["import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    else:\n        return name", "import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    else:\n        return name", "import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    else:\n        return name", "import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    else:\n        return name", "import re\ndef unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    # Remove quotes from the beginning and end of the name\n    return re.sub(r'^[\"\\']|[\"\\']$', '', name)"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9033, "num_retrieved": 32, "inference_time": 1.1433023363351822, "generate_results": ["import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin', 'FreeBSD'] else 0\n    \n    # Regex pattern to split on whitespace, handling quoted strings and escaped characters\n    # This pattern matches sequences of non-whitespace, handles quoted strings, and escapes\n    pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^\"\\\\]|\\\\.)*)(?:\\s+|$)'\n    \n    # Split the string using the pattern\n    parts = re.split(r'(\\s+)', s)\n    \n    # Clean up empty parts and handle quoted strings\n    result = []\n    i = 0\n    while i < len(parts):\n        part = parts[i]\n        if part.strip():\n            # If it's a quoted string, remove quotes and keep the content\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            elif part.startswith(\"'\") and part.endswith(\"'\"):\n                result.append(part[1:-1])\n            else:\n                # Split on whitespace but preserve quoted parts\n                if part.strip():\n                    # For non-quoted parts, split on whitespace\n                    subparts = re.split(r'(\\s+)', part)\n                    for sp in subparts:\n                        if sp.strip():\n                            result.append(sp.strip())\n        i += 1\n    \n    # Handle the case where we have a quoted string with escaped characters\n    # Use a more robust approach to split with proper handling of quotes and escapes\n    result = []\n    # Use a simple approach: split on whitespace, then clean up\n    tokens = []\n    current = \"\"\n    in_quotes = False\n    quote_char = None\n    \n    for char in s:\n        if char in '\"\\'':\n            if in_quotes and char == quote_char:\n                in_quotes = False\n                quote_char = None\n            elif not in_quotes:\n                in_quotes = True\n                quote_char = char\n            else:\n                # Already in quotes, just add to current\n                pass\n        elif char", "import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin', 'FreeBSD'] else 0\n    \n    # Regex pattern to split on whitespace, handling quoted strings and escaped characters\n    # This pattern matches sequences of non-whitespace, quoted strings, and escaped whitespace\n    pattern = r'([^\"\\\\]|\\\\.)+|\"(?:[^\"\\\\]|\\\\.)*\"|\\'(?:[^\\'\\\\]|\\\\.)*\\''\n    \n    # Split the string using the pattern\n    parts = re.split(pattern, s)\n    \n    # Filter out empty strings and strip quotes\n    result = []\n    for part in parts:\n        if part:\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            elif part.startswith(\"'\") and part.endswith(\"'\"):\n                result.append(part[1:-1])\n            else:\n                # Handle escaped characters and unquoted parts\n                result.append(part)\n    \n    # Remove empty strings\n    return [part for part in result if part]", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_is_linux() or os_is_mac() else 0\n    \n    if platform == 1:  # POSIX\n        # Use regex to handle quoted strings and escaped characters\n        # This pattern matches quoted strings and unquoted tokens with escaped quotes\n        pattern = r'([^\"\\\\]|\\\\.)*\"([^\"]*)\"'  # Match quoted strings\n        tokens = []\n        start = 0\n        while True:\n            match = re.search(r'([^\"\\\\]|\\\\.)*\"([^\"]*)\"', s[start:])\n            if not match:\n                break\n            tokens.append(match.group(1) + match.group(2))\n            start += len(match.group(0))\n        \n        # Handle unquoted tokens\n        remaining = s[start:]\n        if remaining:\n            # Split on whitespace, but preserve quoted strings\n            unquoted = re.split(r'(\\s+)', remaining)\n            for part in unquoted:\n                if part.strip():\n                    tokens.append(part.strip())\n        \n        # Remove empty tokens\n        return [token for token in tokens if token]\n    \n    elif platform == 0:  # Windows/CMD\n        # For Windows, split on spaces but preserve quoted strings\n        # Use a more robust approach to handle quoted strings\n        tokens = []\n        start = 0\n        while start < len(s):\n            if s[start] == '\"':\n                # Find the closing quote\n                end = s.find('\"', start + 1)\n                if end != -1:\n                    tokens.append(s[start+1:end])\n                    start = end + 1\n                else:\n                    # No closing quote found\n                    break\n            elif s[start].isspace():\n                start += 1\n            else:\n                # Extract token until next space or quote\n                end = start\n                while end < len(s) and s[end] not in ' \"':\n                    end += 1\n                tokens.append(s[start:end])\n                start = end\n        \n        return tokens\n    \n    else:\n        #", "import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin', 'FreeBSD'] else 0\n    \n    if platform == 1:\n        # POSIX platform (Linux, macOS, etc.)\n        # Use regex to split on whitespace, but preserve quoted strings\n        pattern = r'([^\"\\\\]|\\\\.)*\"([^\"]*)\"'  # Match quoted strings\n        result = []\n        current = \"\"\n        in_quote = False\n        \n        for char in s:\n            if char == '\"' and not in_quote:\n                in_quote = True\n                current = \"\"\n            elif char == '\"' and in_quote:\n                in_quote = False\n                result.append(current)\n                current = \"\"\n            elif not in_quote:\n                if char.isspace():\n                    if current:\n                        result.append(current)\n                        current = \"\"\n                else:\n                    current += char\n        if current:\n            result.append(current)\n            \n        # Split on whitespace, but keep quoted strings intact\n        parts = re.split(r'(\\s+)', s)\n        result = []\n        current = \"\"\n        for part in parts:\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            elif part.startswith('\"'):\n                result.append(part[1:])\n            elif part.endswith('\"'):\n                result.append(part[:-1])\n            else:\n                if part.strip():\n                    result.append(part.strip())\n        return result\n        \n    else:\n        # Windows/CMD platform\n        # Split on spaces, but handle quoted strings\n        result = []\n        current = \"\"\n        in_quote = False\n        \n        for char in s:\n            if char == '\"' and not in_quote:\n                in_quote = True\n                current = \"\"\n            elif char == '\"' and in_quote:\n                in_quote = False\n                result.append(current)\n                current = \"\"\n            elif not in_quote:\n                if char == ' ':\n                    if current:\n                        result.append(current)\n                        current = \"\"\n                else:\n                    current += char\n        if current:\n            result.append(current)", "import re\nimport platform as os_platform\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if os_platform.system() in ['Linux', 'Darwin', 'FreeBSD'] else 0\n    \n    # Regex pattern to match shell-like syntax: quoted strings, escaped quotes, and whitespace\n    # This handles quotes, backslashes, and whitespace properly\n    pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^\"\\\\]|\\\\.)*|[^\"\\\\ ]+)(?:\\s+|$)'\n    \n    # Split the string using the pattern\n    parts = re.split(r'(\\s+)', s)\n    \n    # Clean up empty parts and handle quoted strings\n    result = []\n    i = 0\n    while i < len(parts):\n        part = parts[i]\n        if part.strip():\n            # If it's a quoted string, preserve it\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            elif part.startswith(\"'\") and part.endswith(\"'\"):\n                result.append(part[1:-1])\n            else:\n                # Split on whitespace but preserve quoted parts\n                # For non-quoted parts, split on whitespace\n                if part.strip():\n                    # Handle escaped characters\n                    result.append(part.strip())\n        i += 1\n    \n    # Handle cases where we have multiple consecutive whitespaces\n    # This is a simplified version that works for most cases\n    result = [part for part in result if part.strip()]\n    \n    # For Windows, we need to handle CMD-style parsing\n    if platform == 0:\n        # Windows CMD uses different parsing rules\n        # Split on spaces but preserve quoted strings\n        result = []\n        in_quotes = False\n        current = \"\"\n        for char in s:\n            if char in '\"':\n                if in_quotes:\n                    in_quotes = False\n                    if current:\n                        result.append(current)\n                        current = \"\"\n                else:\n                    in_quotes = True\n                    current = \"\"\n            elif char == ' ' and not in_quotes:\n                if current:\n                    result.append(current)\n                    current ="]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8924, "num_retrieved": 30, "inference_time": 1.1433023363351822, "generate_results": ["import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the output filename if not provided\n    if filename is None:\n        filename = Path(archive_path).name.split('.')[0]\n    \n    output_path = tmp_dir / filename\n    \n    # Use subprocess to extract the archive\n    # We'll use 'tar' as a fallback for various archive types\n    try:\n        # Try to extract using tar first\n        cmd = ['tar', '-xzf', archive_path, '-C', str(tmp_dir)]\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError:\n        # If tar fails, try other common archive types\n        try:\n            # For zip files\n            if archive_path.endswith('.zip'):\n                cmd = ['unzip', '-q', archive_path, '-d', str(tmp_dir)]\n                subprocess.run(cmd, check=True)\n            else:\n                # For other formats, try to use 'gunzip' or similar\n                # This is a fallback - in practice, we'd need to detect the format\n                cmd = ['gunzip', '-c', archive_path]\n                with open(output_path, 'wb') as f:\n                    subprocess.run(cmd, stdout=f, check=True)\n        except subprocess.CalledProcessError:\n            # If all else fails, just return the path as is\n            pass\n    \n    # Return the path to the extracted directory\n    return str(output_path)", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the output filename if not provided\n    if filename is None:\n        filename = Path(archive_path).name.replace(\".tar.gz\", \"\").replace(\".tar.xz\", \"\").replace(\".tar.bz2\", \"\")\n    \n    output_path = tmp_dir / filename\n    \n    # Extract the archive using tar\n    try:\n        # Use tar to extract the archive\n        cmd = [\"tar\", \"-xzf\", archive_path, \"-C\", str(tmp_dir)]\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive {archive_path}: {e}\")\n    \n    # Return the file URL (assuming the extracted file is in the tmp directory)\n    # This is a simplified implementation - in practice, the exact URL would depend on the file structure\n    return str(output_path)", "import subprocess\nimport os\nfrom pathlib import Path\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(exist_ok=True)\n    \n    # Determine the output filename if not provided\n    if filename is None:\n        filename = Path(archive_path).name.rsplit(\".\", 1)[0]\n    \n    output_path = tmp_dir / filename\n    \n    # Extract the archive to the temporary directory\n    try:\n        # Use subprocess to run the tar command\n        cmd = [\"tar\", \"-xzf\", archive_path, \"-C\", str(tmp_dir)]\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the file repo url (path to the extracted directory)\n    return str(output_path)", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n    \n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the output filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).rsplit(\".\", 1)[0]\n    \n    # Create the output path\n    output_path = tmp_dir / filename\n    \n    # Extract the archive using tar\n    try:\n        subprocess.run(\n            [\"tar\", \"-xzf\", archive_path, \"-C\", str(tmp_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the file repo url (assuming the extracted file is in the tmp directory)\n    # This is a simplified implementation - in practice, the exact path would depend on the archive structure\n    return str(output_path)", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Determine the output filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).rsplit('.', 1)[0]\n    \n    output_path = tmp_dir / filename\n    \n    # Extract the archive using tar\n    try:\n        subprocess.run(\n            ['tar', '-xzf', archive_path, '-C', str(tmp_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Return the path to the extracted directory as a file repo url\n    return str(output_path)"]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8779, "num_retrieved": 26, "inference_time": 1.1433023363351822, "generate_results": ["import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter files that are in the ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the filtered files\n        filtered_files.sort()\n        \n        # Join the files with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                                capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter to only include files that are in the ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Join with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter files that are in the provided ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the filtered files\n        filtered_files.sort()\n        \n        # Join the files with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter to only include files that are in the ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Join with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter files that are in the ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Join with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not available, return empty string\n        return \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 3282, "num_retrieved": 30, "inference_time": 1.1433023363351822, "generate_results": ["def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # Based on the context, the supported file types are those related to docopt examples\n    # such as .docopt files (as seen in test cases and examples)\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt') or filename.endswith('.ini') or filename.endswith('.json')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide information about which file types are supported\n    # Based on the retrieved context, there is no explicit list of supported file types\n    # Therefore, we return False by default as no specific pattern is defined\n    \n    return False", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    # The context does not provide specific file types that this module supports\n    # Based on the examples and context, the module appears to handle configuration files\n    # like INI and JSON files, as seen in the config_file_example.py\n    # Therefore, we check if the filename ends with .ini or .json\n    \n    return filename.endswith('.ini') or filename.endswith('.json')"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9014, "num_retrieved": 36, "inference_time": 1.1433023363351822, "generate_results": ["import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Define time units and their corresponding timedelta multipliers\n    unit_map = {\n        'second': 1,\n        'seconds': 1,\n        'minute': 60,\n        'minutes': 60,\n        'hour': 3600,\n        'hours': 3600,\n        'day': 86400,\n        'days': 86400,\n        'week': 604800,\n        'weeks': 604800,\n        'month': 2592000,  # Approximate (30 days)\n        'months': 2592000,\n        'year': 31536000,  # Approximate (365 days)\n        'years': 31536000\n    }\n\n    # Regular expression to match number and unit\n    pattern = r'^(\\d+)\\s*([a-z]+)$'\n    match = re.match(pattern, frequency.strip().lower())\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2)\n\n    if unit not in unit_map:\n        raise ValueError(f'Unknown unit: {unit}')\n\n    # Create timedelta with the appropriate multiplier\n    return timedelta(seconds=number * unit_map[unit])", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f'Cannot parse frequency string: {frequency}')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f'Unknown time unit: {unit}')", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency.lower() == \"always\":\n        return None\n\n    # Define time units and their corresponding timedelta multipliers\n    unit_map = {\n        'second': 1,\n        'seconds': 1,\n        'minute': 60,\n        'minutes': 60,\n        'hour': 3600,\n        'hours': 3600,\n        'day': 86400,\n        'days': 86400,\n        'week': 604800,\n        'weeks': 604800,\n        'month': 2592000,  # Approximate 30 days\n        'months': 2592000,\n        'year': 31536000,  # Approximate 365 days\n        'years': 31536000\n    }\n\n    # Match number and unit using regex\n    match = re.match(r'^(\\d+)\\s*([a-z]+)$', frequency.strip())\n    \n    if not match:\n        raise ValueError(f'Invalid frequency format: {frequency}')\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    if unit not in unit_map:\n        raise ValueError(f'Unknown time unit: {unit}')\n    \n    # Create timedelta based on the number and unit\n    multiplier = unit_map[unit]\n    return timedelta(seconds=number * multiplier)", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Define time units and their corresponding timedelta multipliers\n    unit_map = {\n        'second': 1,\n        'seconds': 1,\n        'minute': 60,\n        'minutes': 60,\n        'hour': 3600,\n        'hours': 3600,\n        'day': 86400,\n        'days': 86400,\n        'week': 604800,\n        'weeks': 604800,\n        'month': 2592000,  # Approximate (30 days)\n        'months': 2592000,\n        'year': 31536000,  # Approximate (365 days)\n        'years': 31536000\n    }\n\n    # Regular expression to match number and unit\n    pattern = r'^(\\d+)\\s*([a-z]+)$'\n    match = re.match(pattern, frequency.strip().lower())\n    \n    if not match:\n        raise ValueError(f'Invalid frequency format: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2)\n\n    if unit not in unit_map:\n        raise ValueError(f'Unknown unit: {unit}')\n\n    # Create timedelta with the appropriate multiplier\n    return timedelta(seconds=number * unit_map[unit])", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n\n    # Define time units and their corresponding timedelta multipliers\n    unit_map = {\n        'second': 1,\n        'seconds': 1,\n        'minute': 60,\n        'minutes': 60,\n        'hour': 3600,\n        'hours': 3600,\n        'day': 86400,\n        'days': 86400,\n        'week': 604800,\n        'weeks': 604800,\n        'month': 2592000,  # Approximate (30 days)\n        'months': 2592000,\n        'year': 31536000,  # Approximate (365 days)\n        'years': 31536000\n    }\n\n    # Match number and unit\n    match = re.match(r'^(\\d+)\\s*([a-z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f'Invalid frequency format: {frequency}')\n\n    number = int(match.group(1))\n    unit = match.group(2).lower()\n\n    if unit not in unit_map:\n        raise ValueError(f'Unknown time unit: {unit}')\n\n    # Create timedelta\n    seconds = number * unit_map[unit]\n    return timedelta(seconds=seconds)"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10167, "num_retrieved": 70, "inference_time": 1.1433023363351822, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip = socket.gethostbyname(host)\n        # localhost is typically 127.0.0.1 or ::1 (IPv6)\n        return ip in ['127.0.0.1', '127.0.1.1', '::1']\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == \"127.0.0.1\"\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not local\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the host to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == \"127.0.0.1\"\n    except socket.gaierror:\n        # If the host cannot be resolved, it's not localhost\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip = socket.gethostbyname(host)\n        # localhost is typically 127.0.0.1 or ::1\n        return ip in ['127.0.0.1', '0.0.0.0', '::1']\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n\n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip = socket.gethostbyname(host)\n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == \"127.0.0.1\"\n    except socket.gaierror:\n        # If hostname resolution fails, it's not localhost\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8835, "num_retrieved": 30, "inference_time": 1.2387736588716507, "generate_results": ["import os\nimport re\nfrom glob import glob\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Replace any existing wildcards with the appropriate glob syntax\n            # Use sh: prefix and add **/*path*/** pattern\n            # Remove any leading/trailing slashes and expand to a glob pattern\n            path = path.strip('/')\n            # Use glob pattern with ** to match any directory path\n            glob_pattern = f'sh:**/*{path}*/**'\n            result.append(glob_pattern)\n    return result", "import os\nimport glob\nfrom itertools import chain\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use glob pattern with ** to match any directory path\n            # Replace any existing / with /* to match subdirectories\n            # Add ** at the beginning and end to match any directory path\n            pattern = path.replace('/', '/*')\n            # Add ** at the beginning and end to match any directory path\n            result.append(f'sh:**/*{pattern}*/**')\n    return result", "import os\nimport glob\nfrom itertools import chain\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragments into glob patterns\n            # Use glob pattern with ** to match any directory path\n            # Replace any existing / with /* to match directories\n            # Add wildcards around the path\n            expanded_path = os.path.expanduser(path)\n            # Convert to glob pattern with **/* pattern\n            # This will match any directory containing the path\n            glob_pattern = f'sh:**/*{expanded_path}*/**'\n            result.append(glob_pattern)\n    return result", "import os\nimport glob\nfrom itertools import chain\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use glob pattern with ** to match any directory depth\n            # Replace any existing / with /* to match subdirectories\n            # Add sh: prefix to make it a Borg pattern\n            expanded_path = os.path.expanduser(path)\n            # Convert to glob pattern with **/* at the end\n            glob_pattern = f'sh:**/*{expanded_path}*/**'\n            result.append(glob_pattern)\n    return result", "import os\nimport glob\nfrom itertools import chain\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path fragments into glob patterns.\n    Pass through existing patterns untouched.\n    \n    For example, given find_paths of:\n      \n      ['foo.txt', 'pp:root/somedir']\n      \n    ... transform that into:\n      \n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    result = []\n    for path in find_paths:\n        if path.startswith('sh:') or path.startswith('pp:'):\n            result.append(path)\n        else:\n            # Transform path fragment into glob pattern\n            # Use glob pattern with ** to match any directory path\n            # Replace any existing / with /* to match subdirectories\n            # Add ** at the beginning and end to match any directory path\n            pattern = path.replace('/', '/*')\n            # Add ** at the beginning and end to match any directory path\n            result.append(f'sh:**/{pattern}/**')\n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10088, "num_retrieved": 67, "inference_time": 1.2387736588716507, "generate_results": ["def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get(\"TERM\", \"\") == \"xterm-256color\" and os.environ.get(\"GIT_BASH\", \"\") != \"\"", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get(\"GIT_BASH\", \"\") != \"\""]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8727, "num_retrieved": 26, "inference_time": 1.2387736588716507, "generate_results": ["import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return\n    \n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    with open(config_filename, 'w', encoding='utf-8') as config_file:\n        config_file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return\n    \n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    with open(config_filename, 'w', encoding='utf-8') as config_file:\n        config_file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return\n    \n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    with open(config_filename, 'w', encoding='utf-8') as config_file:\n        config_file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return\n    \n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    with open(config_filename, 'w', encoding='utf-8') as config_file:\n        config_file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    if not overwrite and os.path.exists(config_filename):\n        return\n    \n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    with open(config_filename, 'w', encoding='utf-8') as config_file:\n        config_file.write(rendered_config)\n    \n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9123, "num_retrieved": 35, "inference_time": 1.2387736588716507, "generate_results": ["def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands, default is \" && \"\n    :return: a single line command string\n    \"\"\"\n    # Wrap the script to ensure proper formatting\n    lines = textwrap.dedent(script).strip().splitlines()\n    # Join non-empty, non-comment lines with the specified separator\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the separator\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands, default is \" && \"\n    :return: a single line command string\n    \"\"\"\n    # Wrap the script content to ensure proper formatting\n    lines = textwrap.dedent(script).strip().splitlines()\n    # Join non-empty, non-comment lines with the specified separator\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the separator\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands, default is \" && \"\n    :return: a single line command string\n    \"\"\"\n    # Wrap the script content with proper formatting\n    # Remove leading and trailing whitespace\n    lines = textwrap.dedent(script).strip().splitlines()\n    \n    # Join non-empty, non-comment lines with the separator\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the specified separator\n    return seperator.join(commands)", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Wrap the script lines with the separator to join them into one command\n    lines = script.strip().splitlines()\n    # Remove empty lines and comments\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the specified separator\n    if commands:\n        return seperator.join(commands)\n    else:\n        return \"\"", "def oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Wrap the script lines with the separator to create a single command\n    lines = script.strip().splitlines()\n    # Remove empty lines and comments\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the specified separator\n    if commands:\n        return seperator.join(commands)\n    else:\n        return \"\""]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8963, "num_retrieved": 32, "inference_time": 1.2387736588716507, "generate_results": ["import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.func = func\n    \n    # Create a string representation of the function call\n    function_call = f\"func({', '.join([repr(arg) for arg in args])})\"\n    \n    # Create the command to execute\n    cmd = [\"python\", \"-c\"]\n    cmd.append(f\"import sys; sys.path.append('{sys.path[0]}'); import {module_name}; {function_call}\")\n    \n    # Set environment variables\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Execute the command with timeout\n    result = subprocess.run(cmd, env=env, timeout=timeout, capture_output=True, text=True)\n    \n    return result", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.__dict__[func.__name__] = func\n    \n    # Create a string representation of the command to execute\n    command = [sys.executable, \"-c\", f\"import {module_name}; {func.__name__}(*{args})\"]\n    \n    # Set up environment variables\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Execute the command with timeout\n    try:\n        result = subprocess.run(command, timeout=timeout, env=env, capture_output=True, text=True)\n        return result.returncode, result.stdout, result.stderr\n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Command timed out\"\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func)}_module\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.func = func\n    \n    # Execute the function in a subprocess\n    try:\n        # Prepare the command to run\n        cmd = [\"python\", \"-c\"]\n        \n        # Build the command string\n        cmd_str = f\"import sys; sys.path.append('{sys.path[0]}'); import {module_name}; {module_name}.func({', '.join([repr(arg) for arg in args])})\"\n        \n        # Add extra environment variables\n        env = dict(os.environ)\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the command with timeout\n        result = subprocess.run(\n            cmd + [cmd_str],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result.returncode, result.stdout, result.stderr\n        \n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Timeout expired\"\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the command to run\n    # We need to import the module containing the function and then execute it\n    # This is a simplified implementation assuming func is a callable from a module\n    # that can be imported via its module name and function name\n    \n    # Extract module and function name from func\n    # This is a simplified approach - in practice, you'd need to parse the function's module\n    # For this implementation, we assume func is accessible via a module that can be imported\n    \n    # Since we don't have access to the actual module or function name, \n    # we'll create a simple wrapper that runs the function in a subprocess\n    \n    # Create a temporary script that imports the module and calls the function\n    script_content = f\"\"\"\nimport sys\nimport importlib\nimport subprocess\n\n# Import the module containing the function\nmodule_name = '{func.__module__}'\nmodule = importlib.import_module(module_name)\n\n# Get the function\nfunc = getattr(module, '{func.__name__}')\n\n# Execute the function with arguments\nresult = func({'', *args})  # This is a placeholder - actual args need to be properly passed\nprint(result)\n\"\"\"\n    \n    # Create a temporary file for the script\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write(script_content)\n        temp_script = f.name\n    \n    try:\n        # Set up environment\n        env = dict(os.environ)\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script in subprocess\n        result = subprocess.run(\n            ['python', temp_script],\n            capture_output=True,\n            text=True,\n            timeout=timeout,\n            env=env\n        )\n        \n        # Return the output\n        return result.stdout.strip() if result.stdout else None\n        \n    except subprocess.TimeoutExpired", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Construct the command to run\n    # We need to import the module and get the function\n    # Since func is a function, we need to find its module\n    module_name = func.__module__\n    \n    # Import the module\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        raise ImportError(f\"Failed to import module {module_name}: {e}\")\n    \n    # Create the command string\n    command = [sys.executable, \"-c\"]\n    \n    # Build the Python script to execute\n    script = f\"\"\"\nimport importlib\nimport sys\nimport subprocess\nimport {module_name}\n\n# Get the function from the module\nfunc = {module_name}.{func.__name__}\n\n# Execute the function with arguments\nresult = func({', '.join([repr(arg) for arg in args])})\nprint(result)\n\"\"\"\n    \n    # Add extra environment variables\n    env = dict(os.environ)\n    if extra_env:\n        env.update(extra_env)\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            command,\n            input=script,\n            text=True,\n            capture_output=True,\n            timeout=timeout,\n            env=env\n        )\n        return result.stdout.strip() if result.stdout else None\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function execution timed out after {timeout} seconds\")\n    except Exception as e:\n        raise RuntimeError(f\"Error running subprocess: {e}\")"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8878, "num_retrieved": 31, "inference_time": 1.2387736588716507, "generate_results": ["import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)\\s*([:-])(.*)\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(3) if match.group(3) else None\n    \n    # If default value is provided, use it if the environment variable is not set\n    if default_value is not None:\n        env_value = os.environ.get(var_name, default_value)\n        return env_value\n    \n    # If no default value is provided, raise an error if the environment variable is not set\n    env_value = os.environ.get(var_name)\n    if env_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return env_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([^:]+)(?::([^}]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(2)\n    \n    env_value = os.environ.get(var_name)\n    if env_value is not None:\n        return env_value\n    \n    if default_value is not None:\n        return default_value\n    \n    raise EnvironmentError(f\"Environment variable '{var_name}' is not defined and no default value provided\")", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)\\s*([:-])(.*)\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(3)\n    \n    # If default value is provided, use it if the environment variable is not set\n    if default_value:\n        env_value = os.environ.get(var_name, default_value)\n        return env_value\n    \n    # If no default value is provided, raise an error if the environment variable is not set\n    env_value = os.environ.get(var_name)\n    if env_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return env_value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)\\s*([:-]([a-zA-Z0-9_]+))?\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(3) if match.group(3) else None\n    \n    value = os.environ.get(var_name)\n    if value is None and default_value is not None:\n        return default_value\n    \n    if value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return value", "import os\nimport re\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    match = re.match(r'\\$\\{([a-zA-Z_][a-zA-Z0-9_]*)\\s*([:-])(.*)\\}', matcher.group(0))\n    if not match:\n        return matcher.group(0)\n    \n    var_name = match.group(1)\n    default_value = match.group(3)\n    \n    # If default value is provided, use it if the environment variable is not set\n    if default_value:\n        env_value = os.environ.get(var_name, default_value)\n        return env_value\n    \n    # If no default value is provided, raise an error if the environment variable is not set\n    env_value = os.environ.get(var_name)\n    if env_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value provided\")\n    \n    return env_value"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9949, "num_retrieved": 59, "inference_time": 1.2387736588716507, "generate_results": ["import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parsed = urllib.parse.urlsplit(image_href)\n        \n        # Validate required components\n        if not parsed.netloc:\n            raise ValueError(\"Invalid image href: missing network location\")\n        \n        # Extract image_id from path (last part of path after any leading path)\n        path = parsed.path or ''\n        image_id = path.split('/')[-1] if path else ''\n        \n        # Validate image_id is a valid UUID\n        if not image_id or not uuidutils.is_uuid_like(image_id):\n            raise ValueError(\"Invalid image href: path does not contain a valid UUID\")\n        \n        # Determine if SSL is used\n        use_ssl = parsed.scheme == 'https'\n        \n        return (image_id, parsed.netloc, use_ssl)\n    \n    except Exception as e:\n        raise ValueError(f\"Error parsing image href: {str(e)}\")", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n    \n    parsed = urllib.parse.urlparse(image_href)\n    \n    # Validate required components\n    if not parsed.scheme or not parsed.netloc:\n        raise ValueError(\"Invalid image href: missing scheme or netloc\")\n    \n    use_ssl = parsed.scheme in ('https', 'http')\n    \n    # Extract image_id from path\n    path = parsed.path or ''\n    image_id = None\n    \n    # Handle cases where path might be empty or contain only a trailing slash\n    if path:\n        # Split by '/' and take the last part\n        path_parts = path.strip('/').split('/')\n        if path_parts:\n            image_id = path_parts[-1]\n    \n    # Validate image_id is present\n    if not image_id or not imageutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image href: missing or malformed image_id\")\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n    \n    parsed = urllib.parse.urlsplit(image_href)\n    \n    if not parsed.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parsed.scheme in ('https', 'http')\n    \n    # Extract image_id from the path\n    path_parts = parsed.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    image_id = path_parts[-1]\n    \n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image ID format\")\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n    \n    parsed = urllib.parse.urlparse(image_href)\n    \n    if not parsed.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    use_ssl = parsed.scheme == 'https'\n    \n    # Extract image_id from path\n    path_parts = parsed.path.strip('/').split('/')\n    if not path_parts:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    image_id = path_parts[-1]\n    \n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image ID format\")\n    \n    return (image_id, parsed.netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    if not image_href:\n        raise ValueError(\"Image href cannot be empty\")\n\n    try:\n        parsed = urllib.parse.urlsplit(image_href)\n    except Exception as e:\n        raise ValueError(f\"Invalid image href format: {str(e)}\")\n\n    if not parsed.netloc:\n        raise ValueError(\"Image href must have a network location (netloc)\")\n\n    use_ssl = parsed.scheme == 'https'\n    netloc = parsed.netloc\n    \n    # Extract image_id from path\n    path_parts = parsed.path.strip('/').split('/', 1)\n    if not path_parts:\n        raise ValueError(\"Image href must have a valid path component\")\n    \n    image_id = path_parts[-1]\n    \n    # Validate image_id is a valid UUID\n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(f\"Invalid image ID format: {image_id}\")\n    \n    return (image_id, netloc, use_ssl)"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8973, "num_retrieved": 32, "inference_time": 1.2387736588716507, "generate_results": ["import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '/'):\n            processed_list.append(item[:-1])\n        else:\n            processed_list.append(item)\n    \n    return processed_list", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '/'):\n            processed_list.append(item[:-1])\n        else:\n            processed_list.append(item)\n    \n    return processed_list", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.path.sep):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 2440, "num_retrieved": 10, "inference_time": 1.2387736588716507, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literally\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literally\n    pattern = pattern.replace(' ', r'\\s*')\n    # Escape special regex characters\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = pattern.replace(' ', r'\\s*')\n    # Replace special regex characters with escaped versions\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace special regex characters with escaped versions\n    pattern = re.escape(pattern)\n    return pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Replace spaces with escaped space to match literal spaces\n    pattern = re.escape(pattern)\n    # Add word boundaries to match exact matches\n    pattern = r'\\b' + pattern + r'\\b'\n    return pattern"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9777, "num_retrieved": 60, "inference_time": 1.2387736588716507, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Prepare environment if provided\n        if env is None:\n            env = os.environ.copy()\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            timeout=None  # No timeout specified\n        )\n        \n        # Check for errors\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n                if result.stderr:\n                    print(f\"Error output: {result.stderr}\")\n            return result.returncode\n        \n        return 0\n        \n    except subprocess.TimeoutExpired:\n        if verbose:\n            print(\"Command timed out\")\n        return 1\n    except FileNotFoundError:\n        if verbose:\n            print(f\"Command not found: {' '.join(cmd)}\")\n        return 1\n    except Exception as e:\n        if verbose:\n            print(f\"Unexpected error: {str(e)}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Prepare environment if provided\n        env_dict = env or os.environ.copy()\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env_dict,\n            timeout=None\n        )\n        \n        # Check for errors\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n                if result.stderr:\n                    print(f\"Error output: {result.stderr}\")\n            return result.returncode\n        \n        return 0\n        \n    except subprocess.TimeoutExpired:\n        if verbose:\n            print(\"Command timed out\")\n        return 1\n    except FileNotFoundError as e:\n        if verbose:\n            print(f\"Command not found: {e}\")\n        return 1\n    except Exception as e:\n        if verbose:\n            print(f\"Unexpected error: {e}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Prepare the environment if provided\n        if env is None:\n            env = os.environ.copy()\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            args=args,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            check=False\n        )\n        \n        # Return the result code\n        return result.returncode\n        \n    except Exception as e:\n        # Return non-zero error code if an exception occurs\n        return -1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Prepare the environment if provided\n        if env is None:\n            env = os.environ.copy()\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            args=args,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            timeout=None\n        )\n        \n        # Check for errors\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n            if result.stderr:\n                print(f\"Error output: {result.stderr}\")\n            return result.returncode\n        \n        return 0\n        \n    except subprocess.TimeoutExpired:\n        if verbose:\n            print(\"Command timed out\")\n        return 1\n    except FileNotFoundError:\n        if verbose:\n            print(f\"Command not found: {' '.join(cmd)}\")\n        return 1\n    except Exception as e:\n        if verbose:\n            print(f\"Unexpected error: {str(e)}\")\n        return 1", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Prepare the environment if provided\n        if env is None:\n            env = os.environ.copy()\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE if not hide_stderr else None,\n            text=True,\n            env=env,\n            timeout=300  # 5 minute timeout\n        )\n        \n        # Check for errors\n        if result.returncode != 0:\n            if verbose:\n                print(f\"Command failed with return code {result.returncode}\")\n                if result.stderr:\n                    print(f\"Error output: {result.stderr}\")\n            return result.returncode\n        \n        return 0\n        \n    except subprocess.TimeoutExpired:\n        if verbose:\n            print(\"Command timed out\")\n        return 1\n    except FileNotFoundError as e:\n        if verbose:\n            print(f\"Command not found: {e}\")\n        return 1\n    except Exception as e:\n        if verbose:\n            print(f\"Unexpected error: {e}\")\n        return 1"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 1.2387736588716507, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9766, "num_retrieved": 66, "inference_time": 1.2387736588716507, "generate_results": ["def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    # A root is a node that has no parent in the hierarchy\n    for s, p, o in graph:\n        if p == prop:\n            # If this node has a parent (o), it's not a root\n            # If this node has no parent (o is None), it's a root\n            if o is None:\n                roots.add(s)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    # A root is a node that has no parent in the hierarchy\n    for s, p, o in graph:\n        if p == prop:\n            # If this node has a parent (o), it's not a root\n            # If this node has no parent (o is None), it's a root\n            if o is None:\n                roots.add(s)\n    \n    # For each node that has the property, check if it's already in roots\n    # If not, it's a root\n    for s, p, o in graph:\n        if p == prop and o is None:\n            roots.add(s)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    # A root is a node that has no parent in the hierarchy\n    for s, p, o in graph:\n        if p == prop:\n            # If this node has a parent (o), it's not a root\n            # If this node has no parent (o is None), it's a root\n            if o is None:\n                roots.add(s)\n    \n    # For each node that has the property, check if it's already a root\n    # We need to find all nodes that are not pointed to by any other node\n    # via the given property\n    \n    # Create a set of all nodes that are parents (objects) of some triple\n    parents = set()\n    for s, p, o in graph:\n        if p == prop:\n            parents.add(o)\n    \n    # Find all nodes that are not in the parents set - these are roots\n    # We need to find all nodes that are not pointed to by any other node\n    # via the given property\n    \n    # Get all nodes that have the property as subject\n    children = set()\n    for s, p, o in graph:\n        if p == prop:\n            children.add(s)\n    \n    # Find all nodes that are not in the children set - these are roots\n    # Actually, we want nodes that have no parent, so we need to find nodes\n    # that are not in the parents set\n    \n    # The roots are the nodes that are not parents\n    # So we need to find all nodes that are not in the parents set\n    # But we also need to consider that a node might be a child of another node\n    \n    # Actually, the roots are the nodes that have no parent\n    # So we need to find all nodes that are", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    # A root is a node that has no parent in the hierarchy\n    for s, p, o in graph:\n        if p == prop:\n            # If this node is the parent of another node, it's not a root\n            # We need to find nodes that are not pointed to by any other node\n            # So we check if there's any node that has this node as its parent\n            # If not, then this node is a root\n            pass\n    \n    # Instead, we can find all nodes that are not the object of any triple with predicate prop\n    # These are the roots\n    seen = set()\n    for s, p, o in graph:\n        if p == prop:\n            seen.add(o)\n    \n    # All nodes that are not in seen are roots\n    all_nodes = set()\n    for s, p, o in graph:\n        all_nodes.add(s)\n        all_nodes.add(o)\n    \n    # Roots are nodes that are not the object of any triple with predicate prop\n    roots = all_nodes - seen\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have the given property with no object (i.e., roots)\n    # A root is a node that has no parent in the hierarchy\n    for s, p, o in graph:\n        if p == prop:\n            # If this node has a parent (o), it's not a root\n            # If this node has no parent (o is None), it's a root\n            if o is None:\n                roots.add(s)\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9371, "num_retrieved": 34, "inference_time": 1.2387736588716507, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 7074, "num_retrieved": 39, "inference_time": 1.2387736588716507, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9123, "num_retrieved": 35, "inference_time": 1.2387736588716507, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9014, "num_retrieved": 38, "inference_time": 1.2387736588716507, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9064, "num_retrieved": 38, "inference_time": 1.5892279148101807, "generate_results": ["import logging\nimport os\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    if not config_filenames:\n        return ({}, ())\n    \n    config_dict = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            schema_filename = schema_filename()\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename, \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n    \n    return (config_dict, logs)", "import logging\nimport os\nfrom typing import Tuple, Dict, Any, Sequence\n\ndef load_configurations(config_filenames: Sequence[str], overrides: Sequence[str] = None, resolve_env: bool = True) -> Tuple[Dict[str, Dict[str, Any]], Sequence[logging.LogRecord]]:\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configurations = {}\n    logs = []\n\n    for config_filename in config_filenames:\n        try:\n            schema_filename = schema_filename()\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename, \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configurations[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except PermissionError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Permission denied when reading configuration file {config_filename}',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n\n    return configurations, logs", "import logging\nimport os\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    if not config_filenames:\n        return ({}, ())\n    \n    configurations = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            schema_filename = schema_filename()\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename, \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configurations[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n    \n    return (configurations, logs)", "import logging\nimport os\nimport yaml\nfrom itertools import chain\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    if not config_filenames:\n        return ({}, ())\n    \n    config_dict = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            # Load the configuration file\n            schema_filename = schema_filename()\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename, \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except Validation_error as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Validation error in configuration file {config_filename}: {error}',\n                args=(),\n                exc_info=None\n            ))\n    \n    return (config_dict, tuple(logs))", "import logging\nimport os\nimport yaml\nfrom itertools import chain\nfrom collections import defaultdict\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    if not config_filenames:\n        return ({}, ())\n    \n    config_dict = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            # Load the configuration file\n            schema_filename = schema_filename()\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename, \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            config_dict[config_filename] = config\n            logs.extend(config_logs)\n        except FileNotFoundError as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Configuration file {config_filename} not found',\n                args=(),\n                exc_info=None\n            ))\n        except Exception as error:\n            logs.append(logging.LogRecord(\n                name='borgmatic',\n                level=logging.ERROR,\n                pathname=str(error),\n                lineno=0,\n                msg=f'Error parsing configuration file {config_filename}: {str(error)}',\n                args=(),\n                exc_info=None\n            ))\n    \n    return (config_dict, tuple(logs))"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9474, "num_retrieved": 60, "inference_time": 1.5892279148101807, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\n    in case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8851, "num_retrieved": 26, "inference_time": 1.5892279148101807, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create the time object\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        tz_offset_minutes, tz_offset_seconds = divmod(tz.utcoffset(datetime(1970, 1, 1)).total_seconds(), 60)\n        if tz_offset_seconds != 0:\n            raise ValueError(\"Bolt protocol does not support sub-second UTC offsets\")\n        zone = pytz.FixedOffset(tz_offset_minutes)\n        time_obj = zone.localize(time_obj)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create the time object\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        tz_offset_minutes, tz_offset_seconds = divmod(tz.utcoffset(None).total_seconds(), 60)\n        zone = pytz.FixedOffset(tz_offset_minutes)\n        time_obj = zone.localize(time_obj)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create the time object\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # Apply timezone if provided\n    if tz is not None:\n        tz_offset_minutes, tz_offset_seconds = divmod(tz.utcoffset(None).total_seconds(), 60)\n        zone = pytz.FixedOffset(tz_offset_minutes)\n        time_obj = zone.localize(time_obj)\n    \n    return time_obj", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a Time object with the parsed hours, minutes, seconds, and nanoseconds\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # If timezone is provided, localize the time\n    if tz is not None:\n        if isinstance(tz, datetime.tzinfo):\n            # For timezone objects, we need to localize the time\n            # However, Time objects don't have a tzinfo attribute directly\n            # So we create a datetime with the time and localize it\n            from datetime import datetime\n            dt = datetime(1970, 1, 1, hours, minutes, seconds, nanoseconds, tzinfo=tz)\n            # Extract the time part and re-create it\n            time_obj = Time(hours, minutes, seconds, nanoseconds)\n            # Apply timezone localization\n            localized_time = tz.localize(datetime(1970, 1, 1, hours, minutes, seconds, nanoseconds))\n            # Since we're only creating a Time object, we just return the time part\n            # The timezone info is not stored in the Time object directly\n            # So we just return the time without timezone\n        else:\n            # Handle integer timezone offset\n            tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\n            zone = pytz.FixedOffset(tz_offset", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise TypeError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    # Create a Time object with the parsed hours, minutes, seconds, and nanoseconds\n    time_obj = Time(hours, minutes, seconds, nanoseconds)\n    \n    # If timezone is provided, localize the time\n    if tz is not None:\n        if isinstance(tz, datetime.tzinfo):\n            time_obj = tz.localize(time_obj)\n        else:\n            # Handle integer timezone offset (minutes)\n            tz_offset_minutes, _ = divmod(tz, 60)\n            zone = pytz.FixedOffset(tz_offset_minutes)\n            time_obj = zone.localize(time_obj)\n    \n    return time_obj"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8599, "num_retrieved": 19, "inference_time": 1.5892279148101807, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we see support for Bolt 3 and Bolt 4.x\n    if protocol_version is None:\n        # Return all supported versions\n        handlers[Version(3, 0)] = Bolt3\n        handlers[Version(4, 1)] = Bolt4x1\n        handlers[Version(4, 0)] = Bolt4x0\n    else:\n        # Return only the handler for the specified version if supported\n        if protocol_version == Version(3, 0):\n            handlers[protocol_version] = Bolt3\n        elif protocol_version == Version(4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == Version(4, 0):\n            handlers[protocol_version] = Bolt4x0\n        # Otherwise, no handler is available\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we have:\n    # - Bolt3: supported by Neo4j versions 3.5, 4.0, 4.1, 4.2, 4.3, 4.4\n    # - Bolt4x1: supported by Neo4j versions 4.1 - 4.4\n    \n    # Add Bolt 3 handler (version 3, 0)\n    handlers[(3, 0)] = Bolt3\n    \n    # Add Bolt 4.1 handler (version 4, 1)\n    handlers[(4, 1)] = Bolt4x1\n    \n    # If protocol_version is specified, filter the dictionary\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we have:\n    # - Bolt3: supported by Neo4j 3.5, 4.0, 4.1, 4.2, 4.3, 4.4\n    # - Bolt4x1: supported by Neo4j 4.1 - 4.4\n    \n    # Add Bolt 3 handler (version 3, 0)\n    handlers[(3, 0)] = Bolt3\n    \n    # Add Bolt 4.1 handler (version 4, 1)\n    handlers[(4, 1)] = Bolt4x1\n    \n    # If protocol_version is specified, filter the dictionary\n    if protocol_version is not None:\n        if protocol_version in handlers:\n            return {protocol_version: handlers[protocol_version]}\n        else:\n            return {}\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we see support for Bolt 3 and Bolt 4.x\n    if protocol_version is None:\n        # Return all supported versions\n        handlers[Version(3, 0)] = Bolt3\n        handlers[Version(4, 1)] = Bolt4x1\n        handlers[Version(4, 0)] = Bolt4x0\n    else:\n        # Return only the handler for the specified version if supported\n        if protocol_version == Version(3, 0):\n            handlers[protocol_version] = Bolt3\n        elif protocol_version == Version(4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == Version(4, 0):\n            handlers[protocol_version] = Bolt4x0\n        # Otherwise, no handler is available for the specified version\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None and not isinstance(protocol_version, tuple):\n        raise TypeError(\"protocol_version must be a tuple or None\")\n\n    handlers = {}\n    \n    # Define supported protocol versions and their corresponding handlers\n    # Based on the context, we see support for Bolt 3 and Bolt 4.x\n    if protocol_version is None:\n        # Return all supported versions\n        handlers[Version(3, 0)] = Bolt3\n        handlers[Version(4, 0)] = Bolt4x0\n        handlers[Version(4, 1)] = Bolt4x1\n        handlers[Version(4, 2)] = Bolt4x2\n        handlers[Version(4, 3)] = Bolt4x3\n        handlers[Version(4, 4)] = Bolt4x4\n    else:\n        # Return only the handler for the specified version if supported\n        if protocol_version == Version(3, 0):\n            handlers[protocol_version] = Bolt3\n        elif protocol_version == Version(4, 0):\n            handlers[protocol_version] = Bolt4x0\n        elif protocol_version == Version(4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == Version(4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == Version(4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == Version(4, 4):\n            handlers[protocol_version] = Bolt4x4\n        else:\n            # Version not supported\n            handlers = {}\n    \n    return handlers"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8668, "num_retrieved": 22, "inference_time": 1.5892279148101807, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9302, "num_retrieved": 49, "inference_time": 1.5892279148101807, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, type(seq))\n    \n    # If seq is not a sequence, try to convert it to one\n    # Based on context, we assume it might be a source or a simple value\n    # For now, we return the original seq as the sequence and its type\n    return (seq, type(seq))", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, type(seq))\n    \n    # If seq is not a sequence, try to convert it to one\n    # Based on context, we assume it might be a source or another element\n    # that needs to be wrapped in a sequence\n    if hasattr(seq, \"__iter__\"):\n        # If it's iterable, we might need to create a sequence from it\n        # For now, we'll return the original seq as a sequence\n        return (seq, type(seq))\n    \n    # If it's a simple value, wrap it in a sequence\n    # This is a fallback - in practice, the actual sequence type would depend on context\n    return (seq, type(seq))", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, type(seq))\n    \n    # If seq is not a sequence, try to convert it to one\n    # Based on context, we assume it might be a source or a simple value\n    # For simplicity, if it's not a sequence, we return it as is with its type\n    return (seq, type(seq))", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, type(seq))\n    \n    # If seq is not a sequence, try to convert it to one\n    # Based on context, we assume it might be a source or another element\n    # that needs to be wrapped in a sequence\n    if hasattr(seq, \"__iter__\"):\n        # If it's iterable, we assume it's a sequence of elements\n        # and we return a sequence wrapper\n        return (seq, type(seq))\n    \n    # If it's a simple value, wrap it in a sequence\n    # This is a fallback - in practice, this might need more context\n    return (seq, type(seq))", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena_sequence.LenaSequence):\n        return (seq, type(seq))\n    \n    # If seq is not a sequence, try to convert it to one\n    # Based on context, we assume it might be a source or another element\n    # that needs to be wrapped in a sequence\n    if hasattr(seq, \"__iter__\"):\n        # If it's iterable, we might need to create a sequence from it\n        # For now, we'll return the original seq as is with its type\n        return (seq, type(seq))\n    \n    # If it's a simple value, wrap it in a sequence if needed\n    # This is a fallback - in practice, the actual sequence type would be determined by context\n    return (seq, type(seq))"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8447, "num_retrieved": 16, "inference_time": 1.5892279148101807, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Only compute if not already computed\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram\n        self._scale = other\n        # Update the histogram data if needed\n        # This would depend on the specific histogram implementation\n        # For now, just set the scale value", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Only compute if not already computed and not asked to recompute\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram to the given value\n        self._scale = other\n        # Update the histogram data if needed\n        # This would depend on the specific implementation of the histogram\n        # For now, we just store the scale value", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Only compute if not already computed and not explicitly asked to recompute\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram to the given value\n        self._scale = other\n        # Update the histogram data if needed\n        # This would depend on the specific histogram implementation\n        # For now, just set the scale value", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Only compute if not already computed and not asked to recompute\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram to other value\n        self._scale = other\n        # Update the histogram data if needed\n        # This would depend on the specific implementation of the histogram\n        # For now, just set the scale value\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\n    If *other* is ``None``, return scale of this histogram.\n    If its scale was not computed before,\n    it is computed and stored for subsequent use\n    (unless explicitly asked to *recompute*).\n    Note that after changing (filling) the histogram\n    one must explicitly recompute the scale\n    if it was computed before.\n\n    If a float *other* is provided, rescale self to *other*.\n\n    Histograms with scale equal to zero can't be rescaled.\n    :exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if not recompute:\n                # Only compute if not already computed and not asked to recompute\n                self._scale = self._compute_scale()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram to the given value\n        self._scale = other\n        # Update the histogram data if needed\n        # This would depend on the specific implementation of the histogram\n        # For now, just store the new scale value"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8404, "num_retrieved": 14, "inference_time": 1.5892279148101807, "generate_results": ["def scale(self, other=None):\n    \"\"\"Get or set the scale of the graph.\n\n    If *other* is None, return the scale of this graph.\n\n    If a numeric *other* is provided, rescale to that value.\n    If the graph has unknown or zero scale, rescaling that will raise :exc:`~.LenaValueError`.\n\n    To get meaningful results, graph's fields are used.\n    Only the last coordinate is rescaled.\n    For example, if the graph has *x* and *y* coordinates,\n    then *y* will be rescaled, and for a 3-dimensional graph\n    *z* will be rescaled.\n    All errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the coordinates of the graph\n    coords = self.coords\n    \n    # Only rescale the last coordinate\n    if len(coords) > 0:\n        # Get the last coordinate (which will be the one to rescale)\n        last_coord = coords[-1]\n        \n        # Rescale the last coordinate and its associated errors\n        # This assumes that the graph has a method to rescale its data\n        # and that the scale is stored in a property called _scale\n        self._scale = other\n        \n        # If the graph has errors, rescale them as well\n        if hasattr(self, 'errors') and self.errors is not None:\n            # Rescale errors in the same way as the coordinate\n            # This is a simplified implementation - actual implementation\n            # would depend on the specific graph structure\n            pass\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"Get or set the scale of the graph.\n\n    If *other* is None, return the scale of this graph.\n\n    If a numeric *other* is provided, rescale to that value.\n    If the graph has unknown or zero scale, rescaling that will raise :exc:`~.LenaValueError`.\n\n    To get meaningful results, graph's fields are used.\n    Only the last coordinate is rescaled.\n    For example, if the graph has *x* and *y* coordinates,\n    then *y* will be rescaled, and for a 3-dimensional graph\n    *z* will be rescaled.\n    All errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the coordinates of the graph\n    coords = self.coords\n    \n    # Only rescale the last coordinate\n    if coords:\n        last_coord_index = len(coords) - 1\n        # Rescale the last coordinate and its associated errors\n        for i, coord in enumerate(coords[last_coord_index]):\n            if hasattr(coord, 'scale'):\n                coord.scale(other)\n    \n    # Set the scale of the graph\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"Get or set the scale of the graph.\n\n    If *other* is None, return the scale of this graph.\n\n    If a numeric *other* is provided, rescale to that value.\n    If the graph has unknown or zero scale, rescaling that will raise :exc:`~.LenaValueError`.\n\n    To get meaningful results, graph's fields are used.\n    Only the last coordinate is rescaled.\n    For example, if the graph has *x* and *y* coordinates,\n    then *y* will be rescaled, and for a 3-dimensional graph\n    *z* will be rescaled.\n    All errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the coordinates of the graph\n    coords = self.coords\n    \n    # Only rescale the last coordinate\n    if coords:\n        # Get the last coordinate (index -1)\n        last_coord = coords[-1]\n        \n        # Rescale the last coordinate and its associated errors\n        # Assuming the graph has a method to scale its data\n        # and that the scale is stored in self._scale\n        self._scale = other\n        \n        # Apply the scale to the last coordinate and its errors\n        # This would typically involve modifying the data in the graph\n        # For now, we just update the scale attribute\n        return self._scale\n    \n    # If no coordinates, raise an error\n    raise lena.core.LenaValueError(\"graph has no coordinates to scale\")", "def scale(self, other=None):\n    \"\"\"Get or set the scale of the graph.\n\n    If *other* is None, return the scale of this graph.\n\n    If a numeric *other* is provided, rescale to that value.\n    If the graph has unknown or zero scale, rescaling that will raise :exc:`~.LenaValueError`.\n\n    To get meaningful results, graph's fields are used.\n    Only the last coordinate is rescaled.\n    For example, if the graph has *x* and *y* coordinates,\n    then *y* will be rescaled, and for a 3-dimensional graph\n    *z* will be rescaled.\n    All errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the coordinates of the graph\n    coords = self.coords\n    \n    # Only rescale the last coordinate\n    if coords:\n        last_coord_index = len(coords) - 1\n        # Rescale the last coordinate and all associated errors\n        for i, coord in enumerate(coords[last_coord_index]):\n            if hasattr(coord, 'scale'):\n                coord.scale(other)\n    \n    # Set the scale of the graph\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"Get or set the scale of the graph.\n\n    If *other* is None, return the scale of this graph.\n\n    If a numeric *other* is provided, rescale to that value.\n    If the graph has unknown or zero scale, rescaling that will raise :exc:`~.LenaValueError`.\n\n    To get meaningful results, graph's fields are used.\n    Only the last coordinate is rescaled.\n    For example, if the graph has *x* and *y* coordinates,\n    then *y* will be rescaled, and for a 3-dimensional graph\n    *z* will be rescaled.\n    All errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale to the provided value\n    if not isinstance(other, (int, float)):\n        raise lena.core.LenaValueError(\"scale must be a number\")\n    \n    # Get the coordinates of the graph\n    coords = self.coords\n    \n    # Only rescale the last coordinate\n    if coords:\n        # Get the last coordinate (index -1)\n        last_coord_idx = len(coords) - 1\n        \n        # Rescale the last coordinate and its associated errors\n        # Assuming the graph has a method to scale its data\n        # and that the scale is stored in self._scale\n        if self._scale is None or self._scale == 0:\n            raise lena.core.LenaValueError(\"cannot rescale to zero or unknown scale\")\n        \n        # Apply the scaling to the last coordinate\n        # This would typically involve modifying the data in the graph\n        # For now, we just update the scale attribute\n        self._scale = other\n        \n        # If the graph has errors, they should be rescaled too\n        # This would depend on the specific implementation of the graph class\n        # For now, we just update the scale\n        \n        return self._scale\n    \n    raise lena.core.LenaValueError(\"graph has no coordinates\")"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8307, "num_retrieved": 10, "inference_time": 1.5892279148101807, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    from lena.structures import graph\n    from lena.structures.histogram import histogram\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names\n    if len(field_names) == 0:\n        raise ValueError(\"field_names must not be empty\")\n    \n    # Determine the dimension of the graph\n    dim = len(field_names)\n    \n    # Validate that make_value returns a value with correct dimension\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Create coordinate values based on get_coordinate\n    coordinates = []\n    for i in range(len(hist.edges)):\n        if i == 0:\n            # For 1D histograms, use the edge values\n            if get_coordinate == \"left\":\n                coord = hist.edges[i][0]\n            elif get_coordinate == \"right\":", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    from lena.structures import graph\n    from lena.structures.histogram import histogram\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names length matches histogram dimension\n    if hasattr(hist, 'dim'):\n        if len(field_names) != hist.dim:\n            raise ValueError(f\"field_names length ({len(field_names)}) must match histogram dimension ({hist.dim})\")\n    \n    # Determine coordinates based on get_coordinate\n    coordinates = []\n    edges = hist.edges\n    \n    if hist.dim == 1:\n        # For 1D histogram, use the edge coordinate\n        if get_coordinate == \"left\":\n            coord = [edges[0][i] for i in range(len(edges[0]))]\n        elif get_coordinate == \"right\":\n            coord = [edges[0][i+1] for i in range(len", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    from lena.structures import graph\n    from lena.structures.histogram import histogram\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise lena.core.LenaValueError(\n            'get_coordinate must be one of \"left\", \"right\" or \"middle\"; '\n            '\"{}\" provided'.format(get_coordinate)\n        )\n    \n    # Validate field_names\n    if len(field_names) == 0:\n        raise lena.core.LenaValueError(\"field_names must not be empty\")\n    \n    # Validate make_value\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Get bin edges and create coordinates\n    edges = hist.edges\n    \n    # Determine the dimension of the histogram\n    dim = len(edges)\n    \n    # Create coordinate values based on get_coordinate\n    coordinates = []\n    for i in range(dim):\n        if dim == 1:\n            # For 1D histogram\n            if get_coordinate == \"", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    from lena.structures import graph\n    from lena.structures.histogram import histogram\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names length matches histogram dimension\n    if hasattr(hist, 'dim'):\n        if len(field_names) != hist.dim:\n            raise ValueError(f\"field_names length ({len(field_names)}) must match histogram dimension ({hist.dim})\")\n    \n    # Determine coordinates based on get_coordinate\n    coordinates = []\n    edges = hist.edges\n    \n    if hasattr(hist, 'dim'):\n        if hist.dim == 1:\n            # For 1D histogram, use the bin edges\n            if get_coordinate == \"left\":\n                coords = [edges[0][0]]\n            elif get_coordinate == \"right\":\n                coords = [edges[0][-1]]\n            else:  # middle\n                coords", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    from lena.structures import graph\n    from lena.structures.histogram import histogram\n    \n    # Validate get_coordinate\n    if get_coordinate not in [\"left\", \"right\", \"middle\"]:\n        raise ValueError(f'get_coordinate must be one of \"left\", \"right\" or \"middle\"; \"{get_coordinate}\" provided')\n    \n    # Validate field_names length matches histogram dimension\n    if hasattr(hist, 'dim'):\n        if len(field_names) != hist.dim:\n            raise ValueError(f\"field_names length ({len(field_names)}) must match histogram dimension ({hist.dim})\")\n    \n    # Set default make_value if not provided\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Determine coordinate based on get_coordinate\n    if hasattr(hist, 'edges'):\n        edges = hist.edges\n        if hist.dim == 1:\n            # For 1D histogram\n            if get_coordinate == \"left\":\n                coordinate = edges[0]\n            elif get_coordinate == \"right\":\n                coordinate = edges"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8546, "num_retrieved": 18, "inference_time": 1.5892279148101807, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n       If multiple methods or attributes are invalid, all such errors\n       are collected and reported. Previously, only the first error was reported.\n       As a special case, if only one such error is present, it is raised\n       alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Missing method {name!r} required by interface {iface!r}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        method = getattr(candidate, name, None)\n        if method is not None:\n            # Check if method has correct signature (simplified - actual signature checking would be more complex)\n            try:\n                # This is a simplified check - in practice, signature verification would require more detailed analysis\n                pass\n            except Exception as e:\n                errors.append(f\"Method {name!r} has incorrect signature: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Missing attribute {name!r", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Missing method {name!r} required by interface {iface!r}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        method = getattr(candidate, name, None)\n        if method is not None:\n            # Check if method has correct signature (simplified - actual signature checking would be more complex)\n            try:\n                # This is a simplified check - in practice, signature verification would involve\n                # inspecting the method's signature and comparing it to the interface's method signature\n                pass\n            except Exception as e:\n                errors.append(f\"Method {name!r} has incorrect signature: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Missing method {name!r} required by interface {iface!r}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        method = getattr(candidate, name, None)\n        if method is not None:\n            # Check if method has correct signature (simplified - actual signature checking would be more complex)\n            try:\n                # This is a simplified check - in practice, signature checking would involve inspecting \n                # the method's parameters, return type, etc.\n                pass\n            except Exception as e:\n                errors.append(f\"Method {name!r} has invalid signature: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Missing attribute {name", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for method_name in iface:\n        if not hasattr(candidate, method_name):\n            errors.append(f\"Missing method {method_name!r} required by interface {iface!r}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for method_name in iface:\n        method = getattr(candidate, method_name, None)\n        if method is not None:\n            # Check if method signature matches (simplified - actual signature checking would be more complex)\n            try:\n                # This is a simplified check - in practice, signature validation would be more detailed\n                pass\n            except TypeError as e:\n                errors.append(f\"Method {method_name!r} has incorrect signature: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for attr_name in iface:\n        if not hasattr(candidate, attr_name):\n            errors.append(f\"Missing attribute {attr_name!r} required", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Candidate does not claim to provide interface {iface!r}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Missing method {name!r} required by interface {iface!r}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        method = getattr(candidate, name, None)\n        if method is not None:\n            # Check if method has correct signature (simplified - actual signature checking would be more complex)\n            try:\n                # This is a simplified check - in practice, signature verification would require more detailed analysis\n                pass\n            except Exception as e:\n                errors.append(f\"Method {name!r} has incorrect signature: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Missing attribute {name!r} required by interface {iface!r}\")"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8546, "num_retrieved": 18, "inference_time": 1.5892279148101807, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Object {candidate} does not define method {name}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        if hasattr(candidate, name):\n            try:\n                method = getattr(candidate, name)\n                # Check if method signature matches (simplified - actual signature checking would be more complex)\n                # This is a placeholder - actual signature checking would require inspecting the method's signature\n                pass\n            except Exception as e:\n                errors.append(f\"Method {name} in object {candidate} has invalid signature: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface:\n        if hasattr(iface, name) and not hasattr(candidate, name):\n            errors.append(f\"Object {candidate} does not define attribute {name}\")\n\n    # Report errors", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Object {candidate} does not define method {name}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        if hasattr(candidate, name):\n            try:\n                method = getattr(candidate, name)\n                # Check if method signature matches interface requirements\n                # This is a simplified check - actual signature validation would require \n                # more detailed inspection of method parameters and return types\n                # For now, we just verify the method exists and is callable\n                if not callable(method):\n                    errors.append(f\"Object {candidate} has non-callable method {name}\")\n            except Exception as e:\n                errors.append(f\"Error accessing method {name} on {candidate}: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface:\n        if hasattr(iface", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Object {candidate} does not define method {name}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        if hasattr(candidate, name):\n            try:\n                method = getattr(candidate, name)\n                # Check if method signature matches interface specification\n                # This is a simplified check - actual signature validation would require\n                # more detailed inspection of method parameters and return types\n                # which is beyond the scope of this implementation\n                pass\n            except Exception as e:\n                errors.append(f\"Method {name} in object {candidate} has invalid signature: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Object {candidate} does not define attribute {name}\")\n\n    #", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check that candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Object {candidate} does not define method {name}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        if hasattr(candidate, name):\n            method = getattr(candidate, name)\n            # Check if method has correct signature (simplified - actual signature checking would be more complex)\n            try:\n                # This is a simplified check - in practice, signature checking would involve inspecting the method's signature\n                # For now, we just verify the method exists and is callable\n                if not callable(method):\n                    errors.append(f\"Method {name} is not callable in object {candidate}\")\n            except Exception as e:\n                errors.append(f\"Error checking method signature for {name}: {e}\")\n\n    # Step 4: Check that candidate defines all necessary attributes\n    for name in iface:", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the interface using ``iface.providedBy`` \n      (unless *tentative* is `True`, in which case this step is skipped). This means that the \n      candidate's class declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>` the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be checked passed.\n    :raises zope.interface.Invalid: If any of the previous conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors are collected and reported. \n        Previously, only the first error was reported. As a special case, if only one such error \n        is present, it is raised alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(f\"Object {candidate} does not claim to provide interface {iface}\")\n\n    # Step 2: Check if candidate defines all necessary methods\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Object {candidate} does not define method {name}\")\n\n    # Step 3: Check method signatures (to the extent possible)\n    for name in iface:\n        if hasattr(candidate, name):\n            try:\n                method = getattr(candidate, name)\n                # Check if method signature matches interface specification\n                # This is a simplified check - actual signature validation would require\n                # more detailed inspection of method parameters and return types\n                # which is beyond the scope of this implementation\n                pass\n            except Exception as e:\n                errors.append(f\"Method {name} in object {candidate} has invalid signature: {e}\")\n\n    # Step 4: Check if candidate defines all necessary attributes\n    for name in iface:\n        if not hasattr(candidate, name):\n            errors.append(f\"Object {candidate} does not define attribute {name}\")\n\n    #"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10208, "num_retrieved": 75, "inference_time": 1.5892279148101807, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative=tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative=tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative=tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative=tentative)", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    from zope.interface.verify import verifyClass\n    return verifyClass(iface, candidate, tentative=tentative)"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10034, "num_retrieved": 70, "inference_time": 1.5892279148101807, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # Default to the standard metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base class has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # Default to the standard metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9404, "num_retrieved": 50, "inference_time": 1.5892279148101807, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        self.__delitem__(key)\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        self.__delitem__(key)\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        self.__delitem__(key)\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        self.__delitem__(key)\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\"\"\"\n    try:\n        value = self[key]\n        self.__delitem__(key)\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9226, "num_retrieved": 44, "inference_time": 1.5892279148101807, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 1.5892279148101807, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 1.73289255797863, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9214, "num_retrieved": 43, "inference_time": 1.73289255797863, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return a random (key, value) pair.\"\"\"\n    import random\n    keys = list(self.__data.keys())\n    if not keys:\n        raise KeyError('%s is empty' % type(self).__name__)\n    key = random.choice(keys)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random (key, value) pair.\n    \"\"\"\n    if not self:\n        raise KeyError('cache is empty')\n    \n    # Get all keys and select a random one\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    \n    value = self.__data.pop(key)\n    del self.__size[key]\n    self.__currsize -= self.__size[key]\n    \n    return (key, value)", "import random\n\ndef popitem(self):\n    \"\"\"\n    Remove and return a random (key, value) pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    value = self[key]\n    del self[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"Remove and return a random (key, value) pair.\"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    keys_list = list(self.__order.keys())\n    key = random.choice(keys_list)\n    value = self.pop(key)\n    return (key, value)", "import random\n\ndef popitem(self):\n    \"\"\"\n    Remove and return a random (key, value) pair.\n    \"\"\"\n    if not self:\n        raise KeyError('cache is empty')\n    \n    keys_list = list(self.keys())\n    key = random.choice(keys_list)\n    value = self.pop(key)\n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8986, "num_retrieved": 31, "inference_time": 1.73289255797863, "generate_results": ["def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the parameter style\n    # This pattern should match in-style parameters according to the style's param_regex\n    # The pattern will match escape sequences, named parameters, numeric parameters, and percent placeholders\n    pattern = self.param_regex\n    \n    # Add escape sequence handling if escape_char is defined\n    if self.escape_char:\n        # Escape the escape character in the pattern to avoid matching it literally\n        escape_char = re.escape(self.escape_char)\n        # Create a pattern that matches escape sequences\n        escape_pattern = f\"({escape_char}(?P<escape>[^{escape_char}]+))\"\n        # Replace the param_regex with the escape pattern\n        pattern = pattern.replace(f\"({self.escape_char}\", f\"({escape_char}\")\n    \n    # Ensure the pattern matches the in-style parameters\n    # The pattern should match named, numeric, or ordinal parameters\n    # and handle escape sequences properly\n    \n    # Compile the pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the parameter style\n    # This pattern will match in-style parameters according to the style's param_regex\n    # and escape sequence\n    pattern = self.param_regex\n    \n    # If there's an escape character, we need to account for it in the pattern\n    # The escape sequence is matched using the escape_regex\n    if self.escape_char:\n        # Escape sequences are matched using escape_regex\n        # We need to ensure that the pattern accounts for the escape character\n        # and the escape sequence\n        escape_pattern = self.escape_regex\n        # The pattern should match either a parameter or an escape sequence\n        # The parameter is matched by param_regex, and escape sequences are matched by escape_regex\n        # We need to ensure that the escape sequence is properly handled\n        # The pattern should be constructed to match the parameter style\n        # and escape sequences\n        pattern = f\"({escape_pattern})|({pattern})\"\n    \n    # Compile the pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a regex pattern that matches in-style parameters\n    # This pattern should match the parameter format defined by the in_style.param_regex\n    # It should also handle escape sequences and out percent placeholders\n    pattern = self._in_style.param_regex\n    \n    # Add escape sequence handling\n    escape_regex = self._in_style.escape_regex\n    if escape_regex:\n        # Escape sequences are matched with the escape_char followed by the escape_regex\n        pattern = f\"({escape_regex})|({pattern})\"\n    \n    # Add out percent placeholder handling\n    # Out percent is matched as a literal % and needs to be escaped\n    pattern = pattern.replace('%', '\\\\%')\n    \n    # Compile the pattern\n    import re\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a regex pattern that matches parameter placeholders\n    # based on the parameter style (named, numeric, or ordinal)\n    pattern = self.param_regex\n    \n    # Escape the escape character to prevent it from being matched\n    # as part of the parameter pattern\n    escape_char = self.escape_char\n    if escape_char:\n        escape_pattern = f\"\\\\{escape_char}\"\n        pattern = pattern.replace(escape_pattern, f\"(?:{escape_pattern})\")\n    \n    # Ensure the pattern matches the parameter format\n    # This is a placeholder implementation - actual pattern would depend\n    # on the specific parameter style (named, numeric, ordinal)\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Construct the regular expression pattern based on the parameter style\n    # This pattern will match in-style parameters according to the style's param_regex\n    # and escape sequence\n    pattern = self.param_regex\n    \n    # If there's an escape character, we need to account for escape sequences\n    if self.escape_char:\n        # Escape the escape character itself to avoid matching it in the pattern\n        escape_char = re.escape(self.escape_char)\n        # Create a pattern that matches escape sequences\n        escape_pattern = f\"({escape_char}(?P<escape>{escape_char})|{escape_char}(?P<out_percent>\\\\%))\"\n        # Combine the pattern with the parameter regex\n        pattern = f\"({escape_pattern})|({pattern})\"\n    \n    # Compile the pattern\n    return re.compile(pattern)"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9134, "num_retrieved": 36, "inference_time": 1.73289255797863, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the appropriate converter based on the in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create the appropriate converter based on style combinations\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the appropriate converter based on the in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create the appropriate converter based on style combinations\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter(\n            escape_char=self._escape_char,\n            expand_tuples=self._expand_tuples,\n            in_regex=self._in_regex,\n            in_style=in_style,\n            out_style=out_style\n        )\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8334, "num_retrieved": 11, "inference_time": 1.73289255797863, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8400, "num_retrieved": 14, "inference_time": 1.73289255797863, "generate_results": ["def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``", "def parse(self, timestr, default=None, ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8800, "num_retrieved": 29, "inference_time": 1.73289255797863, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous and in a fold state\n    if hasattr(self, 'is_ambiguous') and callable(self.is_ambiguous):\n        is_ambiguous = self.is_ambiguous(dt)\n        fold = self._fold(dt)\n        \n        # If ambiguous and in a fold, resolve to the appropriate index\n        if is_ambiguous and fold:\n            idx = self._find_last_transition(dt)\n            if idx is not None:\n                # Adjust index based on fold state\n                idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n                idx -= idx_offset\n                # Apply the transition to get the correct time\n                dt = dt.replace(tzinfo=None)\n                dt = dt.replace(tzinfo=self)\n                dt = dt.astimezone(self)\n                dt = dt.replace(tzinfo=None)\n                return dt\n        else:\n            # For non-ambiguous or non-fold cases, just convert\n            dt = dt.replace(tzinfo=None)\n            dt = dt.replace(tzinfo=self)\n            dt = dt.astimezone(self)\n            dt = dt.replace(tzinfo=None)\n            return dt\n    \n    # Default behavior for non-ambiguous cases\n    dt = dt.replace(tzinfo=None)\n    dt = dt.replace(tzinfo=self)\n    dt = dt.astimezone(self)\n    dt = dt.replace(tzinfo=None)\n    return dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous and in a fold state\n    if hasattr(self, 'is_ambiguous') and callable(self.is_ambiguous):\n        is_ambiguous = self.is_ambiguous(dt)\n        fold = self._fold(dt)\n        \n        # If ambiguous and in a fold, resolve to the appropriate index\n        if is_ambiguous and fold:\n            idx = self._find_last_transition(dt)\n            if idx is not None:\n                # Adjust index based on fold state\n                idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n                return self._resolve_ambiguous_time(dt)\n    \n    # For non-ambiguous cases or when no is_ambiguous method is available,\n    # simply return the datetime with the appropriate offset\n    return dt + self.utcoffset(dt)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous and in a fold state\n    if hasattr(self, 'is_ambiguous') and callable(self.is_ambiguous):\n        is_ambiguous = self.is_ambiguous(dt)\n        fold = self._fold(dt)\n        \n        # If ambiguous and in a fold, resolve to the appropriate index\n        if is_ambiguous and fold:\n            idx = self._find_last_transition(dt)\n            if idx is not None:\n                # Adjust index based on fold\n                idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n                idx -= idx_offset\n                # Create new datetime with adjusted offset\n                dt = dt.replace(tzinfo=None)\n                dt = dt.replace(tzinfo=self).astimezone(UTC).astimezone(self)\n                dt = dt.replace(tzinfo=None)\n                return dt\n        # If not ambiguous or not in fold, just convert normally\n    # Fall back to standard conversion\n    dt = dt.replace(tzinfo=None)\n    dt = dt.replace(tzinfo=self).astimezone(UTC).astimezone(self)\n    dt = dt.replace(tzinfo=None)\n    return dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous and in a fold state\n    if hasattr(self, 'is_ambiguous') and callable(self.is_ambiguous):\n        is_ambiguous = self.is_ambiguous(dt)\n        fold = self._fold(dt)\n        \n        # If ambiguous and in a fold, resolve to the appropriate index\n        if is_ambiguous and fold:\n            idx = self._find_last_transition(dt)\n            if idx is not None:\n                # Adjust index based on fold state\n                idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n                idx -= idx_offset\n                # Apply the transition to get the correct time\n                dt = dt.replace(tzinfo=None)\n                dt = dt.replace(tzinfo=self)\n                dt = dt.astimezone(UTC).astimezone(self)\n                dt = dt.replace(tzinfo=None)\n                return dt\n        else:\n            # For non-ambiguous cases, simply convert\n            dt = dt.replace(tzinfo=None)\n            dt = dt.replace(tzinfo=self)\n            dt = dt.astimezone(UTC).astimezone(self)\n            dt = dt.replace(tzinfo=None)\n            return dt\n    \n    # Default behavior for non-ambiguous cases\n    dt = dt.replace(tzinfo=None)\n    dt = dt.replace(tzinfo=self)\n    dt = dt.astimezone(UTC).astimezone(self)\n    dt = dt.replace(tzinfo=None)\n    return dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # Check if the datetime is ambiguous and in a fold state\n    if hasattr(self, 'is_ambiguous') and callable(self.is_ambiguous):\n        is_ambiguous = self.is_ambiguous(dt)\n        fold = self._fold(dt)\n        \n        # If ambiguous and in a fold, resolve to the appropriate index\n        if is_ambiguous and fold:\n            idx = self._find_last_transition(dt)\n            if idx is not None:\n                # Adjust index based on fold state\n                idx_offset = int(not fold and self.is_ambiguous(dt, idx))\n                idx -= idx_offset\n                # Apply the transition to get the correct time\n                dt = dt.replace(tzinfo=None)\n                dt = dt.replace(tzinfo=self)\n                dt = dt.astimezone(UTC).astimezone(self)\n                dt = dt.replace(tzinfo=None)\n                return dt\n        else:\n            # For non-ambiguous or non-fold cases, just convert\n            dt = dt.replace(tzinfo=None)\n            dt = dt.replace(tzinfo=self)\n            dt = dt.astimezone(UTC).astimezone(self)\n            dt = dt.replace(tzinfo=None)\n            return dt\n    \n    # Default behavior for non-ambiguous cases\n    dt = dt.replace(tzinfo=None)\n    dt = dt.replace(tzinfo=self)\n    dt = dt.astimezone(UTC).astimezone(self)\n    dt = dt.replace(tzinfo=None)\n    return dt"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8512, "num_retrieved": 17, "inference_time": 1.73289255797863, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        dt if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9069, "num_retrieved": 35, "inference_time": 1.73289255797863, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\n\n    Args:\n        before: Split file before these delimiters.\n        after: Split file after these delimiters.\n    \"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9268, "num_retrieved": 42, "inference_time": 1.73289255797863, "generate_results": ["def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n    \n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'private':\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    if request.headers.get('X-Diaspora-Message') or request.headers.get('X-Diaspora-Content'):\n        return \"diaspora\"\n    \n    # If no clear Diaspora indicators, try to identify via protocol\n    try:\n        # Use identify_protocol_by_request to determine the protocol\n        protocol = identify_protocol_by_request(request)\n        if hasattr(protocol, 'PROTOCOL_NAME') and protocol.PROTOCOL_NAME == \"diaspora\":\n            return \"diaspora\"\n    except NoSuitableProtocolFoundError:\n        pass\n    \n    return None", "def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n    \n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'private':\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    if request.headers.get('X-Diaspora-Auth') or request.headers.get('X-Diaspora-Nonce'):\n        return \"diaspora\"\n    \n    # If no clear Diaspora indicators, fall back to protocol identification\n    # This would typically use the identify_protocol_by_request function\n    try:\n        found_protocol = identify_protocol_by_request(request)\n        if hasattr(found_protocol, 'PROTOCOL_NAME') and found_protocol.PROTOCOL_NAME == \"diaspora\":\n            return \"diaspora\"\n    except Exception:\n        pass\n    \n    return None", "def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n    \n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'private':\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    if request.headers.get('X-Diaspora-Envelope') or request.headers.get('X-Diaspora-Signature'):\n        return \"diaspora\"\n    \n    # If none of the above, try to identify using the protocol method\n    try:\n        # Use identify_protocol_by_request to determine the protocol\n        protocol = identify_protocol_by_request(request)\n        if hasattr(protocol, 'PROTOCOL_NAME') and protocol.PROTOCOL_NAME == \"diaspora\":\n            return \"diaspora\"\n    except Exception:\n        pass\n    \n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'private':\n        return \"diaspora\"\n    \n    # Check if it's a legacy payload with Diaspora-specific headers\n    if request.headers.get('X-Diaspora-Handle'):\n        return \"diaspora\"\n    \n    # Check if the request body contains Diaspora-specific XML structure\n    if request.body and b'<XML' in request.body:\n        return \"diaspora\"\n    \n    # If none of the above, it's not a Diaspora request\n    return None", "def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n    \n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if it's a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'public':\n        return \"diaspora\"\n    \n    # Check if it's a private message (legacy payload)\n    if request.headers.get('X-Diaspora-Message-Type') == 'private':\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    if request.headers.get('X-Diaspora-Content-Type') == 'xml':\n        return \"diaspora\"\n    \n    # Check if the body contains a Diaspora-specific signature or structure\n    body = request.body\n    if body and b'<post>' in body and b'<XML>' in body:\n        return \"diaspora\"\n    \n    # If none of the above, it's not a Diaspora request\n    return None"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9384, "num_retrieved": 47, "inference_time": 1.73289255797863, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related patterns\n    url = request.url\n    if url.startswith('https://matrix.') or url.startswith('http://matrix.'):\n        return True\n    \n    # Check if the request headers contain Matrix-related information\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check if the request method is one that Matrix typically uses\n    if request.method in ['POST', 'GET']:\n        # Matrix typically uses specific endpoints like /_matrix/client/r0\n        if '/_matrix/client/r0/' in url or '/_matrix/client/r0' in url:\n            return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related patterns or if headers indicate Matrix\n    # Matrix requests typically use specific URLs or headers that can be detected\n    url = request.url\n    \n    # Matrix requests often have URLs that end with .matrix or contain matrix-related paths\n    if url and any(url.endswith(suffix) for suffix in ['.matrix', '/matrix/', '/_matrix/']):\n        return True\n    \n    # Check for Matrix-specific headers\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Matrix requests might have specific content types\n    content_type = request.headers.get('Content-Type', '')\n    if 'application/matrix' in content_type or 'application/x-matrix' in content_type:\n        return True\n    \n    # Matrix requests might have specific methods\n    if request.method in ['POST', 'PUT'] and any(keyword in request.url for keyword in ['matrix', 'room', 'client']):\n        return True\n    \n    # If none of the above, return False\n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related patterns or if it's a known Matrix endpoint\n    url = request.url\n    if url.startswith('https://matrix.org') or url.startswith('https://matrix.org/') or \\\n       url.startswith('https://localhost:8448') or url.startswith('https://localhost:8448/'):\n        return True\n    \n    # Check for Matrix-specific paths like /_matrix/client/r0/ or /_matrix/federation/r0/\n    if '/_matrix/client/r0/' in url or '/_matrix/federation/r0/' in url:\n        return True\n    \n    # Check if the request headers contain Matrix-specific information\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check if the request method is one that Matrix typically uses\n    if request.method in ['POST', 'GET'] and '/_matrix/' in url:\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related patterns\n    url = request.url\n    if url.startswith('https://matrix.') or url.startswith('http://matrix.'):\n        return True\n    \n    # Check if the request headers contain matrix-related information\n    if request.headers.get('X-Matrix-Client-Id'):\n        return True\n    \n    # Check if the request method is one that Matrix typically uses\n    if request.method in ['POST', 'GET']:\n        # Matrix typically uses specific endpoints\n        if request.path.startswith('/_matrix/'):\n            return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related patterns or if it's a known Matrix endpoint\n    url = request.url\n    if url.startswith('https://matrix.org') or url.startswith('https://matrix.org/') or \\\n       url.startswith('https://localhost:8448') or url.startswith('https://localhost:8448/'):\n        return True\n    \n    # Check for Matrix-specific paths or query parameters\n    if '/matrix/' in url or '/_matrix/' in url or '/_matrix/client' in url or '/_matrix/server' in url:\n        return True\n    \n    # Check if the request headers contain Matrix-related information\n    if request.headers.get('X-Matrix-Client') or request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check if the request method is one that Matrix typically uses\n    if request.method in ['POST', 'GET'] and any(x in request.url for x in ['/_matrix/', '/matrix/']):\n        return True\n    \n    return False"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9464, "num_retrieved": 47, "inference_time": 1.73289255797863, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8926, "num_retrieved": 30, "inference_time": 1.73289255797863, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith(\"```\"):\n            # Inside code block - skip tag processing\n            result_lines.append(line)\n            continue\n            \n        # Process line for tags\n        # Remove tags that are inside code blocks\n        # Tags are in the format #tag\n        # We need to find #tag and replace with replacer(tag) if replacer is provided\n        # But we need to skip if inside code block\n        \n        # Find all hashtags\n        import re\n        tag_pattern = r'#(\\w+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        # Process each match\n        processed_line = line\n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                replacement = replacer(tag)\n                processed_line = processed_line.replace(f\"#{tag}\", replacement)\n        \n        result_lines.append(processed_line)\n    \n    # Join lines back into text\n    result_text = \"\\n\".join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block, skip processing\n            result_lines.append(line)\n            continue\n            \n        # Process non-code lines\n        # Find all tags (words starting with #) that are not inside code blocks\n        # Use regex to find hashtags that are not inside code blocks\n        import re\n        # Pattern to match hashtags that are not inside code blocks\n        # We'll use a simple approach: find hashtags that are not preceded by a backtick\n        # and not inside a code block\n        tag_pattern = r'#([a-zA-Z0-9_\\-]+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        # Process each match\n        line_with_tags = line\n        for match in matches:\n            tag = match.group(1)\n            # Add tag to set\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag with the result of the replacer function\n            if replacer:\n                replacement = replacer(tag)\n                line_with_tags = line_with_tags.replace(f'#{tag}', replacement)\n        \n        result_lines.append(line_with_tags)\n    \n    # Join lines back into text\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith(\"```\"):\n            # Inside code block - skip tag processing\n            result_lines.append(line)\n            continue\n            \n        # Process line for tags\n        # Use regex to find tags (e.g., #tag) that are not inside code blocks\n        # Tags are preceded by # and followed by a non-alphanumeric character or end of string\n        import re\n        tag_pattern = r'#([a-zA-Z0-9_\\-]+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        line_result = line\n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            if replacer:\n                # Replace the tag with the result of the replacer function\n                replacement = replacer(tag)\n                line_result = line_result.replace(f\"#{tag}\", replacement)\n        \n        result_lines.append(line_result)\n    \n    result_text = \"\\n\".join(result_lines)\n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block, skip processing\n            result_lines.append(line)\n            continue\n            \n        # Check if line contains a tag (starts with #)\n        # We need to handle tags that might be inside other text\n        # Use regex to find hashtags that are not inside code blocks\n        import re\n        # Pattern to match hashtags that are not inside code blocks\n        # We'll use a simple approach: find # followed by non-space characters\n        # and ensure it's not inside a code block\n        tag_pattern = r'#([^\\s\\W]+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        # Process each match\n        line_parts = []\n        for match in matches:\n            tag = match.group(1).lower()\n            # Add the tag to our set\n            tags.add(tag)\n            \n            # Replace the tag with the replacer function if provided\n            if replacer:\n                replacement = replacer(tag)\n                line_parts.append(line[:match.start()] + replacement + line[match.end():])\n            else:\n                line_parts.append(line[:match.start()] + line[match.end():])\n        \n        # If no tags found, just add the original line\n        if not line_parts:\n            result_lines.append(line)\n        else:\n            # Join the parts back together\n            result_lines.append(''.join(line_parts))\n    \n    # Join the result lines back into a single string\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with three or more backticks)\n        if line.strip().startswith('```'):\n            # Inside code block - skip tag processing\n            result_lines.append(line)\n            continue\n            \n        # Process line for tags\n        # Remove any existing tags (e.g., #tag) and extract them\n        # We need to handle the case where tags might be inside other text\n        # and avoid matching tags inside code blocks\n        \n        # Find all hashtags in the line\n        import re\n        tag_pattern = r'#([a-zA-Z0-9_\\-]+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        # Process each match\n        line_with_tags = line\n        for match in matches:\n            tag = match.group(1)\n            if tag:\n                tags.add(tag)\n                # If replacer is provided, replace the tag with the result of the replacer function\n                if replacer:\n                    replacement = replacer(tag)\n                    line_with_tags = line_with_tags.replace(f'#{tag}', replacement)\n        \n        result_lines.append(line_with_tags)\n    \n    # Join lines back into a single string\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9620, "num_retrieved": 53, "inference_time": 1.73289255797863, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    if not text:\n        return text\n    \n    # Replace textual links with HTML links\n    # Pattern to match text like [text](url)\n    pattern = r'\\[([^\\]]+)\\]\\((https?://[^\\s\\)]+)\\)'\n    def replace_link(match):\n        text_content = match.group(1)\n        url = match.group(2)\n        # Return HTML link with attributes\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{text_content}</a>'\n    \n    # Replace markdown links\n    result = re.sub(pattern, replace_link, text)\n    \n    # Also handle direct URLs in text (e.g., http://example.com)\n    url_pattern = r'(https?://[^\\s\\)]+)'\n    result = re.sub(url_pattern, lambda m: f'<a href=\"{m.group(0)}\" target=\"_blank\" rel=\"noopener noreferrer\">{m.group(0)}</a>', result)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    if not text:\n        return text\n    \n    # Replace textual links with HTML links\n    # Pattern to match text like [text](url)\n    pattern = r'\\[([^\\]]+)\\]\\((https?://[^\\s\\)]+)\\)'\n    def replace_link(match):\n        text = match.group(1)\n        url = match.group(2)\n        # Return HTML link with attributes\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{text}</a>'\n    \n    # Replace markdown links\n    result = re.sub(pattern, replace_link, text)\n    \n    # Also handle direct URLs in text (e.g., https://example.com)\n    # Pattern to match URLs in text\n    url_pattern = r'(https?://[^\\s\\)]+)'\n    result = re.sub(url_pattern, lambda m: f'<a href=\"{m.group(1)}\" target=\"_blank\" rel=\"noopener noreferrer\">{m.group(1)}</a>', result)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace textual links with HTML links with proper attributes\n    # This is a simplified implementation based on common patterns\n    import re\n    \n    # Pattern to match text links like [text](url)\n    pattern = r'\\[([^\\]]+)\\]\\((https?://[^\\s\\)]+)\\)'\n    \n    def replace_link(match):\n        text = match.group(1)\n        url = match.group(2)\n        # Return HTML link with target and rel attributes\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{text}</a>'\n    \n    # Replace all matches\n    result = re.sub(pattern, replace_link, text)\n    \n    # Also handle direct URLs in text (e.g., https://example.com)\n    # Convert them to clickable links\n    url_pattern = r'(https?://[^\\s\\)]+)'\n    result = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', result)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    if not text:\n        return text\n    \n    # Replace textual links with HTML links\n    # Pattern to match text like [text](url)\n    pattern = r'\\[([^\\]]+)\\]\\((https?://[^\\s\\)]+)\\)'\n    def replace_link(match):\n        text_content = match.group(1)\n        url = match.group(2)\n        # Return as HTML link with attributes\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{text_content}</a>'\n    \n    # Replace URLs in text that are not already in brackets\n    # Pattern to match URLs like http://example.com\n    url_pattern = r'(https?://[^\\s\\)]+)'\n    def replace_url(match):\n        url = match.group(1)\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{url}</a>'\n    \n    # First process links in [text](url) format\n    result = re.sub(pattern, replace_link, text)\n    \n    # Then process standalone URLs\n    result = re.sub(url_pattern, replace_url, result)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    if not text:\n        return text\n    \n    # Replace textual links with HTML links\n    # Pattern to match text like [text](url)\n    pattern = r'\\[([^\\]]+)\\]\\((https?://[^\\s\\)]+)\\)'\n    def replace_link(match):\n        text = match.group(1)\n        url = match.group(2)\n        # Add attributes like target=\"_blank\" and rel=\"noopener\"\n        return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{text}</a>'\n    \n    # Replace links in the text\n    result = re.sub(pattern, replace_link, text)\n    \n    # Also handle direct URLs in text (e.g., https://example.com)\n    # Pattern to match URLs that are not in brackets\n    url_pattern = r'(https?://[^\\s\\)]+)'\n    result = re.sub(url_pattern, lambda m: f'<a href=\"{m.group(1)}\" target=\"_blank\" rel=\"noopener\">{m.group(1)}</a>', result)\n    \n    return result"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9635, "num_retrieved": 53, "inference_time": 1.73289255797863, "generate_results": ["def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    import requests\n    try:\n        response = requests.head(url, timeout=5)\n        if response.status_code == 200:\n            return response.headers.get('content-type')\n    except requests.exceptions.RequestException:\n        pass\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9715, "num_retrieved": 55, "inference_time": 1.73289255797863, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not be empty\n    if len(tag.strip()) == 0:\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Only allow alphanumeric characters and underscores\n    if not re.match(r'^[a-zA-Z0-9_]+$', tag):\n        return False\n    \n    # Tags should not start or end with underscore\n    if tag.startswith('_') or tag.endswith('_'):\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(tag) > 32:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not be empty\n    if len(tag) == 0:\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Only allow alphanumeric characters and underscores\n    if not re.match(r'^[a-zA-Z0-9_]+$', tag):\n        return False\n    \n    # Tags should not start or end with underscore\n    if tag.startswith('_') or tag.endswith('_'):\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(tag) > 32:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not be empty\n    if len(tag.strip()) == 0:\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Only allow alphanumeric characters and underscores\n    if not re.match(r'^[a-zA-Z0-9_]+$', tag):\n        return False\n    \n    # Tags should not start or end with underscore\n    if tag.startswith('_') or tag.endswith('_'):\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(tag) > 32:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not be empty\n    if len(tag.strip()) == 0:\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Only allow alphanumeric characters and underscores\n    if not re.match(r'^[a-zA-Z0-9_]+$', tag):\n        return False\n    \n    # Tags should not start or end with underscore\n    if tag.startswith('_') or tag.endswith('_'):\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(tag) > 32:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should not be empty\n    if len(tag.strip()) == 0:\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Only allow alphanumeric characters and underscores\n    if not re.match(r'^[a-zA-Z0-9_]+$', tag):\n        return False\n    \n    # Tags should not start or end with underscore\n    if tag.startswith('_') or tag.endswith('_'):\n        return False\n    \n    # Tags should not be too long (reasonable limit)\n    if len(tag) > 32:\n        return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8959, "num_retrieved": 31, "inference_time": 1.3292386531829834, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8690, "num_retrieved": 23, "inference_time": 1.3292386531829834, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\n    Basically we've already verified the sender is who they say when receiving the payload. However, the sender might\n    be trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\n    AND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9090, "num_retrieved": 36, "inference_time": 1.3292386531829834, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"links\": {\n            \"self\": f\"{url}/.well-known/nodeinfo\",\n            \"api\": f\"{url}/api/v1\"\n        },\n        \"protocols\": [\n            \"diaspora\"\n        ],\n        \"metadata\": {\n            \"description\": \"Diaspora Federation Node\",\n            \"url\": url,\n            \"host\": url.split(\"//\")[1].split(\"/\")[0]\n        }\n    }\n    \n    # If a custom document path is provided, update the links\n    if document_path:\n        document[\"links\"][\"self\"] = f\"{url}{document_path}\"\n    \n    return document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"protocols\": [\n            \"https://diaspora.github.io/diaspora_federation/federation/protocols/diaspora/\"\n        ],\n        \"links\": {\n            \"self\": url.rstrip(\"/\") + \"/.well-known/nodeinfo\",\n            \"about\": url.rstrip(\"/\") + \"/about\",\n            \"api\": url.rstrip(\"/\") + \"/api\",\n            \"public\": url.rstrip(\"/\") + \"/public\",\n            \"private\": url.rstrip(\"/\") + \"/private\"\n        },\n        \"metadata\": {\n            \"description\": \"Diaspora Federation Node\",\n            \"title\": \"Diaspora Node\",\n            \"url\": url,\n            \"host\": url.split(\"//\")[1].split(\"/\")[0]\n        }\n    }\n    \n    if document_path:\n        document[\"links\"][\"nodeinfo\"] = url.rstrip(\"/\") + document_path\n    else:\n        document[\"links\"][\"nodeinfo\"] = url.rstrip(\"/\") + \"/.well-known/nodeinfo\"\n    \n    return document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"protocols\": [\"diaspora\"],\n        \"links\": {\n            \"self\": url.rstrip(\"/\") + \"/.well-known/nodeinfo\",\n            \"api\": url.rstrip(\"/\") + \"/api\",\n            \"webfinger\": url.rstrip(\"/\") + \"/.well-known/webfinger\"\n        },\n        \"metadata\": {\n            \"description\": \"Diaspora Federation Node\",\n            \"url\": url,\n            \"host\": url.split(\"//\")[1].split(\"/\")[0]\n        },\n        \"services\": {\n            \"public\": {\n                \"description\": \"Public services\",\n                \"url\": url.rstrip(\"/\") + \"/public\"\n            }\n        }\n    }\n    \n    if document_path:\n        document[\"links\"][\"nodeinfo\"] = url.rstrip(\"/\") + document_path\n    \n    return document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"protocols\": [\n            \"https://diaspora.github.io/diaspora_federation/federation/protocols/diaspora/\"\n        ],\n        \"links\": {\n            \"self\": url.rstrip(\"/\") + \"/.well-known/nodeinfo\",\n            \"api\": url.rstrip(\"/\") + \"/api\",\n            \"public\": url.rstrip(\"/\") + \"/public\",\n            \"private\": url.rstrip(\"/\") + \"/private\"\n        },\n        \"metadata\": {\n            \"description\": \"Diaspora Federation Node\",\n            \"title\": \"Diaspora Node\",\n            \"url\": url,\n            \"host\": url.split(\"//\")[1].split(\"/\")[0]\n        }\n    }\n    \n    if document_path:\n        document[\"links\"][\"nodeinfo\"] = url.rstrip(\"/\") + document_path\n    else:\n        document[\"links\"][\"nodeinfo\"] = url.rstrip(\"/\") + \"/.well-known/nodeinfo\"\n    \n    return document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"links\": {\n            \"self\": f\"{url}/.well-known/nodeinfo\",\n            \"api\": f\"{url}/api/v1\",\n            \"public\": f\"{url}/public\"\n        },\n        \"protocols\": [\n            \"diaspora\"\n        ],\n        \"metadata\": {\n            \"description\": \"Diaspora Federation Node\",\n            \"url\": url,\n            \"contact\": \"https://diaspora.software\"\n        }\n    }\n    \n    if document_path:\n        document[\"links\"][\"nodeinfo\"] = f\"{url}{document_path}\"\n    \n    return document"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8966, "num_retrieved": 31, "inference_time": 1.3292386531829834, "generate_results": ["def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Cipher import PKCS1_v1_5\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        import base64\n        \n        # Convert public key to RSA key object\n        rsa_key = RSA.import_key(public_key)\n        \n        # Create hash of the document content\n        doc_text = etree.tostring(doc, encoding='ascii')\n        sig_contents = doc_text + \".\" + base64.b64encode(b\"application/xml\").decode(\"ascii\") + \".\" + \\\n                       base64.b64encode(b\"base64url\").decode(\"ascii\") + \".\" + \\\n                       base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        \n        sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Verify the signature using the public key\n        cipher = PKCS1_v1_5.new(rsa_key)\n        try:\n            cipher.verify(sig_hash, base64.urlsafe_b64decode(signature))\n            return True\n        except ValueError:\n            return False\n            \n    except Exception as e:\n        logger.error(f\"Error verifying relayable signature: {e}\")\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Cipher import PKCS1_v1_5\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        import base64\n        \n        # Convert public key to RSA key object\n        rsa_key = RSA.import_key(public_key)\n        \n        # Create hash of the document content\n        doc_text = etree.tostring(doc, encoding='ascii')\n        sig_contents = doc_text + \".\" + base64.b64encode(b\"application/xml\").decode(\"ascii\") + \".\" + \\\n                       base64.b64encode(b\"base64url\").decode(\"ascii\") + \".\" + \\\n                       base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        \n        sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Verify the signature using the public key\n        cipher = PKCS1_v1_5.new(rsa_key)\n        try:\n            cipher.verify(sig_hash, base64.urlsafe_b64decode(signature))\n            return True\n        except ValueError:\n            return False\n            \n    except Exception as e:\n        logger.warning(f\"Error verifying relayable signature: {e}\")\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Cipher import PKCS1_v1_5\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        import urlsafe_b64decode\n        \n        # Convert public key to RSA key object\n        rsa_key = RSA.import_key(public_key)\n        \n        # Create hash of the document content\n        doc_text = etree.tostring(doc, encoding='ascii')\n        sig_contents = doc_text + \".\" + b64encode(b\"application/xml\").decode(\"ascii\") + \".\" + \\\n                       b64encode(b\"base64url\").decode(\"ascii\") + \".\" + \\\n                       b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        \n        hash_obj = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Verify the signature using the public key\n        cipher = PKCS1_v1_5.new(rsa_key)\n        try:\n            cipher.verify(hash_obj, urlsafe_b64decode(signature))\n            return True\n        except Exception:\n            return False\n            \n    except Exception as e:\n        logger.error(f\"Error verifying relayable signature: {e}\")\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Cipher import PKCS1_v1_5\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        import base64\n        \n        # Convert public key to RSA key object\n        rsa_key = RSA.import_key(public_key)\n        \n        # Create a hash of the document content\n        doc_text = etree.tostring(doc, encoding='ascii')\n        sig_contents = doc_text + \".\" + base64.b64encode(b\"application/xml\").decode(\"ascii\") + \".\" + \\\n                       base64.b64encode(b\"base64url\").decode(\"ascii\") + \".\" + \\\n                       base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        \n        hash_obj = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Verify the signature using the public key\n        cipher = PKCS1_v1_5.new(rsa_key)\n        try:\n            cipher.verify(hash_obj, base64.urlsafe_b64decode(signature))\n            return True\n        except ValueError:\n            return False\n            \n    except Exception as e:\n        logger.error(f\"Error verifying relayable signature: {e}\")\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        from Crypto.PublicKey import RSA\n        from Crypto.Cipher import PKCS1_v1_5\n        from Crypto.Hash import SHA256\n        from lxml import etree\n        import base64\n        \n        # Convert public key to RSA key object\n        rsa_key = RSA.import_key(public_key)\n        \n        # Create a hash of the document content\n        doc_text = etree.tostring(doc, encoding='ascii')\n        sig_contents = doc_text + \".\" + base64.b64encode(b\"application/xml\").decode(\"ascii\") + \".\" + \\\n                       base64.b64encode(b\"base64url\").decode(\"ascii\") + \".\" + \\\n                       base64.b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        \n        hash_obj = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Verify the signature using the public key\n        cipher = PKCS1_v1_5.new(rsa_key)\n        verified_sig = cipher.verify(hash_obj, base64.urlsafe_b64decode(signature))\n        \n        return verified_sig\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9327, "num_retrieved": 43, "inference_time": 1.3292386531829834, "generate_results": ["import json\nimport re\nfrom typing import Dict, Optional\nfrom lxml import etree\nfrom federation.hostmeta.parsers import parse_nodeinfo_document, parse_nodeinfo2_document, parse_statisticsjson_document, parse_mastodon_document, parse_matrix_document, parse_misskey_document\nfrom federation.utils.network import fetch_document\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        doc = json.loads(document)\n        \n        # Extract required fields from JSON webfinger\n        result = {}\n        \n        # Subject field\n        subject = doc.get(\"subject\")\n        if subject:\n            # Extract handle from subject (acct:handle@domain)\n            match = re.match(r\"acct:(.*)@(.*)\", subject)\n            if match:\n                result[\"handle\"] = match.group(1)\n                result[\"domain\"] = match.group(2)\n        \n        # Aliases\n        aliases = doc.get(\"aliases\", [])\n        result[\"aliases\"] = aliases\n        \n        # Links\n        links = doc.get(\"links\", [])\n        result[\"links\"] = []\n        for link in links:\n            rel = link.get(\"rel\")\n            href = link.get(\"href\")\n            type_ = link.get(\"type\")\n            if rel and href:\n                result[\"links\"].append({\n                    \"rel\": rel,\n                    \"href\": href,\n                    \"type\": type_\n                })\n        \n        # If we have a handle and domain, we can also extract the profile URL\n        if result.get(\"handle\") and result.get(\"domain\"):\n            result[\"profile_url\"] = f\"https://{result['domain']}/profile/{result['handle']}\"\n        \n        return result\n        \n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        try:\n            # Parse XRD document\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject_element = root.find(\".//{http://webfinger.net/}subject\")\n            if subject_element is not None:\n                subject = subject_element.text\n                if subject and subject.startswith(\"acct:\"):\n                    handle_match = re.match(r\"acct:(.*)@(.*)\", subject", "import json\nimport re\nfrom typing import Dict, Optional\nfrom lxml import etree\nfrom federation.utils.text import decode_if_bytes\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        doc = json.loads(document)\n        # Extract required fields from JSON webfinger\n        result = {\n            \"subject\": None,\n            \"aliases\": [],\n            \"links\": []\n        }\n        \n        # Extract subject (acct:handle)\n        subject = doc.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"subject\"] = subject\n        \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        if aliases:\n            result[\"aliases\"] = [alias for alias in aliases if alias]\n        \n        # Extract links\n        links = doc.get(\"links\", [])\n        for link in links:\n            rel = link.get(\"rel\")\n            href = link.get(\"href\")\n            type_ = link.get(\"type\")\n            if href:\n                result[\"links\"].append({\n                    \"rel\": rel,\n                    \"href\": href,\n                    \"type\": type_\n                })\n        \n        return result\n        \n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        # Parse XML document\n        xml_doc = etree.fromstring(decode_if_bytes(document))\n        \n        # Extract subject from XRD\n        subject_elem = xml_doc.find(\".//{http://webfinger.net/}subject\")\n        if subject_elem is not None:\n            subject = subject_elem.text\n            if subject and subject.startswith(\"acct:\"):\n                result = {\n                    \"subject\": subject,\n                    \"aliases\": [],\n                    \"links\": []\n                }\n            else:\n                result = {\n                    \"subject\": None,\n                    \"aliases\": [],\n                    \"links\": []\n                }\n        else:\n            result = {\n                \"subject\": None,\n                \"aliases\": [],\n                \"links\": []\n            }\n        \n        # Extract aliases from XRD\n        aliases_elem = xml_doc.find(\".//{http://webfinger.net/}aliases\")\n        if aliases_elem is not None:\n            for alias in aliases_elem", "import json\nimport re\nfrom typing import Dict, Optional\nfrom lxml import etree\nfrom federation.utils.text import decode_if_bytes\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    # Try to parse as JSON first (new format)\n    try:\n        doc = json.loads(document)\n        # Extract required fields from JSON webfinger\n        result = {\n            \"subject\": None,\n            \"aliases\": [],\n            \"links\": []\n        }\n        \n        # Extract subject (acct:handle)\n        subject = doc.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"subject\"] = subject\n        \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        if isinstance(aliases, list):\n            result[\"aliases\"] = [alias for alias in aliases if alias]\n        \n        # Extract links\n        links = doc.get(\"links\", [])\n        if isinstance(links, list):\n            result[\"links\"] = []\n            for link in links:\n                if isinstance(link, dict):\n                    result[\"links\"].append({\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    })\n        \n        return result\n        \n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        # Parse XML content\n        xml_content = decode_if_bytes(document)\n        root = etree.fromstring(xml_content)\n        \n        # Extract subject from XRD\n        subject_elem = root.find(\".//{http://webfingerprint.org/schema/1.0}Subject\")\n        if subject_elem is not None and subject_elem.text:\n            result = {\n                \"subject\": \"acct:\" + subject_elem.text.split(\":\", 1)[1] if \":\" in subject_elem.text else subject_elem.text,\n                \"aliases\": [],\n                \"links\": []\n            }\n        else:\n            result = {\n                \"subject\": None,\n                \"aliases\": [],\n                \"links\": []\n            }\n        \n        # Extract aliases from XRD\n        aliases_elem = root.find(\".//{http://webfingerprint.org/schema/1.0}Aliases\")\n        if aliases_elem is not None:\n            for alias in", "import json\nimport re\nfrom typing import Dict, Optional\nfrom lxml import etree\nfrom federation.hostmeta.parsers import parse_nodeinfo_document, parse_nodeinfo2_document, parse_statisticsjson_document, parse_mastodon_document, parse_matrix_document, parse_misskey_document\nfrom federation.utils.network import fetch_document\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        doc = json.loads(document)\n        \n        # Extract required fields from JSON webfinger\n        result = {\n            \"subject\": None,\n            \"aliases\": [],\n            \"links\": []\n        }\n        \n        # Extract subject (acct:handle)\n        subject = doc.get(\"subject\")\n        if subject and subject.startswith(\"acct:\"):\n            result[\"subject\"] = subject\n        \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        if isinstance(aliases, list):\n            result[\"aliases\"] = [alias for alias in aliases if alias]\n        \n        # Extract links\n        links = doc.get(\"links\", [])\n        if isinstance(links, list):\n            result[\"links\"] = []\n            for link in links:\n                if isinstance(link, dict):\n                    result[\"links\"].append({\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    })\n        \n        return result\n        \n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        try:\n            # Parse XRD document\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject_element = root.find(\".//{http://webfingerprint.org/schema/1.0}Subject\")\n            if subject_element is not None and subject_element.text:\n                result = {\n                    \"subject\": \"acct:\" + subject_element.text.split(\":\", 1)[1] if \":\" in subject_element.text else subject_element.text,\n                    \"aliases\": [],\n                    \"links\": []\n                }\n            else:\n                result = {\n                    \"subject\": None,\n                    \"aliases\": [],\n                    \"links\": []\n                }\n            \n            # Extract aliases from XRD\n            aliases_element = root.find(\".//{http", "import json\nimport re\nfrom typing import Dict, Optional\nfrom lxml import etree\nfrom federation.hostmeta.parsers import parse_nodeinfo_document, parse_nodeinfo2_document, parse_statisticsjson_document, parse_mastodon_document, parse_matrix_document, parse_misskey_document\nfrom federation.utils.network import fetch_document\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        doc = json.loads(document)\n        \n        # Extract required fields from JSON webfinger\n        result = {}\n        \n        # Handle subject and aliases\n        subject = doc.get(\"subject\")\n        if subject:\n            # Extract handle from subject (e.g., \"acct:handle@domain\")\n            match = re.match(r\"acct:(.+?)@(.+)\", subject)\n            if match:\n                result[\"handle\"] = match.group(1)\n                result[\"domain\"] = match.group(2)\n        \n        # Extract aliases\n        aliases = doc.get(\"aliases\", [])\n        result[\"aliases\"] = aliases\n        \n        # Extract links\n        links = doc.get(\"links\", [])\n        result[\"links\"] = []\n        \n        for link in links:\n            rel = link.get(\"rel\")\n            href = link.get(\"href\")\n            type_ = link.get(\"type\")\n            if rel and href:\n                result[\"links\"].append({\n                    \"rel\": rel,\n                    \"href\": href,\n                    \"type\": type_\n                })\n        \n        # Extract additional fields\n        result[\"id\"] = doc.get(\"id\")\n        \n        return result\n        \n    except json.JSONDecodeError:\n        # Fall back to parsing XRD format (old format)\n        try:\n            # Parse XRD document\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject_elem = root.find(\".//{http://webfingerprint.org/schema/1.0}subject\")\n            if subject_elem is not None:\n                subject = subject_elem.text\n                match = re.match(r\"acct:(.+?)@(.+)\", subject)\n                if match:\n                    result = {\n                        \"handle\": match.group(1),\n                        \"domain\": match.group(2)\n                    }\n                else:\n                    result = {\"handle"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9505, "num_retrieved": 50, "inference_time": 1.3292386531829834, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the fetch_document utility\n        document, _, exception = fetch_document(host=handle.split(\"@\")[1] if \"@\" in handle else handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        return document\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the fetch_document utility\n        document, _, _ = fetch_document(host=handle.split(\"@\")[1] if \"@\" in handle else handle, path='/.well-known/webfinger')\n        return document\n    except Exception:\n        # Return None if retrieval fails, without raising an error\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the fetch_document utility\n        document, _, exception = fetch_document(host=handle.split(\"@\")[1] if \"@\" in handle else handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        return document\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the fetch_document utility\n        document, _, exception = fetch_document(host=handle.split(\"@\")[1] if \"@\" in handle else handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        return document\n    except Exception:\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the fetch_document utility\n        document, _, exception = fetch_document(host=handle.split(\"@\")[1] if \"@\" in handle else handle, path='/.well-known/webfinger')\n        if exception:\n            return None\n        return document\n    except Exception:\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9357, "num_retrieved": 43, "inference_time": 1.3292386531829834, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Extract the self link with type application/activity+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            # Return the parsed document with the profile ID\n            return {\n                \"handle\": handle,\n                \"profile_id\": link[\"href\"]\n            }\n    \n    logger.debug(\"retrieve_and_parse_diaspora_webfinger: found webfinger but it has no as2 self href\")\n    return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Extract the self link with type application/activity+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            # Return the parsed document with the profile ID\n            return {\n                \"handle\": handle,\n                \"profile_id\": link[\"href\"]\n            }\n    \n    logger.debug(\"retrieve_and_parse_diaspora_webfinger: found webfinger but it has no as2 self href\")\n    return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Extract the self link with type application/activity+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            # Return the parsed document with the profile ID\n            return {\n                \"handle\": handle,\n                \"profile_id\": link[\"href\"]\n            }\n    \n    logger.debug(\"retrieve_and_parse_diaspora_webfinger: found webfinger but it has no as2 self href\")\n    return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Extract the self link with type application/activity+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            # Return the parsed document with the profile ID\n            return {\n                \"handle\": handle,\n                \"profile_id\": link[\"href\"]\n            }\n    \n    logger.debug(\"retrieve_and_parse_diaspora_webfinger: found webfinger but it has no as2 self href\")\n    return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if not document:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Extract the self link with type application/activity+json\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            # Return the parsed document with the profile ID\n            return {\n                \"handle\": handle,\n                \"profile_id\": link[\"href\"]\n            }\n    \n    logger.debug(\"retrieve_and_parse_diaspora_webfinger: found webfinger but it has no as2 self href\")\n    return None"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9803, "num_retrieved": 58, "inference_time": 1.3292386531829834, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n        \n        # Create a DiasporaHostMeta instance with the parsed data\n        # Assuming the host-meta document contains a webfinger_host field\n        webfinger_host = doc.get('webfinger_host')\n        if not webfinger_host:\n            return None\n            \n        # Create the host-meta object\n        hostmeta = DiasporaHostMeta(webfinger_host=webfinger_host)\n        \n        return hostmeta\n        \n    except (json.JSONDecodeError, KeyError) as e:\n        logger.error(\"Failed to parse host-meta document: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n        \n        # Create a DiasporaHostMeta instance with the parsed data\n        # Assuming the host-meta document contains a webfinger_host field\n        webfinger_host = doc.get('webfinger_host')\n        if not webfinger_host:\n            return None\n            \n        # Create the host-meta object\n        hostmeta = DiasporaHostMeta(webfinger_host=webfinger_host)\n        \n        return hostmeta\n        \n    except (json.JSONDecodeError, KeyError) as e:\n        logger.error(\"Failed to parse host-meta document: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n        \n        # Create a DiasporaHostMeta instance with the parsed data\n        # Assuming the host-meta document contains a webfinger_host field\n        webfinger_host = doc.get('webfinger_host')\n        if not webfinger_host:\n            return None\n            \n        # Create the host-meta object\n        hostmeta = DiasporaHostMeta(webfinger_host=webfinger_host)\n        \n        return hostmeta\n        \n    except (json.JSONDecodeError, KeyError) as e:\n        logger.error(\"Failed to parse host-meta document: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n        \n        # Create a DiasporaHostMeta instance with the parsed data\n        # Assuming the host-meta document contains a webfinger_host field\n        webfinger_host = doc.get('webfinger_host')\n        if not webfinger_host:\n            return None\n            \n        # Create the host-meta object\n        hostmeta = DiasporaHostMeta(webfinger_host=webfinger_host)\n        \n        return hostmeta\n        \n    except (json.JSONDecodeError, KeyError) as e:\n        logger.error(\"Failed to parse host-meta document: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Fetch the host-meta document from the remote host\n    document, status_code, error = fetch_document(host=host, path='/host-meta')\n    \n    if not document:\n        return None\n    \n    try:\n        # Parse the document as JSON\n        doc = json.loads(document)\n        \n        # Create a DiasporaHostMeta instance with the parsed data\n        # Assuming the host-meta document contains a webfinger_host field\n        webfinger_host = doc.get('webfinger_host')\n        if not webfinger_host:\n            return None\n            \n        # Create the host-meta object\n        hostmeta = DiasporaHostMeta(webfinger_host=webfinger_host)\n        \n        return hostmeta\n        \n    except (json.JSONDecodeError, KeyError) as e:\n        logger.error(\"Failed to parse host-meta document: %s\", e)\n        return None"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8642, "num_retrieved": 20, "inference_time": 1.3292386531829834, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(method, url, data=data, timeout=timeout, *args, **kwargs)\n        return response.status_code, None\n    except requests.exceptions.RequestException as e:\n        return None, e"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8836, "num_retrieved": 25, "inference_time": 1.3292386531829834, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    To insert value for a nested key, all ancestor keys should be given as method's arguments.\n\n    Example:\n        dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = val if not keys else {}\n    for k in keys:\n        current = current[k]\n    current = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    To insert value for a nested key, all ancestor keys should be given as method's arguments.\n\n    Example:\n        dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = val if not keys else {}\n    for k in keys:\n        current = current[k]\n    current = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    To insert value for a nested key, all ancestor keys should be given as method's arguments.\n\n    Example:\n        dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Set the first key\n    current[key] = {}\n    # Iterate through the remaining keys to create the nested structure\n    for k in keys:\n        current = current[k]\n    # Assign the value to the final nested location\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    To insert value for a nested key, all ancestor keys should be given as method's arguments.\n\n    Example:\n        dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = val\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"Insert a value of a nested key into a dictionary.\n\n    To insert value for a nested key, all ancestor keys should be given as method's arguments.\n\n    Example:\n        dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    current[key] = val\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9078, "num_retrieved": 33, "inference_time": 1.3292386531829834, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class ListOfFilesType(ComplexType):\n        \"\"\"Complex type to handle a list of file names from a string or list input.\"\"\"\n        \n        def resolve(self, value):\n            if isinstance(value, string_types):\n                # Split by comma and strip whitespace\n                file_names = [item.strip() for item in value.split(',')]\n            else:\n                file_names = [item.strip() for item in value]\n            \n            # Validate each file name against settings_dirs\n            valid_file_names = []\n            for file_name in file_names:\n                if not file_name:\n                    continue\n                # Check if file name is in settings_dirs or a valid path\n                if any(file_name.startswith(dir_name) for dir_name in settings_dirs):\n                    valid_file_names.append(file_name)\n            \n            return valid_file_names\n    \n    return ListOfFilesType(spec_option['name'], (self.vars, self.defaults, self.plugin_path), spec_option['subcommand'], spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class ListOfFilesType(ComplexType):\n        \"\"\"Returns a list of file names from a string or list of strings\"\"\"\n\n        def resolve(self, value):\n            if isinstance(value, string_types):\n                return [v.strip() for v in value.split(',') if v.strip()]\n            else:\n                return [v.strip() for v in value if v.strip()]\n    \n    return ListOfFilesType(spec_option['name'], (settings_dirs, None, None), None, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class ListOfFilesType(ComplexType):\n        \"\"\"Returns a list of file names from a string or list of strings\"\"\"\n\n        def resolve(self, value):\n            if isinstance(value, string_types):\n                return [v.strip() for v in value.split(',') if v.strip()]\n            else:\n                return [v.strip() for v in value if v.strip()]\n    \n    return ListOfFilesType(spec_option['name'], (settings_dirs, None, None), None, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class ListOfFilesType(ComplexType):\n        \"\"\"Returns a list of file names from a string or list of strings\"\"\"\n\n        def resolve(self, value):\n            if isinstance(value, string_types):\n                return [v.strip() for v in value.split(',') if v.strip()]\n            else:\n                return [v.strip() for v in value if v.strip()]\n    \n    return ListOfFilesType(spec_option['name'], (settings_dirs, None, None), None, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"Create a new IniType complex type\"\"\"\n    class ListOfFilesType(ComplexType):\n        \"\"\"Returns a list of file names from a string or list of strings\"\"\"\n        \n        def resolve(self, value):\n            if isinstance(value, string_types):\n                return [v.strip() for v in value.split(',') if v.strip()]\n            else:\n                return [v.strip() for v in value if v.strip()]\n    \n    return ListOfFilesType(spec_option['name'], (settings_dirs, None, None), None, spec_option)"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10453, "num_retrieved": 76, "inference_time": 1.3292386531829834, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    return CoreServices.ansible_config_manager()"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10612, "num_retrieved": 81, "inference_time": 1.3292386531829834, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton and should be initialized\n    # when first accessed, we return an instance of WorkspaceManager\n    if not hasattr(cls, '_workspace_manager'):\n        cls._workspace_manager = WorkspaceManager(workspaces_base_dir=os.path.join(os.path.expanduser(\"~\"), \".infrared\", \"workspaces\"))\n    return cls._workspace_manager", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and state management within the application.\n    # Based on the retrieved context, it appears that the workspace manager is\n    # initialized in the WorkspaceManagerSpec class and accessed via CoreServices.\n    # Since no explicit implementation is provided in the context, we return\n    # a dummy instance for demonstration purposes.\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # Based on the context, it appears that the workspace manager is likely\n    # initialized in the WorkspaceManagerSpec class and accessed via CoreServices.\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # Based on the context, it appears that the workspace manager is likely\n    # initialized as a singleton or through a CoreServices instance.\n    # Since the exact implementation is not provided in the context,\n    # we return a dummy instance for demonstration purposes.\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class context and how the workspace manager\n    # is instantiated or retrieved in the application.\n    # Based on the context, it appears that the workspace manager is a class\n    # that manages workspaces and is initialized with a workspaces base directory.\n    # Since the exact implementation is not provided in the context, we return\n    # a dummy instance of WorkspaceManager.\n    from infrared.core.services.workspaces import WorkspaceManager\n    return WorkspaceManager(workspaces_base_dir=os.path.join(os.getcwd(), \"workspaces\"))"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 10026, "num_retrieved": 64, "inference_time": 1.3292386531829834, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # Assuming the plugin manager is a singleton and already initialized\n    # based on the context where CoreServices.plugins_manager() is used\n    # and the InfraredPluginManager class is defined in core/services/plugins.py\n    from infrared.core.services import CoreServices\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    return InfraredPluginManager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    return InfraredPluginManager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8717, "num_retrieved": 21, "inference_time": 1.3292386531829834, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"Validates that spec (YAML) content has all required fields.\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML format in spec file: {str(exc)}\")\n\n    # Define required fields (adjust based on actual spec structure)\n    required_fields = [\n        'name',           # Required field for plugin name\n        'description',    # Required field for plugin description\n        'plugin_type',    # Required field for plugin type\n        'subparsers'      # Required field for command structure\n    ]\n\n    # Validate presence of required fields\n    for field in required_fields:\n        if field not in spec:\n            raise exceptions.IRValidatorException(f\"Missing required field '{field}' in spec file\")\n\n    # Additional validation can be added here based on specific requirements\n    # For example, validate that subparsers has at least one parser, etc.\n\n    return spec", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"Validates that spec (YAML) content has all required fields.\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML format in spec file: {str(exc)}\")\n\n    # Define required fields that must be present\n    required_fields = [\n        'name',           # plugin name\n        'description',    # plugin description\n        'plugin_type',    # type of plugin\n        'subparsers'      # subparsers configuration\n    ]\n\n    # Validate presence of required fields\n    for field in required_fields:\n        if field not in spec:\n            raise exceptions.IRValidatorException(f\"Missing required field '{field}' in spec file\")\n\n    # Additional validation for subparsers (ensure it's a dictionary)\n    if not isinstance(spec['subparsers'], dict):\n        raise exceptions.IRValidatorException(\"'subparsers' must be a dictionary in spec file\")\n\n    # Additional validation for plugin_type (ensure it's a string)\n    if not isinstance(spec['plugin_type'], str):\n        raise exceptions.IRValidatorException(\"'plugin_type' must be a string in spec file\")\n\n    # Additional validation for description (ensure it's a string)\n    if not isinstance(spec['description'], str):\n        raise exceptions.IRValidatorException(\"'description' must be a string in spec file\")\n\n    # Additional validation for name (ensure it's a string)\n    if not isinstance(spec['name'], str):\n        raise exceptions.IRValidatorException(\"'name' must be a string in spec file\")\n\n    return spec", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"Validates that spec (YAML) content has all required fields.\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML format in spec file: {str(exc)}\")\n\n    # Define required fields that must be present\n    required_fields = [\n        'name',           # plugin name\n        'description',    # plugin description\n        'plugin_type',    # type of plugin\n        'subparsers'      # subparsers configuration\n    ]\n\n    # Validate presence of required fields\n    for field in required_fields:\n        if field not in spec:\n            raise exceptions.IRValidatorException(f\"Missing required field '{field}' in spec file\")\n\n    # Additional validation for subparsers (ensure it's a dictionary)\n    if not isinstance(spec['subparsers'], dict):\n        raise exceptions.IRValidatorException(\"subparsers must be a dictionary\")\n\n    # Validate that plugin_type is a string\n    if not isinstance(spec['plugin_type'], str):\n        raise exceptions.IRValidatorException(\"plugin_type must be a string\")\n\n    # Validate that description is a string\n    if not isinstance(spec['description'], str):\n        raise exceptions.IRValidatorException(\"description must be a string\")\n\n    # Validate that name is a string\n    if not isinstance(spec['name'], str):\n        raise exceptions.IRValidatorException(\"name must be a string\")\n\n    return spec", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"Validates that spec (YAML) content has all required fields.\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML format in spec file: {str(exc)}\")\n\n    # Define required fields that must be present\n    required_fields = [\n        'name', 'description', 'plugin_type', 'subparsers'\n    ]\n\n    # Validate presence of required fields\n    for field in required_fields:\n        if field not in spec:\n            raise exceptions.IRValidatorException(f\"Missing required field '{field}' in spec file\")\n\n    # Additional validation can be added here based on specific requirements\n    # For example, validate that subparsers has at least one parser, etc.\n\n    return spec", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"Validates that spec (YAML) content has all required fields.\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise exceptions.IRValidatorException(f\"Invalid YAML format in spec file: {str(exc)}\")\n\n    # Define required fields that must be present\n    required_fields = [\n        'name',           # plugin name\n        'description',    # plugin description\n        'plugin_type',    # type of plugin\n        'subparsers'      # subparsers configuration\n    ]\n\n    # Validate presence of required fields\n    for field in required_fields:\n        if field not in spec:\n            raise exceptions.IRValidatorException(f\"Missing required field '{field}' in spec file\")\n\n    # Additional validation for subparsers\n    if 'subparsers' in spec:\n        if not isinstance(spec['subparsers'], dict):\n            raise exceptions.IRValidatorException(\"'subparsers' must be a dictionary\")\n\n    # Return the validated and loaded spec data\n    return spec"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8753, "num_retrieved": 22, "inference_time": 1.3292386531829834, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise exceptions.IRFileNotFoundException([yaml_file])\n\n    with open(yaml_file, 'r') as file:\n        try:\n            data = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            raise exceptions.IRValidatorException(f\"Invalid YAML format: {str(exc)}\")\n\n    # Validate required fields - this should be defined in the class context\n    # For now, assuming basic validation based on common requirements\n    if data is None:\n        raise exceptions.IRValidatorException(\"YAML file is empty or contains no valid data\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise exceptions.IRFileNotFoundException([yaml_file])\n\n    with open(yaml_file, 'r') as file:\n        try:\n            data = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            raise exceptions.IRValidatorException(f\"Invalid YAML format in file: {yaml_file}\\n{str(exc)}\")\n\n    # Validate required fields - this should be defined in the class context\n    # For now, assuming the class has a list of required fields\n    required_fields = getattr(cls, 'REQUIRED_FIELDS', [])\n    \n    for field in required_fields:\n        if field not in data:\n            raise exceptions.IRValidatorException(f\"Missing required field '{field}' in YAML file: {yaml_file}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise exceptions.IRFileNotFoundException(yaml_file)\n\n    with open(yaml_file, 'r') as file:\n        try:\n            data = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            raise exceptions.IRValidatorException(f\"Invalid YAML format: {str(exc)}\")\n\n    # Validate required fields (example - adjust based on actual required fields)\n    # This is a placeholder - actual validation should be based on the specific requirements\n    if data is None:\n        raise exceptions.IRValidatorException(\"YAML file is empty or invalid\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise exceptions.IRFileNotFoundException(yaml_file)\n\n    with open(yaml_file, 'r') as file:\n        try:\n            data = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            raise exceptions.IRValidatorException(f\"Invalid YAML format: {str(exc)}\")\n\n    # Validate required fields - this should be defined in the class context\n    # For now, assuming the class has a list of required fields\n    required_fields = getattr(cls, 'REQUIRED_FIELDS', [])\n    \n    for field in required_fields:\n        if field not in data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    return data", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise exceptions.IRFileNotFoundException(yaml_file)\n\n    with open(yaml_file, 'r') as file:\n        try:\n            data = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            raise exceptions.IRValidatorException(f\"Invalid YAML format: {str(exc)}\")\n\n    # Validate required fields - this should be defined in the class context\n    # For now, we'll assume the required fields are defined in class attributes\n    required_fields = getattr(cls, 'REQUIRED_FIELDS', [])\n    \n    for field in required_fields:\n        if field not in data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n\n    return data"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9146, "num_retrieved": 34, "inference_time": 1.1500456035137177, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name not in shared_groups:\n            shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name not in shared_groups:\n            shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name not in shared_groups:\n            shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name not in shared_groups:\n            shared_groups.append(group_name)\n    self.spec_helper.spec_dict['shared_groups'] = shared_groups", "def _include_groups(self, parser_dict):\n    \"\"\"Resolves the include dict directive in the spec files.\"\"\"\n    include_groups = parser_dict.get('include_groups', [])\n    if not include_groups:\n        return\n    \n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    for group_name in include_groups:\n        if group_name not in shared_groups:\n            shared_groups.append(group_name)\n    \n    self.spec_helper.spec_dict['shared_groups'] = shared_groups"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9678, "num_retrieved": 53, "inference_time": 1.1500456035137177, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Iterate over all parsers and options to build defaults\n    for parser, option_spec in self.iterate_option_specs():\n        option_name = option_spec['name']\n        option_default = option_spec.get('default')\n        \n        if option_default is not None:\n            defaults[option_name] = option_default\n    \n    # Add defaults from command-specific sections\n    for command_name, command_dict in self.spec_dict.get('commands', {}).items():\n        for option_name, option_spec in command_dict.get('options', {}).items():\n            option_default = option_spec.get('default')\n            if option_default is not None:\n                defaults[option_name] = option_default\n    \n    # Add defaults from shared groups if present\n    for group in self.spec_dict.get('shared_groups', []):\n        for option_name, option_spec in group.get('options', {}).items():\n            option_default = option_spec.get('default')\n            if option_default is not None:\n                defaults[option_name] = option_default\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Iterate over all parsers and options to collect default values\n    for parser, option_spec in self.iterate_option_specs():\n        option_name = option_spec['name']\n        default_value = option_spec.get('default')\n        \n        if default_value is not None:\n            # If default value is provided in spec, use it\n            defaults[option_name] = default_value\n    \n    # Add any defaults from the main spec dictionary\n    for command_name, command_spec in self.spec_dict.get('commands', {}).items():\n        for option_name, option_spec in command_spec.get('options', {}).items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n    \n    # Add defaults from shared groups if they exist\n    for group in self.spec_dict.get('shared_groups', []):\n        for option_name, option_spec in group.get('options', {}).items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Iterate over all parsers and options\n    for parser, option_spec in self.iterate_option_specs():\n        option_name = option_spec['name']\n        option_type = option_spec.get('type', 'string')\n        \n        # Get default value from spec if defined\n        default_value = option_spec.get('default')\n        \n        # If default is defined, use it\n        if default_value is not None:\n            # Handle special types like flag, list, dict, etc.\n            if option_type == 'flag':\n                defaults[option_name] = True\n            elif option_type == 'list':\n                defaults[option_name] = default_value if isinstance(default_value, list) else []\n            elif option_type == 'dict':\n                defaults[option_name] = default_value if isinstance(default_value, dict) else {}\n            else:\n                defaults[option_name] = default_value\n    \n    # Add defaults from command-specific sections\n    for command_name, command_dict in self.spec_dict.get('commands', {}).items():\n        for option_name, option_spec in command_dict.get('options', {}).items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[f\"{command_name}.{option_name}\"] = default_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Iterate over all parsers and options to collect default values\n    for parser, option_spec in self.iterate_option_specs():\n        option_name = option_spec['name']\n        default_value = option_spec.get('default')\n        \n        if default_value is not None:\n            # If default value is provided in spec, use it\n            defaults[option_name] = default_value\n    \n    # Add any defaults from the main spec dictionary\n    for command_name, command_spec in self.spec_dict.get('commands', {}).items():\n        for option_name, option_spec in command_spec.get('options', {}).items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n    \n    # Add defaults from shared groups if available\n    for group in self.spec_dict.get('shared_groups', []):\n        for option_name, option_spec in group.get('options', {}).items():\n            default_value = option_spec.get('default')\n            if default_value is not None:\n                defaults[option_name] = default_value\n    \n    return defaults", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    defaults = {}\n    \n    # Iterate over all parsers and options to collect default values\n    for parser, option_spec in self.iterate_option_specs():\n        option_name = option_spec['name']\n        option_default = option_spec.get('default')\n        \n        if option_default is not None:\n            defaults[option_name] = option_default\n    \n    # Add defaults from command-specific sections\n    for command_name, command_dict in self.spec_dict.get('commands', {}).items():\n        for option_name, option_spec in command_dict.get('options', {}).items():\n            option_default = option_spec.get('default')\n            if option_default is not None:\n                defaults[option_name] = option_default\n    \n    # Add defaults from subparsers\n    for subparser_name, subparser_dict in self.spec_dict.get('subparsers', {}).items():\n        for option_name, option_spec in subparser_dict.get('options', {}).items():\n            option_default = option_spec.get('default')\n            if option_default is not None:\n                defaults[option_name] = option_default\n    \n    return defaults"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9719, "num_retrieved": 51, "inference_time": 1.1500456035137177, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the docstring, it should return a dictionary of deprecated options\n    # However, no implementation details or context about deprecated options are provided\n    # Therefore, returning an empty dictionary as a placeholder\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    # TODO: Implement this function\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8957, "num_retrieved": 31, "inference_time": 1.1500456035137177, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific deprecation rules\n    deprecated_args = []\n    \n    # Example logic: check for known deprecated arguments in both sources\n    # This is a template - actual implementation would need specific deprecation mapping\n    known_deprecated = {\n        'old_option': 'new_option',\n        'legacy_flag': 'modern_flag'\n    }\n    \n    for arg_name in cli_args:\n        if arg_name in known_deprecated:\n            deprecated_args.append((arg_name, known_deprecated[arg_name], 'cli'))\n    \n    for arg_name in answer_file_args:\n        if arg_name in known_deprecated:\n            deprecated_args.append((arg_name, known_deprecated[arg_name], 'file'))\n    \n    if deprecated_args:\n        print(\"Deprecated arguments detected:\")\n        for arg, replacement, source in deprecated_args:\n            print(f\"  {arg} (from {source}) is deprecated. Use {replacement} instead.\")\n    \n    # Return True if any deprecations found, False otherwise\n    return len(deprecated_args) > 0", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n    \n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8862, "num_retrieved": 25, "inference_time": 1.1500456035137177, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9127, "num_retrieved": 35, "inference_time": 1.1500456035137177, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"Gets the specification for the specified option name.\"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9276, "num_retrieved": 38, "inference_time": 1.1500456035137177, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, silenced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9109, "num_retrieved": 33, "inference_time": 1.1500456035137177, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    absent = []\n    for arg in args:\n        val = self.module.params.get(arg, None)\n        if val is None:\n            absent.append(arg)\n    if absent:\n        raise UtilError(\"Expected {} to be specified\".format(absent))"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9061, "num_retrieved": 32, "inference_time": 1.1500456035137177, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        # required_when is a dict with condition and value\n        condition_met = False\n        condition_dict = required_when\n        \n        # Evaluate the condition based on args\n        for condition_key, condition_value in condition_dict.items():\n            # Get the value of the condition key from args\n            condition_arg_value = args.get(command_name, {}).get(condition_key)\n            \n            # If condition is a string, check if it matches the value\n            if isinstance(condition_value, str):\n                if condition_arg_value == condition_value:\n                    condition_met = True\n                    break\n            # If condition is a boolean, check if it matches\n            elif isinstance(condition_value, bool):\n                if condition_arg_value == condition_value:\n                    condition_met = True\n                    break\n            # If condition is a list, check if value is in the list\n            elif isinstance(condition_value, list):\n                if condition_arg_value in condition_value:\n                    condition_met = True\n                    break\n        \n        # If condition is met, add the option name to matched_args\n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        # required_when is a dict with condition and value\n        condition_met = False\n        condition_dict = required_when\n        \n        # Evaluate the condition based on args\n        for condition_key, condition_value in condition_dict.items():\n            # Get the value of the condition key from args\n            condition_arg_value = args.get(command_name, {}).get(condition_key)\n            \n            # If the condition is a boolean check\n            if isinstance(condition_value, bool):\n                condition_met = condition_arg_value == condition_value\n            else:\n                # If it's a string or other type, check if it matches\n                if condition_arg_value is not None and str(condition_arg_value) == str(condition_value):\n                    condition_met = True\n                    break\n        \n        # If condition is met, add the argument to matched list\n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    required_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        condition_met = False\n        condition_args = required_when.split(',')\n        \n        # Evaluate the condition based on args\n        for condition in condition_args:\n            # Parse condition: \"arg_name==value\" or \"arg_name!=value\"\n            if '=' in condition:\n                condition_part = condition.split('==')\n                if len(condition_part) == 2:\n                    condition_arg, condition_value = condition_part\n                    condition_arg = condition_arg.strip()\n                    condition_value = condition_value.strip()\n                    # Check if the argument exists and matches the value\n                    if condition_arg in args.get(command_name, {}) and \\\n                       str(args[command_name][condition_arg]) == condition_value:\n                        condition_met = True\n                        break\n            elif '!=' in condition:\n                condition_part = condition.split('!=')\n                if len(condition_part) == 2:\n                    condition_arg, condition_value = condition_part\n                    condition_arg = condition_arg.strip()\n                    condition_value = condition_value.strip()\n                    # Check if the argument exists and does not match the value\n                    if condition_arg in args.get(command_name, {}) and \\\n                       str(args[command_name][condition_arg]) != condition_value:\n                        condition_met = True\n                        break\n        \n        if condition_met:\n            required_args.append(option_name)\n    \n    return required_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        # required_when is a dict with condition and value\n        condition_met = False\n        condition_dict = required_when\n        \n        # Evaluate the condition based on args\n        for condition_key, condition_value in condition_dict.items():\n            # Get the value of the condition key from args\n            condition_arg_value = args.get(command_name, {}).get(condition_key)\n            \n            # If condition is a string, check if it matches the value\n            if isinstance(condition_value, str):\n                if condition_arg_value == condition_value:\n                    condition_met = True\n                    break\n            # If condition is a boolean, check if it matches\n            elif isinstance(condition_value, bool):\n                if condition_arg_value == condition_value:\n                    condition_met = True\n                    break\n            # If condition is a list, check if value is in the list\n            elif isinstance(condition_value, list):\n                if condition_arg_value in condition_value:\n                    condition_met = True\n                    break\n        \n        # If condition is met, add the option name to matched_args\n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec, args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when`` condition\n    \"\"\"\n    matched_args = []\n    for option in options_spec:\n        option_name = option['name']\n        required_when = option.get('required_when')\n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        # required_when is a dict with condition and value\n        condition_met = False\n        \n        # Extract the condition and value from required_when\n        condition = required_when.get('condition')\n        value = required_when.get('value')\n        \n        # Get the value of the condition from args\n        condition_value = None\n        if condition:\n            # The condition is typically a string that refers to an option name\n            condition_option_name = condition\n            condition_value = args.get(command_name, {}).get(condition_option_name)\n        \n        # Check if the condition is met\n        if condition_value is not None:\n            # If value is provided, check if it matches the required value\n            if value is not None and condition_value == value:\n                condition_met = True\n            elif value is None and condition_value is not None:\n                condition_met = True\n            elif value is None and condition_value is None:\n                condition_met = True\n        \n        if condition_met:\n            matched_args.append(option_name)\n    \n    return matched_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9140, "num_retrieved": 34, "inference_time": 1.1500456035137177, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value, length in self._get_length_constraints():\n        if arg_value and len(arg_value) > length:\n            invalid_options.append((arg_name, arg_value, length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value, length in self._get_length_specs():\n        if arg_value and len(arg_value) > length:\n            invalid_options.append((arg_name, arg_value, length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value, length in self._get_length_specs():\n        if arg_value and len(arg_value) > length:\n            invalid_options.append((arg_name, arg_value, length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value, length in self._get_length_specs():\n        if arg_value and len(arg_value) > length:\n            invalid_options.append((arg_name, arg_value, length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for arg_name, arg_value, length in self._get_length_constraints():\n        if arg_value and len(arg_value) > length:\n            invalid_options.append((arg_name, arg_value, length))\n    \n    if invalid_options:\n        raise IRInvalidLengthException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9122, "num_retrieved": 33, "inference_time": 1.1500456035137177, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs(self.command_name)\n    \n    invalid_options = []\n    \n    for option_spec in option_specs:\n        if option_spec.get('choices'):\n            option_name = option_spec['name']\n            option_choices = option_spec['choices']\n            \n            # Get the value from args\n            if option_name in args.get(self.command_name, {}):\n                option_value = args[self.command_name][option_name]\n                \n                # Check if the value is in the available choices\n                if option_value not in option_choices:\n                    invalid_options.append((\n                        option_name,\n                        option_value,\n                        option_choices\n                    ))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs(self.command_name)\n    \n    invalid_options = []\n    \n    for option_spec in option_specs:\n        if option_spec.get('choices'):\n            option_name = option_spec['name']\n            option_value = args.get(self.command_name, {}).get(option_name)\n            \n            if option_value is not None:\n                available_choices = option_spec['choices']\n                if option_value not in available_choices:\n                    invalid_options.append((\n                        option_name,\n                        option_value,\n                        available_choices\n                    ))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    option_specs = self.get_parser_option_specs(self.command_name)\n    \n    invalid_options = []\n    \n    for option_spec in option_specs:\n        if option_spec.get('choices'):\n            option_name = option_spec['name']\n            option_choices = option_spec['choices']\n            \n            # Get the value from args\n            if option_name in args.get(self.command_name, {}):\n                option_value = args[self.command_name][option_name]\n                \n                # Check if the value is in available choices\n                if option_value not in option_choices:\n                    invalid_options.append((\n                        option_name,\n                        option_value,\n                        option_choices\n                    ))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    \n    # Get all option specs from the parser\n    option_specs = self.get_parser_option_specs(self.command_name)\n    \n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        available_choices = option_spec.get('choices', [])\n        \n        # Check if this option has choices defined\n        if not available_choices:\n            continue\n            \n        # Get the value for this option from args\n        option_value = args.get(self.command_name, {}).get(option_name)\n        \n        # Skip if option is not present in args\n        if option_value is None:\n            continue\n            \n        # Check if the value is in available choices\n        if option_value not in available_choices:\n            invalid_options.append((option_name, option_value, available_choices))\n    \n    # If there are invalid options, raise an exception\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    \n    # Get all the option specs for the command\n    option_specs = self.get_parser_option_specs(self.command_name)\n    \n    for option_spec in option_specs:\n        option_name = option_spec['name']\n        choices = option_spec.get('choices', [])\n        \n        # Check if this option has choices defined\n        if not choices:\n            continue\n            \n        # Get the value of this option from args\n        option_value = args.get(self.command_name, {}).get(option_name)\n        \n        # Skip if option is not set\n        if option_value is None:\n            continue\n            \n        # Check if the value is in the available choices\n        if option_value not in choices:\n            invalid_options.append((option_name, option_value, choices))\n    \n    # If there are invalid options, raise an exception\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8994, "num_retrieved": 30, "inference_time": 1.1500456035137177, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific argument validation requirements\n    # This function should iterate through arguments and validate their values against min/max constraints\n    # Since specific min/max constraints are not defined in the context, this implementation\n    # provides a skeleton that could be extended with actual validation logic\n    \n    # Example structure:\n    # For each argument with min/max constraints, check if value is within range\n    # This would typically involve accessing command-specific specs or configuration\n    \n    # As no specific constraints or command specs are provided in context,\n    # this function returns True by default\n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific argument validation requirements\n    # This function should iterate through arguments and validate their values against min/max constraints\n    # Since specific min/max constraints are not provided in the context, this implementation\n    # would need to be customized based on the actual validation rules\n    \n    # Example structure (to be implemented with actual min/max values):\n    # for arg_name, arg_value in args.items():\n    #     if arg_value is not None:\n    #         min_val, max_val = get_min_max_for(arg_name)\n    #         if arg_value < min_val or arg_value > max_val:\n    #             raise ValueError(f\"Argument '{arg_name}' value {arg_value} is out of range [{min_val}, {max_val}]\")\n    \n    # As no specific validation rules are provided in the context, \n    # this function returns without any validation\n    pass", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific argument validation requirements\n    # This function should iterate through arguments and validate their values against min/max constraints\n    # Since specific min/max constraints are not provided in the context, this implementation\n    # would need to be customized based on the actual validation rules\n    \n    # Example structure (to be implemented with actual min/max values):\n    # for arg_name, arg_value in args.items():\n    #     if arg_value is not None:\n    #         min_val, max_val = get_min_max_for(arg_name)\n    #         if arg_value < min_val or arg_value > max_val:\n    #             raise ValueError(f\"Argument '{arg_name}' value {arg_value} is outside min/max range\")\n    \n    # Since no specific min/max values or validation rules are defined in context,\n    # this function returns without validation\n    pass", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific argument validation requirements\n    # This function should iterate through arguments and validate their values against min/max constraints\n    # Since specific min/max constraints are not provided in the context, this implementation\n    # would need to be customized based on the actual validation rules\n    \n    # Example structure:\n    # For each argument with min/max constraints, check if value is within range\n    # This is a stub - actual implementation would require specific validation rules\n    \n    # No specific validation rules provided in context, so returning True by default\n    # In a real implementation, this would check against defined min/max values for each argument\n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific argument validation requirements\n    # This function should iterate through arguments and validate their values against min/max constraints\n    # Since specific min/max constraints are not provided in the context, this implementation is minimal\n    \n    # Example structure:\n    # For each argument with min/max constraints, check if value is within range\n    # This would typically involve accessing a configuration or spec that defines min/max values\n    \n    # As no specific constraints or data structure for min/max values are provided,\n    # this function returns True by default (no validation performed)\n    return True"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9286, "num_retrieved": 38, "inference_time": 1.1500456035137177, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create the complex type instance based on the type name\n    if type_name == 'Value':\n        return Value(option_name, settings_dirs=None, sub_command_name=subcommand, spec_option=spec_option)\n    elif type_name == 'Bool':\n        return Bool(option_name, settings_dirs=None, sub_command_name=subcommand, spec_option=spec_option)\n    elif type_name == 'Flag':\n        return Flag(option_name, settings_dirs=None, sub_command_name=subcommand, spec_option=spec_option)\n    elif type_name == 'ComplexType':\n        return ComplexType(option_name, settings_dirs=None, sub_command_name=subcommand, spec_option=spec_option)\n    else:\n        # Default to ComplexType if type is not recognized\n        return ComplexType(option_name, settings_dirs=None, sub_command_name=subcommand, spec_option=spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create the complex type instance based on the type name\n    if type_name == 'value':\n        return Value(option_name, spec_option=spec_option)\n    elif type_name == 'bool':\n        return Bool(option_name, spec_option=spec_option)\n    elif type_name == 'flag':\n        return Flag(option_name, spec_option=spec_option)\n    else:\n        # Default to ComplexType if type is not recognized\n        return ComplexType(\n            arg_name=option_name,\n            settings_dirs=None,\n            sub_command_name=subcommand,\n            spec_option=spec_option\n        )", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create the complex type instance based on the type name\n    if type_name == 'value':\n        return Value(option_name, spec_option=spec_option)\n    elif type_name == 'bool':\n        return Bool(option_name, spec_option=spec_option)\n    elif type_name == 'flag':\n        return Flag(option_name, spec_option=spec_option)\n    else:\n        # For any other complex type, create a ComplexType instance\n        return ComplexType(\n            arg_name=option_name,\n            settings_dirs=None,  # Not specified in context\n            sub_command_name=subcommand,\n            spec_option=spec_option\n        )", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create the complex type instance based on the type name\n    if type_name == 'value':\n        return Value(option_name, spec_option=spec_option)\n    elif type_name == 'bool':\n        return Bool(option_name, spec_option=spec_option)\n    elif type_name == 'flag':\n        return Flag(option_name, spec_option=spec_option)\n    elif type_name == 'complex':\n        # For complex types, create a ComplexType instance\n        return ComplexType(\n            arg_name=option_name,\n            settings_dirs=None,  # Not specified in context\n            sub_command_name=subcommand,\n            spec_option=spec_option\n        )\n    else:\n        # Default to Value if type is not recognized\n        return Value(option_name, spec_option=spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"Build the complex argument type\"\"\"\n    # Create the complex type instance based on the type name\n    if type_name == 'value':\n        return Value(option_name)\n    elif type_name == 'bool':\n        return Bool(option_name)\n    elif type_name == 'flag':\n        return Flag(option_name)\n    elif type_name == 'complex':\n        # For complex types, use the base ComplexType with provided specifications\n        return ComplexType(\n            arg_name=option_name,\n            settings_dirs=None,  # Not specified in context\n            sub_command_name=subcommand,\n            spec_option=spec_option\n        )\n    else:\n        # Default to Value if type is not recognized\n        return Value(option_name)"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8638, "num_retrieved": 19, "inference_time": 1.1500456035137177, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Iterate through all arguments\n    for arg, value in args.items():\n        # Check if the argument starts with a control flag (e.g., --debug, --dry-run)\n        if arg.startswith('debug') or arg.startswith('dry-run') or \\\n           arg.startswith('output') or arg.startswith('verbose') or \\\n           arg.startswith('extra-vars') or arg.startswith('ansible-args'):\n            control_args[arg] = value\n        else:\n            # All other arguments are considered nested arguments\n            # They will be used by Ansible playbooks and put into the spec yml file\n            nested_args[arg] = value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Define common control arguments that should not be nested\n    control_keys = [\n        'debug', 'dry-run', 'generate-answers-file', 'output', 'verbose',\n        'extra-vars', 'ansible-args'\n    ]\n    \n    for key, value in args.items():\n        if key in control_keys:\n            control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            # They will be processed and put into the spec yml file\n            nested_args[key] = value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Iterate through all args and classify them\n    for key, value in args.items():\n        # Check if the key starts with a control argument prefix\n        # Common control arguments typically start with '--' or have specific control names\n        if key.startswith('debug') or key.startswith('dry-run') or key.startswith('output') or \\\n           key.startswith('verbose') or key.startswith('extra-vars') or key.startswith('generate-answers-file'):\n            control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by Ansible playbooks and placed in the spec yml file\n            nested_args[key] = value\n    \n    # Handle custom arguments - these are typically passed as key=value pairs\n    # and should be treated as custom ansible variables\n    # In this implementation, we assume that any argument not in control_args\n    # is a nested argument, and custom arguments are a subset of nested arguments\n    # If custom arguments are explicitly defined, they would be handled separately\n    \n    # Return the split dictionaries\n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Define common control arguments that should not be put into spec yml\n    control_keys = [\n        'debug', 'dry-run', 'generate-answers-file', 'output', 'verbose',\n        'extra-vars', 'ansible-args'\n    ]\n    \n    for key, value in args.items():\n        if key in control_keys:\n            control_args[key] = value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be put into the spec yml file\n            nested_args[key] = value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Iterate through all arguments\n    for arg, value in args.items():\n        # Check if the argument is a control argument (common IR behavior flags)\n        # These typically start with '--' and are known control flags\n        if arg.startswith('--') and arg not in ['--dry-run', '--generate-answers-file', '--output', '--verbose', '--debug', '--extra-vars']:\n            control_args[arg] = value\n        elif arg.startswith('--') and arg in ['--dry-run', '--generate-answers-file', '--output', '--verbose', '--debug', '--extra-vars']:\n            # These are control arguments but are handled specially\n            control_args[arg] = value\n        else:\n            # All other arguments are considered nested or custom\n            # Check if it's a custom argument (starts with '--' and has a value)\n            if arg.startswith('--'):\n                # If it's a custom argument, it will be processed as nested\n                # in the VarsDictManager\n                nested_args[arg] = value\n            else:\n                # If it's not a flag, it might be a custom variable\n                # We treat it as nested for now\n                nested_args[arg] = value\n    \n    # Extract custom arguments from extra-vars\n    if '--extra-vars' in control_args:\n        extra_vars = control_args['--extra-vars']\n        # Parse extra-vars into nested arguments\n        if extra_vars and '=' in extra_vars:\n            # Split extra-vars by semicolon and process each\n            for extra_var in extra_vars.split(';'):\n                extra_var = extra_var.strip()\n                if '=' in extra_var:\n                    key, value = extra_var.split('=', 1)\n                    # Split key by dots to create nested structure\n                    key_parts = key.split('.')\n                    nested_key = key_parts[0]"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9324, "num_retrieved": 40, "inference_time": 1.1500456035137177, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if extra_var:\n            # Split the extra-var string by '=' to get key and value\n            parts = extra_var.split('=', 1)\n            if len(parts) == 2:\n                key, value = parts\n                vars_dict[key] = value\n            else:\n                # If no '=' found, treat as key with empty value\n                vars_dict[parts[0]] = \"\"\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if extra_var:\n            # Split the extra-var string by '=' to get key and value\n            parts = extra_var.split('=', 1)\n            if len(parts) == 2:\n                key, value = parts\n                vars_dict[key] = value\n            else:\n                # If no value is provided, treat the key as a boolean True\n                vars_dict[parts[0]] = True\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if extra_var:\n            # Split the extra-var string by '=' to get key and value\n            parts = extra_var.split('=', 1)\n            if len(parts) == 2:\n                key, value = parts\n                # Handle nested dictionaries using dict_utils.dict_insert\n                dict_utils.dict_insert(vars_dict, value, key)\n            else:\n                # If no value is provided, treat as key with empty value\n                dict_utils.dict_insert(vars_dict, '', parts[0])\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if extra_var:\n            # Split the extra-var string by '=' to get key and value\n            parts = extra_var.split('=', 1)\n            if len(parts) == 2:\n                key, value = parts\n                vars_dict[key] = value\n            elif len(parts) == 1:\n                # If only a key is provided, treat it as a key with empty value\n                vars_dict[parts[0]] = \"\"\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if extra_var:\n            # Split the extra-var string by '=' to get key and value\n            parts = extra_var.split('=', 1)\n            if len(parts) == 2:\n                key, value = parts\n                vars_dict[key] = value\n            elif len(parts) == 1:\n                # If no value is provided, treat the key as a boolean True\n                vars_dict[parts[0]] = True\n    \n    return vars_dict"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8642, "num_retrieved": 20, "inference_time": 1.1500456035137177, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Register the plugin's roles path if provided\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n\n    # Execute the ansible playbook with provided parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n\n    # Restore the original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Register the plugin's roles path in ANSIBLE_ROLES_PATH environment variable\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n\n    # Execute the Ansible playbook with provided arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n\n    # Restore the original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n\n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Register the plugin's roles path if provided\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the ansible playbook with provided arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    # Restore the original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible command with required arguments\n    ansible_cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbose level if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        vars_arg = '--extra-vars'\n        vars_str = ','.join([f\"{k}={v}\" for k, v in extra_vars.items()])\n        ansible_cmd.extend([vars_arg, vars_str])\n    \n    # Add ansible arguments if provided\n    if ansible_args:\n        for key, value in ansible_args.items():\n            ansible_cmd.extend(['--' + key, str(value)])\n    \n    # Execute the ansible playbook command\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare ansible arguments\n    ansible_args_list = []\n    if ansible_args:\n        # Convert dict to list of arguments\n        for arg, value in ansible_args.items():\n            if isinstance(value, (list, tuple)):\n                # Handle lists/tuples as multiple arguments\n                for item in value:\n                    ansible_args_list.append(f\"--{arg}={item}\")\n            else:\n                ansible_args_list.append(f\"--{arg}={value}\")\n    \n    # Combine all arguments\n    all_args = []\n    if verbose is not None:\n        all_args.append(f\"--verbose={verbose}\")\n    \n    all_args.extend(ansible_args_list)\n    \n    # Execute ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=verbose,\n        extra_vars=extra_vars,\n        ansible_args=all_args\n    )\n    \n    return result"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8474, "num_retrieved": 16, "inference_time": 1.7180655002593994, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse the CLI arguments to extract relevant options\n    parser = argparse.ArgumentParser()\n    # Assuming the plugin's spec defines the arguments, we use the plugin's configuration\n    # to build the parser and parse the arguments\n    # In practice, this would be constructed from the plugin's spec file\n    \n    # For now, we'll simulate parsing the arguments and extract the necessary ones\n    # This is a simplified version assuming cli_args is already parsed\n    control_args = {}\n    for arg in cli_args:\n        if arg.startswith('--'):\n            # Extract key-value pairs from arguments\n            key, value = arg.split('=', 1) if '=' in arg else (arg, None)\n            control_args[key] = value\n    \n    # Set up the Ansible playbook execution with the provided variables\n    # and workspace context\n    ansible_result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=control_args.get('verbose', None),\n        extra_vars=vars_dict,\n        ansible_args=control_args.get('ansible-args', None)\n    )\n    \n    return ansible_result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse the command line arguments to extract ansible-specific options\n    parser = argparse.ArgumentParser()\n    # Add common ansible arguments (these would typically be defined in the plugin spec)\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--extra-vars', help='Extra vars to pass to ansible')\n    parser.add_argument('--output', help='Output file for vars dict')\n    parser.add_argument('--dry-run', action='store_true', help='Only generate vars dict, do not execute playbook')\n    parser.add_argument('--generate-answers-file', action='store_true', help='Only generate answers file')\n    \n    # Parse the arguments\n    parsed_args = parser.parse_args(cli_args)\n    \n    # Prepare the extra vars for Ansible\n    ansible_extra_vars = vars_dict\n    \n    # Update extra vars with any additional vars from cli_args\n    if parsed_args.extra_vars:\n        ansible_extra_vars.update(yaml.safe_load(parsed_args.extra_vars))\n    \n    # Set up the Ansible playbook execution\n    ansible_args = {}\n    if parsed_args.verbose:\n        ansible_args['verbose'] = True\n    \n    # If dry-run or generate-answers-file, return early without executing playbook\n    if parsed_args.dry_run or parsed_args.generate_answers_file:\n        return None\n    \n    # Set up the roles path if defined in the plugin\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse the command line arguments to extract ansible-specific options\n    parser = argparse.ArgumentParser()\n    # Add common ansible arguments that might be passed in cli_args\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--extra-vars', help='Extra variables to pass to ansible')\n    parser.add_argument('--output', help='Output file for vars dict')\n    parser.add_argument('--dry-run', action='store_true', help='Only generate vars dict, do not execute playbook')\n    parser.add_argument('--generate-answers-file', action='store_true', help='Only generate answers file')\n    \n    # Parse the arguments\n    parsed_args = parser.parse_args(cli_args)\n    \n    # Prepare the extra vars for Ansible\n    ansible_extra_vars = parsed_args.extra_vars or {}\n    \n    # Update vars_dict with any extra vars from cli_args\n    if ansible_extra_vars:\n        vars_dict.update(ansible_extra_vars)\n    \n    # If dry-run or generate-answers-file is specified, return early\n    if parsed_args.dry_run or parsed_args.generate_answers_file:\n        return None\n    \n    # Set verbose mode if requested\n    verbose = parsed_args.verbose\n    \n    # Execute the Ansible playbook with the provided vars_dict\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=verbose,\n        extra_vars=vars_dict,\n        ansible_args=parsed_args.__dict__\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse the CLI arguments to extract ansible-specific options\n    parser = argparse.ArgumentParser()\n    # Add common ansible arguments (these would typically be defined in the plugin spec)\n    parser.add_argument('--verbose', action='store_true', help='Enable verbose output')\n    parser.add_argument('--extra-vars', help='Extra variables to pass to Ansible')\n    parser.add_argument('--dry-run', action='store_true', help='Only generate vars dict, do not execute')\n    parser.add_argument('--generate-answers-file', action='store_true', help='Only generate answers file')\n    parser.add_argument('-o', '--output', help='Write vars dict to file')\n    \n    # Parse the arguments\n    parsed_args = parser.parse_args(cli_args)\n    \n    # Prepare the extra vars for Ansible\n    ansible_extra_vars = vars_dict\n    \n    # Update with any additional extra-vars from CLI\n    if parsed_args.extra_vars:\n        ansible_extra_vars.update(yaml.safe_load(parsed_args.extra_vars))\n    \n    # Set verbose mode if requested\n    verbose = parsed_args.verbose\n    \n    # Determine if we should run in dry-run or generate answers only\n    if parsed_args.dry_run or parsed_args.generate_answers_file:\n        return None\n    \n    # Set output file if specified\n    output_file = parsed_args.output\n    if output_file:\n        with open(output_file, 'w') as f:\n            f.write(yaml.safe_dump(ansible_extra_vars, default_flow_style=False))\n    \n    # Execute the Ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=verbose,\n        extra_vars=ansible_extra_vars,\n        ansible_args=parsed_args.__dict__\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Parse the CLI arguments to extract relevant options\n    parser = argparse.ArgumentParser()\n    # Assuming the plugin's spec defines the arguments, we use the plugin's configuration\n    # to build the argument parser (this is a simplified version)\n    \n    # Extract control arguments from cli_args\n    control_args = {}\n    # This is a simplified implementation - in reality, the arguments would be parsed\n    # based on the plugin's spec and defined options\n    for arg in cli_args:\n        if arg.startswith('--'):\n            # Extract key-value pairs\n            key, value = arg[2:].split('=', 1) if '=' in arg[2:] else (arg[2:], None)\n            control_args[key] = value\n    \n    # Set up Ansible environment variables\n    ansible_roles_path = os.environ.get('ANSIBLE_ROLES_PATH', '')\n    \n    # Update Ansible roles path if defined in plugin\n    if ir_plugin.roles_path:\n        role_path = os.path.join(ir_plugin.path, ir_plugin.roles_path)\n        if not os.path.exists(role_path):\n            LOG.warning(\"Plugin's config.role_path: %s, doesn't exist\", role_path)\n        roles_path = os.path.join(role_path, '../')\n        if ansible_roles_path:\n            new_path = ':'.join([ansible_roles_path, roles_path])\n        else:\n            new_path = roles_path\n        os.environ['ANSIBLE_ROLES_PATH'] = new_path\n    \n    # Execute the Ansible playbook with the provided variables\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=ir_plugin.playbook,\n        verbose=control_args.get('verbose', None),\n        extra_vars=vars_dict,\n        ansible_args=control_args.get('ansible-args', None)\n    )\n    \n    # Restore original ANSIBLE_ROLES_PATH\n    os.environ['ANSIBLE_ROLES_PATH'] = ansible_roles_path\n    \n    return result"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 8843, "num_retrieved": 25, "inference_time": 1.7180655002593994, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Get the option specifications for the given parser_name\n    options = self.get_parser_option_specs(parser_name)\n    \n    # Iterate through each option and convert its value to the appropriate type\n    for option in options:\n        option_name = option['name']\n        option_type = option.get('type')\n        option_default = option.get('default')\n        \n        # If the option has a type specified, convert the value\n        if option_type:\n            value = values_dict.get(parser_name, {}).get(option_name)\n            if value is not None:\n                # Handle specific types like int, float, bool, etc.\n                if option_type == 'int':\n                    try:\n                        values_dict[parser_name][option_name] = int(value)\n                    except (ValueError, TypeError):\n                        pass  # Keep as string if conversion fails\n                elif option_type == 'float':\n                    try:\n                        values_dict[parser_name][option_name] = float(value)\n                    except (ValueError, TypeError):\n                        pass  # Keep as string if conversion fails\n                elif option_type == 'bool':\n                    if isinstance(value, str):\n                        values_dict[parser_name][option_name] = value.lower() in ('true', '1', 'yes', 'on')\n                    else:\n                        values_dict[parser_name][option_name] = bool(value)\n                elif option_type == 'choice':\n                    # For choice types, we assume the value is already valid\n                    values_dict[parser_name][option_name] = value\n                elif option_type == 'path':\n                    # For path types, we assume the value is a string path\n                    values_dict[parser_name][option_name] = value\n                elif option_type == 'list':\n                    # For list types, split the string by comma\n                    if isinstance(value, str):\n                        values_dict[parser_name][option_name] = [v.strip() for v in value.split(',')]\n                    else:\n                        values_dict[parser_name][option_name] = []\n                else:\n                    # For any other type, keep as string\n                    values_dict[parser_name][option_name] = value", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Define type conversions based on common argument types and parser names\n    # This is a placeholder implementation - actual logic would depend on\n    # specific command specifications and expected types\n    \n    # Example type conversions based on common patterns\n    if parser_name == \"ssh\":\n        # For SSH command, convert node_name to string (already string)\n        # remote_command to string (already string)\n        pass\n    \n    elif parser_name == \"plugin\":\n        # For plugin commands, convert names to strings\n        # versions to strings or integers as needed\n        pass\n    \n    elif parser_name == \"main\":\n        # For main command, convert workspace names to strings\n        # boolean flags to boolean values\n        pass\n    \n    # Default: no conversion needed, values remain as strings\n    # In a real implementation, this would reference specific spec definitions\n    # or use a mapping of option names to expected types\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Define type conversions based on common argument types and command names\n    # This is a placeholder implementation - actual type conversion logic\n    # would depend on the specific command and its expected argument types\n    \n    # Example conversions based on common patterns:\n    # - boolean flags (store_true, store_false)\n    # - integers (count, number of items)\n    # - floats (values like memory, time)\n    # - lists (comma-separated values)\n    \n    # For now, we'll only handle basic type conversions based on known patterns\n    # in the context of common infrared commands\n    \n    if parser_name == 'ssh':\n        # SSH command specific handling\n        if 'remote_command' in values_dict:\n            # remote_command is typically a string, but could be None\n            if values_dict['remote_command'] is not None:\n                # Ensure it's a string\n                values_dict['remote_command'] = str(values_dict['remote_command'])\n    \n    elif parser_name == 'plugin':\n        # Plugin management commands\n        if 'revision' in values_dict:\n            # revision is typically a string, but could be None\n            if values_dict['revision'] is not None:\n                values_dict['revision'] = str(values_dict['revision'])\n        \n        if 'src' in values_dict:\n            # src can be a list of strings\n            if values_dict['src'] is not None:\n                values_dict['src'] = [str(item) for item in values_dict['src']]\n    \n    elif parser_name == 'workspace':\n        # Workspace management commands\n        if 'name' in values_dict:\n            # name is typically a string\n            if values_dict['name'] is not None:\n                values_dict['name'] = str(values_dict['name'])\n    \n    # Additional type conversions can be added based on specific command requirements\n    # For example, converting integers, floats, booleans, etc.\n    \n    # Default: ensure all values are strings\n    for key, value in values_dict.items():\n        if value is not None:\n            values_dict[key] = str(value)\n    \n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Define type conversion rules based on parser name and option specifications\n    # This is a placeholder implementation - actual logic would depend on the specific\n    # options defined for each parser\n    \n    # Example: For SSH spec, convert remote_command to string if needed\n    if parser_name == 'ssh':\n        if 'remote_command' in values_dict.get(parser_name, {}):\n            # Ensure remote_command is a string\n            values_dict[parser_name]['remote_command'] = str(values_dict[parser_name]['remote_command'])\n    \n    # Example: For workspace commands, convert names to strings\n    if parser_name in ['create', 'delete', 'list', 'inventory']:\n        for key in values_dict.get(parser_name, {}).keys():\n            if key in ['name', 'workspacename', 'filename']:\n                values_dict[parser_name][key] = str(values_dict[parser_name][key])\n    \n    # Example: For plugin commands, convert names to strings\n    if parser_name in ['add', 'remove', 'update', 'search']:\n        for key in values_dict.get(parser_name, {}).keys():\n            if key in ['name', 'src', 'revision', 'src_path']:\n                values_dict[parser_name][key] = str(values_dict[parser_name][key])\n    \n    # For any other parser, ensure all values are strings\n    for key, value in values_dict.items():\n        if key == parser_name:\n            for subkey, subvalue in value.items():\n                if subvalue is not None:\n                    values_dict[key][subkey] = str(subvalue)\n    \n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Define type conversion rules based on command name and option specifications\n    # This is a placeholder implementation - actual logic would depend on the specific\n    # command and its expected argument types\n    \n    # Get the option specifications for the given parser_name\n    options_specs = self.get_parser_option_specs(parser_name)\n    \n    # Map option names to their expected types\n    type_map = {}\n    for option_spec in options_specs:\n        option_name = option_spec['name']\n        option_type = option_spec.get('type', str)\n        if option_type != str:\n            type_map[option_name] = option_type\n    \n    # Convert values based on their type specifications\n    for option_name, value in values_dict.get(parser_name, {}).items():\n        if option_name in type_map:\n            option_type = type_map[option_name]\n            try:\n                if option_type == int:\n                    values_dict[parser_name][option_name] = int(value)\n                elif option_type == float:\n                    values_dict[parser_name][option_name] = float(value)\n                elif option_type == bool:\n                    values_dict[parser_name][option_name] = value.lower() in ('true', '1', 'yes', 'on')\n                elif option_type == list:\n                    values_dict[parser_name][option_name] = value.split(',') if value else []\n                elif option_type == dict:\n                    # Convert string to dict if needed\n                    if value and isinstance(value, str):\n                        values_dict[parser_name][option_name] = eval(value)\n            except (ValueError, SyntaxError):\n                # If conversion fails, keep original string value\n                pass\n    \n    return values_dict"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9293, "num_retrieved": 38, "inference_time": 1.7180655002593994, "generate_results": ["def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n    \n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    \n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec_dict = yaml.safe_load(stream) or {}\n    \n    # Flatten the dictionary by removing nested structures\n    flat_dict = {}\n    \n    def flatten_dict(data, parent_key='', sep='.'):\n        for k, v in data.items():\n            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n            if isinstance(v, dict):\n                flatten_dict(v, new_key, sep)\n            else:\n                flat_dict[new_key] = v\n    \n    flatten_dict(spec_dict)\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    \n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec_dict = yaml.safe_load(stream) or {}\n    \n    # Flatten the dictionary by recursively processing nested keys\n    def flatten_dict(d, parent_key='', sep='.'):\n        items = []\n        for k, v in d.items():\n            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n            if isinstance(v, dict):\n                items.extend(flatten_dict(v, new_key, sep=sep).items())\n            elif isinstance(v, list):\n                for i, item in enumerate(v):\n                    if isinstance(item, dict):\n                        items.extend(flatten_dict(item, f\"{new_key}.{i}\", sep=sep).items())\n                    else:\n                        items.append((f\"{new_key}.{i}\", item))\n            else:\n                items.append((new_key, v))\n        return dict(items)\n    \n    return flatten_dict(spec_dict)", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    \n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as f:\n        spec_content = f.read()\n    \n    spec_dict = yaml.safe_load(spec_content) or {}\n    \n    # Flatten the dictionary by removing nested structures\n    flat_dict = {}\n    \n    def flatten_dict(data, parent_key='', sep='.'):\n        for k, v in data.items():\n            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n            if isinstance(v, dict):\n                flatten_dict(v, new_key, sep)\n            elif isinstance(v, list):\n                for i, item in enumerate(v):\n                    if isinstance(item, dict):\n                        flatten_dict(item, f\"{new_key}{sep}{i}\", sep)\n                    else:\n                        flat_dict[f\"{new_key}{sep}{i}\"] = item\n            else:\n                flat_dict[new_key] = v\n    \n    flatten_dict(spec_dict)\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    \n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as stream:\n        spec_dict = yaml.safe_load(stream) or {}\n    \n    # Flatten the dictionary by recursively processing nested keys\n    def flatten_dict(d, parent_key='', sep='.'):\n        items = []\n        for k, v in d.items():\n            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n            if isinstance(v, dict):\n                items.extend(flatten_dict(v, new_key, sep=sep).items())\n            else:\n                items.append((new_key, v))\n        return dict(items)\n    \n    return flatten_dict(spec_dict)", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    import os\n    import yaml\n    \n    spec_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.exists(spec_path):\n        return {}\n    \n    with open(spec_path, 'r') as f:\n        spec_content = f.read()\n    \n    spec_dict = yaml.safe_load(spec_content) or {}\n    \n    # Flatten the dictionary by removing nested structures\n    flat_dict = {}\n    \n    def flatten_dict(data, parent_key='', sep='.'):\n        for k, v in data.items():\n            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n            if isinstance(v, dict):\n                flatten_dict(v, new_key, sep)\n            elif isinstance(v, list):\n                for i, item in enumerate(v):\n                    if isinstance(item, dict):\n                        flatten_dict(item, f\"{new_key}.{i}\", sep)\n                    else:\n                        flat_dict[f\"{new_key}.{i}\"] = item\n            else:\n                flat_dict[new_key] = v\n    \n    flatten_dict(spec_dict)\n    \n    return flat_dict"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9321, "num_retrieved": 38, "inference_time": 1.7180655002593994, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = self.config_path\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = self.config_path\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = self.config_path\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"Set the environment variable for config path, if it is undefined.\"\"\"\n    # Determine the common config path from the infrared core package\n    version_info = version.VersionInfo('infrared')\n    common_path = pkg.resource_filename(version_info.package, 'common')\n    \n    # Set environment variables for Ansible configuration paths\n    env_vars = {\n        'ANSIBLE_ROLES_PATH': 'roles',\n        'ANSIBLE_FILTER_PLUGINS': 'filter_plugins',\n        'ANSIBLE_CALLBACK_PLUGINS': 'callback_plugins',\n        'ANSIBLE_LIBRARY': 'library'\n    }\n    \n    for envvar, specific_dir in env_vars.items():\n        conf_path = os.environ.get(envvar, '')\n        additional_conf_path = os.path.join(common_path, specific_dir)\n        if conf_path:\n            full_conf_path = ':'.join([additional_conf_path, conf_path])\n        else:\n            full_conf_path = additional_conf_path\n        os.environ[envvar] = full_conf_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    # Check if the config path is already set in environment variables\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        # Set the config path to the infrared home directory\n        # Assuming infrared_home is available in self or core settings\n        config_path = os.path.join(self.infrared_home, 'infrared.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9421, "num_retrieved": 43, "inference_time": 1.7180655002593994, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would involve parsing the spec and adding\n    # appropriate CLI arguments to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This implementation assumes the spec object has a spec_dict that defines\n    # the CLI options and groups to be added\n    spec_dict = self.kwargs.get('spec_dict', {})\n    \n    # Add shared groups if defined\n    if 'shared_groups' in spec_dict:\n        for group_dict in spec_dict['shared_groups']:\n            group = root_subparsers.add_argument_group(group_dict['title'])\n            for opt_name, opt_dict in group_dict.get('options', {}).items():\n                group.add_argument(\n                    opt_name,\n                    **opt_dict\n                )\n    \n    # Add main parser options\n    if 'options' in spec_dict:\n        for opt_name, opt_dict in spec_dict['options'].items():\n            root_subparsers.add_argument(\n                opt_name,\n                **opt_dict\n            )\n    \n    # Add subparsers if defined\n    if 'subparsers' in spec_dict:\n        for subparser_name, subparser_dict in spec_dict['subparsers'].items():\n            subparser = root_subparsers.add_parser(subparser_name, **subparser_dict)\n            # Recursively extend the subparser with its own options\n            if 'options' in subparser_dict:\n                for opt_name, opt_dict in subparser_dict['options'].items():\n                    subparser.add_argument(\n                        opt_name,\n                        **opt_dict\n                    )\n            if 'groups' in subparser_dict:\n                for group_dict in subparser_dict['groups']:\n                    group = subparser.add_argument_group(group_dict['title'])\n                    for opt_name, opt_dict in group_dict.get('options', {}).items():\n                        group.add_argument(\n                            opt_name,\n                            **opt_dict\n                        )", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This implementation assumes the spec object has a spec_dict that defines\n    # the CLI options and groups to be added\n    spec_dict = self.kwargs.get('spec_dict', {})\n    \n    # Add shared groups if defined\n    if 'shared_groups' in spec_dict:\n        for group_dict in spec_dict['shared_groups']:\n            group = root_subparsers.add_argument_group(group_dict['title'])\n            for opt_name, opt_dict in group_dict.get('options', {}).items():\n                group.add_argument(\n                    opt_name,\n                    **opt_dict\n                )\n    \n    # Add main parser options\n    if 'options' in spec_dict:\n        for opt_name, opt_dict in spec_dict['options'].items():\n            root_subparsers.add_argument(\n                opt_name,\n                **opt_dict\n            )\n    \n    # Add subparsers if defined\n    if 'subparsers' in spec_dict:\n        for subparser_name, subparser_dict in spec_dict['subparsers'].items():\n            subparser = root_subparsers.add_parser(subparser_name)\n            # Recursively extend the subparser with its own options\n            if 'options' in subparser_dict:\n                for opt_name, opt_dict in subparser_dict['options'].items():\n                    subparser.add_argument(\n                        opt_name,\n                        **opt_dict\n                    )\n            if 'groups' in subparser_dict:\n                for group_dict in subparser_dict['groups']:\n                    group = subparser.add_argument_group(group_dict['title'])\n                    for opt_name, opt_dict in group_dict.get('options', {}).items():\n                        group.add_argument(\n                            opt_name,\n                            **opt_dict\n                        )", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # Create a new subparser for this spec\n    subparser = root_subparsers.add_parser(self.name, help=self.get_name())\n    \n    # Extend the subparser with options from the spec\n    spec_dict = self.spec_helper.spec_dict\n    parser_data = spec_dict.get('subparsers', {}).get(self.name, {})\n    \n    # Add groups and options to the subparser\n    if parser_data:\n        self._add_groups(self, subparser, self.name, parser_data)", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to the main entry point.\n\n    :param root_subparsers: the subparser object to extend.\n    \"\"\"\n    # This implementation assumes the spec object has a spec_dict that defines\n    # the CLI options and groups to be added\n    spec_dict = self.kwargs.get('spec_dict', {})\n    \n    # Add shared groups if defined\n    if 'shared_groups' in spec_dict:\n        for group_dict in spec_dict['shared_groups']:\n            group = root_subparsers.add_argument_group(group_dict['title'])\n            for opt_name, opt_dict in group_dict.get('options', {}).items():\n                group.add_argument(\n                    opt_name,\n                    **opt_dict\n                )\n    \n    # Add main parser options\n    if 'options' in spec_dict:\n        for opt_name, opt_dict in spec_dict['options'].items():\n            root_subparsers.add_argument(\n                opt_name,\n                **opt_dict\n            )\n    \n    # Add subparsers if defined\n    if 'subparsers' in spec_dict:\n        for subparser_name, subparser_dict in spec_dict['subparsers'].items():\n            subparser = root_subparsers.add_parser(subparser_name)\n            # Recursively extend the subparser with its own options\n            if 'options' in subparser_dict:\n                for opt_name, opt_dict in subparser_dict['options'].items():\n                    subparser.add_argument(\n                        opt_name,\n                        **opt_dict\n                    )\n            if 'groups' in subparser_dict:\n                for group_dict in subparser_dict['groups']:\n                    group = subparser.add_argument_group(group_dict['title'])\n                    for opt_name, opt_dict in group_dict.get('options', {}).items():\n                        group.add_argument(\n                            opt_name,\n                            **opt_dict\n                        )"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "jaccard", "context_length": 8192, "actual_tokens": 9435, "num_retrieved": 60, "inference_time": 1.7180655002593994, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise Exception(f\"Path {path} does not start with root {root}\")"]}
