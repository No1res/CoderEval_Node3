{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72199, "num_retrieved": 220, "inference_time": 10.63707809150219, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 <= nanoseconds < 86400000000000)\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 <= nanoseconds < 86400000000000)\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight (0 <= nanoseconds < 86400000000000)\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71023, "num_retrieved": 182, "inference_time": 10.63707809150219, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :return: dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    # Convert to days, seconds, and nanoseconds\n    days = int(seconds // 86400)\n    remaining_seconds = int(seconds % 86400)\n    nanoseconds = int((seconds - days * 86400 - remaining_seconds) * 1000000000)\n    \n    # Ensure nanoseconds are within valid range\n    if nanoseconds < 0:\n        nanoseconds += 1000000000\n        remaining_seconds -= 1\n    \n    return Structure(b\"i\", days, remaining_seconds, nanoseconds)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: a timedelta value\n    :type value: timedelta\n    :return: a dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    days = int(seconds // 86400)\n    seconds = int(seconds % 86400)\n    return Structure(b\"d\", days, seconds)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: a timedelta value\n    :type value: timedelta\n    :return: a dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    # Convert to days, seconds, and nanoseconds\n    days = int(seconds // 86400)\n    remaining_seconds = int(seconds % 86400)\n    nanoseconds = int((seconds - days * 86400 - remaining_seconds) * 1000000000)\n    \n    # Ensure nanoseconds are within valid range\n    if nanoseconds < 0:\n        nanoseconds = 0\n    \n    return Structure(b\"d\", days, remaining_seconds, nanoseconds)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: a timedelta value\n    :type value: timedelta\n    :return: a dehydrated representation of the timedelta\n    \"\"\"\n    return temporal.dehydrate_timedelta(value)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: a timedelta value\n    :type value: timedelta\n    :return: a dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    # Convert to days, seconds, and nanoseconds\n    days = int(seconds // 86400)\n    remainder = seconds % 86400\n    seconds = int(remainder)\n    nanoseconds = int((seconds - int(seconds)) * 1000000000)\n    return Structure(b\"d\", days, seconds, nanoseconds)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72576, "num_retrieved": 233, "inference_time": 10.63707809150219, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71326, "num_retrieved": 189, "inference_time": 10.63707809150219, "generate_results": ["def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", coordinates[0], coordinates[1]) if len(coordinates) >= 2 else Structure(b\"X\", coordinates[0])", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", coordinates[0], coordinates[1]) if len(coordinates) >= 2 else Structure(b\"X\", coordinates[0])", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", coordinates[0], coordinates[1]) if len(coordinates) >= 2 else Structure(b\"X\", coordinates[0])", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", coordinates[0], coordinates[1]) if len(coordinates) >= 2 else Structure(b\"X\", coordinates[0])", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", coordinates[0], coordinates[1]) if len(coordinates) >= 2 else Structure(b\"X\", coordinates[0])"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71691, "num_retrieved": 203, "inference_time": 10.63707809150219, "generate_results": ["def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69605, "num_retrieved": 136, "inference_time": 10.63707809150219, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (3, 0):\n            handlers[protocol_version] = Bolt3\n        elif protocol_version == (5, 0):\n            handlers[protocol_version] = Bolt5x0\n        elif protocol_version == (4, 0):\n            handlers[protocol_version] = Bolt4x0\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # Return all available versions\n    handlers = {\n        (3, 0): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5,", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (3, 5):\n            handlers[protocol_version] = Bolt3\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (5, 0):\n            handlers[protocol_version] = Bolt5x0\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # Return all available versions\n    handlers = {\n        (3, 5): Bolt3,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5, 2): Bolt5x2,\n        (5, 3): Bolt5x3,\n        (5,", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific protocol version is supported\n        handlers = {}\n        # Check for specific versions in the context\n        if protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (3, 0):\n            handlers[protocol_version] = Bolt3\n        elif protocol_version == (5, 0):\n            handlers[protocol_version] = Bolt5x0\n        elif protocol_version == (4, 0):\n            handlers[protocol_version] = Bolt4x0\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # If no specific version is provided, return all available handlers\n    handlers = {\n        (3, 0): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        # Check for Bolt 3.x\n        if protocol_version == (3, 5):\n            handlers[protocol_version] = Bolt3\n        # Check for Bolt 4.x\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        # Check for Bolt 5.x\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (5, 0):\n            handlers[protocol_version] = Bolt5x0\n            \n        return handlers\n        \n    # If no specific version is provided, return all available handlers\n    handlers = {\n        (3, 5): Bolt3,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (3, 0):\n            handlers[protocol_version] = Bolt3\n        elif protocol_version == (5, 0):\n            handlers[protocol_version] = Bolt5x0\n        elif protocol_version == (4, 0):\n            handlers[protocol_version] = Bolt4x0\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # Return all available versions\n    handlers = {\n        (3, 0): Bolt3,\n        (4, 0): Bolt4x0,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5,"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69189, "num_retrieved": 122, "inference_time": 10.63707809150219, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\n    For example, a timeout may be applied::\n\n        from neo4j import unit_of_work\n\n        @unit_of_work(timeout=100)\n        def count_people_tx(tx):\n            result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n            record = result.single()\n            return record[\"persons\"]\n\n    :param metadata:\n        a dictionary with metadata.\n        Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n        It will also get logged to the ``query.log``.\n        This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n\n    :param timeout:\n        the transaction timeout in seconds.\n        Transactions that execute longer than the configured timeout will be terminated by the database.\n        This functionality allows to limit query/transaction execution time.\n        Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n        Value should not represent a negative duration.\n        A zero duration will make the transaction execute indefinitely.\n        None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        # Attach metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\n    For example, a timeout may be applied::\n\n        from neo4j import unit_of_work\n\n        @unit_of_work(timeout=100)\n        def count_people_tx(tx):\n            result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n            record = result.single()\n            return record[\"persons\"]\n\n    :param metadata:\n        a dictionary with metadata.\n        Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n        It will also get logged to the ``query.log``.\n        This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n\n    :param timeout:\n        the transaction timeout in seconds.\n        Transactions that execute longer than the configured timeout will be terminated by the database.\n        This functionality allows to limit query/transaction execution time.\n        Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n        Value should not represent a negative duration.\n        A zero duration will make the transaction execute indefinitely.\n        None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        # Attach metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\n    For example, a timeout may be applied::\n\n        from neo4j import unit_of_work\n\n        @unit_of_work(timeout=100)\n        def count_people_tx(tx):\n            result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n            record = result.single()\n            return record[\"persons\"]\n\n    :param metadata:\n        a dictionary with metadata.\n        Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n        It will also get logged to the ``query.log``.\n        This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n\n    :param timeout:\n        the transaction timeout in seconds.\n        Transactions that execute longer than the configured timeout will be terminated by the database.\n        This functionality allows to limit query/transaction execution time.\n        Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n        Value should not represent a negative duration.\n        A zero duration will make the transaction execute indefinitely.\n        None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        # Attach metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\n    For example, a timeout may be applied::\n\n        from neo4j import unit_of_work\n\n        @unit_of_work(timeout=100)\n        def count_people_tx(tx):\n            result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n            record = result.single()\n            return record[\"persons\"]\n\n    :param metadata:\n        a dictionary with metadata.\n        Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n        It will also get logged to the ``query.log``.\n        This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n\n    :param timeout:\n        the transaction timeout in seconds.\n        Transactions that execute longer than the configured timeout will be terminated by the database.\n        This functionality allows to limit query/transaction execution time.\n        Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n        Value should not represent a negative duration.\n        A zero duration will make the transaction execute indefinitely.\n        None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        # Attach metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\n    For example, a timeout may be applied::\n\n        from neo4j import unit_of_work\n\n        @unit_of_work(timeout=100)\n        def count_people_tx(tx):\n            result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n            record = result.single()\n            return record[\"persons\"]\n\n    :param metadata:\n        a dictionary with metadata.\n        Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n        It will also get logged to the ``query.log``.\n        This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n    :type metadata: dict\n\n    :param timeout:\n        the transaction timeout in seconds.\n        Transactions that execute longer than the configured timeout will be terminated by the database.\n        This functionality allows to limit query/transaction execution time.\n        Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n        Value should not represent a negative duration.\n        A zero duration will make the transaction execute indefinitely.\n        None will use the default timeout configured in the database.\n    :type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        # Attach metadata and timeout to the function\n        wrapper.metadata = metadata\n        wrapper.timeout = timeout\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71908, "num_retrieved": 209, "inference_time": 10.63707809150219, "generate_results": ["def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70211, "num_retrieved": 156, "inference_time": 10.63707809150219, "generate_results": ["def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [self._record.value(key) for key in keys] if keys else [self._record.value()]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [self._record.value(key) for key in keys] if keys else [self._record.value()]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [self._record.value(key) for key in keys] if keys else [self._record.value()]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [self._record.value(key) for key in keys] if keys else [self._record.value()]", "def values(self, *keys):\n    \"\"\"Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    return [self._record.value(key) for key in keys] if keys else [self._record.value()]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70028, "num_retrieved": 149, "inference_time": 10.63707809150219, "generate_results": ["def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises IndexError: if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if not keys:\n        # If no keys are provided, return all values\n        for key in self.keys():\n            record_dict[key] = self.value(key)\n    else:\n        # Process each key provided\n        for key in keys:\n            if isinstance(key, int):\n                # Handle index\n                if key < 0 or key >= len(self.keys()):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self.keys()[key]] = self.value(self.keys()[key])\n            else:\n                # Handle key name\n                record_dict[key] = self.value(key)\n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises IndexError: if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if keys:\n        for key in keys:\n            if isinstance(key, int):\n                if key < 0 or key >= len(self.keys()):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self.keys()[key]] = self.values(key)\n            else:\n                record_dict[key] = self.value(key)\n    else:\n        for key in self.keys():\n            record_dict[key] = self.value(key)\n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises IndexError: if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if keys:\n        for key in keys:\n            if isinstance(key, int):\n                if key < 0 or key >= len(self.keys()):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self.keys()[key]] = self.values()[key]\n            else:\n                record_dict[key] = self.value(key)\n    else:\n        for key in self.keys():\n            record_dict[key] = self.value(key)\n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises IndexError: if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if keys:\n        for key in keys:\n            if isinstance(key, int):\n                if key < 0 or key >= len(self.keys()):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self.keys()[key]] = self.values()[key]\n            else:\n                record_dict[key] = self.value(key)\n    else:\n        for key in self.keys():\n            record_dict[key] = self.value(key)\n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises IndexError: if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    if not keys:\n        # If no keys are provided, return all values\n        for key in self.keys():\n            record_dict[key] = self.value(key)\n    else:\n        # Process each key provided\n        for key in keys:\n            if isinstance(key, int):\n                # Handle index\n                if key < 0 or key >= len(self.keys()):\n                    raise IndexError(f\"Index {key} is out of bounds\")\n                record_dict[self.keys()[key]] = self.value(self.keys()[key])\n            else:\n                # Handle key name\n                record_dict[key] = self.value(key)\n    return record_dict"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70419, "num_retrieved": 161, "inference_time": 10.63707809150219, "generate_results": ["def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read a 16-bit unsigned integer\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop u16\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop u16\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop u16\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop u16\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70128, "num_retrieved": 152, "inference_time": 10.63707809150219, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\" Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69844, "num_retrieved": 144, "inference_time": 10.63707809150219, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, \" \".join(map(repr, fields)))\n    self._append(b\"\\"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71329, "num_retrieved": 192, "inference_time": 10.63707809150219, "generate_results": ["def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = abs(n - integer_part)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = abs(n - integer_part)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = abs(n - integer_part)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even number\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even number.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = abs(n - integer_part)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even number\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round to the nearest integer, with ties rounding to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the integer part and fractional part\n    integer_part = int(n)\n    fractional_part = abs(n - integer_part)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional_part < 0.5:\n        return integer_part\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional_part > 0.5:\n        return integer_part + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71654, "num_retrieved": 200, "inference_time": 10.63707809150219, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class using type() with the name, base classes, and attributes\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            # Ensure the number of coordinates matches the expected fields\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(coordinates)}\")\n            \n            # Store the coordinates\n            self._coordinates = list(coordinates)\n            \n            # Set the SRID from the srid_map if available\n            if len(fields) > 0:\n                srid = srid_map.get(fields[0], None)\n                if srid is not None:\n                    self.srid = srid\n                else:\n                    self.srid = None\n        \n        def __repr__(self):\n            return f\"{name}({self._coordinates}, srid={self.srid})\"\n        \n        def __len__(self):\n            return len(self._coordinates)\n        \n        def __getitem__(self, index):\n            return self._coordinates[index]\n    \n    # Add the class to the global namespace\n    globals()[name] = PointSubclass\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            # Store the coordinates and SRID from the fields\n            self._coordinates = coordinates\n            # Set the SRID from the srid_map\n            self.srid = srid_map.get(name, 0)\n            \n        def __repr__(self):\n            return f\"{name}({self._coordinates}, srid={self.srid})\"\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class using type() with the name, base classes, and attributes\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            # Store the coordinates and SRID\n            self._coordinates = coordinates\n            self._srid = srid_map.get(name)\n            \n            # Validate coordinates based on the SRID\n            if self._srid is not None:\n                dim = len(coordinates)\n                if dim != fields:\n                    raise ValueError(f\"SRID {self._srid} requires {fields} coordinates ({dim} provided)\")\n            \n            # Set the SRID if it was provided\n            if self._srid is not None:\n                super().__init__(coordinates)\n                self.srid = self._srid\n            else:\n                super().__init__(coordinates)\n    \n    # Return the dynamically created class\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class using type() with the name, base classes, and attributes\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Point requires {len(fields)} coordinates, got {len(coordinates)}\")\n            super().__init__(coordinates)\n        \n        @classmethod\n        def from_srid(cls, srid, *coordinates):\n            if srid not in srid_map:\n                raise ValueError(f\"SRID {srid} not found in srid_map\")\n            srid_class, dim = srid_map[srid]\n            if len(coordinates) != dim:\n                raise ValueError(f\"SRID {srid} requires {dim} coordinates, got {len(coordinates)}\")\n            return srid_class(coordinates)\n    \n    # Set the class name and add it to the namespace\n    PointSubclass.__name__ = name\n    PointSubclass.__module__ = Point.__module__\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class using type() with the name, base classes, and attributes\n    class PointSubclass(Point):\n        def __init__(self, *coordinates):\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Point requires {len(fields)} coordinates, got {len(coordinates)}\")\n            super().__init__(coordinates)\n            self._fields = fields\n        \n        def __repr__(self):\n            return f\"{name}({self._fields}, srid={self.srid})\"\n    \n    # Register the new class in the srid_map\n    srid_map[name] = (PointSubclass, len(fields))\n    \n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71417, "num_retrieved": 194, "inference_time": 10.63707809150219, "generate_results": ["def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        import warnings\n        import functools\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        \n        return wrapper\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 52035, "num_retrieved": 224, "inference_time": 3.528088077902794, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts up\n    return f'options(stringsAsFactors = FALSE, warn = 1)\\n{code}'", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    # Common R options might include setting working directory, memory limits, or other runtime settings\n    r_options = [\n        # Example options that might be needed\n        'options(warn=1)',  # Enable warnings\n        'options(digits=5)',  # Set number of significant digits\n        'options(scipen=999)',  # Prevent scientific notation\n        'options(stringsAsFactors=FALSE)',  # Prevent string factors\n    ]\n    \n    # Insert R options at the beginning of the code\n    r_options_code = '\\n'.join(r_options)\n    return f'{r_options_code}\\n{code}'", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    r_options = [\n        '--vanilla',  # Use vanilla R without user-specific settings\n        '--no-save',  # Don't save the workspace\n        '--no-restore',  # Don't restore the workspace\n        '--no-readline',  # Disable readline for better compatibility\n    ]\n    \n    # Insert R options at the beginning of the code\n    r_options_code = ' '.join(r_options)\n    return f'{r_options_code} {code}'", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    return f\"options(scipen = 999, digits = 7, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    r_options = [\n        '--vanilla',  # Use vanilla R with no user options\n        '--no-save',  # Don't save the workspace\n        '--no-restore',  # Don't restore the workspace\n        '--no-site-file',  # Don't load site-specific files\n        '--no-init-file',  # Don't load init files\n    ]\n    \n    # Insert R options at the beginning of the code\n    r_options_code = ' '.join(r_options)\n    return f'{r_options_code} {code}'"]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 58886, "num_retrieved": 257, "inference_time": 3.528088077902794, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs that runs the command\n    # on each file in varargs, with concurrency controlled by target_concurrency\n    # and color support based on the color parameter.\n    \n    # If target_concurrency is not provided, use the default from target_concurrency()\n    if target_concurrency == 1:\n        # Run one command per file sequentially\n        retcode = 0\n        output = b''\n        for file_arg in varargs:\n            new_cmd = list(cmd)\n            new_cmd.extend([file_arg])\n            try:\n                # Execute the command with the file argument\n                result = subprocess.run(new_cmd, capture_output=True, text=False, **kwargs)\n                retcode = result.returncode\n                output += result.stdout + result.stderr\n            except Exception as e:\n                retcode = 1\n                output += str(e).encode()\n        return retcode, output\n    else:\n        # For concurrency, we would typically use multiprocessing or threading\n        # However, since this is a simplified implementation, we'll just run\n        # sequentially for now to avoid complexity.\n        return 0, b''", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs that runs the command\n    # on each file argument in parallel with the specified concurrency\n    # The actual implementation would use subprocess.Popen with a pool\n    # of processes to run the command on each file argument.\n    \n    # For now, we'll just run the command on each file argument sequentially\n    # This is a placeholder implementation that would need to be expanded\n    # to handle concurrency and color properly.\n    \n    # If no files are provided, return success with empty output\n    if not varargs:\n        return 0, b''\n    \n    # Initialize result variables\n    result_code = 0\n    all_output = b''\n    \n    # Process each file argument\n    for file_arg in varargs:\n        # Create a command with the current file argument\n        current_cmd = cmd + (file_arg,)\n        \n        # Execute the command and capture output\n        try:\n            # This is a simplified version - in reality, this would use\n            # subprocess to run the command\n            # For now, we'll just return a dummy result\n            retcode, output = cmd_output_b(*current_cmd)\n            result_code = max(result_code, retcode)\n            all_output += output\n        except Exception as e:\n            result_code = 1\n            all_output += b'Error: ' + str(e).encode()\n    \n    return result_code, all_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs\n    # In a real implementation, this would spawn multiple processes\n    # to run the command on each file argument concurrently\n    \n    # For now, we'll just run the command sequentially on each file\n    # This is a placeholder implementation that would need to be expanded\n    # to handle concurrency and color properly\n    \n    # If no files are provided, return success with empty output\n    if not varargs:\n        return 0, b''\n    \n    # Process each file argument\n    total_retcode = 0\n    all_output = b''\n    \n    for file_arg in varargs:\n        # Create a new command with the current file argument\n        new_cmd = cmd + (file_arg,)\n        \n        # Execute the command and capture output\n        try:\n            # In a real implementation, we would use subprocess.Popen\n            # with proper concurrency and error handling\n            # For now, we'll simulate the behavior\n            retcode, output = cmd_output_b(*new_cmd, **kwargs)\n            total_retcode = max(total_retcode, retcode)\n            all_output += output\n        except Exception as e:\n            total_retcode = 1\n            all_output += b'Error: ' + str(e).encode()\n    \n    return total_retcode, all_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs\n    # It runs the command on each argument in varargs\n    # with the specified concurrency\n    \n    # If target_concurrency is not provided, use the default\n    if target_concurrency == 1:\n        # Run one at a time\n        result = []\n        for arg in varargs:\n            result.append(cmd_output_b(*cmd, input=arg))\n        return result[0][0], result[0][1]\n    \n    # For concurrency, we would typically use multiprocessing or threading\n    # However, without a specific implementation of concurrency control,\n    # we return a simplified version that just runs the command on each argument\n    \n    # In a real implementation, we would:\n    # 1. Split varargs into chunks based on target_concurrency\n    # 2. Run each chunk in parallel\n    # 3. Collect results and return the final exit code and output\n    \n    # Since we don't have the full implementation of concurrency,\n    # we fall back to sequential execution\n    \n    return_code = 0\n    output = b''\n    \n    for arg in varargs:\n        try:\n            retcode, out = cmd_output_b(*cmd, input=arg)\n            if retcode != 0:\n                return_code = retcode\n            output += out\n        except Exception as e:\n            return_code = 1\n            output += f\"Error: {str(e)}\\n\".encode()\n    \n    return return_code, output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs\n    # It runs the command on each argument in varargs\n    # with the specified concurrency\n    \n    # If target_concurrency is not provided, use the default\n    if target_concurrency == 1:\n        # Run one command at a time\n        retcode = 0\n        output = b''\n        for arg in varargs:\n            new_cmd = cmd + (arg,)\n            try:\n                # Execute the command with the current argument\n                result = subprocess.run(new_cmd, capture_output=True, text=False, **kwargs)\n                retcode = result.returncode\n                output += result.stdout + result.stderr\n            except Exception as e:\n                retcode = 1\n                output += str(e).encode()\n        return retcode, output\n    else:\n        # For concurrency, we would typically use multiprocessing\n        # or threading to run commands in parallel\n        # This is a simplified version that just runs sequentially\n        # for demonstration purposes\n        retcode = 0\n        output = b''\n        for arg in varargs:\n            new_cmd = cmd + (arg,)\n            try:\n                result = subprocess.run(new_cmd, capture_output=True, text=False, **kwargs)\n                retcode = result.returncode\n                output += result.stdout + result.stderr\n            except Exception as e:\n                retcode = 1\n                output += str(e).encode()\n        return retcode, output"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 28187, "num_retrieved": 116, "inference_time": 3.528088077902794, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Use a fixed seed for deterministic shuffling\n    random.shuffle(result)\n    return result", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Deterministic shuffle using fixed seed\n    random.shuffle(result)\n    return result", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Use deterministic seed for consistent shuffling\n    random.shuffle(result)\n    return result", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Deterministic shuffle using fixed seed\n    random.shuffle(result)\n    return result", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Use a fixed seed for deterministic shuffling\n    random.shuffle(result)\n    return result"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 45680, "num_retrieved": 198, "inference_time": 3.528088077902794, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    version = []\n    for part in parts:\n        try:\n            version.append(int(part))\n        except ValueError:\n            # If a part is not a valid integer, treat it as 0\n            version.append(0)\n    \n    # Ensure we have at least one part\n    if not version:\n        return (0,)\n    \n    return tuple(version)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 62520, "num_retrieved": 288, "inference_time": 3.528088077902794, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the command might have a shebang that needs to be processed\n    # On Windows, shebangs are not parsed by the system, so we need to ensure\n    # that the command is properly formatted for Windows execution\n    \n    # If the first element of the command is a shebang (starts with #!), \n    # we need to replace it with the actual executable path\n    if cmd and cmd[0].startswith('#!'):\n        # Extract the executable path from the shebang\n        # On Windows, we need to ensure the path is properly formatted\n        # and that it's a valid executable\n        executable = cmd[0][2:]  # Remove the '#!' prefix\n        # Normalize the path to ensure it's in the correct format for Windows\n        # This includes handling paths with backslashes and ensuring they're absolute\n        if executable.startswith('/'):\n            # Convert forward slashes to backslashes for Windows\n            executable = executable.replace('/', '\\\\')\n        # Ensure the path is absolute\n        if not executable.startswith('\\\\'):\n            executable = os.path.abspath(executable)\n        \n        # Replace the shebang with the executable path\n        new_cmd = (executable,) + cmd[1:]\n        return new_cmd\n    \n    # For non-shebang commands, we just return the original command\n    # This handles the case where the command is already properly formatted\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the first element is a shebang path\n    if cmd and cmd[0].startswith('#!'):\n        # If it's a deep-path shebang, we need to ensure it's properly resolved\n        # For Windows, we need to handle the path differently\n        if sys.platform == 'win32':\n            # On Windows, we need to ensure the shebang path is properly formatted\n            # and that it's not being misinterpreted\n            shebang_path = cmd[0]\n            # Normalize the path to ensure it's in the correct format\n            # This handles deep-path shebangs by ensuring the path is properly formatted\n            # and that it's compatible with Windows\n            if shebang_path.startswith('#!'):\n                # Extract the actual path from the shebang\n                path_part = shebang_path[2:]\n                # Ensure the path is properly formatted for Windows\n                if path_part.startswith('python'):\n                    # For Python shebangs, we need to ensure the path is properly formatted\n                    # and that it's compatible with Windows\n                    return (path_part,) + cmd[1:]\n                else:\n                    # For other shebangs, we need to ensure the path is properly formatted\n                    # and that it's compatible with Windows\n                    return (path_part,) + cmd[1:]\n            else:\n                # If it's not a shebang, just return the original command\n                return cmd\n        else:\n            # On non-Windows systems, just return the original command\n            return cmd\n    \n    # For non-shebang cases, just return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the command might have a shebang that needs to be processed\n    # On Windows, shebangs are not parsed by the system, so we need to ensure\n    # that the command is properly normalized to work with the system\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure that the command is properly formatted\n        # and that any shebangs are handled correctly\n        # We also need to ensure that deep-path shebangs work properly\n        # This is done by normalizing the command to ensure it's in a format\n        # that Windows can handle\n        return tuple(\n            os.path.normpath(os.path.join(*cmd)) if cmd else cmd\n        )\n    \n    # For non-Windows platforms, we can use the default behavior\n    # which is to normalize the command path\n    return tuple(\n        os.path.normpath(os.path.join(*cmd)) if cmd else cmd\n    )", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the command might be a shebang path\n    # On Windows, we need to ensure that the command is properly normalized\n    # and that shebangs are handled correctly\n    \n    # If the first element is a path that might be a shebang, we need to\n    # ensure it's properly resolved and handled\n    if cmd and cmd[0].startswith('#!'):\n        # For shebangs, we need to ensure the path is properly resolved\n        # and that it's not being misinterpreted on Windows\n        # This is a workaround for Windows not parsing shebangs properly\n        # and for the issue described in https://bugs.python.org/issue8557\n        # We normalize the path to ensure it's in a format that Windows can handle\n        first_cmd = cmd[0]\n        # Extract the path part after the shebang\n        path_part = first_cmd[2:] if first_cmd.startswith('#!') else first_cmd\n        # Normalize the path for Windows\n        if sys.platform == 'win32':\n            # Convert forward slashes to backslashes for Windows compatibility\n            path_part = path_part.replace('/', '\\\\')\n            # Ensure the path is properly formatted for Windows\n            path_part = path_part.replace('\\\\', '\\\\\\\\')\n        # Replace the original command with the normalized one\n        return (path_part,) + cmd[1:]\n    \n    # For non-shebang cases, we just return the original command\n    # This ensures that commands without shebangs are not modified\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the first element is a shebang path\n    if cmd and cmd[0].startswith('#!'):\n        # If it's a shebang, we need to ensure it's properly formatted\n        # and that the path is absolute\n        shebang_path = cmd[0]\n        if not os.path.isabs(shebang_path):\n            # Convert to absolute path\n            shebang_path = os.path.abspath(shebang_path)\n        # Replace the first element with the absolute path\n        return (shebang_path,) + cmd[1:]\n    \n    # For Windows, we need to ensure that paths are properly handled\n    # and that the command is not broken by the shebang parsing\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure that the command is properly\n        # formatted and that any paths are absolute\n        new_cmd = []\n        for arg in cmd:\n            # Convert any relative paths to absolute paths\n            if os.path.isabs(arg):\n                new_cmd.append(arg)\n            else:\n                new_cmd.append(os.path.abspath(arg))\n        return tuple(new_cmd)\n    \n    # For other platforms, just return the original command\n    return cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 10175, "num_retrieved": 54, "inference_time": 3.528088077902794, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Try to get the value from cache\n            if cache_instance.get(cache_key) is not None:\n                return cache_instance[cache_key]\n            \n            # If not in cache, compute the value and store it\n            result = func(*args, **kwargs)\n            cache_instance[cache_key] = result\n            return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_value = key(*args, **kwargs)\n            \n            # Try to get the result from cache\n            if key_value in cache_instance:\n                return cache_instance[key_value]\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache_instance[key_value] = result\n            return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args, **key_kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[cache_key]\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n                return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[key_args]\n            except KeyError:\n                # If not in cache, compute the result\n                result = func(*args, **kwargs)\n                # Store the result in cache\n                cache_instance[key_args] = result\n            \n            return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 10517, "num_retrieved": 52, "inference_time": 3.528088077902794, "generate_results": ["import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    def decorator(func):\n        cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n        if typed:\n            key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[key(*args, **kwargs)]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key(*args, **kwargs)] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize, ttl, timer=timer)\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    return cached(cache, key=key)", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    def decorator(func):\n        cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n        if typed:\n            key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[key(*args, **kwargs)]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key(*args, **kwargs)] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize, ttl, timer=timer)\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    return cached(cache, key=key)", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize, ttl, timer=timer)\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    return cached(cache, key=key)"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 3.528088077902794, "generate_results": ["def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return typedkey(*args, **kwargs)\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return typedkey(*args, **kwargs)\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return typedkey(*args, **kwargs)\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return typedkey(*args, **kwargs)\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 3.528088077902794, "generate_results": ["import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, typedkey)\n    else:\n        cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            return cache.get((args, tuple(sorted(kwargs.items()))), func(*args, **kwargs))\n        return wrapper\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: _HashedTuple(args + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, typedkey)\n    else:\n        cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            return cache.get((args, tuple(sorted(kwargs.items()))), func(*args, **kwargs))\n        return wrapper\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: _HashedTuple(args + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: _HashedTuple(args + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 10041, "num_retrieved": 49, "inference_time": 3.528088077902794, "generate_results": ["from functools import wraps\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return typedkey(*args, **kwargs)\n        else:\n            # Use hashkey for standard hashing\n            key_func = hashkey\n            \n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "from functools import wraps\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return typedkey(*args, **kwargs)\n        else:\n            # Use hashkey for standard hashing\n            key_func = hashkey\n        \n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        if typed:\n            key_func = lambda *args, **kwargs: typedkey(*args, **kwargs)\n        else:\n            key_func = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "from functools import wraps\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return typedkey(*args, **kwargs)\n        else:\n            # Use hashkey for standard hashing\n            key_func = hashkey\n            \n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from cachetools import LFUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = typedkey if typed else hashkey\n        \n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 8936, "num_retrieved": 45, "inference_time": 3.528088077902794, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n        return (key, self.pop(key))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n        return (key, self.pop(key))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 11144, "num_retrieved": 57, "inference_time": 3.528088077902794, "generate_results": ["def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 11342, "num_retrieved": 59, "inference_time": 3.528088077902794, "generate_results": ["def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 11327, "num_retrieved": 60, "inference_time": 3.528088077902794, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        key_func = key if callable(key) else (lambda *args, **kwargs: key(*args, **kwargs))\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the method call\n            k = key_func(self, *args, **kwargs)\n            \n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            \n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                with lock:\n                    cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n            \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache.hits, cache.misses\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    cache.hits = 0\n                    cache.misses = 0\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle caching\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the value from cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Key not found, call the original function\n                    v = func(self, *args, **kwargs)\n                    # Store the result in cache\n                    try:\n                        cache.setdefault(k, v)\n                    except ValueError:\n                        # Value too large, return the result directly\n                        return v\n            \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        wrapper.cache_info = lambda: _CacheInfo(cache.hits, cache.misses, cache.maxsize, cache.currsize)\n        wrapper.cache_clear = lambda: cache.clear()\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        cache_key = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the cache based on the method's arguments\n            k = cache_key(self, *args, **kwargs)\n            \n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            \n            # Call the original method and store the result\n            v = func(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                with lock:\n                    cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n            \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache._stats if hasattr(cache, '_stats') else (0, 0)\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    if hasattr(cache, '_stats'):\n                        cache._stats = [0, 0]\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Use the provided cache or create a new one if cache is None\n        if cache is None:\n            cache = _UnboundCache()\n        \n        # Create a wrapper function\n        def wrapper(self, *args, **kwargs):\n            # Generate the key using the provided key function\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            with lock:\n                try:\n                    cache.setdefault(k, v)\n                except ValueError:\n                    return v  # value too large\n        \n        # Add cache info and clear methods to the wrapper\n        wrapper.cache_info = lambda: _CacheInfo(0, 0, cache.maxsize, cache.currsize)\n        wrapper.cache_clear = lambda: cache.clear()\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        cache_key = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the cache based on the method's arguments\n            k = cache_key(self, *args, **kwargs)\n            \n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            \n            # Call the original method and store the result\n            v = func(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                with lock:\n                    cache.setdefault(k, v)\n            except ValueError:\n                # Value too large\n                return v\n                \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache.hits, cache.misses\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    cache.hits = 0\n                    cache.misses = 0\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 7109, "num_retrieved": 23, "inference_time": 3.528088077902794, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback\n    tb = traceback.extract_tb(e.__traceback__, max_level)\n    \n    # Format the exception\n    exc_type = e.__class__.__name__\n    exc_msg = e.args[0] if e.args else \"No message\"\n    \n    # Build the trace string\n    trace_lines = []\n    for frame in tb:\n        filename = frame.filename\n        lineno = frame.lineno\n        name = frame.name\n        line = frame.line\n        \n        # Limit path level\n        if filename.startswith('/'):\n            path_parts = filename.split('/')\n            if len(path_parts) > max_path_level:\n                filename = '/'.join(path_parts[:max_path_level]) + '/...'\n        \n        trace_lines.append(f\"{filename}:{lineno} in {name}\")\n        if line:\n            trace_lines.append(f\"    {line}\")\n    \n    # Join the lines\n    trace_str = \"\\n\".join(trace_lines)\n    \n    # Return formatted string\n    return f\"{exc_type}: {exc_msg}\\n{trace_str}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            trace_lines.append(f\"{filename}:{lineno}:{name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        result = f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n        \n        return result\n        \n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the formatted string\n        result = f\"{exc_type}: {exc_msg}\"\n        \n        # Add traceback if there is any\n        if tb:\n            result += \"\\nTraceback (most recent call last):\\n\"\n            for frame in tb:\n                result += f\"  File \\\"{frame.filename}\\\", line {frame.lineno}, in {frame.name}\\n\"\n        \n        return result\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback\n    tb = traceback.extract_tb(e.__traceback__, max_level)\n    \n    # Format the exception\n    exc_type = e.__class__.__name__\n    exc_msg = e.args[0] if e.args else \"No message\"\n    \n    # Build the formatted string\n    result = f\"{exc_type}: {exc_msg}\"\n    \n    # Add traceback if there is any\n    if tb:\n        result += \"\\nTraceback (most recent call last):\\n\"\n        for frame in tb:\n            result += f\"  File \\\"{frame.filename}\\\", line {frame.lineno}, in {frame.name}\\n\"\n    \n    return result", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the formatted string\n        result = f\"{exc_type}: {exc_msg}\"\n        \n        # Add traceback if there is any\n        if tb:\n            result += \"\\nTraceback (most recent call last):\\n\"\n            for frame in tb:\n                result += f\"  File \\\"{frame.filename}\\\", line {frame.lineno}, in {frame.name}\\n\"\n        \n        return result\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 11406, "num_retrieved": 43, "inference_time": 3.528088077902794, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return the number of bytes written (length of text_buffer in bytes)\n        return len(bytes_buffer)\n        \n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except Exception as e:\n        # Log error (if logging is available) and return negative value\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return the number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except Exception as e:\n        # Log error (if logging is available) and return negative value\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return the number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 11524, "num_retrieved": 44, "inference_time": 8.318515926599503, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 11387, "num_retrieved": 44, "inference_time": 8.318515926599503, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 5115, "num_retrieved": 16, "inference_time": 8.318515926599503, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real implementation, this might involve clearing handlers, resetting log levels, or removing filters\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real scenario, this would involve clearing handlers, resetting log levels, etc.\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real implementation, this might involve clearing handlers, resetting log levels, or removing filters\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real implementation, this might involve clearing handlers, resetting log levels, or removing filters\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real scenario, this would involve clearing handlers, resetting log levels, etc.\n    pass"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71630, "num_retrieved": 218, "inference_time": 8.318515926599503, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This function should return the actual implementation class\n    # without the 'Py' or 'Fallback' suffix\n    # The actual implementation would typically be determined\n    # by the context or by checking for C optimizations\n    # based on environment variables or other factors\n    return self._getTargetClassImplementation()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This function should return the actual implementation class\n    # without the 'Py' or 'Fallback' suffix\n    # The actual implementation would typically be determined\n    # by the context of the test or the specific class being tested\n    # For the purpose of this implementation, we return a placeholder\n    # that would be replaced with the actual implementation in a real scenario\n    return self._getTargetClassImplementation()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This function should return the actual implementation class\n    # without the 'Py' or 'Fallback' suffix\n    # The actual implementation would typically be determined\n    # by the context or by checking for C optimizations\n    # based on environment variables or other factors\n    # For now, we return a placeholder implementation\n    # that would be replaced in a real implementation\n    return self._getTargetClassImplementation()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This function should return the actual implementation class\n    # without the 'Py' or 'Fallback' suffix\n    # The actual implementation would typically be determined\n    # by the context of the test or the specific class being tested\n    # For now, we return a placeholder implementation\n    # In a real implementation, this would check for C optimizations\n    # and return the appropriate class\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This function should return the actual implementation class\n    # without the 'Py' or 'Fallback' suffix\n    # The actual implementation would typically be determined\n    # by the context of the test or the specific class being tested\n    # For the purpose of this implementation, we return a placeholder\n    # that would be replaced with the actual implementation in a real scenario\n    return self._getTargetClassImplementation()"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71692, "num_retrieved": 220, "inference_time": 8.318515926599503, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Sort the orderings by their first element\n    sorted_orderings = sorted(orderings)\n    \n    # Build the result ordering\n    result = []\n    seen = set()\n    \n    # Process each ordering in order\n    for ordering in sorted_orderings:\n        # Add elements that are not already in the result\n        for element in ordering:\n            if element not in seen:\n                # Find the position of this element in the current ordering\n                # and ensure it's consistent with other orderings\n                result.append(element)\n                seen.add(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Sort the orderings by their first element\n    sorted_orderings = sorted(orderings, key=lambda x: x[0])\n    \n    # Build the merged ordering\n    result = []\n    seen = set()\n    \n    # Process each ordering in the sorted order\n    for ordering in sorted_orderings:\n        # Add elements from this ordering that haven't been seen yet\n        for element in ordering:\n            if element not in seen:\n                # Check if this element appears in other orderings\n                # and if the suffix starting with this element is present\n                # in all orderings that contain it\n                valid = True\n                for other_ordering in orderings:\n                    if element in other_ordering:\n                        # Find the position of this element in the other ordering\n                        pos_in_other = other_ordering.index(element)\n                        # Check if the suffix starting with this element is present\n                        suffix = other_ordering[pos_in_other:]\n                        if suffix not in [ordering[pos_in_other:] for ordering in orderings]:\n                            valid = False\n                            break\n                \n                if valid:\n                    result.append(element)\n                    seen.add(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = {}\n            positions[element][i] = j\n    \n    # Sort the orderings by the first element that appears in them\n    # This ensures that we process the orderings in a consistent way\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else '')\n    \n    # Build the result ordering\n    result = []\n    seen = set()\n    \n    # Process each ordering in the sorted order\n    for ordering in sorted_orderings:\n        # For each element in the current ordering, if it's not already in the result\n        # and if it appears in other orderings, ensure that the suffix starting with it\n        # is preserved in the result\n        for element in ordering:\n            if element not in seen:\n                # Check if this element appears in other orderings\n                # If it does, we need to ensure that the suffix starting with it\n                # is preserved in the result\n                # We'll just add it to the result and mark it as seen\n                result.append(element)\n                seen.add(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as the base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering should be inserted\n        # We need to preserve the order of elements in the current ordering\n        # and ensure that any common suffix is maintained\n        \n        # Create a list to hold the merged result\n        merged = []\n        \n        # Keep track of the current position in the result\n        result_idx = 0\n        ordering_idx = 0\n        \n        # Merge elements from both orderings\n        while result_idx < len(result) and ordering_idx < len(ordering):\n            if result[result_idx] == ordering[ordering_idx]:\n                # Elements match, add to merged\n                merged.append(result[result_idx])\n                result_idx += 1\n                ordering_idx += 1\n            elif result[result_idx] < ordering[ordering_idx]:\n                # Current result element is smaller, add it\n                merged.append(result[result_idx])\n                result_idx += 1\n            else:\n                # Current ordering element is smaller, add it\n                merged.append(ordering[ordering_idx])\n                ordering_idx += 1\n        \n        # Add any remaining elements from either ordering\n        while result_idx < len(result):\n            merged.append(result[result_idx])\n            result_idx += 1\n        while ordering_idx < len(ordering):\n            merged.append(ordering[ordering_idx])\n            ordering_idx += 1\n        \n        # Update result with the merged list\n        result = merged\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Start with the first ordering as the base\n    result = orderings[0][:]\n    \n    # For each subsequent ordering, merge it into the result\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts\n        # We need to find the first element in the current ordering\n        # that is not already in the result\n        i = 0\n        while i < len(ordering):\n            # If we find an element that is already in the result,\n            # we need to check if the suffix starting from that element\n            # is present in the result\n            if ordering[i] in result:\n                # Find the position of this element in the result\n                pos = result.index(ordering[i])\n                # Check if the suffix starting at this position matches\n                # the suffix in the current ordering\n                suffix_in_result = result[pos:]\n                suffix_in_ordering = ordering[i:]\n                \n                # If they don't match, we need to insert the elements\n                # from the ordering after the position\n                if suffix_in_result != suffix_in_ordering:\n                    # Insert the elements from the ordering after the position\n                    # but only if they are not already in the result\n                    for j in range(i + 1, len(ordering)):\n                        if ordering[j] not in result:\n                            result.insert(pos + len(suffix_in_result), ordering[j])\n                break\n            i += 1\n        else:\n            # If we didn't find any element in the ordering that is in the result,\n            # we need to insert the entire ordering\n            result.extend(ordering)\n    \n    return result"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73448, "num_retrieved": 282, "inference_time": 8.318515926599503, "generate_results": ["def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # If provides is a tuple or list, convert to a declaration\n    if isinstance(provides, (list, tuple)):\n        from zope.interface.interfaces import Declaration\n        return Declaration(*provides)\n    \n    # If provides is already a declaration, return it directly\n    if hasattr(provides, 'flattened'):\n        return provides\n    \n    # Fallback to creating a declaration from the provides attribute\n    from zope.interface.interfaces import Declaration\n    return Declaration(provides)", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # If provides is a tuple or list, convert to a declaration\n    if isinstance(provides, (list, tuple)):\n        from zope.interface.interfaces import Declaration\n        return Declaration(*provides)\n    \n    # If provides is already a declaration, return it directly\n    if hasattr(provides, '__iter__'):\n        from zope.interface.interfaces import Declaration\n        return Declaration(*provides)\n    \n    # Fallback to empty declaration if none of the above\n    from zope.interface.interfaces import IDeclaration\n    return IDeclaration()", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # If provides is a tuple or list, convert to a declaration\n    if isinstance(provides, (list, tuple)):\n        from zope.interface.interfaces import Declaration\n        return Declaration(*provides)\n    \n    # If provides is already a Declaration, return it directly\n    if hasattr(provides, 'flattened'):\n        return provides\n    \n    # Fallback to creating a declaration from the provides attribute\n    from zope.interface.interfaces import Declaration\n    return Declaration(provides)", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # If provides is a tuple or list, convert to a declaration\n    if isinstance(provides, (list, tuple)):\n        from zope.interface.interfaces import Declaration\n        return Declaration(*provides)\n    \n    # If provides is already a Declaration, return it directly\n    if hasattr(provides, 'flattened'):\n        return provides\n    \n    # Fallback to creating a declaration from the provides value\n    from zope.interface.interfaces import Declaration\n    return Declaration(provides)", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return an empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # If provides is a tuple or list, convert to a declaration\n    if isinstance(provides, (tuple, list)):\n        from zope.interface.declarations import Declaration\n        return Declaration(*provides)\n    \n    # If provides is already a declaration, return it\n    if hasattr(provides, '__contains__'):\n        return provides\n    \n    # Fallback to empty declaration\n    from zope.interface.interfaces import IDeclaration\n    return IDeclaration()"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72555, "num_retrieved": 251, "inference_time": 8.318515926599503, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of what constitutes the \"ordered minimum equivalent\"\n    \n    # If classes is empty, return empty list\n    if not classes:\n        return []\n    \n    # Remove duplicates by converting to a set and back to list\n    unique_classes = list(set(classes))\n    \n    # Sort the unique classes to ensure a consistent order\n    # This is a minimal reduction that maintains the set of classes\n    # while ensuring a deterministic order\n    unique_classes.sort()\n    \n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of what constitutes the \"ordered minimum equivalent\"\n    \n    # If the input is empty, return empty list\n    if not classes:\n        return []\n    \n    # Sort classes by their module and name for consistent ordering\n    # This is a simple heuristic to create a minimal ordered list\n    sorted_classes = sorted(classes, key=lambda c: (c.__module__, c.__name__))\n    \n    # Remove duplicates while preserving order\n    unique_classes = []\n    seen = set()\n    for cls in sorted_classes:\n        cls_key = (cls.__module__, cls.__name__)\n        if cls_key not in seen:\n            seen.add(cls_key)\n            unique_classes.append(cls)\n    \n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of what constitutes the \"ordered minimum equivalent\"\n    \n    # If classes is empty, return empty list\n    if not classes:\n        return []\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by class name (or some other ordering criterion)\n    # This is a minimal implementation that just removes duplicates\n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of what constitutes the \"ordered minimum equivalent\"\n    \n    # If classes is empty, return empty list\n    if not classes:\n        return []\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by class name (or some other ordering criterion)\n    # This is a minimal implementation that just removes duplicates\n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of what constitutes the \"ordered minimum equivalent\"\n    \n    # If classes is empty, return empty list\n    if not classes:\n        return []\n    \n    # Remove duplicates while preserving order\n    seen = set()\n    unique_classes = []\n    for cls in classes:\n        if cls not in seen:\n            seen.add(cls)\n            unique_classes.append(cls)\n    \n    # Sort by class name (or some other ordering criterion)\n    # This is a minimal implementation that just removes duplicates\n    return unique_classes"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 74430, "num_retrieved": 314, "inference_time": 8.318515926599503, "generate_results": ["def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if hasattr(desc, 'getName'):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base interfaces\n            names_and_descriptions = []\n            # Get all attributes from the interface and its base interfaces\n            for base in self.__bases__:\n                if hasattr(base, 'namesAndDescriptions'):\n                    for name, desc in base.namesAndDescriptions(all=True):\n                        names_and_descriptions.append((name, desc))\n            # Add attributes defined by this interface\n            for name, desc in self.__dict__.items():\n                if hasattr(desc, 'getName'):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            return [(name, desc) for name, desc in self.direct.items()]\n        else:\n            # Return all attributes including those defined by base interfaces\n            result = []\n            # Add directly defined attributes\n            for name, desc in self.direct.items():\n                result.append((name, desc))\n            # Add attributes from base interfaces\n            for base in self.__bases__:\n                if hasattr(base, 'namesAndDescriptions'):\n                    for name, desc in base.namesAndDescriptions(all=True):\n                        result.append((name, desc))\n            return result", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            return [(name, desc) for name, desc in self.direct.items()]\n        else:\n            # Return all attributes including those defined by base interfaces\n            result = []\n            # Add directly defined attributes\n            for name, desc in self.direct.items():\n                result.append((name, desc))\n            # Add attributes from base interfaces\n            for base in self.__bases__:\n                if hasattr(base, 'namesAndDescriptions'):\n                    for name, desc in base.namesAndDescriptions(all=True):\n                        result.append((name, desc))\n            return result", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if hasattr(desc, 'getName'):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base interfaces\n            names_and_descriptions = []\n            # Get all attributes from the interface and its base interfaces\n            for base in self.__bases__:\n                if hasattr(base, 'namesAndDescriptions'):\n                    for name, desc in base.namesAndDescriptions(all=True):\n                        names_and_descriptions.append((name, desc))\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if hasattr(desc, 'getName'):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            return [(name, desc) for name, desc in self.direct.items()]\n        else:\n            # Return all attributes including those defined by base interfaces\n            result = []\n            # Start with directly defined attributes\n            for name, desc in self.direct.items():\n                result.append((name, desc))\n            # Add attributes from base interfaces\n            for base in self.__bases__:\n                if hasattr(base, 'namesAndDescriptions'):\n                    for name, desc in base.namesAndDescriptions(all=True):\n                        result.append((name, desc))\n            return result"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 74346, "num_retrieved": 309, "inference_time": 8.318515926599503, "generate_results": ["def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        # Return all attribute names including those from base interfaces\n        names = []\n        for base in self.__bases__:\n            names.extend(base.names(all=True))\n        return list(set(names))", "def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        return list(self.__dict__.keys()) + list(self.__bases__)", "def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        return list(self.__dict__.keys()) + list(self.__bases__)", "def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.get('__names__', []))\n    else:\n        return list(self.__dict__.get('__names__', []) + self.__dict__.get('__all_names__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        return list(self.__dict__.keys()) + list(self.__bases__)"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 66665, "num_retrieved": 273, "inference_time": 8.318515926599503, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, IDeclaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, IDeclaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, IDeclaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence.flattened())\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence.flattened())\n    else:\n        output.append(sequence)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70900, "num_retrieved": 195, "inference_time": 8.318515926599503, "generate_results": ["def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        import zope.interface._c_optimizations\n        return True\n    except ImportError:\n        return False", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _use_c_impl\n        return _use_c_impl\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        import zope.interface._c_optimizations\n        return True\n    except ImportError:\n        return False", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _use_c_impl\n        return _use_c_impl\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        import zope.interface._c_optimizations\n        return True\n    except ImportError:\n        return False"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71259, "num_retrieved": 208, "inference_time": 8.318515926599503, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\n    This takes into account whether we're on PyPy and the value of the\n    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy, in which case we don't attempt C optimizations\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    # Default to attempting C optimizations\n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\n    This takes into account whether we're on PyPy and the value of the\n    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy, in which case we don't attempt C optimizations\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    # Default to attempting C optimizations\n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\n    This takes into account whether we're on PyPy and the value of the\n    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy, which does not support C optimizations\n    import sys\n    if sys.implementation.name == 'pypy':\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '0')\n    \n    # Return True if PURE_PYTHON is not set or is not '1'\n    return pure_python != '1'", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\n    This takes into account whether we're on PyPy and the value of the\n    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy, in which case we don't attempt C optimizations\n    if hasattr(sys, 'pypy_version_info'):\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    # Default to attempting C optimizations\n    return True", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\n    This takes into account whether we're on PyPy and the value of the\n    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # Check if we're on PyPy, which does not support C optimizations\n    import sys\n    if sys.implementation.name == 'pypy':\n        return False\n    \n    # Check the PURE_PYTHON environment variable\n    import os\n    pure_python = os.environ.get('PURE_PYTHON', '').lower()\n    if pure_python in ('1', 'true', 'yes'):\n        return False\n    \n    # Default to attempting C optimizations\n    return True"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72292, "num_retrieved": 242, "inference_time": 8.318515926599503, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71337, "num_retrieved": 210, "inference_time": 8.318515926599503, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68305, "num_retrieved": 109, "inference_time": 8.318515926599503, "generate_results": ["def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, '_make_bins') and callable(self._make_bins):\n        self._data = self._make_bins()\n    else:\n        self._data = [self._initial_value] * len(self._bins)", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, '_make_bins'):\n        self._bins = self._make_bins()\n    else:\n        self._bins = self._initial_value", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, '_make_bins') and callable(self._make_bins):\n        self._data = self._make_bins()\n    else:\n        self._data = [self._initial_value] * len(self._bins)", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, '_make_bins') and callable(self._make_bins):\n        self._data = self._make_bins()\n    else:\n        self._data = [self._initial_value] * len(self._bins)", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._cur_context = {}\n    if hasattr(self, '_make_bins') and callable(self._make_bins):\n        self._data = self._make_bins()\n    else:\n        self._data = [self._initial_value] * len(self._bins)"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68226, "num_retrieved": 106, "inference_time": 8.318515926599503, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    raise NotImplementedError(\"to_csv is deprecated and not used. Use ToCSV instead.\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    raise NotImplementedError(\"to_csv is deprecated and not used. Use ToCSV instead.\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    raise NotImplementedError(\"to_csv is deprecated and not used. Use ToCSV instead.\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    raise NotImplementedError(\"to_csv is deprecated and not used. Use ToCSV instead.\")", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    raise NotImplementedError(\"to_csv is deprecated and not used. Use ToCSV instead.\")"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69190, "num_retrieved": 140, "inference_time": 11.12499937415123, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    for err in errors:\n        if err[1] == coord_name:\n            return err[3]\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    for err in errors:\n        if err[1] == coord_name:\n            return err[3]\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    for err in errors:\n        if err[1] == coord_name:\n            return err[3]\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    for err in errors:\n        if err[1] == coord_name:\n            return err[3]\n    return None", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    errors = self._parsed_error_names\n    for err in errors:\n        if err[1] == coord_name:\n            return err[3]\n    return None"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68109, "num_retrieved": 102, "inference_time": 11.12499937415123, "generate_results": ["def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error indices from the graph\n    error_x_ind = self._error_x_ind\n    error_y_ind = self._error_y_ind\n    \n    # Create error subcontext\n    error_context = {}\n    \n    # Add x error index if it exists\n    if error_x_ind >= 0:\n        error_context[\"x\"] = {\"index\": error_x_ind}\n    \n    # Add y error index if it exists\n    if error_y_ind >= 0:\n        error_context[\"y\"] = {\"index\": error_y_ind}\n    \n    # Append to context.error if it exists, otherwise create it\n    if \"error\" in context:\n        context[\"error\"].update(error_context)\n    else:\n        context[\"error\"] = error_context", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error indices from the graph\n    error_x_ind = self._error_x_ind\n    error_y_ind = self._error_y_ind\n    \n    # Create error subcontext\n    error_subcontext = {}\n    \n    # Add x error index if it exists\n    if error_x_ind >= 0:\n        error_subcontext[\"x\"] = {\"index\": error_x_ind}\n    \n    # Add y error index if it exists\n    if error_y_ind >= 0:\n        error_subcontext[\"y\"] = {\"index\": error_y_ind}\n    \n    # Append to context.error if it exists, otherwise create it\n    if \"error\" in context:\n        context[\"error\"].update(error_subcontext)\n    else:\n        context[\"error\"] = error_subcontext", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Create error subcontext if it doesn't exist\n    if \"error\" not in context:\n        context[\"error\"] = {}\n    \n    # Add error indices based on the graph's error fields\n    if self._error_x_ind != -1:\n        context[\"error\"][\"x\"] = {\"index\": self._error_x_ind}\n    if self._error_y_ind != -1:\n        context[\"error\"][\"y\"] = {\"index\": self._error_y_ind}\n    \n    # Note: The docstring mentions \"x_low\", \"y_low\", etc. but the implementation\n    # follows the pattern of using \"x\" and \"y\" as the primary error fields\n    # as per the context description and the code structure.", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Create error subcontext if it doesn't exist\n    if \"error\" not in context:\n        context[\"error\"] = {}\n    \n    # Add error indices based on the graph's error fields\n    if self._error_x_ind != -1:\n        context[\"error\"][\"x\"] = {\"index\": self._error_x_ind}\n    if self._error_y_ind != -1:\n        context[\"error\"][\"y\"] = {\"index\": self._error_y_ind}\n    \n    # Note: The docstring mentions \"x_low\", \"y_low\", etc. but the implementation\n    # follows the pattern of using \"x\" and \"y\" as the primary error fields\n    # as per the context description and the error field naming convention.", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Create error subcontext if it doesn't exist\n    if \"error\" not in context:\n        context[\"error\"] = {}\n    \n    # Add error indices based on the graph's error fields\n    if self._error_x_ind != -1:\n        context[\"error\"][\"x\"] = {\"index\": self._error_x_ind}\n    \n    if self._error_y_ind != -1:\n        context[\"error\"][\"y\"] = {\"index\": self._error_y_ind}\n    \n    # Add error field names for x and y coordinates\n    # Using \"x\", \"y\", \"z\" as per docstring\n    if self._error_x_ind != -1:\n        context[\"error\"][\"x_low\"] = {\"index\": self._error_x_ind}\n    \n    if self._error_y_ind != -1:\n        context[\"error\"][\"y_low\"] = {\"index\": self._error_y_ind}\n    \n    # Ensure existing values in context.value and its subcontexts are preserved\n    # (no removal of existing values)"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68593, "num_retrieved": 118, "inference_time": 11.12499937415123, "generate_results": ["def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Handle 1D case\n    if len(edges) == 1:\n        edges = edges[0]\n        bin_widths = [edges[i+1] - edges[i] for i in range(len(edges) - 1)]\n        return sum(b * w for b, w in zip(bins, bin_widths))\n    \n    # Handle multidimensional case\n    # Use the product of bin widths for each dimension\n    total_integral = 0\n    for i in range(len(edges)):\n        # For each dimension, compute the bin widths\n        dim_edges = edges[i]\n        dim_widths = [dim_edges[j+1] - dim_edges[j] for j in range(len(dim_edges) - 1)]\n        \n        # For each bin in this dimension, multiply by the product of all other dimension widths\n        dim_product = 1\n        for j in range(len(edges)):\n            if i != j:\n                dim_product *= 1  # This is a placeholder - we need to compute the actual product\n        \n        # This is a simplified implementation - the actual implementation would need to\n        # properly compute the volume of each bin and multiply by the bin value\n        pass\n    \n    # The actual implementation would need to properly compute the integral\n    # based on the multidimensional mesh and bin values\n    # For now, we return a placeholder\n    return sum(bins)", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Handle 1D case\n    if len(edges) == 1:\n        edges = edges[0]\n        bin_width = edges[1] - edges[0]\n        return sum(bins) * bin_width\n    \n    # Handle multidimensional case\n    # For each bin, compute the volume (product of bin widths)\n    total_integral = 0.0\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # For 2D, compute the area of each bin\n            dx = edges[0][i+1] - edges[0][i]\n            dy = edges[1][j+1] - edges[1][j]\n            bin_value = bins[i][j]\n            total_integral += bin_value * dx * dy\n    \n    # For higher dimensions, we need to compute the volume of each bin\n    # This is a generalization of the 1D and 2D cases\n    # We'll use the product of bin widths for each dimension\n    total_integral = 0.0\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            dx = edges[0][i+1] - edges[0][i]\n            dy = edges[1][j+1] - edges[1][j]\n            bin_value = bins[i][j]\n            total_integral += bin_value * dx * dy\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Handle 1D case\n    if len(edges) == 1:\n        edges = edges[0]\n        # Compute the integral as sum of bin values * bin width\n        integral = 0\n        for i in range(len(edges) - 1):\n            bin_width = edges[i + 1] - edges[i]\n            integral += bins[i] * bin_width\n        return integral\n    \n    # Handle multidimensional case\n    # Use the product of bin widths for each dimension\n    integral = 0\n    # For each bin, compute the product of bin widths\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # For 2D, compute the product of bin widths\n            bin_width_x = edges[0][i + 1] - edges[0][i]\n            bin_width_y = edges[1][j + 1] - edges[1][j]\n            bin_volume = bin_width_x * bin_width_y\n            # Add contribution of this bin\n            integral += bins[i][j] * bin_volume\n    \n    return integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Handle 1D case\n    if len(edges) == 1:\n        edges = edges[0]\n        bin_widths = [edges[i+1] - edges[i] for i in range(len(edges) - 1)]\n        return sum(b * w for b, w in zip(bins, bin_widths))\n    \n    # Handle multi-dimensional case\n    # For each bin, compute the volume (product of bin widths)\n    total_integral = 0.0\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # For 2D case, compute the area of each bin\n            bin_width_x = edges[0][i+1] - edges[0][i]\n            bin_width_y = edges[1][j+1] - edges[1][j]\n            bin_value = bins[i][j]\n            total_integral += bin_value * bin_width_x * bin_width_y\n    \n    # For higher dimensions, we would need to extend this logic\n    # But based on the context, this appears to be a 2D implementation\n    # The general case would involve computing the volume of each bin\n    # and multiplying by the bin value\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Handle 1D case\n    if len(edges) == 1:\n        edges = edges[0]\n        bin_widths = [edges[i+1] - edges[i] for i in range(len(edges) - 1)]\n        return sum(b * w for b, w in zip(bins, bin_widths))\n    \n    # Handle multidimensional case\n    # For each bin, compute the volume (product of bin widths)\n    # and multiply by the bin value\n    total_integral = 0.0\n    dim = len(edges)\n    \n    # Iterate over all bins\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # For 2D case, compute the area of the bin\n            # For higher dimensions, compute the volume\n            bin_value = bins[i][j]\n            bin_volume = 1.0\n            for k in range(dim):\n                bin_volume *= (edges[k][i+1] - edges[k][i])\n            total_integral += bin_value * bin_volume\n    \n    return total_integral"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 67977, "num_retrieved": 99, "inference_time": 11.12499937415123, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, sequence.Sequence):\n        # Check if the sequence contains a FillRequest element\n        try:\n            from .check_sequence_type import is_fill_request_el\n            # We need to check if any element in the sequence is a FillRequest\n            for el in seq._seq:\n                if isinstance(el, fill_request_seq.FillRequest):\n                    return True\n                elif hasattr(el, 'run') and hasattr(el, 'fill'):\n                    # Check if it's a FillRequest element through its methods\n                    if hasattr(el, 'request'):\n                        return True\n        except Exception:\n            pass\n    \n    # Check if it's a Source sequence (which should not be allowed)\n    if isinstance(seq, source.Source):\n        return False\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, sequence.Sequence):\n        # Check if the sequence contains a FillRequest element\n        try:\n            from .check_sequence_type import is_fill_request_el\n            # Check if any element in the sequence is a FillRequest element\n            for element in seq._seq:\n                if hasattr(element, 'run') and is_fill_request_el(element):\n                    return True\n            return False\n        except ImportError:\n            return False\n    \n    # If it's a Source sequence, return False\n    if isinstance(seq, source.Source):\n        return False\n    \n    # For other types, check if they can be converted to FillRequestSeq\n    # This is a simplified check based on the context\n    try:\n        from .check_sequence_type import is_fill_request_el\n        # Check if seq can be converted to a FillRequestSeq\n        # This would typically involve checking if it has a FillRequest element\n        # or if it can be transformed into one\n        return False\n    except ImportError:\n        return False\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, sequence.Sequence):\n        # Check if the sequence contains a FillRequest element\n        try:\n            from .check_sequence_type import is_fill_request_el\n            # Check if any element in the sequence is a FillRequest element\n            for el in seq._seq:\n                if isinstance(el, fill_request_seq.FillRequest):\n                    return True\n                if hasattr(el, 'run') and hasattr(el, 'fill'):\n                    # Check if it's a FillRequest element through its methods\n                    if hasattr(el, 'request'):\n                        return True\n        except (AttributeError, ImportError):\n            pass\n    \n    # Check if it's a Source sequence (which should not be allowed)\n    if isinstance(seq, source.Source):\n        return False\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, sequence.Sequence):\n        # Check if the sequence contains a FillRequest element\n        try:\n            from .check_sequence_type import is_fill_request_el\n            # We need to check if any element in the sequence is a FillRequest\n            seq_elements = seq._seq if hasattr(seq, '_seq') else []\n            for elem in seq_elements:\n                if isinstance(elem, fill_request_seq.FillRequest):\n                    return True\n                if hasattr(elem, 'run') and hasattr(elem, 'fill'):\n                    # Check if it's a FillRequest element through its methods\n                    if hasattr(elem, 'request'):\n                        return True\n            return False\n        except Exception:\n            return False\n    \n    # Check if seq is a Source sequence\n    if isinstance(seq, source.Source):\n        return False\n    \n    # Check if seq is a FillComputeSeq or other sequence types\n    if isinstance(seq, fill_compute_seq.FillComputeSeq):\n        return False\n    \n    # For other types, check if they contain a FillRequest element\n    try:\n        from .check_sequence_type import is_fill_request_el\n        # If seq is a sequence, check its elements\n        if hasattr(seq, '_seq'):\n            for elem in seq._seq:\n                if is_fill_request_el(elem):\n                    return True\n    except Exception:\n        pass\n    \n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    \n    if isinstance(seq, sequence.Sequence):\n        # Check if the sequence contains a FillRequest element\n        try:\n            from .check_sequence_type import is_fill_request_el\n            # Check if any element in the sequence is a FillRequest element\n            for element in seq._seq:\n                if hasattr(element, 'run') and is_fill_request_el(element):\n                    return True\n            return False\n        except ImportError:\n            return False\n        except Exception:\n            return False\n    \n    # Check if seq is a Source sequence\n    if isinstance(seq, source.Source):\n        return False\n    \n    # For other types, check if they can be converted to FillRequestSeq\n    # This is a fallback - if seq is not a FillRequestSeq or contains FillRequest,\n    # and is not a Source, return False\n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68987, "num_retrieved": 136, "inference_time": 11.12499937415123, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill', None)) and callable(getattr(obj, 'request', None))", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill', None)) and callable(getattr(obj, 'request', None))", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill', None)) and callable(getattr(obj, 'request', None))", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill', None)) and callable(getattr(obj, 'request', None))", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(getattr(obj, 'fill', None)) and callable(getattr(obj, 'request', None))"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69367, "num_retrieved": 148, "inference_time": 11.12499937415123, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run', None))"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68992, "num_retrieved": 136, "inference_time": 11.12499937415123, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 67954, "num_retrieved": 97, "inference_time": 11.12499937415123, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # For level > 1, recursively process\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Level > 1 - recursive check\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Recursive case - check up to the specified level\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Level > 1 - recursive check\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n    *level* sets the maximum depth of recursion. For infinite recursion,\n    set that to -1. For level 1,\n    if a key is present both in *d1* and *d2* but has different values,\n    it is included into the difference.\n    See :func:`intersection` for more details.\n\n    *d1* and *d2* remain unchanged. However, *d1* or some of its\n    subdictionaries may be returned directly.\n    Make a deep copy of the result when appropriate.\n\n    .. versionadded:: 0.5\n       add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Level > 1 - recursive check\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69176, "num_retrieved": 138, "inference_time": 11.12499937415123, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the bin edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if the coordinate is within the edges\n    if len(edges) == 1:\n        # 1D case\n        if coord < edges[0][0] or coord >= edges[0][-1]:\n            return  # Coordinate is outside the edges\n    else:\n        # Multi-dimensional case\n        try:\n            # Get the bin indices for the coordinate\n            bin_indices = []\n            for i, edge in enumerate(edges):\n                # Find the bin index for this dimension\n                if coord < edge[0] or coord >= edge[-1]:\n                    return  # Coordinate is outside the edges\n                # Find the index of the bin that contains the coordinate\n                bin_idx = np.digitize(coord, edge)\n                bin_indices.append(bin_idx - 1)  # -1 because digitize returns 1-indexed\n        except:\n            return  # Coordinate is outside the edges\n    \n    # Get the bin content at the calculated indices\n    try:\n        bin_content = bins[bin_indices]\n    except:\n        return  # Invalid bin indices\n    \n    # Update the bin content with the weight\n    bin_content += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    # Get the bin edges and determine which bin contains the coordinate\n    bin_indices = get_bin_on_value(coord, self.edges)\n    \n    # Check if the coordinate is within the valid range\n    if any(idx < 0 or idx >= len(edges) - 1 for idx, edges in zip(bin_indices, self.edges)):\n        return\n    \n    # Get the bin content and update it with the weight\n    bin_content = self.bins[bin_indices]\n    bin_content += weight\n    \n    # Update the histogram bins\n    self.bins[bin_indices] = bin_content", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the bin edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if the coordinate is within the edges\n    if len(edges) == 1:\n        # 1D case\n        if not (edges[0][0] <= coord <= edges[0][-1]):\n            return\n        # Find the bin index\n        bin_idx = int(np.digitize(coord, edges[0]) - 1)\n        if bin_idx < 0 or bin_idx >= len(bins):\n            return\n        # Add weight to the bin\n        bins[bin_idx] += weight\n    else:\n        # Multi-dimensional case\n        # Find the bin indices for each dimension\n        bin_indices = []\n        for i, edge in enumerate(edges):\n            if not (edge[0] <= coord[i] <= edge[-1]):\n                return\n            bin_idx = int(np.digitize(coord[i], edge) - 1)\n            if bin_idx < 0 or bin_idx >= len(bins[i]):\n                return\n            bin_indices.append(bin_idx)\n        \n        # Add weight to the bin\n        bin_content = bins[bin_indices[0]]\n        if isinstance(bin_content, list):\n            bin_content[bin_indices[1]] += weight\n        else:\n            bin_content += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the bin edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if the coordinate is within the edges\n    if len(edges) == 1:\n        # 1D case\n        if coord < edges[0][0] or coord >= edges[0][-1]:\n            return  # Coordinate is outside the edges\n    else:\n        # Multi-dimensional case\n        try:\n            # Get the bin indices for the coordinate\n            bin_indices = []\n            for i, edge in enumerate(edges):\n                if len(edge) < 2:\n                    raise ValueError(\"Edges must have at least two elements\")\n                # Find the bin index for this coordinate\n                if coord < edge[0] or coord >= edge[-1]:\n                    return  # Coordinate is outside the edges\n                # Use binary search to find the bin index\n                # This is a simplified version - in practice, we'd use numpy.searchsorted\n                bin_indices.append(np.searchsorted(edge, coord, side='left') - 1)\n            # If we get here, the coordinate is within the edges\n            # Use the bin indices to get the bin content\n            bin_content = bins[bin_indices[0]]\n            # Update the bin content with the weight\n            bin_content += weight\n        except Exception:\n            return\n    \n    # Update the bin content with the weight\n    # This is a simplified version - in practice, we'd use the actual bin indices\n    # to update the bin content\n    pass", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\n    Coordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the bin edges and bins from the histogram\n    edges = self.edges\n    bins = self.bins\n    \n    # Check if the coordinate is within the valid range\n    if not isinstance(edges, (list, tuple)) or not isinstance(edges[0], (list, tuple)):\n        # Handle 1D case\n        if isinstance(coord, (int, float)):\n            # Find the bin index for the coordinate\n            if coord < edges[0] or coord >= edges[-1]:\n                return  # Coordinate is outside the valid range\n            bin_idx = int(np.searchsorted(edges, coord, side='right')) - 1\n            if bin_idx < 0 or bin_idx >= len(edges) - 1:\n                return  # Coordinate is outside the valid range\n            # Update the bin content\n            bins[bin_idx] += weight\n        else:\n            return  # Invalid coordinate type\n    else:\n        # Handle multi-dimensional case\n        # Get the bin indices for each coordinate\n        bin_indices = []\n        for i, edge in enumerate(edges):\n            if isinstance(coord, (int, float)):\n                # For 1D case, we already handled it above\n                pass\n            else:\n                # For multi-dimensional case, check if the coordinate is within the valid range\n                if coord[i] < edge[0] or coord[i] >= edge[-1]:\n                    return  # Coordinate is outside the valid range\n                bin_idx = int(np.searchsorted(edge, coord[i], side='right')) - 1\n                if bin_idx < 0 or bin_idx >= len(edge) - 1:\n                    return  # Coordinate is outside the valid range\n                bin_indices.append(bin_idx)\n        \n        # Update the bin content\n        bin_content = bins\n        for i, idx in enumerate(bin_indices):\n            bin_content = bin_content[idx]\n        bin_content += weight"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69307, "num_retrieved": 135, "inference_time": 11.12499937415123, "generate_results": ["def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate label keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({\"key\": str(key), \"value\": \"Label key must be a string\"})\n        else:\n            # Check if key matches the expected regex pattern\n            # Assuming a regex pattern is defined somewhere in the codebase\n            # For this implementation, we'll just check if it's a valid string\n            # and if it contains only allowed characters\n            if not key.isalnum() and not any(c in key for c in \"._-\"):\n                errors.append({\"key\": key, \"value\": \"Label key does not match the regex pattern\"})\n    \n    # Validate label values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            # If value is not a string, check if it's a list or other iterable\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({\"key\": str(item), \"value\": \"Expected string or bytes-like object\"})\n            else:\n                errors.append({\"key\": str(value), \"value\": \"Expected string or bytes-like object\"})\n    \n    if errors:\n        raise ValidationError(errors", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate label keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        elif not key.match(r\"^[a-z][a-z0-9_]*$\"):\n            errors.append({str(key): \"Label key does not match the regex [a-z][a-z0-9_]*\"})\n    \n    # Validate label values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            errors.append({str(value): \"Label value must be a string\"})\n        elif not value.match(r\"^[a-zA-Z0-9][a-zA-Z0-9_.-]*$\"):\n            errors.append({str(value): \"Label value does not match the regex [a-zA-Z0-9][a-zA-Z0-9_.-]*\"})\n    \n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate label keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        elif not re.match(r\"^[a-z][a-z0-9_]*$\", key):\n            errors.append({key: \"Label key does not match the regex [a-z][a-z0-9_]*\"})\n    \n    # Validate label values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            # If value is not a string, check if it's a list or other iterable\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): \"Expected string or bytes-like object\"})\n            else:\n                errors.append({str(value): \"Expected string or bytes-like object\"})\n        else:\n            # If value is a string, ensure it matches the expected regex\n            if not re.match(r\"^[a-zA-Z0-9_\\-\\.]*$\", value):\n                errors.append({value: \"Label value does not match the regex [a-zA-Z0-9_\\-\\.]*\"})\n    \n    if errors:\n        raise ValidationError", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate label keys\n    for key in labels.keys():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n        # Check if key matches the regex pattern (example pattern - adjust as needed)\n        elif not re.match(r\"^[a-z][a-z0-9_]*$\", key):\n            errors.append({key: \"Label key does not match the regex pattern\"})\n    \n    # Validate label values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            # If value is not a string, check if it's a list or other iterable\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({str(item): \"Expected string or bytes-like object\"})\n            else:\n                errors.append({str(value): \"Expected string or bytes-like object\"})\n    \n    if errors:\n        raise ValidationError(errors)", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate label keys\n    for key in labels.keys():\n        if not isinstance(key, str):\n            errors.append({\"key\": str(key), \"value\": \"Label key must be a string\"})\n        elif not key.isidentifier() or not re.match(r\"^[a-z][a-z0-9_]*$\", key):\n            errors.append({\"key\": key, \"value\": \"Label key does not match the regex pattern\"})\n    \n    # Validate label values\n    for key, value in labels.items():\n        if not isinstance(value, str):\n            if isinstance(value, (list, tuple)):\n                for item in value:\n                    if not isinstance(item, str):\n                        errors.append({\"key\": str(item), \"value\": \"Expected string or bytes-like object\"})\n            else:\n                errors.append({\"key\": str(value), \"value\": \"Expected string or bytes-like object\"})\n        elif not value.isalnum() and not re.match(r\"^[a-zA-Z0-9._-]+$\", value):\n            errors.append({\"key\": key, \"value\": \"Label value does not match the regex pattern\"})\n    \n    if errors:\n        raise ValidationError(errors)"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69847, "num_retrieved": 150, "inference_time": 11.12499937415123, "generate_results": ["def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    # Minimum length of 1 character\n    pattern = re.compile(r\"^[a-zA-Z0-9]([a-zA-Z0-9_-])*[a-zA-Z0-9]$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    # Minimum length of 1 character\n    pattern = re.compile(r\"^[a-zA-Z0-9]([a-zA-Z0-9_-])*[a-zA-Z0-9]$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    # Minimum length of 1 character\n    pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9_-])*[a-zA-Z0-9]$\"\n    return re.compile(pattern)", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    # Minimum length of 1 character\n    pattern = r\"^[a-zA-Z0-9]([a-zA-Z0-9_-])*[a-zA-Z0-9]$\"\n    return re.compile(pattern)", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # and can contain letters, numbers, hyphens, and underscores\n    # No consecutive hyphens or underscores\n    # No leading or trailing hyphens or underscores\n    # No special characters like spaces or punctuation\n    pattern = re.compile(r'^[a-zA-Z0-9]([a-zA-Z0-9_-])*[a-zA-Z0-9]$')\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69934, "num_retrieved": 157, "inference_time": 11.12499937415123, "generate_results": ["def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    \n    # Define the regular expression pattern for valid values\n    # This pattern allows alphanumeric characters, underscores, hyphens, and dots\n    # It also allows optional leading and trailing whitespace\n    pattern = r'^[a-zA-Z0-9_\\-\\.]+$'\n    \n    if not re.match(pattern, str(value)):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression.\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    \n    # Define the regular expression pattern for valid values\n    # This pattern allows alphanumeric characters, underscores, hyphens, and dots\n    # It also allows optional leading/trailing whitespace\n    pattern = r'^[a-zA-Z0-9_\\-\\.]+$'\n    \n    if not re.match(pattern, str(value)):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression.\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    \n    # Define the regular expression pattern for valid values\n    # This pattern allows alphanumeric characters, underscores, hyphens, and dots\n    # It also allows optional leading and trailing whitespace\n    pattern = r'^[a-zA-Z0-9_\\-\\.]+$'\n    \n    if not re.match(pattern, str(value)):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression.\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    \n    # Define the regular expression pattern for valid values\n    # This pattern allows alphanumeric characters, underscores, hyphens, and dots\n    # It also allows optional leading and trailing whitespace\n    pattern = r'^[a-zA-Z0-9_\\-\\.]+$'\n    \n    if not re.match(pattern, str(value)):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression.\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    import re\n    \n    # Define the regular expression pattern for valid values\n    # This pattern allows alphanumeric characters, underscores, hyphens, and dots\n    # It also allows optional leading/trailing whitespace\n    pattern = r'^[a-zA-Z0-9_\\-\\.]+$'\n    \n    if not re.match(pattern, str(value)):\n        raise ValidationError(f\"Value '{value}' is not conform to the regular expression.\")"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69869, "num_retrieved": 157, "inference_time": 11.12499937415123, "generate_results": ["def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Define the regular expression pattern for valid keys\n    # Keys should be alphanumeric, with hyphens, underscores, and dots allowed\n    # and should not start or end with a hyphen or underscore\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*[a-zA-Z0-9]$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    import marshmallow.exceptions as ValidationError\n    \n    # Define the regular expression pattern for valid keys\n    # Keys should be alphanumeric, with hyphens, underscores, and dots allowed\n    # and should not start or end with a hyphen or underscore\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*[a-zA-Z0-9]$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    from marshmallow import ValidationError\n    \n    # Regular expression pattern for valid keys (example pattern - adjust as needed)\n    # This pattern typically matches alphanumeric characters, hyphens, and underscores\n    # and ensures it starts and ends with alphanumeric characters\n    pattern = r'^[a-zA-Z0-9][a-zA-Z0-9_-]*[a-zA-Z0-9]$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}. Key must match the pattern {pattern}.\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    import marshmallow.exceptions as ValidationError\n    \n    # Define the regular expression pattern for valid keys\n    # Keys should be alphanumeric, with hyphens, underscores, and dots allowed\n    # and should not start or end with a hyphen or underscore\n    pattern = r'^[a-zA-Z][a-zA-Z0-9._-]*[a-zA-Z0-9]$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    import re\n    import marshmallow.exceptions as ValidationError\n    \n    # Define the regular expression pattern for valid keys\n    # Keys should be alphanumeric, with hyphens, underscores, and dots allowed\n    # and should not start or end with a hyphen or underscore\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-\\.]*[a-zA-Z0-9]$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69712, "num_retrieved": 146, "inference_time": 11.12499937415123, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # If first_level is True, we are dealing with the complete observer schema\n    # for a resource, so we need to copy the identifying fields from the manifest\n    if first_level:\n        try:\n            # Copy apiVersion and kind from the manifest\n            observer_schema[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n            observer_schema[\"kind\"] = manifest_dict.get(\"kind\")\n            \n            # Copy metadata from the manifest\n            metadata = manifest_dict.get(\"metadata\", {})\n            if metadata:\n                observer_schema[\"metadata\"] = {}\n                # Copy name from metadata\n                observer_schema[\"metadata\"][\"name\"] = metadata.get(\"name\")\n                \n                # For other metadata fields, set to None if not present\n                for key, value in metadata.items():\n                    if key not in [\"name\"]:\n                        observer_schema[\"metadata\"][key] = None\n                        \n        except (KeyError, TypeError):\n            # If any required field is missing, raise an assertion error\n            raise AssertionError(\"Missing required fields in manifest dictionary\")\n    \n    # Iterate through all key-value pairs in manifest_dict\n    for key, value in manifest_dict.items():\n        # If the value is a dict or list, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value)\n        else:\n            # Replace all non-list and non", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # If first_level is True, we are dealing with the complete observer schema\n    # for a resource, so we need to copy the identifying fields from the manifest\n    if first_level:\n        try:\n            observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            observer_schema[\"metadata\"] = {}\n            \n            # Copy metadata fields that are present\n            try:\n                observer_schema[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n                \n            # For other metadata fields, we'll set them to None\n            for key in metadata.keys():\n                if key not in [\"name\"]:\n                    observer_schema[\"metadata\"][key] = None\n        except KeyError:\n            pass\n    \n    # Recursively process all values in the manifest dictionary\n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # For lists, we would typically call the list version of this function\n            # But since this function is for dictionaries, we'll just set to None\n            observer_schema[key] = None\n        else:\n            # Replace all non-list and non-dict values with None", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # If first_level is True, we are dealing with the complete observer schema\n    # for a resource, so we need to copy the identifying fields from the manifest\n    if first_level:\n        try:\n            # Copy apiVersion and kind from the manifest\n            observer_schema[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n            observer_schema[\"kind\"] = manifest_dict.get(\"kind\")\n            \n            # Copy metadata fields\n            metadata = manifest_dict.get(\"metadata\", {})\n            if metadata:\n                observer_schema[\"metadata\"] = {}\n                observer_schema[\"metadata\"][\"name\"] = metadata.get(\"name\")\n                \n                # Copy other metadata fields if present\n                for key, value in metadata.items():\n                    if key not in [\"name\"]:\n                        observer_schema[\"metadata\"][key] = value\n                        \n        except (KeyError, TypeError):\n            # If any required field is missing, raise an assertion error\n            raise AssertionError(\"Missing required fields in manifest\")\n    \n    # Recursively process all values in the manifest_dict\n    for key, value in manifest_dict.items():\n        # If the value is a dictionary, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        # If the value is a list, recursively process it (handled by the list function)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value)\n        # For all other values, replace with", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # If first_level is True, we are dealing with the complete observer schema\n    # for a resource, so we need to copy the identifying fields from the manifest\n    if first_level:\n        try:\n            observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            observer_schema[\"metadata\"] = {}\n            \n            # Copy metadata fields that are present\n            try:\n                observer_schema[\"metadata\"][\"name\"] = metadata[\"name\"]\n            except KeyError:\n                pass\n                \n        except (KeyError, TypeError):\n            pass\n    \n    # Recursively process all values in the manifest_dict\n    for key, value in manifest_dict.items():\n        if isinstance(value, dict):\n            # Recursively process nested dictionaries\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            # Process lists using the list version of the function\n            observer_schema[key] = generate_default_observer_schema_list(value)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n    \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # If first_level is True, we are dealing with the complete observer schema\n    # for a resource, so we need to copy the identifying fields from the manifest\n    if first_level:\n        try:\n            # Copy apiVersion and kind from the manifest\n            observer_schema[\"apiVersion\"] = manifest_dict.get(\"apiVersion\")\n            observer_schema[\"kind\"] = manifest_dict.get(\"kind\")\n            \n            # Copy metadata from the manifest\n            metadata = manifest_dict.get(\"metadata\", {})\n            if metadata:\n                observer_schema[\"metadata\"] = {}\n                # Copy name from metadata\n                observer_schema[\"metadata\"][\"name\"] = metadata.get(\"name\")\n                \n                # For other metadata fields, set to None if not present\n                for key, value in metadata.items():\n                    if key not in [\"name\"]:\n                        observer_schema[\"metadata\"][key] = None\n                        \n        except (KeyError, TypeError):\n            # If any of the required fields are missing, raise an assertion error\n            raise AssertionError(\"Missing required fields in manifest dictionary\")\n    \n    # Recursively process all values in the manifest dictionary\n    for key, value in manifest_dict.items():\n        # If the value is a dictionary, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        # If the value is a list, recursively process it\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69093, "num_retrieved": 120, "inference_time": 11.12499937415123, "generate_results": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:`update_last_applied_manifest_dict_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If the observed element is not present in the response, add it to the manifest\n            last_applied_manifest.append(value)\n        elif idx < len(response):\n            # If the observed element is present in both observer_schema and response,\n            # recursively update the nested fields\n            if isinstance(value, dict):\n                # Update the dictionary fields recursively\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            elif isinstance(value, list):\n                # Update the list fields recursively\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            else:\n                # For scalar values, just ensure they are present in the manifest\n                if idx >= len(last_applied_manifest):\n                    last_applied_manifest.append(value)\n                else:\n                    last_applied_manifest[idx] = value", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If the observed element is not present in the response, we need to add it\n            # to the last_applied_manifest\n            last_applied_manifest.append(value)\n        elif idx < len(response):\n            # If the observed element is present in the response, we need to update\n            # the value in last_applied_manifest\n            if isinstance(value, dict):\n                # Recursively update the dictionary\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            elif isinstance(value, list):\n                # Recursively update the list\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            else:\n                # For scalar values, just update the value in last_applied_manifest\n                last_applied_manifest[idx] = response[idx]", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If the observed element is not present in the response, we need to add it\n            # to the last_applied_manifest with a None value\n            last_applied_manifest.append(None)\n        elif idx >= len(last_applied_manifest):\n            # If the response has more elements than the last_applied_manifest,\n            # we need to extend the list\n            last_applied_manifest.extend([None] * (len(response) - len(last_applied_manifest)))\n        \n        # If the current element in observer_schema is a dictionary, recursively update\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        # If the current element in observer_schema is a list, recursively update\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        # If the current element is a primitive value, initialize it in the response\n        else:\n            if idx < len(response):\n                last_applied_manifest[idx] = response[idx]", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If the observed element is not present in the response, we need to add it\n            # to the last_applied_manifest with a None value\n            last_applied_manifest.append(None)\n        else:\n            # If the observed element is present in the response, update the value\n            # in last_applied_manifest\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n            else:\n                # For scalar values, just set the value from the response\n                if idx < len(last_applied_manifest):\n                    last_applied_manifest[idx] = response[idx]\n                else:\n                    last_applied_manifest.append(response[idx])", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        if idx >= len(response):\n            # If the observed element is not present in the response, we need to add it\n            # to the last_applied_manifest with a None value\n            last_applied_manifest.append(None)\n        elif idx >= len(last_applied_manifest):\n            # If the response has more elements than last_applied_manifest, we need to\n            # extend last_applied_manifest\n            last_applied_manifest.extend([None] * (len(response) - len(last_applied_manifest)))\n        \n        # If the current element in observer_schema is a dictionary, recursively update\n        # the corresponding dictionary in last_applied_manifest\n        if isinstance(value, dict):\n            if idx < len(last_applied_manifest) and idx < len(response):\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n        # If the current element in observer_schema is a list, recursively update\n        # the corresponding list in last_applied_manifest\n        elif isinstance(value, list):\n            if idx < len(last_applied_manifest) and idx < len(response):\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], observer_schema[idx], response[idx]\n                )\n        # If the current element is a primitive value, set it in last_applied_manifest\n        else:\n            if idx < len(last_applied_manifest):\n                last_applied_manifest[idx] = value"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69038, "num_retrieved": 120, "inference_time": 11.12499937415123, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:`update_last_applied_manifest_list_from_resp`, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response.\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initializes their value in\n    last_applied_manifest if they are not yet present.\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            else:\n                # If the key already exists, we don't need to update it\n                # as we're only initializing missing fields\n                pass", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            else:\n                # If the key already exists, we don't need to update it\n                # as it's already in the manifest\n                pass", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            else:\n                last_applied_manifest[key] = value", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            else:\n                last_applied_manifest[key] = value", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field {key} not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = {}\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = []\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], value, response[key]\n            )\n        else:\n            if key not in last_applied_manifest:\n                last_applied_manifest[key] = value\n            else:\n                last_applied_manifest[key] = value"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70077, "num_retrieved": 154, "inference_time": 6.919085115194321, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest or []\n    observer_schema = app.spec.observer_schema or []\n    \n    # If custom observer schema is already provided, no need to generate default\n    if observer_schema:\n        return\n    \n    # Generate default observer schema for each resource in manifest\n    for resource in manifest:\n        # Extract resource metadata\n        api_version = resource.get(\"apiVersion\")\n        kind = resource.get(\"kind\")\n        name = resource.get(\"metadata\", {}).get(\"name\")\n        namespace = resource.get(\"metadata\", {}).get(\"namespace\")\n        \n        if not api_version or not kind or not name:\n            continue\n            \n        # Create default observer schema entry\n        observer_entry = {\n            \"apiVersion\": api_version,\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name\n            }\n        }\n        \n        # Add namespace if present\n        if namespace:\n            observer_entry[\"metadata\"][\"namespace\"] = namespace\n            \n        # Add default fields to observe (this is a simplified default)\n        # In a real implementation, this would include specific fields to observe\n        # based on the resource type\n        observer_entry[\"spec\"] = {}\n        \n        # Add default observer schema for the resource\n        observer_schema.append(observer_entry)\n    \n    # Set the generated schema on the application\n    app.spec.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Initialize the default observer schema\n    default_observer_schema = []\n    \n    # Get the manifest from the application spec\n    manifest = app.spec.manifest or []\n    \n    # For each resource in the manifest, create a default observer schema entry\n    for resource in manifest:\n        # Extract the resource metadata\n        api_version = resource.get(\"apiVersion\")\n        kind = resource.get(\"kind\")\n        name = resource.get(\"metadata\", {}).get(\"name\")\n        \n        # Skip if any required field is missing\n        if not api_version or not kind or not name:\n            continue\n            \n        # Create a default observer schema entry for the resource\n        observer_entry = {\n            \"apiVersion\": api_version,\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name\n            }\n        }\n        \n        # Add the observer entry to the default observer schema\n        default_observer_schema.append(observer_entry)\n    \n    # Set the generated default observer schema on the application\n    app.spec.observer_schema = default_observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest or []\n    observer_schema = app.spec.observer_schema or []\n    \n    # If custom observer schema is already provided, no need to generate default\n    if observer_schema:\n        return\n    \n    # Generate default observer schema for each resource in manifest\n    for resource in manifest:\n        # Extract resource metadata\n        api_version = resource.get(\"apiVersion\")\n        kind = resource.get(\"kind\")\n        name = resource.get(\"metadata\", {}).get(\"name\")\n        namespace = resource.get(\"metadata\", {}).get(\"namespace\")\n        \n        if not api_version or not kind or not name:\n            continue\n            \n        # Create default observer schema entry\n        observer_entry = {\n            \"apiVersion\": api_version,\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name\n            }\n        }\n        \n        # Add namespace if present\n        if namespace:\n            observer_entry[\"metadata\"][\"namespace\"] = namespace\n            \n        # Add default fields to observe (this is a simplified default)\n        # In a real implementation, this would include specific fields to observe\n        # based on the resource type\n        observer_entry[\"spec\"] = {}\n        \n        # Add default observer schema for the resource\n        observer_schema.append(observer_entry)\n    \n    # Set the generated schema in the application\n    app.spec.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest or []\n    observer_schema = app.spec.observer_schema or []\n    \n    # If custom observer schema is already provided, no need to generate default\n    if observer_schema:\n        return\n    \n    # Generate default observer schema for each resource in manifest\n    for resource in manifest:\n        # Extract resource metadata\n        api_version = resource.get(\"apiVersion\")\n        kind = resource.get(\"kind\")\n        name = resource.get(\"metadata\", {}).get(\"name\")\n        namespace = resource.get(\"metadata\", {}).get(\"namespace\")\n        \n        if not api_version or not kind or not name:\n            continue\n            \n        # Create default observer schema entry\n        observer_entry = {\n            \"apiVersion\": api_version,\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name,\n                \"namespace\": namespace\n            }\n        }\n        \n        # Add to observer schema\n        observer_schema.append(observer_entry)\n    \n    # Set the generated observer schema\n    app.spec.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    manifest = app.spec.manifest or []\n    observer_schema = app.spec.observer_schema or []\n    \n    # If custom observer schema is already provided, no need to generate default\n    if observer_schema:\n        return\n    \n    # Generate default observer schema for each resource in manifest\n    for resource in manifest:\n        # Extract resource metadata\n        api_version = resource.get(\"apiVersion\")\n        kind = resource.get(\"kind\")\n        name = resource.get(\"metadata\", {}).get(\"name\")\n        namespace = resource.get(\"metadata\", {}).get(\"namespace\")\n        \n        if not api_version or not kind or not name:\n            continue\n            \n        # Create default observer schema entry\n        observer_entry = {\n            \"apiVersion\": api_version,\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name\n            }\n        }\n        \n        # Add namespace if present\n        if namespace:\n            observer_entry[\"metadata\"][\"namespace\"] = namespace\n            \n        # Add default fields to observe (this would typically be based on the resource type)\n        # For now, we just create a minimal schema with the required fields\n        observer_entry[\"metadata\"] = {\n            \"name\": name,\n            \"namespace\": namespace\n        }\n        \n        # Add a list of fields to observe (this would be resource-specific)\n        # For simplicity, we add a placeholder for fields\n        observer_entry[\"fields\"] = {\n            \"metadata\": {\n                \"name\": True,\n                \"namespace\": True\n            }\n        }\n        \n        # Add to observer schema list\n        observer_schema.append(observer_entry)\n    \n    # Set the generated schema on the application\n    app.spec.observer_schema = observer_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 28364, "num_retrieved": 74, "inference_time": 6.919085115194321, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 28438, "num_retrieved": 74, "inference_time": 6.919085115194321, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 41738, "num_retrieved": 112, "inference_time": 6.919085115194321, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if path is not None:\n            validator.obj_fs = open_fs(path, create=False)\n        return validator.validate(path)\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if self.root_fs is not None:\n            # If we have a root filesystem, open the path in it\n            obj_fs = self.root_fs.opendir(path)\n        else:\n            # Otherwise, open the path directly\n            obj_fs = open_fs(path)\n        \n        # Validate the object\n        if validator.validate(obj_fs):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if path is None:\n            # Validate root filesystem\n            validator.validate(ocfl_opendir(self.root_fs, '/'))\n        else:\n            # Validate object at path\n            validator.validate(ocfl_opendir(self.root_fs, path))\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False\n    \n    # Check if any errors were found\n    return validator.log.num_errors == 0", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if path is None:\n            # Validate root filesystem\n            validator.validate(ocfl_opendir(self.root_fs, '/'))\n        else:\n            # Validate object at path\n            validator.validate(ocfl_opendir(self.root_fs, path))\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False\n    \n    # Check if there are any errors\n    return validator.log.num_errors == 0", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if path is None:\n            # Validate root filesystem\n            validator.validate(ocfl_opendir(self.root_fs, '/'))\n        else:\n            # Validate object at path\n            validator.validate(ocfl_opendir(self.root_fs, path))\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False\n    \n    # Check if there are any errors\n    return validator.log.num_errors == 0"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 50103, "num_retrieved": 122, "inference_time": 6.919085115194321, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"VALID\" if self.log.num_errors == 0 else \"INVALID\")\n    return \" | \".join(status)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"Status: \" + (\"VALID\" if self.log.num_errors == 0 else \"INVALID\"))\n    if self.log.num_warnings > 0:\n        status.append(f\"Warnings: {self.log.num_warnings}\")\n    if self.log.num_errors > 0:\n        status.append(f\"Errors: {self.log.num_errors}\")\n    return \" | \".join(status)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"VALID\" if self.log.num_errors == 0 else \"INVALID\")\n    return \" | \".join(status)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"VALID\" if self.log.num_errors == 0 else \"INVALID\")\n    return \" | \".join(status)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"Status: \" + (\"VALID\" if self.log.num_errors == 0 else \"INVALID\"))\n    if self.log.num_warnings > 0:\n        status.append(f\"Warnings: {self.log.num_warnings}\")\n    if self.log.num_errors > 0:\n        status.append(f\"Errors: {self.log.num_errors}\")\n    return \" | \".join(status)"]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 43124, "num_retrieved": 108, "inference_time": 6.919085115194321, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 49205, "num_retrieved": 120, "inference_time": 6.919085115194321, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 41818, "num_retrieved": 110, "inference_time": 6.919085115194321, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from type value\n            if 'type' in inventory and inventory['type']:\n                # Extract version from type URL\n                m = re.search(r'/(\\d+\\.\\d+)', inventory['type'])\n                if m:\n                    spec_version = m.group(1)\n                    inv_validator.spec_version = spec_version\n                    self.spec_version = spec_version\n            else:\n                # Use self.spec_version as fallback\n                pass\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='standalone', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the type value in inventory\n            if 'type' in inventory:\n                type_value = inventory['type']\n                # Extract version from type URL (e.g., https://ocfl.io/1.0/spec/#inventory)\n                match = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n                if match:\n                    spec_version = match.group(1)\n                    inv_validator.spec_version = spec_version\n                else:\n                    # If type value doesn't match expected format, use self.spec_version\n                    pass\n            # If no type value or invalid type value, use self.spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the type value in inventory\n            type_value = inventory.get('type')\n            if type_value:\n                # Extract version from type value (e.g., https://ocfl.io/1.0/spec/#inventory)\n                match = re.search(r'/(\\d+\\.\\d+)/', type_value)\n                if match:\n                    spec_version = match.group(1)\n                    inv_validator.spec_version = spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the type value in the inventory\n            if 'type' in inventory:\n                type_value = inventory['type']\n                # Extract version from type URL (e.g., https://ocfl.io/1.0/spec/#inventory)\n                match = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n                if match:\n                    spec_version = match.group(1)\n                    inv_validator.spec_version = spec_version\n                else:\n                    # If type value doesn't match expected format, use self.spec_version\n                    pass\n            # If no type value or invalid type value, use self.spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='standalone', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the type value in inventory\n            if 'type' in inventory:\n                type_value = inventory['type']\n                # Extract version from type URL (e.g., https://ocfl.io/1.0/spec/#inventory)\n                m = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n                if m:\n                    inv_validator.spec_version = m.group(1)\n                else:\n                    # If type value doesn't match expected format, use self.spec_version\n                    pass\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 40674, "num_retrieved": 99, "inference_time": 6.919085115194321, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used[filepath]:\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used.get(filepath, []):\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used[filepath]:\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used[filepath]:\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used[filepath]:\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath, missing_digest=digest)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 41982, "num_retrieved": 113, "inference_time": 6.919085115194321, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version's spec version is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version's spec version is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version's spec version is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 42172, "num_retrieved": 118, "inference_time": 6.919085115194321, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 41181, "num_retrieved": 104, "inference_time": 6.919085115194321, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that fixity block has the correct structure\n    for digest_algorithm in fixity:\n        if not isinstance(fixity[digest_algorithm], dict):\n            self.log.error('E091', where='root', digest_algorithm=digest_algorithm)\n            return\n            \n        for digest in fixity[digest_algorithm]:\n            if not isinstance(fixity[digest_algorithm][digest], list):\n                self.log.error('E091', where='root', digest_algorithm=digest_algorithm, digest=digest)\n                return\n                \n            # Check that all files in fixity are present in manifest\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E094', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)\n                    return", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that fixity block has the correct structure\n    for digest_algorithm in fixity:\n        if not isinstance(fixity[digest_algorithm], dict):\n            self.log.error('E091', where='root', digest_algorithm=digest_algorithm)\n            return\n            \n        for digest in fixity[digest_algorithm]:\n            if not isinstance(fixity[digest_algorithm][digest], list):\n                self.log.error('E091', where='root', digest_algorithm=digest_algorithm, digest=digest)\n                return\n                \n            # Check that all files in fixity are present in manifest\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E094', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)\n                    return", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that fixity block has the correct structure\n    for digest_algorithm in fixity:\n        if not isinstance(fixity[digest_algorithm], dict):\n            self.log.error('E091', where='root', digest_algorithm=digest_algorithm)\n            continue\n            \n        for digest in fixity[digest_algorithm]:\n            if not isinstance(digest, str):\n                self.log.error('E091', where='root', digest_algorithm=digest_algorithm, digest=digest)\n                continue\n                \n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E094', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all fixity entries reference files that are in the manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all fixity entries reference files in the manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72795, "num_retrieved": 249, "inference_time": 6.919085115194321, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zip_file:\n            return [os.path.basename(file) for file in zip_file.namelist()]\n    elif file_utils.is_folder(path):\n        return [os.path.basename(file) for file in os.listdir(path)]\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zip_file:\n            return [os.path.basename(file) for file in zip_file.namelist()]\n    elif file_utils.is_folder(path):\n        return [os.path.basename(file) for file in os.listdir(path)]\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zip_file:\n            return [os.path.basename(file) for file in zip_file.namelist()]\n    elif file_utils.is_folder(path):\n        return [os.path.basename(file) for file in os.listdir(path)]\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zip_file:\n            return [os.path.basename(file) for file in zip_file.namelist()]\n    elif file_utils.is_folder(path):\n        return [os.path.basename(file) for file in os.listdir(path)]\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        with ZipFile(path, 'r') as zip_file:\n            return [os.path.basename(file) for file in zip_file.namelist()]\n    elif file_utils.is_folder(path):\n        return [os.path.basename(file) for file in os.listdir(path)]\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70659, "num_retrieved": 178, "inference_time": 6.919085115194321, "generate_results": ["def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    for file in files:\n        file_basename = os.path.basename(file)\n        # Extract the XML filename without extension\n        xml_basename = os.path.splitext(file_basename)[0]\n        \n        # Check if this file is related to an XML file\n        for xml in xmls:\n            xml_basename_no_ext = os.path.splitext(xml)[0]\n            if xml_basename_no_ext == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    for file in files:\n        file_basename = os.path.basename(file)\n        # Extract the XML filename without extension\n        xml_basename = os.path.splitext(file_basename)[0]\n        \n        # Check if this file is related to an XML file\n        for xml_file in xmls:\n            xml_basename_match = os.path.splitext(xml_file)[0]\n            if xml_basename_match == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    for file in files:\n        file_basename = os.path.basename(file)\n        # Extract the XML filename without extension\n        xml_basename = os.path.splitext(file_basename)[0]\n        \n        # Check if the file is related to an XML file\n        for xml in xmls:\n            xml_basename_no_ext = os.path.splitext(xml)[0]\n            if xml_basename_no_ext == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    for file in files:\n        basename = os.path.basename(file)\n        _, ext = os.path.splitext(basename)\n        if ext.lower() == '.xml':\n            xml_basename = os.path.splitext(basename)[0]\n            if xml_basename in xmls:\n                grouped_files[xml_basename] = file\n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    for file in files:\n        file_basename = os.path.basename(file)\n        # Extract the XML filename without extension\n        xml_basename = os.path.splitext(file_basename)[0]\n        \n        # Check if the file is related to an XML file\n        for xml in xmls:\n            xml_basename_match = os.path.splitext(xml)[0]\n            if xml_basename_match == xml_basename:\n                if xml_basename not in grouped_files:\n                    grouped_files[xml_basename] = []\n                grouped_files[xml_basename].append(file)\n                break\n    \n    return grouped_files"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72387, "num_retrieved": 228, "inference_time": 6.919085115194321, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72750, "num_retrieved": 235, "inference_time": 6.919085115194321, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71798, "num_retrieved": 207, "inference_time": 10.097951903939247, "generate_results": ["def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to store package data grouped by XML basename\n    package_data = defaultdict(dict)\n    \n    # Get all files in the folder\n    files = os.listdir(folder)\n    \n    # Process each file\n    for filename in files:\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Extract basename (without extension)\n        basename, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract package data from the XML file\n        # This is a placeholder - actual implementation would parse the XML\n        # and extract relevant metadata\n        package_data[basename]['xml'] = filename\n        \n        # Add other file types (e.g., renditions, assets) if needed\n        # For now, we only capture the XML file\n        \n    return dict(package_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to store package data grouped by XML basename\n    package_data = defaultdict(dict)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get the file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract the basename (without extension) of the XML file\n        basename = os.path.splitext(filename)[0]\n        \n        # Add the file path to the package data under the basename\n        package_data[basename]['xml'] = file_path\n        \n        # Look for related files (assets, renditions) that match the basename pattern\n        for related_filename in os.listdir(folder):\n            related_path = os.path.join(folder, related_filename)\n            if not os.path.isfile(related_path):\n                continue\n                \n            related_basename, related_ext = os.path.splitext(related_filename)\n            \n            # Check if the related file has a name that matches the XML basename pattern\n            # This could be a rendition or asset with a similar name\n            if related_basename.startswith(basename):\n                # Add to the package data under the basename\n                if related_ext.lower() in ['.pdf', '.jpg', '.jpeg', '.png', '.gif']:\n                    # Assume it's a rendition or asset\n                    package_data[basename][related_ext.lower()] = related_path\n    \n    # Convert defaultdict to regular dict\n    return dict(package_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to store package data grouped by XML basename\n    package_data = defaultdict(dict)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Extract file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # Add file to the package data under its basename\n        package_data[basename]['files'] = package_data[basename].get('files', []) + [filename]\n        \n        # Add metadata about the file\n        package_data[basename]['file_path'] = file_path\n    \n    return dict(package_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to store package data grouped by XML basename\n    package_data = defaultdict(dict)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Extract file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # Add the file path to the package data under the basename\n        package_data[basename]['xml_files'] = package_data[basename].get('xml_files', []) + [file_path]\n        \n        # Check if there are any associated files (e.g., assets, renditions) with the same basename\n        # This is a simplified implementation - in a real scenario, you might want to scan for specific patterns\n        # For now, we just group by XML basename\n        \n    # Convert defaultdict to regular dict and return\n    return dict(package_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to store package data grouped by XML basename\n    package_data = defaultdict(dict)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Extract file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract basename (without extension)\n        basename = os.path.splitext(filename)[0]\n        \n        # Add the file path to the package data under the basename\n        package_data[basename]['xml_files'] = package_data[basename].get('xml_files', []) + [file_path]\n        \n        # Check if there are any associated files (e.g., assets, renditions)\n        # This is a simplified implementation - in a real scenario, you might\n        # want to look for specific patterns or file naming conventions\n        for file in os.listdir(folder):\n            file_path = os.path.join(folder, file)\n            if os.path.isfile(file_path):\n                file_ext = os.path.splitext(file)[1].lower()\n                # Check if the file is related to the XML (e.g., assets, renditions)\n                # This is a heuristic - in practice, you might have more sophisticated rules\n                if file_ext in ['.pdf', '.jpg', '.jpeg', '.png', '.tiff']:\n                    # Extract the base name of the file\n                    file_basename = os.path.splitext(file)[0]\n                    # Check if the file basename matches the XML basename\n                    if file_basename.startswith(basename):\n                        package_data[basename][file_ext] = package_data[basename].get(file_ext, []) + [file_path]\n    \n    # Convert defaultdict to regular dict and return\n    return dict(package_data)"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72201, "num_retrieved": 217, "inference_time": 10.097951903939247, "generate_results": ["def _eval_file(prefix, file_path, pdf_langs=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in pdf_langs or []:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, pdf_langs=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'renditions', 'path': file_path}\n        for lang in pdf_langs or []:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'renditions', 'path': file_path}\n    return {'type': 'assets', 'path': file_path}", "def _eval_file(prefix, file_path, pdf_langs=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'renditions', 'path': file_path}\n        for lang in pdf_langs or []:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'renditions', 'path': file_path}\n    return {'type': 'assets', 'path': file_path}", "def _eval_file(prefix, file_path, pdf_langs=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in pdf_langs or []:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, pdf_langs=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'renditions', 'path': file_path}\n        for lang in pdf_langs or []:\n            if file == f'{prefix}-{lang}':\n                return {'type': 'renditions', 'path': file_path}\n    return {'type': 'assets', 'path': file_path}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 56190, "num_retrieved": 181, "inference_time": 10.097951903939247, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies do pacote.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF da rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome original e o nome da rendio em um idioma especfico\n    \"\"\"\n    # Cria um dicionrio com o nome original e o nome da rendio\n    rendition_dict = {\n        'original': os.path.basename(file_path),\n        lang: f\"{os.path.splitext(file_path)[0]}-{lang}.pdf\"\n    }\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies com base na lngua e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Cdigo da lngua do PDF (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com a chave da lngua e o valor sendo o nome do arquivo de rendio\n        Exemplo: {\"pt\": \"artigo02.pdf\", \"en\": \"artigo02-en.pdf\"}\n    \"\"\"\n    # Extrai o nome do arquivo sem a extenso\n    filename = os.path.basename(file_path)\n    name, ext = os.path.splitext(filename)\n    \n    # Cria o nome da rendio com base na lngua\n    if lang == 'en':\n        rendition_name = f\"{name}-en{ext}\"\n    else:\n        rendition_name = f\"{name}{ext}\"\n    \n    return {lang: rendition_name}", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies com base na linguagem e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com a rendio, onde a chave  o idioma e o valor  o nome do arquivo\n        Exemplo: {\"pt\": \"artigo02.pdf\", \"en\": \"artigo02-en.pdf\"}\n    \"\"\"\n    # Extrai o nome do arquivo sem a extenso\n    filename = os.path.basename(file_path)\n    name, ext = os.path.splitext(filename)\n    \n    # Cria o nome da rendio com base na linguagem\n    if lang == 'en':\n        rendition_name = f\"{name}-en{ext}\"\n    else:\n        rendition_name = f\"{name}{ext}\"\n    \n    return {lang: rendition_name}", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies do pacote.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF da rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome original e o nome da rendio em um idioma especfico\n    \"\"\"\n    # Cria um dicionrio com o nome original e o nome da rendio\n    rendition_dict = {\n        'original': os.path.basename(file_path),\n        lang: f\"{os.path.splitext(file_path)[0]}-{lang}.pdf\"\n    }\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies com base na linguagem e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome original e o nome da rendio na linguagem especificada\n        Exemplo: {\"original\": \"artigo02.pdf\", \"en\": \"artigo02-en.pdf\"}\n    \"\"\"\n    # Extrai o nome do arquivo sem a extenso\n    base_name = os.path.splitext(file_path)[0]\n    \n    # Cria o nome da rendio com base na linguagem\n    rendition_name = f\"{base_name}-{lang}.pdf\"\n    \n    # Retorna um dicionrio com o nome original e o nome da rendio\n    return {\n        \"original\": file_path,\n        \"en\": rendition_name if lang == \"en\" else None\n    }"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 54083, "num_retrieved": 224, "inference_time": 10.097951903939247, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(basename)[1].lower()\n    if ext in ['.tiff', '.tif']:\n        self._assets[basename] = file_path\n    elif ext in ['.jpg', '.jpeg']:\n        # Create a jpg version with the same basename\n        jpg_basename = basename.rsplit('.', 1)[0] + '.jpg'\n        self._assets[jpg_basename] = file_path\n    elif ext in ['.png']:\n        # Create a png version with the same basename\n        png_basename = basename.rsplit('.', 1)[0] + '.png'\n        self._assets[png_basename] = file_path\n    else:\n        # For other extensions, just add the original basename\n        self._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(basename)[1].lower()\n    if ext in ['.tiff', '.tif']:\n        # For TIFF files, keep the original extension\n        self[basename] = file_path\n    elif ext in ['.jpg', '.jpeg']:\n        # For JPEG files, create a thumbnail and add both original and thumbnail\n        thumbnail_path = os.path.splitext(file_path)[0] + \".thumbnail.jpg\"\n        # Assuming there's a method to create thumbnails, or it's handled elsewhere\n        # For now, just store the original path\n        self[basename] = file_path\n    elif ext in ['.png']:\n        # For PNG files, create a PNG version and add it\n        png_path = os.path.splitext(file_path)[0] + \".png\"\n        self[basename] = file_path\n    else:\n        # For other extensions, store the original path\n        self[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(basename)[1].lower()\n    if ext in ['.tiff', '.tif']:\n        # For TIFF files, keep the original extension\n        self[basename] = file_path\n    elif ext in ['.jpg', '.jpeg']:\n        # For JPG files, create a thumbnail and add both original and thumbnail\n        thumbnail_name = os.path.splitext(basename)[0] + \".thumbnail.jpg\"\n        thumbnail_path = file_path.replace(basename, thumbnail_name)\n        self[basename] = file_path\n        self[thumbnail_name] = thumbnail_path\n    elif ext in ['.png']:\n        # For PNG files, add the original file\n        self[basename] = file_path\n    else:\n        # For other extensions, add the original file\n        self[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(basename)[1].lower()\n    if ext in ['.tif', '.tiff']:\n        # Keep original file\n        self._assets[basename] = file_path\n    elif ext in ['.jpg', '.jpeg']:\n        # Create a JPEG version with specific-use=\"scielo-web\"\n        new_basename = basename.replace(ext, '.jpg')\n        new_file_path = file_path\n        self._assets[new_basename] = new_file_path\n    elif ext in ['.png']:\n        # Create a PNG version with specific-use=\"scielo-web\"\n        new_basename = basename.replace(ext, '.png')\n        new_file_path = file_path\n        self._assets[new_basename] = new_file_path\n    else:\n        # For other extensions, create a PNG version with specific-use=\"scielo-web\"\n        new_basename = basename.replace(ext, '.png')\n        new_file_path = file_path\n        self._assets[new_basename] = new_file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(basename)[1].lower()\n    if ext in ['.tif', '.tiff']:\n        # Keep original file\n        self._assets[basename] = file_path\n    elif ext in ['.jpg', '.jpeg']:\n        # Create a JPEG version with specific-use=\"scielo-web\"\n        new_basename = basename.replace(ext, '.jpg')\n        new_file_path = file_path\n        self._assets[new_basename] = new_file_path\n    elif ext in ['.png']:\n        # Create a PNG version with specific-use=\"scielo-web\"\n        new_basename = basename.replace(ext, '.png')\n        new_file_path = file_path\n        self._assets[new_basename] = new_file_path\n    else:\n        # For other extensions, create a PNG version with specific-use=\"scielo-web\"\n        new_basename = basename.replace(ext, '.png')\n        new_file_path = file_path\n        self._assets[new_basename] = new_file_path"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72488, "num_retrieved": 231, "inference_time": 10.097951903939247, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(filename)\n    return data"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72908, "num_retrieved": 253, "inference_time": 10.097951903939247, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zip_file:\n        for file_info in zip_file.infolist():\n            files.append(file_info.filename)\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zip_file:\n        for file_info in zip_file.infolist():\n            files.append(file_info.filename)\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zip_file:\n        for file_info in zip_file.infolist():\n            files.append(file_info.filename)\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zip_file:\n        for file_info in zip_file.infolist():\n            files.append(file_info.filename)\n    return files", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zip_file:\n        for file_info in zip_file.infolist():\n            files.append(file_info.filename)\n    return files"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73960, "num_retrieved": 280, "inference_time": 10.097951903939247, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \n    Parameters\n    ----------\n    content : str\n        Text to be processed.\n        \n    Returns\n    -------\n    str\n        Text with 'w:st=\"' replaced by 'w-st=\"'.\n    \"\"\"\n    return content.replace('w:st=\"', 'w-st=\"')", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \n    Parameters\n    ----------\n    content : str\n        Text to be processed.\n        \n    Returns\n    -------\n    str\n        Text with 'w:st=\"' replaced by 'w-st=\"'.\n    \"\"\"\n    return content.replace('w:st=\"', 'w-st=\"')", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \n    Parameters\n    ----------\n    content : str\n        Text to be processed.\n        \n    Returns\n    -------\n    str\n        Text with 'w:st=\"' converted to 'w-st=\"'.\n    \"\"\"\n    return content.replace('w:st=\"', 'w-st=\"')", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \n    Parameters\n    ----------\n    content : str\n        Text to be processed.\n        \n    Returns\n    -------\n    str\n        Text with 'w:st=\"' replaced by 'w-st=\"'.\n    \"\"\"\n    return content.replace('w:st=\"', 'w-st=\"')", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'.\n    \n    Parameters\n    ----------\n    content : str\n        Text to be processed.\n        \n    Returns\n    -------\n    str\n        Text with 'w:st=\"' replaced by 'w-st=\"'.\n    \"\"\"\n    return content.replace('w:st=\"', 'w-st=\"')"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71849, "num_retrieved": 214, "inference_time": 10.097951903939247, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72603, "num_retrieved": 240, "inference_time": 10.097951903939247, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and remove empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract number and supplement\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if part is a number (digits only)\n        if part.isdigit():\n            number = int(part)\n            break\n        # Check if part contains a supplement (e.g., \"s1\", \"s2\")\n        elif part.startswith('s') and part[1:].isdigit():\n            supplement = part[1:]\n            break\n    \n    # If no number found, return 0\n    if number is None:\n        number = 0\n    \n    # If no supplement found, return empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and filter out empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Try to extract number and supplement\n    number = None\n    supplement = None\n    \n    for part in parts:\n        # Check if part contains a number (digits only)\n        if part.isdigit():\n            number = int(part)\n            break\n        # Check if part contains a supplement (e.g., \"s1\", \"s2\")\n        elif part.startswith('s') and part[1:].isdigit():\n            supplement = part[1:]\n            break\n    \n    # If no number found, return 0\n    if number is None:\n        number = 0\n    \n    # If no supplement found, return empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and remove any non-numeric characters\n    parts = issue.strip().split()\n    number = \"\"\n    supplement = \"\"\n    \n    for part in parts:\n        # Remove non-numeric characters and non-alphabetic characters\n        cleaned_part = ''.join(c for c in part if c.isdigit() or c.isalpha())\n        \n        # If the cleaned part is numeric, it could be the number\n        if cleaned_part.isdigit():\n            number = cleaned_part\n        # If the cleaned part contains 's' or 'suppl', it could be the supplement\n        elif 's' in cleaned_part.lower() or 'suppl' in cleaned_part.lower():\n            supplement = cleaned_part\n    \n    # If no number was found, return 0\n    if not number:\n        number = \"0\"\n    \n    # If no supplement was found, return empty string\n    if not supplement:\n        supplement = \"\"\n    \n    return int(number), supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Remove any non-printable characters\n    issue = issue.strip()\n    \n    # Split the issue string by spaces and filter out empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Extract number and supplement\n    number = \"\"\n    supplement = \"\"\n    \n    for part in parts:\n        # Check if the part is a number (digits only)\n        if part.isdigit():\n            number = part\n        # Check if the part is a supplement (starts with 's' or 'S')\n        elif part.startswith('s') or part.startswith('S'):\n            supplement = part[1:] if len(part) > 1 else \"\"\n    \n    # If no number is found, return 0\n    if not number:\n        number = \"0\"\n    \n    # If no supplement is found, return empty string\n    if not supplement:\n        supplement = \"\"\n    \n    return int(number), supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Split the issue string by spaces and remove empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Initialize number and supplement\n    number = \"\"\n    supplement = \"\"\n    \n    # Check if the first part is a number or a supplement\n    if parts:\n        first_part = parts[0]\n        # Try to extract number (digits only)\n        number_digits = ''.join(c for c in first_part if c.isdigit())\n        if number_digits:\n            number = number_digits\n        else:\n            # If no digits, check if it's a supplement (e.g., \"s1\", \"s2\")\n            if first_part.startswith('s'):\n                supplement = first_part[1:]\n            else:\n                # If it's a volume, skip it\n                pass\n    \n    # Check for supplement in the rest of the parts\n    for part in parts[1:]:\n        if part.startswith('s'):\n            supplement = part[1:]\n            break\n    \n    # Return number and supplement\n    return int(number) if number else 0, supplement"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 10.097951903939247, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return \"NOT \" + self.children[0].pretty(indent + 1, debug)\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 10.097951903939247, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it matches absorption or negative absorption patterns\n        # This is a simplified implementation assuming expressions are in a form that can be evaluated\n        \n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using Boolean algebra rules\n        # We'll assume the expressions are already in a form that can be processed\n        # and that the absorption rules can be applied directly\n        \n        # In practice, this would require parsing and evaluating the expression tree\n        # For now, we return the original expression as a placeholder\n        # A full implementation would require traversing the expression tree\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # This is a simplified implementation - actual logic would depend on expression structure\n        # In practice, this would require parsing the expression tree\n        \n        # For now, we return the original expression as a placeholder\n        # A full implementation would need to traverse the expression tree\n        result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        # Absorption: A & (A | B) = A, A | (A & B) = A\n        # Negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(expr, Expression):\n            # Check for absorption cases\n            # Case 1: A & (A | B) = A\n            # Case 2: A | (A & B) = A\n            # Case 3: A & (~A | B) = A & B\n            # Case 4: A | (~A & B) = A | B\n            \n            # We need to evaluate the structure of the expression\n            # This is a simplified implementation assuming we can detect patterns\n            # In a full implementation, we would parse the expression tree\n            \n            # For now, return the expression as is (placeholder)\n            result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we just return the original expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            # This is a simplified implementation that assumes the expression\n            # is already in a form that can be checked for these laws\n            # In a full implementation, we would need to parse the expression tree\n            # and apply the rules recursively\n            result.append(expr)\n        else:\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69771, "num_retrieved": 148, "inference_time": 10.097951903939247, "generate_results": ["def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook].append(handler)\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified\n        hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook] = handler\n        return handler\n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69955, "num_retrieved": 163, "inference_time": 10.097951903939247, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 53581, "num_retrieved": 195, "inference_time": 10.097951903939247, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For UTC, return the original dt\n    if isinstance(self, tzutc):\n        return dt\n    \n    # For other timezones, we need to determine if the datetime is ambiguous\n    # and in a fold state\n    \n    # First, check if the datetime is ambiguous in this timezone\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # If it's not ambiguous, we can simply convert\n    if not is_ambiguous:\n        # Convert from UTC to local time\n        dt_utc = dt.replace(tzinfo=UTC)\n        dt_local = dt_utc.astimezone(self)\n        return dt_local\n    \n    # If it is ambiguous, we need to determine which fold to use\n    # The fold attribute indicates whether we're in the first or second occurrence\n    # of an ambiguous time\n    \n    # Check if the datetime is in a fold state\n    fold = getattr(dt, 'fold', 0)\n    \n    # Create the two possible times (fold 0 and fold 1)\n    dt_fold_0 = enfold(dt, fold=0)\n    dt_fold_1 = enfold(dt, fold=1)\n    \n    # Determine which fold to use based on the timezone's rules\n    # If the fold is 0, use the first occurrence (earlier)\n    # If the fold is 1, use the second occurrence (later)\n    \n    # For timezones with DST transitions, the fold state determines which\n    # of the two possible times is valid\n    \n    # Convert to local time using the appropriate fold\n    if fold == 0:\n        dt_local = dt_fold_0.astimezone(self)\n    else:\n        dt_local = dt_fold_1.astimezone(self)\n    \n    return dt_local", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Create a timezone-aware datetime with the current timezone\n    dt_with_tz = dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt_with_tz)\n    \n    # If it's ambiguous, determine which fold to use\n    if is_ambiguous:\n        # Use the fold value to determine which occurrence to use\n        # The fold value is 0 for the earlier occurrence, 1 for the later\n        fold = 0 if self._fold(dt_with_tz) else 1\n        return enfold(dt_with_tz, fold=int(fold))\n    else:\n        # If not ambiguous, just return the datetime\n        return dt_with_tz", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Convert the datetime to UTC\n    utc_dt = dt.astimezone(UTC)\n    \n    # Determine if the original datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # If the datetime is ambiguous, determine which fold to use\n    # Fold 0 is the earlier occurrence, fold 1 is the later occurrence\n    fold = 0 if is_ambiguous else 1\n    \n    # Convert back to the original timezone with the appropriate fold\n    return enfold(utc_dt, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Convert the datetime to UTC\n    utc_dt = dt.astimezone(UTC)\n    \n    # Determine if the original datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # If the datetime is ambiguous, we need to determine which fold to use\n    # The fold is determined by the transition rules in the timezone\n    if is_ambiguous:\n        # Use the fold value from the timezone\n        fold = self._fold(dt)\n        # Create the enfolded datetime with the appropriate fold\n        return enfold(utc_dt, fold=int(fold))\n    else:\n        # For non-ambiguous times, simply convert to UTC and back\n        return enfold(utc_dt, fold=0)", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Create a timezone-aware datetime with the current timezone\n    dt_with_tz = dt.replace(tzinfo=self)\n    \n    # Determine if the datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt_with_tz)\n    \n    # If it's ambiguous, determine which fold to use\n    # Fold 0 is the earlier occurrence, fold 1 is the later occurrence\n    fold = 0 if is_ambiguous else 1\n    \n    # Calculate the offset from UTC to this timezone\n    utc_offset = self.utcoffset(dt_with_tz)\n    \n    # Create the result datetime by adding the offset to UTC\n    result = dt + utc_offset\n    \n    # Apply the fold if needed\n    if is_ambiguous:\n        result = enfold(result, fold=fold)\n    \n    return result"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 53378, "num_retrieved": 203, "inference_time": 10.097951903939247, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through each attribute and convert fractional values to integers\n    # by rounding to the nearest integer\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days':\n                # Handle days with fractional components\n                if value != int(value):\n                    # Add the fractional part to the next day\n                    days = int(value) + 1\n                    # Adjust hours to account for the extra day\n                    hours = int(getattr(self, 'hours', 0)) + (value - int(value)) * 24\n                else:\n                    days = int(value)\n            elif attr == 'hours':\n                # Handle hours with fractional components\n                if value != int(value):\n                    # Add the fractional part to the next day\n                    hours = int(value) + (value - int(value)) * 24\n                else:\n                    hours = int(value)\n            else:\n                # For other attributes, just convert to integer\n                if value != int(value):\n                    value = int(value) + 1\n                else:\n                    value = int(value)\n            \n            # Set the attribute on the result object\n            setattr(result, attr, value)\n    \n    # Adjust the days and hours to ensure they are consistent\n    # If there are fractional days, add them to the hours\n    if hasattr(self, 'days') and self.days != int(self.days):\n        fractional_days = self.days - int(self.days)\n        hours = int(self.hours) + int(fractional_days * 24)\n        result.hours = hours\n    \n    # Ensure all values are integers\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(result, attr, 0)\n        if value != 0", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Handle fractional values by converting to integer\n            if attr == 'days':\n                # Convert fractional days to hours\n                hours = int(value * 24)\n                # Add the hours to the hours attribute\n                hours += getattr(self, 'hours', 0)\n                # Convert hours to days and add to days attribute\n                days = hours // 24\n                hours = hours % 24\n                # Set the days and hours attributes\n                setattr(result, 'days', days)\n                setattr(result, 'hours', hours)\n            elif attr == 'hours':\n                # Convert fractional hours to minutes\n                minutes = int(value * 60)\n                # Add the minutes to the minutes attribute\n                minutes += getattr(self, 'minutes', 0)\n                # Convert minutes to hours and add to hours attribute\n                hours = minutes // 60\n                minutes = minutes % 60\n                # Set the hours and minutes attributes\n                setattr(result, 'hours', hours)\n                setattr(result, 'minutes', minutes)\n            elif attr == 'minutes':\n                # Convert fractional minutes to seconds\n                seconds = int(value * 60)\n                # Add the seconds to the seconds attribute\n                seconds += getattr(self, 'seconds', 0)\n                # Convert seconds to minutes and add to minutes attribute\n                minutes = seconds // 60\n                seconds = seconds % 60\n                # Set the minutes and seconds attributes\n                setattr(result, 'minutes', minutes)\n                setattr(result, 'seconds', seconds)\n            elif attr == 'weeks':\n                # Convert weeks to days\n                days = int(value * 7)\n                # Add the days to the days", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all attributes and convert fractional values to integers\n    # by rounding to the nearest integer\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Round to nearest integer\n                rounded_value = round(value)\n                # If the value is zero, keep it as zero\n                if rounded_value == 0:\n                    setattr(result, attr, 0)\n                else:\n                    setattr(result, attr, rounded_value)\n            else:\n                # For integer values, just copy them over\n                setattr(result, attr, value)\n    \n    # Handle the special case where we need to adjust for overflow\n    # This is a simplified version - in practice, we'd need to handle\n    # cases where adding hours might cause day overflow, etc.\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current object to avoid modifying the original\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    for attr in self.__slots__:\n        value = getattr(self, attr)\n        if value is not None:\n            # Handle fractional values by converting to integer\n            if isinstance(value, float):\n                # Convert fractional values to integers\n                if attr == 'days':\n                    days = int(value)\n                    # If fractional part exists, add the extra days\n                    if value != days:\n                        # Add the extra days to the hours\n                        hours = getattr(self, 'hours', 0) + int((value - days) * 24)\n                        # Update the hours attribute\n                        setattr(result, 'hours', hours)\n                    else:\n                        setattr(result, 'days', days)\n                elif attr == 'hours':\n                    hours = int(value)\n                    # If fractional part exists, add the extra minutes\n                    if value != hours:\n                        minutes = getattr(self, 'minutes', 0) + int((value - hours) * 60)\n                        # Update the minutes attribute\n                        setattr(result, 'minutes', minutes)\n                    else:\n                        setattr(result, 'hours', hours)\n                elif attr == 'minutes':\n                    minutes = int(value)\n                    # If fractional part exists, add the extra seconds\n                    if value != minutes:\n                        seconds = getattr(self, 'seconds', 0) + int((value - minutes) * 60)\n                        # Update the seconds attribute\n                        setattr(result, 'seconds', seconds)\n                    else:\n                        setattr(result, 'minutes', minutes)\n                else:\n                    # For other attributes, just convert to integer\n                    setattr(result, attr, int(value))\n            else:\n                # For non-fractional values, just assign directly\n                setattr(result, attr, value)\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through each attribute and convert fractional values to integers\n    # by rounding to the nearest integer\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days':\n                # Handle days with fractional components\n                if value != int(value):\n                    # Add the fractional part to the next day\n                    days = int(value) + 1\n                    # Adjust hours to account for the extra day\n                    hours = int(getattr(self, 'hours', 0))\n                    if hours > 0:\n                        # Add the hours to the day count\n                        days += hours // 24\n                        hours = hours % 24\n                    # Set the days and hours\n                    setattr(result, attr, days)\n                    setattr(result, 'hours', hours)\n                else:\n                    setattr(result, attr, int(value))\n            elif attr == 'hours':\n                # Handle hours with fractional components\n                if value != int(value):\n                    # Add the fractional part to the next day\n                    hours = int(value) + 1\n                    # Adjust days to account for the extra day\n                    days = int(getattr(self, 'days', 0))\n                    if days > 0:\n                        # Add the days to the hour count\n                        hours += days * 24\n                    # Set the hours and days\n                    setattr(result, attr, hours)\n                    setattr(result, 'days', days)\n                else:\n                    setattr(result, attr, int(value))\n            else:\n                # For other attributes, just convert to integer\n                setattr(result, attr, int(value))\n    \n    # Ensure all values are positive\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds']:\n        value = getattr(result, attr, 0)"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 51772, "num_retrieved": 185, "inference_time": 10.097951903939247, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 16408, "num_retrieved": 60, "inference_time": 6.794711157679558, "generate_results": ["def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 13626, "num_retrieved": 43, "inference_time": 6.794711157679558, "generate_results": ["def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 16253, "num_retrieved": 58, "inference_time": 6.794711157679558, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"plus\" not in pieces else \".\"\n    return \"+\" if not pieces[\"closest-tag\"] else \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"plus\" not in pieces else \".\"\n    return \"+\" if not pieces[\"closest-tag\"] else \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"plus\" not in pieces else \".\"\n    return \"+\" if not pieces[\"closest-tag\"] else \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"\n    return \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"plus\" not in pieces else \".\"\n    return \"+\" if not pieces[\"closest-tag\"] else \".\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 10252, "num_retrieved": 29, "inference_time": 6.794711157679558, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        raise", "import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Construct the full command list\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or os.environ.copy()\n    \n    # Set up cwd\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT if hide_stderr else subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hidden\n        if verbose and result.stdout:\n            print(result.stdout)\n            \n        return result\n        \n    except subprocess.SubprocessError as e:\n        print(f\"Error running command: {e}\", file=sys.stderr)\n        return None", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(f\"Error running command: {e}\", file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        raise"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 16221, "num_retrieved": 59, "inference_time": 6.794711157679558, "generate_results": ["def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 12606, "num_retrieved": 37, "inference_time": 6.794711157679558, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 46559, "num_retrieved": 104, "inference_time": 6.794711157679558, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the root filesystem to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 45289, "num_retrieved": 92, "inference_time": 6.794711157679558, "generate_results": ["def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = 'unknown'\n    if self.spec_version is None:\n        self.spec_version = '1.0'\n    if self.digest_algorithm is None:\n        self.digest_algorithm = 'sha512'\n    if self.content_directory is None:\n        self.content_directory = 'content'\n    \n    # Initialize the inventory and other required components\n    self.inventory = {}\n    self.all_versions = []\n    self.manifest_files = None\n    self.unnormalized_digests = None\n    self.head = 'UNKNOWN'\n    \n    # Set up the root inventory structure\n    self.inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        self.inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        self.inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            self.inventory['fixity'][fixity_type] = {}\n    \n    # Initialize the root inventory digest file\n    self.inventory_digest_files = {}", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object's filesystem if not already set\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        # This will be handled by the open_fs call in other methods\n        pass\n    \n    # Initialize the object's metadata\n    # The actual initialization of the object's metadata is typically\n    # handled by the validation and parsing methods rather than this method\n    \n    # Set up the object's state and configuration\n    # This includes setting up the inventory, versions, and other metadata\n    # which is typically done during validation or parsing\n    \n    # No specific initialization actions are required beyond setting up\n    # the object's state for future operations\n    pass", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = 'unknown'\n    if self.spec_version is None:\n        self.spec_version = '1.0'\n    if self.digest_algorithm is None:\n        self.digest_algorithm = 'sha512'\n    if self.content_directory is None:\n        self.content_directory = 'content'\n    \n    # Initialize the inventory and related structures\n    self.inventory = {}\n    self.all_versions = []\n    self.manifest_files = None\n    self.unnormalized_digests = None\n    self.head = 'UNKNOWN'\n    \n    # Set up the root inventory structure\n    self.inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        self.inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        self.inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            self.inventory['fixity'][fixity_type] = {}\n    \n    # Initialize the extensions directory\n    self.extensions_dir = 'extensions'\n    \n    # Set up the object root structure\n    self.root_inv_validator = None\n    self.inventory_digest_files = {}", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object's filesystem if not already set\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        # This will be handled by the object creation process\n        pass\n    \n    # Initialize the inventory and other object metadata\n    # This is typically done when the object is first created\n    # and the inventory is generated\n    \n    # Set up the object's root inventory with default values\n    self.id = self.id or None\n    self.spec_version = self.spec_version or '1.1'\n    self.digest_algorithm = self.digest_algorithm or 'sha512'\n    self.content_directory = self.content_directory or 'content'\n    \n    # Initialize the inventory structure\n    self.inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        self.inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        self.inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            self.inventory['fixity'][fixity_type] = {}\n    \n    # Initialize the extensions directory\n    self.extensions_dir = 'extensions'\n    \n    # Initialize the object's state\n    self.all_versions = []\n    self.manifest_files = None\n    self.unnormalized_digests = None\n    self.head = 'UNKNOWN'\n    \n    # Initialize the validation logger if not already set\n    if self.log is None:\n        self.log = ValidationLogger(show_warnings=False, show_errors=True, lang='en')", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object's filesystem if not already done\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the object's metadata\n    self.id = self.id or self._get_id_from_inventory()\n    \n    # Set up the initial inventory structure\n    self.spec_version = self.spec_version or '1.1'\n    \n    # Create the necessary directories\n    self.obj_fs.makedir('content')\n    self.obj_fs.makedir('extensions')\n    \n    # Create the object declaration file\n    self.object_declaration_object().write(pyfs=self.obj_fs, dir='')\n    \n    # Create the initial inventory file\n    inventory = self.start_inventory()\n    inventory_file = 'inventory.json'\n    with self.obj_fs.open(inventory_file, 'w') as fh:\n        json.dump(inventory, fh, sort_keys=True, indent=2)\n    \n    # Create the initial inventory digest file\n    digest_algorithm = self.digest_algorithm or 'sha512'\n    digest_file = inventory_file + '.' + digest_algorithm\n    digest = file_digest(inventory_file, digest_algorithm, pyfs=self.obj_fs)\n    with self.obj_fs.open(digest_file, 'w') as fh:\n        fh.write(digest + ' ' + inventory_file)\n    \n    # Initialize the extensions directory\n    self.validate_extensions_dir()"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 49588, "num_retrieved": 114, "inference_time": 6.794711157679558, "generate_results": ["def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if the version is zero-prefixed\n    # If it's zero-prefixed, we need to find the padding size\n    # by checking if there's a version with a different padding\n    # pattern that starts with v1\n    \n    # If the version is not zero-prefixed, just increment\n    if version_num < 999999:\n        return 'v' + str(version_num + 1)\n    \n    # If we reach here, the version is at the maximum limit\n    # This should not happen in normal use cases\n    raise ValueError(\"Version number exceeds maximum limit\")", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if version is zero-prefixed (has leading zeros)\n    # If it's zero-prefixed, we need to find the padding size\n    # by checking if there's a version with a different padding\n    # pattern\n    \n    # First, check if we have a zero-prefixed version\n    # Look for a version with the same number of digits as the current one\n    # and see if it's zero-prefixed\n    \n    # If we have a zero-prefixed version, the next version should have the same padding\n    # Otherwise, we just increment the number\n    \n    # Check if the current version has leading zeros\n    if version_num < 10:\n        # If version is v1, v2, v3, ..., v9, then next is v10\n        if version_num == 9:\n            return 'v10'\n        return 'v' + str(version_num + 1).zfill(2)\n    \n    # For versions >= 10, we need to check if there's a zero-prefixed pattern\n    # by looking for a version with the same number of digits\n    \n    # Check if we have a version with the same number of digits\n    # For example, if we have v10, v11, v12, ..., v19, then next is v20\n    # If we have v100, v101, ..., v199, then next is v200\n    \n    # Check if the version is zero-prefixed by checking if it has leading zeros\n    # For example, v01, v02, v03, ..., v09, v10, v11, ..., v19\n    \n    # If the version is zero-prefixed, we need to find the padding size\n    # by checking if there's a version with a different padding\n    \n    # Check if we have a version with the same number of digits\n    # For example, if we have v01, v02, v", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if it's zero-prefixed (has leading zeros)\n    if version_num < 10:\n        # For versions 1-9, next version is v10\n        return 'v10'\n    \n    # For versions >= 10, increment by 1\n    next_num = version_num + 1\n    \n    # Format with leading zeros based on the original pattern\n    # If original had 1 digit, next should have 1 digit (v10)\n    # If original had 2 digits, next should have 2 digits (v11)\n    # If original had 3 digits, next should have 3 digits (v101)\n    # We need to determine the padding size from the original version\n    \n    # For v10, v11, v12, etc., we can infer the pattern\n    # If version is v10, v11, v12, etc., we just increment\n    # If version is v100, v101, etc., we just increment\n    \n    # The pattern is determined by the number of digits in the version number\n    # For v1, v2, v3, ..., v9, v10, v11, v12, ...\n    # The next version should have the same number of digits\n    \n    # For v1 to v9, next is v10\n    # For v10 to v99, next is v11 to v99+1\n    # For v100 to v999, next is v101 to v999+1\n    \n    # Determine the number of digits in the version number\n    num_digits = len(str(version_num))\n    \n    # Format the next version with the same number of digits\n    next_version_str = 'v' + str(next_num).zfill(num_digits)\n    \n    return next_version_str", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if version is zero-prefixed (e.g., v001, v002)\n    if version_num < 10:\n        # For versions 1-9, we need to determine the padding\n        # If it's v1, v2, ..., v9, then next is v10\n        # If it's v01, v02, ..., v09, then next is v10\n        # If it's v001, v002, ..., v009, then next is v010\n        # We need to find the padding size\n        padding_size = len(version) - 2\n        if padding_size > 0:\n            # Format with the same padding\n            return 'v' + str(version_num + 1).zfill(padding_size)\n        else:\n            return 'v' + str(version_num + 1)\n    \n    # For versions >= 10, we need to determine the padding\n    # If it's v10, v11, ..., v99, then next is v100\n    # If it's v010, v011, ..., v099, then next is v100\n    # If it's v0010, v0011, ..., v0099, then next is v0100\n    # We need to find the padding size\n    padding_size = len(version) - 2\n    if padding_size > 0:\n        # Format with the same padding\n        return 'v' + str(version_num + 1).zfill(padding_size)\n    else:\n        return 'v' + str(version_num + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if it's zero-prefixed (e.g., v001, v002)\n    if version_num < 10:\n        # For versions 1-9, we need to determine the padding\n        # If it's v1, v2, ..., v9, then next is v10\n        # If it's v01, v02, ..., v09, then next is v10\n        return 'v10'\n    \n    # For versions >= 10, we just increment\n    next_num = version_num + 1\n    \n    # Determine the padding based on the original version\n    # If the original version had 3 digits (e.g., v001), then we keep 3 digits\n    # If the original version had 2 digits (e.g., v01), then we keep 2 digits\n    # If the original version had 1 digit (e.g., v1), then we keep 1 digit\n    # But we need to find the pattern\n    \n    # Check if the version was zero-prefixed\n    if version[1:].startswith('0'):\n        # Find the length of the zero-prefixed part\n        zero_padding_length = len(version) - 2\n        # Pad the next version with the same length\n        return 'v' + str(next_num).zfill(zero_padding_length)\n    \n    # For non-zero prefixed versions, just increment\n    return 'v' + str(next_num)"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 44686, "num_retrieved": 124, "inference_time": 6.794711157679558, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point.\n    \n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    # Initialize a set to track digests that are different from the root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if the version directory exists\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E043\", version=vdir)\n            return\n            \n        # Read the inventory for this version\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E044\", version=vdir)\n            return\n            \n        # Parse and validate the inventory\n        try:\n            inventory = self.parse_inventory(inventory_path)\n            self.validate(inventory)\n            \n            # Extract the manifest and state blocks\n            manifest = inventory.get('manifest', {})\n            state = inventory.get('versions', {}).get(vdir, {}).get('state', {})\n            \n            # Compare digests in state with those in the root inventory\n            # This is a simplified check - in a real implementation, we would\n            # need to compare with the root inventory's manifest\n            if 'manifest' in inventory and 'versions' in inventory:\n                # For now, we'll just record any digests that are present in state\n                # but not in the root manifest (this would be a more complete check)\n                pass\n                \n        except Exception as e:\n            self.error(\"E045\", version=vdir, error=str(e))\n            return\n            \n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root inventory.\"\"\"\n    # Validate that version_dirs is a non-empty list\n    if not version_dirs:\n        self.error(\"E044\")\n        return\n    \n    # Initialize a set to track digests that are different from root inventory\n    different_digests = set()\n    \n    # Assume we have a root inventory with a manifest that we'll compare against\n    # For each version directory, validate that it has an inventory and check for different digests\n    for vdir in version_dirs:\n        # Check if the version directory exists and has an inventory\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E044\", version=vdir)\n            return\n            \n        # Read the inventory for this version\n        inventory = self.parse_inventory(vdir)\n        if inventory is None:\n            self.error(\"E041b\", version=vdir)\n            return\n            \n        # Validate the inventory structure and content\n        self.validate(inventory, extract_spec_version=False)\n        \n        # Extract the manifest from the inventory\n        manifest = inventory.get('manifest', {})\n        \n        # Compare the manifest with the root inventory manifest to find different digests\n        # This is a simplified implementation - in a real scenario, we would have access to the root manifest\n        # For now, we'll just assume we're tracking digests that are not in the root manifest\n        # In a full implementation, we would compare against the root manifest to find differences\n        for digest in manifest:\n            # In a real implementation, we would compare this digest against the root inventory's manifest\n            # For now, we'll just add it to our set of different digests\n            different_digests.add(digest)\n    \n    # Return the set of digests that are different from the root inventory\n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root.\n\n    Each version directory should have an inventory file that represents the state up to that version.\n    Also records any content digests that differ from the root inventory for later validation.\n\n    Args:\n        version_dirs: Array of version directory names in sequence (1, 2, 3...)\n    \"\"\"\n    # Initialize a set to track digests that differ from root\n    differing_digests = set()\n    \n    # Validate each version directory\n    for vdir in version_dirs:\n        # Check if inventory file exists in version directory\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            continue\n            \n        # Load the inventory for this version\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            continue\n            \n        # Validate the version inventory structure\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            continue\n            \n        # Get the root inventory (first version)\n        if 'head' not in version_inventory:\n            self.error(\"E036d\", version=vdir)\n            continue\n            \n        # Compare manifest digests with root inventory\n        # This is a simplified approach - in a real implementation, we would\n        # compare the manifest digests with the root inventory's manifest\n        # to identify any differences\n        root_manifest = self.inventory['manifest'] if self.inventory else {}\n        \n        # For each digest in the current version's manifest\n        for digest in version_inventory['manifest']:\n            if digest not in root_manifest:\n                differing_digests.add(digest)\n    \n    # Return the set of differing digests\n    return differing_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root.\n\n    Each version SHOULD have an inventory up to that point.\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in version sequence (1, 2, 3...).\n    \"\"\"\n    # Initialize a set to track digests that are different from the root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if the version directory exists\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E043\", version=vdir)\n            continue\n            \n        # Read the inventory for this version\n        inventory = self.parse_inventory()\n        \n        # Validate the inventory structure\n        if 'manifest' not in inventory:\n            self.error(\"E041a\", version=vdir)\n            continue\n            \n        # Validate the manifest block\n        (manifest_files, manifest_files_correct_format, unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n        \n        # Check if the manifest files are valid\n        if not manifest_files:\n            self.error(\"E041c\", version=vdir)\n            continue\n            \n        # Validate the versions block\n        if 'versions' not in inventory:\n            self.error(\"E041b\", version=vdir)\n            continue\n            \n        # Extract the root inventory's manifest digests\n        root_manifest_digests = set(manifest_files.values())\n        \n        # Extract the current version's manifest digests\n        current_manifest_digests = set(manifest_files.values())\n        \n        # Find digests that are different from the root inventory\n        for digest in current_manifest_digests:\n            if digest not in root_manifest_digests:\n                different_digests.add(digest)\n    \n    # Return the set of different digests\n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point.\n    \n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    # Validate that version_dirs is a non-empty list\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Initialize a set to track digests that are different from the root inventory\n    different_digests = set()\n    \n    # Process each version directory in sequence\n    for vdir in version_dirs:\n        # Validate that the version directory exists and has a valid inventory\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E044\", version=vdir)\n            return\n            \n        # Read the inventory for this version\n        inventory = self.parse_inventory(vdir)\n        if inventory is None:\n            self.error(\"E041a\", version=vdir)\n            return\n            \n        # Validate the inventory structure\n        self.validate(inventory, extract_spec_version=False)\n        \n        # Extract the manifest from the inventory\n        manifest = inventory.get('manifest', {})\n        \n        # Compare the manifest with the root inventory manifest to find any different digests\n        # This assumes we have access to the root inventory manifest\n        if hasattr(self, 'root_manifest') and self.root_manifest:\n            root_manifest_digests = set(self.root_manifest.keys())\n            version_manifest_digests = set(manifest.keys())\n            \n            # Find digests that are in the version but not in the root\n            different_digests.update(version_manifest_digests - root_manifest_digests)\n            \n            # Find digests that are in the root but not in the version\n            # (This would be caught in other validation steps)\n            \n        # Validate that the version has a valid head reference\n        if 'head' not in inventory:\n            self.error(\"E036d\", version=vdir)\n            return\n            \n        # Validate that the head points to a valid version\n        if inventory['head'] not in version_dirs:\n            self.error(\"E040\", version=vdir, expected=inventory['head'])\n            return\n    \n    # Return the set of different digests for further validation\n    return different"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 50776, "num_retrieved": 127, "inference_time": 6.794711157679558, "generate_results": ["def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs, limit=1)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* file found, likely a file\n            \n        # Check if the path is a directory\n        if not fs.exists(path):\n            return 'file'  # Path doesn't exist\n            \n        # Check if it's a directory with 0=* files\n        # OCFL Storage Root has 0=ocfl_object_<version> files\n        # OCFL Object has 0=ocfl_object_<version> files in its root\n        # If we have a 0=* file, it's either a root or an object\n        \n        # Check if the path is a directory (not a file)\n        if not fs.isdir(path):\n            return 'file'\n            \n        # Check if the namaste files are of the expected format\n        # For a storage root, we expect 0=ocfl_object_<version> files\n        # For an object, we expect 0=ocfl_object_<version> files in the root\n        \n        # If we have a 0=* file, it's either a root or an object\n        # We need to check if there are other directories that might indicate an object\n        # But the spec says that the root should have the 0= file\n        \n        # If we have a 0=* file, it's either a root or an object\n        # We'll return 'root' if it's a storage root, 'object' if it's an object\n        \n        # Check if there are any version directories\n        version_dirs = []\n        for entry in fs.scandir(''):\n            if entry.is_dir and re.match(r'v\\d+$', entry.name):\n                version_dirs.append(entry.name)", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* file found, likely a file\n        \n        # Check the filename of the first namaste file\n        first_namaste = namastes[0]\n        filename = first_namaste.filename\n        \n        # Check if it's a root (has a version directory structure)\n        if filename.startswith('0=ocfl_object_'):\n            # Check if there are any version directories\n            version_dirs = []\n            for entry in fs.scandir(''):\n                if entry.is_dir and re.match(r'v\\d+$', entry.name):\n                    version_dirs.append(entry.name)\n            \n            if len(version_dirs) > 0:\n                return 'root'\n            else:\n                return 'object'\n        \n        # If we get here, it's likely a file\n        return 'file'\n        \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* files found, likely a file\n        \n        # Check the filename of the first namaste file\n        first_namaste = namastes[0]\n        filename = first_namaste.filename\n        \n        # Check if it's a root (has a version directory structure)\n        # Root should have a 0=ocfl_object_x.x file\n        if filename.startswith('0=ocfl_object_'):\n            # Check if there are any version directories\n            version_dirs = []\n            for entry in fs.scandir(''):\n                if entry.is_dir and re.match(r'v\\d+$', entry.name):\n                    version_dirs.append(entry.name)\n            \n            if len(version_dirs) > 0:\n                return 'root'\n            else:\n                return 'object'\n        \n        # If we have a 0=* file but it's not the object declaration\n        # This could be a file with a 0=* namaste\n        return 'file'\n        \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs, limit=1)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* file found, likely a file\n        \n        # Check if the path is a directory\n        if not fs.exists(path):\n            return 'file'  # Path doesn't exist\n        \n        # Check if it's a directory with a 0=* file\n        if not fs.isdir(path):\n            return 'file'  # Path is not a directory\n        \n        # Check if the 0=* file is in the root directory\n        # If it's in the root, it's likely an OCFL Storage Root\n        # If it's in a subdirectory, it's likely an OCFL Object\n        namaste_filename = namastes[0].filename\n        if namaste_filename.startswith('0=ocfl_object_'):\n            # This is an object declaration\n            return 'object'\n        elif namaste_filename == '0=ocfl_object_1.0':\n            # This is a root declaration\n            return 'root'\n        else:\n            # Unknown 0=* file\n            return 'file'\n            \n    except Exception as e:\n        return 'error: ' + str(e)", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* file found, likely a file\n        \n        # Check the filename of the first namaste file\n        first_namaste = namastes[0]\n        filename = first_namaste.filename\n        \n        # Check if it's a root (has a version directory structure)\n        if filename.startswith('0=ocfl_object_'):\n            # Check if there are any version directories\n            version_dirs = []\n            for entry in fs.scandir(''):\n                if entry.is_dir and re.match(r'v\\d+$', entry.name):\n                    version_dirs.append(entry.name)\n            \n            if len(version_dirs) > 0:\n                return 'root'\n            else:\n                return 'object'\n        \n        # If we get here, it's not a standard OCFL object or root\n        return 'file'\n        \n    except Exception as e:\n        return f\"Error accessing path {path}: {str(e)}\""]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73270, "num_retrieved": 245, "inference_time": 6.794711157679558, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical usage, it likely modifies the params\n    # to include additional fields or conditions for querying Bugzilla\n    # However, no specific implementation details are provided in the context\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical usage, it likely modifies the params\n    # to include additional fields or conditions for querying Bugzilla\n    # However, no specific implementation details are provided in the context\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical usage, it likely modifies the params\n    # to include additional fields or conditions for querying Bugzilla\n    # However, no specific implementation details are provided in the context\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical usage, it likely modifies the params\n    # to include additional fields or conditions for bug queries\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical usage, it likely modifies the params\n    # to include additional fields or conditions for bug queries\n    pass"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 63340, "num_retrieved": 192, "inference_time": 6.794711157679558, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 62185, "num_retrieved": 198, "inference_time": 6.794711157679558, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # The actual implementation would involve setting up an ArgumentParser\n    # and parsing the provided arguments, but no such implementation is present\n    # in the retrieved context.\n    pass", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace object or similar structure\n    \n    # Since the context does not contain the actual implementation of parse_arguments,\n    # we return a dummy implementation that would be used in the actual codebase\n    \n    parser = ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--config-paths', nargs='+', help='Paths to configuration files')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Syslog verbosity level')\n    parser.add_argument('--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Log file verbosity level')\n    parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Monitoring verbosity level')\n    parser.add_argument('--log-file', help='Path to log file')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (actual implementation would have more specific subparsers)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories')\n    \n    prune_parser = subparsers.add_parser('prune', help='Prune old archives')\n    prune_parser.add_argument('--repository', help='Repository path')\n    prune_parser.add_argument", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # The actual implementation would involve setting up an ArgumentParser\n    # and parsing the provided arguments, but no such implementation is present\n    # in the retrieved context.\n    pass", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace object or similar structure\n    parser = ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true')\n    parser.add_argument('--config-paths', nargs='+', default=[])\n    parser.add_argument('--dry-run', action='store_true')\n    parser.add_argument('--no-color', action='store_true')\n    parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some')\n    parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some')\n    parser.add_argument('--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some')\n    parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some')\n    parser.add_argument('--log-file', type=str)\n    parser.add_argument('--repository', type=str)\n    parser.add_argument('--archive', type=str)\n    parser.add_argument('--prefix', type=str)\n    parser.add_argument('--glob-archives', type=str)\n    parser.add_argument('--paths', nargs='+', default=[])\n    parser.add_argument('--json', action='store_true')\n    parser.add_argument('--lock-wait', type=int)\n    parser.add_argument('--umask', type=int)\n    parser.add_argument('--remote-path', type=str)\n    parser.add_argument('--info', action='store_true')\n    parser.add_argument('--debug', action='store_true')\n    parser.add_argument('--progress', action='store_true')\n    parser.add_argument('--strip-components', type=int)\n    parser.add_argument('--extract-to-stdout', action='store_true')\n    parser.add_argument('--extract-to', type=str)\n    parser.add_argument('--source-config-filename', type=str)\n    parser.add_argument('--source-excludes-filename', type=str)\n    parser.add_argument('--destination-config-filename', type=str)\n    parser.add_argument('--overwrite', action='store_true')\n    \n    #", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the arguments\n    parser = ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--help', action='help', help='Show this help message and exit')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Example subparsers (actual implementation would be more detailed)\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Specific archive to list')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    \n    return parser.parse_args(arguments)"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 62481, "num_retrieved": 189, "inference_time": 6.794711157679558, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(sorted(flags))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(sorted(flags))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(sorted(flags))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(sorted(flags))"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 61648, "num_retrieved": 204, "inference_time": 6.794711157679558, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic version 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration overrides in the form section.option=value')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parsers for each subcommand\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Files to exclude from backup')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Patterns to exclude if present')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')\n    create_parser.add_argument('--keep-daily', type=int, help='Keep daily archives", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Create parsers for each subcommand\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Exclude patterns from files')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude patterns if present')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')\n    create_parser.add_argument('--keep-daily', type=int, help='Keep daily archives')\n    create_parser.add_argument('--keep-weekly', type=int, help='Keep weekly archives')\n    create_parser.add_argument('--keep-monthly', type=int, help='Keep monthly archives')\n    \n    extract_parser = subparsers.add_parser('extract", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Create parsers for each subcommand\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='File containing exclude patterns')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Patterns to exclude if present')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude directories that should not be dumped')\n    create_parser.add_argument('--patterns-from', nargs='+', help='File containing patterns')\n    \n    extract_parser = subparsers.add_parser('extract', help='Extract a backup')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Create parsers for each subcommand\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Files to exclude from backup')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')\n    create_parser.add_argument('--patterns-from', nargs='+', help='File containing patterns to use')\n    \n    extract_parser = subparsers.add_parser('extract', help='Extract a backup')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration overrides in the form section.option=value')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Create parser for create command\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Files to exclude from backup')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')\n    create_parser.add_argument('--keep-daily', type=int, help='Keep daily archives')\n    create"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 61825, "num_retrieved": 205, "inference_time": 11.095412090420723, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Parse the remaining arguments with the current subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Parse the arguments using the subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If parsing fails, skip this subparser and continue with the next\n            continue\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Parse the arguments using the subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If the parser raises SystemExit, it means the arguments are not valid\n            # We need to handle this case\n            break\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Parse the arguments using the subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If parsing fails, continue to next subparser\n            continue\n    \n    return arguments, remaining_arguments"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 60165, "num_retrieved": 180, "inference_time": 11.095412090420723, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='borgmatic - A tool for managing Borg backups',\n        prog='borgmatic'\n    )\n    \n    # Add global flags\n    global_flags = top_level_parser.add_argument_group('global')\n    global_flags.add_argument(\n        '--version', action='version', version='borgmatic %(prog)s %(version)s'\n    )\n    global_flags.add_argument(\n        '--bash-completion', action='store_true', help='Print bash completion script'\n    )\n    global_flags.add_argument(\n        '--config-paths', nargs='+', default=[], help='Paths to configuration files'\n    )\n    global_flags.add_argument(\n        '--dry-run', action='store_true', help='Run in dry-run mode'\n    )\n    global_flags.add_argument(\n        '--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Set verbosity level'\n    )\n    global_flags.add_argument(\n        '--no-color', action='store_true', help='Disable color output'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action', help='Available actions'\n    )\n    \n    # Create subparsers for each action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('source', help='Source directories to back up')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--exclude', nargs='+', help='Exclude patterns')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    info_parser = subparsers.add_parser('info', help='Display archive information')\n    info_parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    \n    # Add more subparsers as needed\n    \n    return top_level_parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='borgmatic - A tool for managing Borg backups',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    # Add global arguments\n    global_args = top_level_parser.add_argument_group('global')\n    global_args.add_argument(\n        '--version', action='version', version='borgmatic %(prog)s'\n    )\n    global_args.add_argument(\n        '--bash-completion', action='store_true', help='Print bash completion script'\n    )\n    global_args.add_argument(\n        '--config-paths', action='append', default=[], help='Paths to configuration files'\n    )\n    global_args.add_argument(\n        '--dry-run', action='store_true', help='Run in dry-run mode'\n    )\n    global_args.add_argument(\n        '--log-file', type=str, help='Path to log file'\n    )\n    global_args.add_argument(\n        '--no-color', action='store_true', help='Disable color output'\n    )\n    global_args.add_argument(\n        '--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Verbosity level (error, warning, some, lots)'\n    )\n    global_args.add_argument(\n        '--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Syslog verbosity level'\n    )\n    global_args.add_argument(\n        '--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Log file verbosity level'\n    )\n    global_args.add_argument(\n        '--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Monitoring verbosity level'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action', help='Available actions'\n    )\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument(\n        '--source-directories', action='append', help='Source directories to back up'\n    )\n    create_parser.add_argument(\n        '--exclude-from', action='append', help='File containing exclude patterns'\n    )\n    create_parser.add_argument(", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='borgmatic - Backup and restore Borg repositories',\n        prog='borgmatic'\n    )\n    \n    # Add global flags\n    global_flags = top_level_parser.add_argument_group('global')\n    global_flags.add_argument(\n        '--version', action='version', version='borgmatic %(prog)s %(version)s'\n    )\n    global_flags.add_argument(\n        '--bash-completion', action='store_true', help='Print bash completion script'\n    )\n    global_flags.add_argument(\n        '--config-paths', action='append', default=[], help='Configuration file paths'\n    )\n    global_flags.add_argument(\n        '--dry-run', action='store_true', help='Run in dry-run mode'\n    )\n    global_flags.add_argument(\n        '--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some',\n        help='Verbosity level (error, warning, some, lots)'\n    )\n    global_flags.add_argument(\n        '--no-color', action='store_true', help='Disable color output'\n    )\n    global_flags.add_argument(\n        '--log-file', help='Log to a file'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action', help='Available actions'\n    )\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument(\n        '--source-directories', action='append', help='Source directories to back up'\n    )\n    create_parser.add_argument(\n        '--exclude-from', action='append', help='File containing exclude patterns'\n    )\n    create_parser.add_argument(\n        '--exclude-caches', action='store_true', help='Exclude cache directories'\n    )\n    create_parser.add_argument(\n        '--exclude-if-present', action='append', help='Exclude files if present'\n    )\n    create_parser.add_argument(\n        '--keep-exclude-tags', action='store_true', help='Keep exclude tags'\n    )\n    create_parser.add_argument(\n        '--exclude-nodump', action='store_true', help='Exclude files marked as nodump'\n    )\n    create_parser.add_argument(\n        '--patterns-from', action='append', help='File containing patterns'\n    )\n    create_parser.add", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool for managing Borg repositories',\n        prog='borgmatic'\n    )\n    \n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    # Note: The actual implementation would depend on the specific actions defined in the codebase\n    # This is a simplified version based on typical structure\n    \n    # Example subparsers (these would be defined in the actual code)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--source', help='Source directories to back up')\n    create_parser.add_argument('--archive', help='Archive name')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Specific archive to list')\n    \n    info_parser = subparsers.add_parser('info', help='Get information about archives')\n    info_parser.add_argument('--archive', help='Specific archive to get info about')\n    \n    # Add more subparsers as needed\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='borgmatic - A tool for managing Borg backups',\n        prog='borgmatic'\n    )\n    \n    # Add global flags\n    global_flags = top_level_parser.add_argument_group('global options')\n    global_flags.add_argument(\n        '--version', \n        action='version', \n        version='borgmatic %(version)s'\n    )\n    global_flags.add_argument(\n        '--bash-completion', \n        action='store_true', \n        help='Print bash completion script'\n    )\n    global_flags.add_argument(\n        '--config-paths', \n        nargs='+', \n        default=['/etc/borgmatic/config.yaml'], \n        help='Paths to configuration files'\n    )\n    global_flags.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode'\n    )\n    global_flags.add_argument(\n        '--verbosity', \n        choices=['error', 'warning', 'some', 'lots'], \n        default='some', \n        help='Verbosity level'\n    )\n    global_flags.add_argument(\n        '--no-color', \n        action='store_true', \n        help='Disable color output'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument(\n        '--source-directories', \n        nargs='+', \n        help='Source directories to back up'\n    )\n    create_parser.add_argument(\n        '--exclude-from', \n        nargs='+', \n        help='Paths to exclude from backup'\n    )\n    create_parser.add_argument(\n        '--exclude-if-present', \n        nargs='+', \n        help='Exclude files if present'\n    )\n    create_parser.add_argument(\n        '--exclude-caches', \n        action='store_true', \n        help='Exclude cache directories'\n    )\n    create_parser.add_argument(\n        '--keep-exclude-tags', \n        action='store_true', \n        help='Keep exclude tags'\n    )\n    create_parser.add_argument(\n        '--exclude-nodump', \n        action='store_true', \n        help='Exclude files marked as nodump'"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70047, "num_retrieved": 170, "inference_time": 11.095412090420723, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70042, "num_retrieved": 168, "inference_time": 11.095412090420723, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace or a dictionary of parsed arguments\n    \n    # Since the actual implementation details are not available in the context,\n    # we return a dummy implementation that would need to be replaced with the actual parsing logic\n    \n    import argparse\n    \n    # Create the parser\n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add parser for bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', help='Repository path')\n    bootstrap_parser.add_argument('--archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add parser for generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source', help='Source configuration file')\n    generate_parser.add_argument('--destination', help='Destination configuration file')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file')\n    \n    # Add parser for validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('--show', action='store_true', help='Show configuration content')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('Global arguments')\n    global_parser.add_argument('--version', action='store_true', help='Show version')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace or a dictionary of parsed arguments\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add parser for bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', help='Repository path')\n    bootstrap_parser.add_argument('--archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add parser for generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source', help='Source configuration file')\n    generate_parser.add_argument('--destination', help='Destination configuration file')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file')\n    \n    # Add parser for validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('--show', action='store_true', help='Show configuration content')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('Global arguments')\n    global_parser.add_argument('--version', action='store_true', help='Show version')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Show bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Show fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n        return args\n    except SystemExit:\n        # If parsing fails, re-raise the exception to exit", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace or parser object\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add common arguments\n    global_parser = parser.add_argument_group('global arguments')\n    global_parser.add_argument('--version', action='store_true', help='Show version information')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-file', type=str, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, help='Format for log file')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add action-specific arguments (simplified)\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', type=str, required=True, help='Repository path')\n    bootstrap_parser.add_argument('--archive', type=str, required=True, help='Archive name')\n    bootstrap_parser.add_argument('--destination', type=str, required=True, help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, default=0, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    generate_parser", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace or parser object\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be defined in the actual implementation)\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', help='Repository path')\n    bootstrap_parser.add_argument('--archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source', help='Source configuration file')\n    generate_parser.add_argument('--destination', help='Destination configuration file')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file')\n    \n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('--show', action='store_true', help='Show configuration content')\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('Global arguments')\n    global_parser.add_argument('--version', action='store_true', help='Show version')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Show bash completion')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Show fish completion')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n    except SystemExit:\n        # If parsing fails, re-raise the exception\n        raise\n        \n    return args", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a stub implementation as the full parser definition is not provided in the context\n    # In a real implementation, this would parse the arguments using argparse and return the parsed namespace\n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true')\n    global_parser.add_argument('--fish-completion', action='store_true')\n    global_parser.add_argument('--dry-run', action='store_true')\n    global_parser.add_argument('--log-file', type=str)\n    global_parser.add_argument('--log-file-format', type=str)\n    global_parser.add_argument('--log-json', action='store_true')\n    global_parser.add_argument('--verbosity', type=int, default=0)\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0)\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0)\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0)\n    global_parser.add_argument('--no-color', action='store_true')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', type=str, required=True)\n    bootstrap_parser.add_argument('--archive', type=str, required=True)\n    bootstrap_parser.add_argument('--destination', type=str, required=True)\n    bootstrap_parser.add_argument('--strip-components', type=int, default=0)\n    bootstrap_parser.add_argument('--progress', action='store_true')\n    \n    # Generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('--source', type=str)\n    generate_parser.add_argument('--destination', type=str, required=True)\n    generate_parser.add_argument('--overwrite', action='store_true')\n    \n    # Validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('--show', action='store_true')"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70335, "num_retrieved": 180, "inference_time": 11.095412090420723, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71031, "num_retrieved": 205, "inference_time": 11.095412090420723, "generate_results": ["def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                    if [[ \"$installed_script\" != \"$(cat \"$this_filename\" 2> /dev/null)\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            '$this_filename',\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion() {{\n    local cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    local all_args=( $(commandline -poc) )\n    if [[ ${#all_args[@]} -lt 2 ]]; then\n        return 1\n    fi\n    for arg in \"${all_args[@]}\"; do\n        if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n            return 0\n        fi\n    done\n    return 1\n}}'''\n            for action_name, action_parser in action_parsers.choices.items()\n        )\n        + ('\\n# global flags',)\n        + tuple(\n            f'''_complete_borgmatic_global() {{\n    local cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    local all_args=( $(commandline", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2>/dev/null)\n                if [[ -f \"$this_filename\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                    if [[ \"$installed_script\" != \"$(cat \"$this_filename\" 2>/dev/null)\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            '$this_filename',\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''complete -F _borgmatic_action_parser_completion -o bashdefault -o default -o nospace borgmatic -n \"$action_parser_condition\" -n \"$exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for action_name, action_parser in action_parsers.choices.items()\n        )\n        + ('\\n# global flags',)\n        + tuple(\n            f'''complete -F _borgmatic_global_option_completion -o bashdefault -o default -o nospace borgmatic -n \"$exact_option_condition\" -a '{' '.join(action.option_strings)}' -d {shlex.quote(action.help)}{exact_options_completion(action)}'''\n            for action in global_plus_action_parser._actions\n            if len(action.option_strings", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                    if [[ \"$installed_script\" != \"$(cat \"$this_filename\" 2> /dev/null)\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            '$this_filename',\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n            '''\n        )\n        + ('\\n# action parser completions',)\n        + tuple(\n            f'''complete -F _borgmatic_action_parser_completion -o default -o bashdefault -o nospace borgmatic -n \"$action_parser_condition\" -n \"$exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for action_name, action_parser in action_parsers.choices.items()\n        )\n        + ('\\n# global flags',)\n        + tuple(\n            f'''complete -F _borgmatic_global_option_completion -o default -o bashdefault -o nospace borgmatic -n \"$exact_option_condition\" -a '{' '.join(action.option_strings)}' -d {shlex.quote(action.help)}{exact_options_completion(action)}'''\n            for action in global_plus_action_parser._actions\n            if len(action", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                    if [[ \"$installed_script\" != \"$(cat \"$this_filename\" 2> /dev/null)\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            '$this_filename',\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion() {{\n    local cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    local all_args=( $(commandline -poc) )\n    if [[ ${#all_args[@]} -lt 2 ]]; then\n        return 1\n    fi\n    for arg in \"${all_args[@]}\"; do\n        if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n            return 0\n        fi\n    done\n    return 1\n}}'''\n            for action_name, action_parser in action_parsers.choices.items()\n        )\n        + ('\\n# global flags',)\n        + tuple(\n            f'''_complete_borgmatic_global() {{\n    local cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    local all_args=( $(commandline", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _borgmatic_check_version() {{\n                local this_filename=$(basename \"$0\")\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ -f \"$this_filename\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                    if [[ \"$installed_script\" != \"$(cat \"$this_filename\" 2> /dev/null)\" ]]; then\n                        echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                            'bash',\n                            'borgmatic --bash-completion | sudo tee $this_filename',\n                            '$this_filename',\n                        )}\"\n                    fi\n                fi\n            }}\n            _borgmatic_check_version\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''_init_completion() {{\n    local cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    local all_args=( $(commandline -poc) )\n    if [[ ${#all_args[@]} -lt 2 ]]; then\n        return 1\n    fi\n    for arg in \"${all_args[@]}\"; do\n        if [[ \"$arg\" == \"${all_args[-1]}\" ]]; then\n            return 0\n        fi\n    done\n    return 1\n}}'''\n            for action_name, action_parser in action_parsers.choices.items()\n        )\n        + ('\\n# global flags',)\n        + tuple(\n            f'''_complete_borgmatic_global() {{\n    local cur=\"${COMP_WORDS[COMP_CWORD]}\"\n    local all_args=( $(commandline"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69811, "num_retrieved": 160, "inference_time": 11.095412090420723, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, help='Format for log file')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', type=str, help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', type=str, help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, help='Format for log file')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    bootstrap_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    \n    # Add", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Log JSON output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, help='Format for log file')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', type=str, default=None, help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', type=str, help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, default=0, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Log output in JSON format')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, help='Format for log file')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Log JSON output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Log file path')\n    global_parser.add_argument('--log-file-format', type=str, help='Log file format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', type=str, help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', type=str, help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69544, "num_retrieved": 151, "inference_time": 11.095412090420723, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Try to parse arguments for each subparser in order\n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, break out of the loop\n            break\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Try to parse arguments using the subparser\n        try:\n            # Parse the arguments using the subparser\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, break out of the loop\n            break\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    # Try to parse arguments for each subparser in the order they appear in subparsers\n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, don't add to arguments and continue\n            pass\n    \n    return (arguments, remaining_arguments)"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69823, "num_retrieved": 162, "inference_time": 11.095412090420723, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    # Create the top-level parser\n    global_parser = argparse.ArgumentParser(\n        description='borgmatic - A Borg backup tool with configuration and automation',\n        prog='borgmatic',\n        add_help=False,\n    )\n    \n    # Add global arguments to the parser\n    global_parser.add_argument(\n        '--version',\n        action='version',\n        version='borgmatic ' + importlib_metadata.version('borgmatic'),\n    )\n    \n    global_parser.add_argument(\n        '--bash-completion',\n        action='store_true',\n        help='Print bash completion script',\n    )\n    \n    global_parser.add_argument(\n        '--fish-completion',\n        action='store_true',\n        help='Print fish completion script',\n    )\n    \n    global_parser.add_argument(\n        '--config-paths',\n        nargs='+',\n        default=[],\n        help='Paths to configuration files',\n    )\n    \n    global_parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Run in dry run mode',\n    )\n    \n    global_parser.add_argument(\n        '--verbosity',\n        choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set verbosity level',\n    )\n    \n    global_parser.add_argument(\n        '--log-json',\n        action='store_true',\n        help='Output log messages in JSON format',\n    )\n    \n    # Create subparsers for different actions\n    action_parsers = argparse._SubParsersAction(\n        parser=global_parser,\n        title='actions',\n        description='Available actions',\n    )\n    \n    # Add subparsers for each action\n    # Note: This is a simplified implementation - actual implementation would include\n    # all the specific action parsers from borgmatic.commands.arguments\n    subparsers = {\n        'config': borgmatic.commands.arguments.make_config_parser(),\n        'create': borgmatic.commands.arguments.make_create_parser(),\n        'extract': borgmatic.commands.arguments.make_extract_parser(),\n        'info': borgmatic.commands.arguments.make_info_parser(),\n        'list': borgmatic.commands.arguments.make_list_parser(),\n        'mount': borgmatic.commands.arguments.make_mount_parser(),\n        'prune': borgmatic.commands.arguments.make_prune_parser(),\n        'restore': borgmatic.commands.arguments.make_restore_parser(),\n        'rcreate':", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    # Create the top-level parser\n    global_parser = argparse.ArgumentParser(\n        description='borgmatic - A Borg backup tool with configuration management and hooks',\n        prog='borgmatic',\n        add_help=False,\n    )\n    \n    # Add global arguments to the parser\n    global_parser.add_argument(\n        '--version',\n        action='version',\n        version='borgmatic ' + importlib_metadata.version('borgmatic'),\n    )\n    \n    global_parser.add_argument(\n        '--bash-completion',\n        action='store_true',\n        help='Print bash completion script',\n    )\n    \n    global_parser.add_argument(\n        '--fish-completion',\n        action='store_true',\n        help='Print fish completion script',\n    )\n    \n    global_parser.add_argument(\n        '--config-paths',\n        nargs='+',\n        default=[],\n        help='Paths to configuration files',\n    )\n    \n    global_parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Run in dry-run mode',\n    )\n    \n    global_parser.add_argument(\n        '--verbosity',\n        choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Verbosity level',\n    )\n    \n    global_parser.add_argument(\n        '--log-json',\n        action='store_true',\n        help='Log output in JSON format',\n    )\n    \n    global_parser.add_argument(\n        '--no-color',\n        action='store_true',\n        help='Disable color output',\n    )\n    \n    # Create subparsers for different actions\n    action_parsers = argparse._SubParsersAction(\n        parser=global_parser,\n        title='actions',\n        description='Available actions',\n    )\n    \n    # Add subparsers for each action\n    # Note: This is a simplified implementation - actual implementation would depend on the specific actions\n    # and their arguments defined in borgmatic.commands.arguments\n    \n    # Example subparsers (these would be populated with actual action parsers)\n    # The actual implementation would use the borgmatic.commands.arguments module to create the parsers\n    \n    # Create a parser for the \"create\" action\n    create_parser = argparse.ArgumentParser(\n        prog='create',\n        description='Create a backup archive',\n    )\n    create_parser.add_argument(\n        '--archive',\n        help='Name of the archive to", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    # Create the top-level parser\n    global_parser = argparse.ArgumentParser(\n        description='borgmatic - A Borg backup tool with configuration and management capabilities',\n        add_help=False,\n    )\n    \n    # Add global arguments to the parser\n    global_parser.add_argument(\n        '--version',\n        action='version',\n        version='borgmatic ' + importlib_metadata.version('borgmatic'),\n    )\n    \n    global_parser.add_argument(\n        '--bash-completion',\n        action='store_true',\n        help='Print bash completion script',\n    )\n    \n    global_parser.add_argument(\n        '--fish-completion',\n        action='store_true',\n        help='Print fish completion script',\n    )\n    \n    global_parser.add_argument(\n        '--config-paths',\n        nargs='+',\n        default=[],\n        help='Paths to configuration files',\n    )\n    \n    global_parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Run in dry-run mode',\n    )\n    \n    global_parser.add_argument(\n        '--verbosity',\n        choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set verbosity level',\n    )\n    \n    global_parser.add_argument(\n        '--log-json',\n        action='store_true',\n        help='Output logs in JSON format',\n    )\n    \n    global_parser.add_argument(\n        '--log-file',\n        help='Path to log file',\n    )\n    \n    global_parser.add_argument(\n        '--log-file-format',\n        help='Format for log file output',\n    )\n    \n    # Create subparsers for different actions\n    action_parsers = argparse._SubParsersAction(\n        parser=global_parser,\n        title='actions',\n        description='Available actions',\n    )\n    \n    # Add subparsers for each action\n    for action_name, action_parser in borgmatic.commands.arguments.ACTIONS.items():\n        action_parser = argparse.ArgumentParser(\n            prog=action_name,\n            description=action_parser.description,\n            add_help=False,\n        )\n        \n        # Add action-specific arguments\n        for action in action_parser._actions:\n            if action.option_strings:\n                action_parser.add_argument(\n                    *action.option_strings,\n                    help=action.help,\n                    **action.__dict__\n                )\n        \n        action_parsers.add_parser(action_name, parents=[", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    # Create the top-level parser\n    global_parser = argparse.ArgumentParser(\n        description='borgmatic - A Borg backup tool with configuration and automation',\n        add_help=False,\n    )\n    \n    # Add global arguments to the parser\n    global_parser.add_argument(\n        '--version',\n        action='version',\n        version='borgmatic ' + importlib_metadata.version('borgmatic'),\n    )\n    \n    global_parser.add_argument(\n        '--bash-completion',\n        action='store_true',\n        help='Print bash completion script',\n    )\n    \n    global_parser.add_argument(\n        '--fish-completion',\n        action='store_true',\n        help='Print fish completion script',\n    )\n    \n    global_parser.add_argument(\n        '--config-paths',\n        nargs='+',\n        default=[],\n        help='Paths to configuration files',\n    )\n    \n    global_parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Run in dry run mode',\n    )\n    \n    global_parser.add_argument(\n        '--verbosity',\n        choices=['error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set verbosity level',\n    )\n    \n    global_parser.add_argument(\n        '--log-json',\n        action='store_true',\n        help='Output logs in JSON format',\n    )\n    \n    global_parser.add_argument(\n        '--no-color',\n        action='store_true',\n        help='Disable color output',\n    )\n    \n    # Create subparsers for different actions\n    action_parsers = argparse._SubParsersAction(\n        parser=global_parser,\n        title='actions',\n        description='Available actions',\n    )\n    \n    # Add subparsers for each action\n    # Note: This is a simplified version - actual implementation would depend on the specific actions\n    # and their arguments as defined in borgmatic.commands.arguments\n    \n    # Example subparsers (these would be populated with actual action parsers)\n    # The actual implementation would use the borgmatic.commands.arguments module to create these\n    \n    # Create a parser for the 'create' action\n    create_parser = argparse.ArgumentParser(\n        prog='create',\n        description='Create a backup archive',\n        add_help=False,\n    )\n    create_parser.add_argument(\n        '--archive',\n        help='Name of the archive to create',\n    )", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    # Create the top-level parser\n    global_parser = argparse.ArgumentParser(\n        description='borgmatic - A tool for managing Borg repositories',\n        prog='borgmatic',\n        add_help=False,\n    )\n    \n    # Add global arguments to the top-level parser\n    global_parser.add_argument(\n        '--version',\n        action='version',\n        version='borgmatic ' + importlib_metadata.version('borgmatic'),\n        help='Show version information and exit',\n    )\n    \n    global_parser.add_argument(\n        '--bash-completion',\n        action='store_true',\n        help='Print bash completion script and exit',\n    )\n    \n    global_parser.add_argument(\n        '--fish-completion',\n        action='store_true',\n        help='Print fish completion script and exit',\n    )\n    \n    # Create subparsers for different actions\n    action_parsers = argparse._SubParsersAction(\n        parser=global_parser,\n        title='actions',\n        description='Available actions',\n    )\n    \n    # Add each action parser\n    for action_name, action_parser in borgmatic.commands.arguments.ACTIONS.items():\n        action_parser = argparse.ArgumentParser(\n            prog=f'borgmatic {action_name}',\n            description=action_parser.description,\n            add_help=False,\n        )\n        \n        # Add action-specific arguments\n        for action in action_parser._actions:\n            if action.option_strings:\n                action_parser.add_argument(*action.option_strings, **action.__dict__)\n        \n        action_parsers.add_parser(action_name, parents=[global_parser], help=action_parser.description)\n    \n    # Create the global_plus_action_parser that combines global arguments with action-specific arguments\n    global_plus_action_parser = argparse.ArgumentParser(\n        prog='borgmatic',\n        description='borgmatic - A tool for managing Borg repositories',\n        add_help=False,\n    )\n    \n    # Add global arguments to the global_plus_action_parser\n    global_plus_action_parser.add_argument(\n        '--version',\n        action='version',\n        version='borgmatic ' + importlib_metadata.version('borgmatic'),\n        help='Show version information and exit',\n    )\n    \n    global_plus_action_parser.add_argument(\n        '--bash-completion',\n        action='store_true',\n        help='Print bash completion script and exit',\n    )\n    \n    global_plus_action_parser.add_argument(\n        '--fish-completion',\n        action"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68296, "num_retrieved": 93, "inference_time": 11.095412090420723, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    # Handle negative max_results by setting to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # Yield the first page\n    if len(response) <= max_results:\n        yield response\n    else:\n        # Split the response into pages\n        for i in range(0, len(response), max_results):\n            yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    # Handle negative max_results by setting to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # Yield the first page\n    if len(response) <= max_results:\n        yield response\n    else:\n        # Split the response into pages\n        for i in range(0, len(response), max_results):\n            yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or max_results <= 0:\n        return\n    \n    # Handle negative max_results by setting to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # Yield the first page\n    if len(response) <= max_results:\n        yield response\n    else:\n        # Split the response into pages\n        for i in range(0, len(response), max_results):\n            yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or max_results <= 0:\n        return\n    \n    # Handle negative max_results by setting to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # Yield the first page\n    if len(response) <= max_results:\n        yield response\n    else:\n        # Split response into pages\n        for i in range(0, len(response), max_results):\n            yield response[i:i + max_results]", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n    \n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response or not max_results:\n        return\n    \n    # Handle negative max_results as per the context\n    if max_results < 0:\n        max_results = 1000\n    \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n    \n    # Yield the response in chunks of max_results\n    for i in range(0, len(response), max_results):\n        yield response[i:i + max_results]"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 20441, "num_retrieved": 146, "inference_time": 11.095412090420723, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit\n    if 'K' in size:\n        # Extract the number part\n        num_part = size.replace('K', '').strip()\n        return int(num_part) * 1024\n    elif 'M' in size:\n        # Extract the number part\n        num_part = size.replace('M', '').strip()\n        return int(num_part) * 1024 * 1024\n    elif 'G' in size:\n        # Extract the number part\n        num_part = size.replace('G', '').strip()\n        return int(num_part) * 1024 * 1024 * 1024\n    else:\n        # No unit, assume it's in bytes\n        return int(size)"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71531, "num_retrieved": 235, "inference_time": 11.095412090420723, "generate_results": ["def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 23160, "num_retrieved": 60, "inference_time": 11.095412090420723, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70126, "num_retrieved": 153, "inference_time": 11.095412090420723, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    \n    # Check for common representations of None in strings\n    return val.lower() in ['none', 'null', 'nil', 'n/a', 'na', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        # Check for common string representations of None\n        return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'undefined', '')\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        # Check for common string representations of None\n        return val.lower() in ['none', 'null', 'nil', 'n/a', '']\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        # Check for common string representations of None\n        return val.lower() in ('none', 'null', 'nil', '', 'undefined', '0')\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    \n    # Check for common representations of None in strings\n    return val.strip() in ['none', 'None', 'NULL', 'null', '']"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70335, "num_retrieved": 180, "inference_time": 11.095412090420723, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\n    string.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69427, "num_retrieved": 138, "inference_time": 11.095412090420723, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolute path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if absolute_path in processed:\n        if verbose:\n            print(\"Path {} has already been processed.\".format(absolute_path))\n        return True\n    \n    # Add to processed set\n    processed.add(absolute_path)\n    \n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Expand the path name to an absolute path\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if the path is already in the processed set\n    if absolute_path in processed:\n        if verbose:\n            print(\"Path {} has already been processed.\".format(absolute_path))\n        return True\n    \n    # Add the path to the processed set\n    processed.add(absolute_path)\n    \n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Expand the path name to an absolute path\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if the path is already in the processed set\n    if absolute_path in processed:\n        if verbose:\n            print(\"Path {} has already been processed.\".format(absolute_path))\n        return True\n    \n    # Add the path to the processed set\n    processed.add(absolute_path)\n    \n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Expand the path name to an absolute path\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if the path is already in the processed set\n    if absolute_path in processed:\n        if verbose:\n            print(\"Path {} has already been processed.\".format(absolute_path))\n        return True\n    \n    # If not already processed, add it to the set\n    processed.add(absolute_path)\n    \n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Expand the path name to an absolute path\n    absolute_path = os.path.abspath(path_name)\n    \n    # Check if the path is already in the processed set\n    if absolute_path in processed:\n        if verbose:\n            print(\"Path {} has already been processed.\".format(absolute_path))\n        return True\n    \n    # Add the path to the processed set\n    processed.add(absolute_path)\n    \n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 74368, "num_retrieved": 326, "inference_time": 7.865174949169159, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of (prev, curr, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and the two adjacent vertices\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of (prev, curr, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of the three points\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 1167, "num_retrieved": 5, "inference_time": 7.865174949169159, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    \n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    \n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73299, "num_retrieved": 259, "inference_time": 7.865174949169159, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"Replace value from flows to given register number.\n\n    'register_value' key in dictionary will be replaced by register number\n    given by 'register_number'.\n\n    :param flow_params: Dictionary containing defined flows\n    :param register_number: The number of register where value will be stored\n    :param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 37299, "num_retrieved": 244, "inference_time": 7.865174949169159, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72023, "num_retrieved": 266, "inference_time": 7.865174949169159, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 1161, "num_retrieved": 5, "inference_time": 7.865174949169159, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 20613, "num_retrieved": 149, "inference_time": 7.865174949169159, "generate_results": ["import requests\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"-xzf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"-xzf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"-xzf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"-xzf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_call(['tar', '-xzf', str(archive_path), '-C', str(destination_path)])\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72370, "num_retrieved": 225, "inference_time": 7.865174949169159, "generate_results": ["import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system().lower() == \"darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system().lower() == \"darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system().lower() == \"darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system().lower() == \"darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system().lower() == \"darwin\""]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70113, "num_retrieved": 162, "inference_time": 7.865174949169159, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ensure it matches the pattern\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add anchors to make it match the full path\n        regex_key = '^' + regex_key + '$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ensure it matches the pattern\n        regex_key = re.compile(f\"^{key.replace('*', '.*')}$\")\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add the regex pattern with parentheses to capture the matched group\n        regex_key = re.sub(r'^\\.', r'^(.*)', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = '^' + regex_key + '$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Escape special regex characters in the pattern\n        regex_key = re.escape(regex_key)\n        # Replace the original key with the regex pattern\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .* to match any sequence of characters\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        # Add the regex pattern to the result\n        result[re.compile(regex_key)] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72482, "num_retrieved": 250, "inference_time": 7.865174949169159, "generate_results": ["def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove quotes from the beginning and end of the string\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove quotes from the beginning and end of the string\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove quotes from the beginning and end of the string\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72238, "num_retrieved": 219, "inference_time": 7.865174949169159, "generate_results": ["import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules (no quotes, spaces separate)\n    if platform == 0:\n        # Split on spaces, but preserve quoted strings\n        # In Windows CMD, spaces are separators unless inside quotes\n        # We use a regex to handle quoted strings\n        # This is a simplified version that handles basic cases\n        # For full compatibility, we'd need to parse the full command\n        # But for simplicity, we'll use a basic split\n        return shlex.split(s)\n    \n    # POSIX (Linux, macOS) uses shlex.split\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # POSIX platform (Linux, macOS) - use shell expansion\n    if platform == 1:\n        # Use regex to split on whitespace, but preserve quoted strings\n        # This handles quotes, escaped characters, and complex shell syntax\n        # Pattern matches sequences of non-whitespace, with quoted strings\n        # and escaped characters\n        pattern = r'([^\"\\s]+|\"[^\"]*\")'\n        parts = re.findall(pattern, s)\n        # Remove empty strings and fix quoted parts\n        result = []\n        for part in parts:\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            else:\n                result.append(part)\n        return result\n    \n    # Windows platform (CMD) - use shell expansion\n    elif platform == 0:\n        # For Windows CMD, we need to handle the fact that it doesn't\n        # support the same shell expansion as POSIX\n        # We'll use a simple split on whitespace, but preserve quoted strings\n        # This is a simplified version that handles basic quoting\n        pattern = r'([^\"\\s]+|\"[^\"]*\")'\n        parts = re.findall(pattern, s)\n        result = []\n        for part in parts:\n            if part.startswith('\"') and part.endswith('\"'):\n                result.append(part[1:-1])\n            else:\n                result.append(part)\n        return result\n    \n    # Default to POSIX behavior if platform is invalid\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules (no quotes, no escaping)\n    if platform == 0:\n        # Split on spaces, but preserve quoted strings\n        # Windows CMD doesn't handle complex quoting like POSIX\n        # Simple split on spaces, treating quoted strings as single tokens\n        # This is a simplified version for Windows CMD\n        return re.split(r'(\\s+)', s)\n    \n    # POSIX (Linux, macOS) uses shlex.split behavior\n    # This handles quotes, escaping, and complex command line parsing\n    if platform == 1:\n        # Use shlex.split for POSIX compatibility\n        return shlex.split(s)\n    \n    # Default to POSIX behavior if platform is unknown\n    return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # POSIX platform (Linux, macOS) - use shell-like splitting\n    if platform == 1:\n        # Use regex to handle quoted strings and escaped characters\n        # This pattern matches quoted strings and unquoted tokens\n        pattern = r'([^\"\\\\]|\\\\.)+|\"([^\"]*)\"'\n        tokens = []\n        match = re.finditer(pattern, s)\n        for m in match:\n            if m.group(1):\n                # Unquoted part - split on whitespace\n                unquoted = m.group(1).strip()\n                if unquoted:\n                    tokens.extend(unquoted.split())\n            else:\n                # Quoted part - keep as is\n                tokens.append(m.group(2))\n        return tokens\n    \n    # Windows platform (CMD) - use shell-like splitting\n    elif platform == 0:\n        # Windows CMD uses different quoting and escaping\n        # Split on whitespace, but preserve quoted strings\n        tokens = []\n        # Use a simple approach: split on whitespace and handle quoted strings\n        # This is a simplified version that works for basic cases\n        parts = re.split(r'(\\s+)', s)\n        current = \"\"\n        for part in parts:\n            if part.startswith('\"') and part.endswith('\"'):\n                # Quoted string - keep as is\n                tokens.append(part)\n            elif part.startswith('\"'):\n                # Start of quoted string\n                current = part\n            elif part.endswith('\"'):\n                # End of quoted string\n                current += part\n                tokens.append(current)\n                current = \"\"\n            elif part.strip():\n                # Unquoted part - split on whitespace\n                if current:\n                    current += part\n                else:\n                    tokens.extend(part.split())\n        # Handle any remaining current string\n        if current:\n            tokens.append(current)\n        return tokens\n    \n    # Default to shlex.split for other platforms\n    else:\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform_val = 1 if platform.system() in ['Linux', 'Darwin'] else 0\n    elif platform == 1:\n        platform_val = 1\n    elif platform == 0:\n        platform_val = 0\n    else:\n        raise ValueError(\"Invalid platform value\")\n    \n    # Pattern to match shell-like syntax: quoted strings, escaped characters, and word boundaries\n    # This pattern handles quotes, backslashes, and spaces\n    pattern = r'((?:[^\"\\\\]|\\\\.)*\"|(?:[^\"\\\\]|\\\\.)*|[^\"\\\\ ]+)(?:\\s+|$)'\n    \n    # Use regex to split the string, handling quoted strings and escaped characters\n    parts = re.findall(pattern, s)\n    \n    # Remove empty strings and strip whitespace from each part\n    result = [part.strip() for part in parts if part.strip()]\n    \n    # For Windows, we need to handle the fact that some commands might have different syntax\n    # But since we're using a regex that handles quotes and escaping, we can just return the result\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72855, "num_retrieved": 241, "inference_time": 7.865174949169159, "generate_results": ["import os\nimport tarfile\nfrom pathlib import Path\nfrom typing import Optional\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_dir / \"extracted\"\n    extract_dir.mkdir(exist_ok=True)\n    \n    # Open the archive and extract it\n    with tarfile.open(archive_path, 'r') as tar:\n        tar.extractall(path=extract_dir)\n    \n    # Determine the filename to use for the repository URL\n    if filename:\n        return f\"file://{extract_dir}/{filename}\"\n    else:\n        # Use the first file in the extracted directory as the filename\n        files = list(extract_dir.glob(\"*\"))\n        if files:\n            return f\"file://{extract_dir}/{files[0].name}\"\n        else:\n            return f\"file://{extract_dir}\"", "import os\nimport tarfile\nfrom pathlib import Path\nfrom typing import Optional\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_dir / \"extracted\"\n    extract_dir.mkdir(exist_ok=True)\n    \n    # Open the archive and extract it\n    with tarfile.open(archive_path, 'r') as tar:\n        tar.extractall(path=extract_dir)\n    \n    # Determine the filename to use for the repository URL\n    if filename:\n        return f\"file://{extract_dir}/{filename}\"\n    else:\n        # Use the first file in the extracted directory as the filename\n        files = list(extract_dir.glob(\"*\"))\n        if files:\n            return f\"file://{extract_dir}/{files[0].name}\"\n        else:\n            return f\"file://{extract_dir}\"", "import os\nimport tempfile\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # Extract the archive to a temporary directory\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(exist_ok=True)\n    \n    # Create a temporary path for the extracted files\n    extract_path = tmp_dir / \"extracted\"\n    extract_path.mkdir(exist_ok=True)\n    \n    # Determine the filename to use if not provided\n    if filename is None:\n        # Extract the filename from the archive path\n        filename = os.path.basename(archive_path).split('.')[0]\n    \n    # Use the context from the retrieved context to extract the archive\n    # This is a simplified version based on the context provided\n    with tempfile.NamedTemporaryFile(delete=False, suffix='.tar.gz') as tmp_file:\n        tmp_file.write(Path(archive_path).read_bytes())\n        tmp_file_path = tmp_file.name\n    \n    try:\n        # Extract the archive to the temporary directory\n        with tarfile.open(tmp_file_path) as tar:\n            tar.extractall(path=extract_path)\n        \n        # Construct the repository URL from the extracted files\n        # This is a placeholder - the actual URL construction would depend on the archive structure\n        # and the specific repository format\n        repo_url = f\"file://{extract_path}\"\n        \n        return repo_url\n        \n    finally:\n        # Clean up the temporary file\n        if os.path.exists(tmp_file_path):\n            os.unlink(tmp_file_path)", "import subprocess\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_dir / \"extracted\"\n    extract_dir.mkdir(exist_ok=True)\n    \n    # Determine the filename if not provided\n    if filename is None:\n        filename = os.path.basename(archive_path).split('.')[0]\n    \n    # Use tar to extract the archive\n    try:\n        subprocess.run(\n            ['tar', '-xzf', archive_path, '-C', str(extract_dir)],\n            check=True\n        )\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(f\"Failed to extract archive: {e}\")\n    \n    # Construct the repo URL based on the extracted directory\n    # This is a placeholder - the actual URL construction would depend on the archive content\n    # and the expected structure of the extracted files\n    base_url = f\"file://{extract_dir}\"\n    \n    return base_url", "import os\nimport tarfile\nfrom pathlib import Path\nfrom typing import Optional\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # Ensure tmp_path is a Path object\n    tmp_dir = Path(tmp_path)\n    \n    # Extract the archive to a temporary directory\n    extract_dir = tmp_dir / \"extracted\"\n    extract_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Open the archive and extract it\n    with tarfile.open(archive_path, 'r') as tar:\n        tar.extractall(path=extract_dir)\n    \n    # Determine the filename to use for the repository URL\n    if filename:\n        # Use the provided filename\n        repo_url = f\"file://{extract_dir}/{filename}\"\n    else:\n        # Use the first file in the extracted directory\n        files = list(extract_dir.glob(\"*\"))\n        if not files:\n            raise FileNotFoundError(f\"No files found in extracted directory: {extract_dir}\")\n        repo_url = f\"file://{extract_dir}/{files[0].name}\"\n    \n    return repo_url"]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 30634, "num_retrieved": 110, "inference_time": 7.865174949169159, "generate_results": ["import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=False)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter only the files that are in the ignored list\n        filtered_files = []\n        for file in ignored_files:\n            if file in ignored:\n                filtered_files.append(file)\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Return as a comma-separated string\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError:\n        return \"\"\n    except FileNotFoundError:\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=False)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter only the files that are in the ignored list\n        filtered_files = []\n        for file in ignored_files:\n            if file in ignored:\n                filtered_files.append(file)\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Return as a comma-separated string\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError:\n        return \"\"\n    except FileNotFoundError:\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=False)\n        if result.returncode != 0:\n            return \"\"\n        \n        # Split the output into lines and filter out empty lines\n        ignored_files = [line.strip() for line in result.stdout.splitlines() if line.strip()]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return \",\".join(ignored_files)\n    except FileNotFoundError:\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=False)\n        if result.returncode != 0:\n            return \"\"\n        \n        # Split the output into lines and filter out empty lines\n        ignored_files = [line.strip() for line in result.stdout.splitlines() if line.strip()]\n        \n        # Sort the files alphabetically\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return \",\".join(ignored_files)\n    except FileNotFoundError:\n        return \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 3282, "num_retrieved": 30, "inference_time": 7.865174949169159, "generate_results": ["def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    supported_extensions = ['.docopt', '.py']\n    for ext in supported_extensions:\n        if filename.endswith(ext):\n            return True\n    return False", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    supported_extensions = ['.docopt', '.py', '.ini', '.json']\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in supported_extensions)"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69799, "num_retrieved": 161, "inference_time": 7.865174949169159, "generate_results": ["import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (case-insensitive)\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Map units to timedelta components\n    unit_map = {\n        'second': timedelta(seconds=number),\n        'seconds': timedelta(seconds=number),\n        'minute': timedelta(minutes=number),\n        'minutes': timedelta(minutes=number),\n        'hour': timedelta(hours=number),\n        'hours': timedelta(hours=number),\n        'day': timedelta(days=number),\n        'days': timedelta(days=number),\n        'week': timedelta(weeks=number),\n        'weeks': timedelta(weeks=number),\n        'month': None,  # Months are not directly supported in timedelta\n        'months': None,  # Months are not directly supported in timedelta\n        'year': None,  # Years are not directly supported in timedelta\n        'years': None,  # Years are not directly supported in timedelta\n    }\n    \n    # Handle months and years - they are not directly supported in timedelta\n    if unit in ['month', 'months', 'year', 'years']:\n        raise ValueError(f\"Frequency unit '{unit}' is not supported. Use seconds, minutes, hours, days, or weeks.\")\n    \n    return unit_map[unit]", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (with optional whitespace)\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Map units to timedelta components\n    unit_map = {\n        'second': timedelta(seconds=number),\n        'seconds': timedelta(seconds=number),\n        'minute': timedelta(minutes=number),\n        'minutes': timedelta(minutes=number),\n        'hour': timedelta(hours=number),\n        'hours': timedelta(hours=number),\n        'day': timedelta(days=number),\n        'days': timedelta(days=number),\n        'week': timedelta(weeks=number),\n        'weeks': timedelta(weeks=number),\n        'month': timedelta(days=number * 30),  # Approximate\n        'months': timedelta(days=number * 30),  # Approximate\n        'year': timedelta(days=number * 365),  # Approximate\n        'years': timedelta(days=number * 365),  # Approximate\n    }\n    \n    if unit not in unit_map:\n        raise ValueError(f\"Unknown unit '{unit}' in frequency '{frequency}'\")\n    \n    return unit_map[unit]", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (with optional whitespace)\n    match = re.match(r'^(\\d+)\\s*(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f\"Unknown time unit '{unit}' in frequency '{frequency}'\")", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (with optional whitespace)\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Map units to timedelta components\n    unit_map = {\n        'second': timedelta(seconds=number),\n        'seconds': timedelta(seconds=number),\n        'minute': timedelta(minutes=number),\n        'minutes': timedelta(minutes=number),\n        'hour': timedelta(hours=number),\n        'hours': timedelta(hours=number),\n        'day': timedelta(days=number),\n        'days': timedelta(days=number),\n        'week': timedelta(weeks=number),\n        'weeks': timedelta(weeks=number),\n        'month': None,  # Not supported - months vary in length\n        'months': None,  # Not supported - months vary in length\n        'year': None,  # Not supported - years vary in length\n        'years': None,  # Not supported - years vary in length\n    }\n    \n    # Check if the unit is supported\n    if unit not in unit_map:\n        raise ValueError(f\"Unsupported unit '{unit}' in frequency '{frequency}'\")\n    \n    # For months, years - we don't support them as they vary in length\n    if unit in ['month', 'months', 'year', 'years']:\n        raise ValueError(f\"Frequency units '{unit}' are not supported\")\n    \n    return unit_map[unit]", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    \"\"\"\n    Given a frequency string with a number and a unit of time, return a corresponding\n    datetime.timedelta instance or None if the frequency is None or \"always\".\n\n    For instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\n    Raise ValueError if the given frequency cannot be parsed.\n    \"\"\"\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match number and unit (with optional whitespace)\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Cannot parse frequency string: {frequency}\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Map units to timedelta components\n    unit_map = {\n        'second': timedelta(seconds=number),\n        'seconds': timedelta(seconds=number),\n        'minute': timedelta(minutes=number),\n        'minutes': timedelta(minutes=number),\n        'hour': timedelta(hours=number),\n        'hours': timedelta(hours=number),\n        'day': timedelta(days=number),\n        'days': timedelta(days=number),\n        'week': timedelta(weeks=number),\n        'weeks': timedelta(weeks=number),\n        'month': timedelta(days=number * 30),  # Approximate\n        'months': timedelta(days=number * 30),  # Approximate\n        'year': timedelta(days=number * 365),  # Approximate\n        'years': timedelta(days=number * 365),  # Approximate\n    }\n    \n    if unit not in unit_map:\n        raise ValueError(f\"Unknown unit in frequency: {unit}\")\n    \n    return unit_map[unit]"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72070, "num_retrieved": 214, "inference_time": 7.865174949169159, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == \"127.0.0.1\"\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP address is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP address is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == \"127.0.0.1\"\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69834, "num_retrieved": 163, "inference_time": 7.911143198609352, "generate_results": ["import re\nfrom os.path import sep\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path.startswith(sep) else f'sh:**/*{path}*/**' \n        if path != path.replace('.', sep) and path.count('.') > 0 else path\n        for path in find_paths\n    ]", "import re\nfrom os.path import sep\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path.startswith(sep) else f'sh:**/*{path}*/**' \n        if path != path.replace('.', sep) and path != path.replace('/', sep) \n        else path \n        for path in find_paths\n    ]", "import re\nfrom os import path\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path.basename(pattern)}*/**' if pattern.startswith('/') or pattern.endswith('.txt') or pattern.endswith('.log') or pattern.endswith('.conf') or pattern.endswith('.yaml') or pattern.endswith('.yml') else f'sh:**/*{pattern}*/**' \n        if pattern not in ['pp:', 'sh:'] and not pattern.startswith('sh:') and not pattern.startswith('pp:') \n        else pattern \n        for pattern in find_paths\n    ]", "import re\nfrom os.path import sep\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('sh:', 'pp:', 'sh:', 'pp:')) else path\n        for path in find_paths\n    ]", "import re\nfrom os.path import sep\n\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('sh:', 'pp:')) else path\n        for path in find_paths\n    ]"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72127, "num_retrieved": 216, "inference_time": 7.911143198609352, "generate_results": ["def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('GIT_BASH') is not None", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM') == 'xterm-256color' and os.environ.get('GIT_BASH') == '1'", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('GIT_BASH') is not None", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('TERM', '') == 'xterm-256color' and os.environ.get('GIT_BASH', '') != ''", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os.environ.get('GIT_BASH') is not None"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69724, "num_retrieved": 158, "inference_time": 7.911143198609352, "generate_results": ["import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and if overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and if overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and if overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and if overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file permissions\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72663, "num_retrieved": 232, "inference_time": 7.911143198609352, "generate_results": ["import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to a single line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Remove leading and trailing whitespace\n    script = script.strip()\n    \n    # Split the script into lines\n    lines = script.splitlines()\n    \n    # Filter out empty lines and comments\n    commands = []\n    for line in lines:\n        stripped = line.strip()\n        if stripped and not stripped.startswith(\"#\"):\n            commands.append(stripped)\n    \n    # Join commands with the separator\n    if commands:\n        return seperator.join(commands)\n    else:\n        return \"\"", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to a single line\n    :param seperator: the separator to join commands with (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    return seperator.join(lines)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to join commands with (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Remove leading and trailing whitespace and split into lines\n    lines = script.strip().splitlines()\n    \n    # Strip whitespace from each line and filter out empty lines\n    commands = [line.strip() for line in lines if line.strip()]\n    \n    # Join commands with the specified separator\n    one_line = seperator.join(commands)\n    \n    # Wrap the command to fit within reasonable width if needed\n    wrapped = textwrap.fill(one_line, width=80)\n    \n    return wrapped", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to a single line\n    :param seperator: the separator to join commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Remove leading and trailing whitespace\n    lines = script.strip().splitlines()\n    \n    # Strip whitespace from each line\n    stripped_lines = [line.strip() for line in lines]\n    \n    # Filter out empty lines and comments\n    commands = []\n    for line in stripped_lines:\n        if line and not line.startswith(\"#\"):\n            commands.append(line)\n    \n    # Join commands with the specified separator\n    if commands:\n        return seperator.join(commands)\n    else:\n        return \"\"", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: a single line command with all commands joined by the separator\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Filter out empty lines and comments\n    commands = []\n    for line in lines:\n        if line.startswith(\"#\") or not line:\n            continue\n        commands.append(line)\n    \n    # Join the commands with the separator\n    if commands:\n        return seperator.join(commands)\n    else:\n        return \"\""]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68974, "num_retrieved": 124, "inference_time": 7.911143198609352, "generate_results": ["import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script to execute the function\n    script_content = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Write the script to a temporary file\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write(script_content)\n        temp_script = f.name\n    \n    try:\n        # Set up environment variables\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script with timeout\n        result = subprocess.run(\n            [sys.executable, temp_script],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n        \n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function {func.__name__} timed out after {timeout} seconds\")\n    except Exception as e:\n        raise e\n    finally:\n        # Clean up the temporary script file\n        if os.path.exists(temp_script):\n            os.unlink(temp_script)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script to execute the function\n    script_content = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Write the script to a temporary file\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write(script_content)\n        temp_script = f.name\n    \n    try:\n        # Set up environment variables\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script with timeout\n        result = subprocess.run(\n            [sys.executable, temp_script],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n        \n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function {func.__name__} timed out after {timeout} seconds\")\n    except Exception as e:\n        raise e\n    finally:\n        # Clean up the temporary script file\n        if os.path.exists(temp_script):\n            os.unlink(temp_script)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script to execute the function\n    script_content = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Write the script to a temporary file\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write(script_content)\n        temp_script = f.name\n    \n    try:\n        # Set up environment variables\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script with timeout\n        result = subprocess.run(\n            [sys.executable, temp_script],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n        \n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function {func.__name__} timed out after {timeout} seconds\")\n    except Exception as e:\n        raise e\n    finally:\n        # Clean up the temporary script file\n        if os.path.exists(temp_script):\n            os.unlink(temp_script)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script to execute the function\n    script_content = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Write the script to a temporary file\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as tmp_file:\n        tmp_file.write(script_content)\n        tmp_file_path = tmp_file.name\n    \n    try:\n        # Set up the environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script using subprocess\n        result = subprocess.run(\n            [sys.executable, tmp_file_path],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        # Return the result\n        return result\n        \n    finally:\n        # Clean up the temporary file\n        if os.path.exists(tmp_file_path):\n            os.unlink(tmp_file_path)", "import subprocess\nimport sys\nimport os\nfrom importlib import import_module\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary script to execute the function\n    script_content = f\"\"\"\nimport sys\nimport os\nimport {func.__module__}\n{func.__name__}({', '.join([repr(arg) for arg in args])})\n\"\"\"\n    \n    # Write the script to a temporary file\n    import tempfile\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n        f.write(script_content)\n        temp_script = f.name\n    \n    try:\n        # Set up the environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n        \n        # Run the script with timeout\n        result = subprocess.run(\n            [sys.executable, temp_script],\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n        \n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Function {func.__name__} timed out after {timeout} seconds\")\n    except Exception as e:\n        raise e\n    finally:\n        # Clean up the temporary script\n        if os.path.exists(temp_script):\n            os.unlink(temp_script)"]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69624, "num_retrieved": 157, "inference_time": 7.911143198609352, "generate_results": ["import os\nfrom typing import Optional\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${VAR} or ${VAR:-DEFAULT}\n    match = matcher.group(0)\n    if match.startswith('${'):\n        # Extract the variable name and optional default value\n        var_part = match[2:-1]\n        \n        # Check if there's a default value (contains ':')\n        if ':' in var_part:\n            var_name, default_value = var_part.split(':', 1)\n        else:\n            var_name = var_part\n            default_value = None\n            \n        # Get the environment variable value\n        env_value = os.environ.get(var_name)\n        \n        # If the variable is not defined and no default value is provided, raise an error\n        if env_value is None and default_value is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n            \n        # Return the value (either from environment or default)\n        return env_value if env_value is not None else default_value\n        \n    return None", "import os\nimport sys\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format is either ${VAR} or ${VAR:-DEFAULT}\n    match = matcher.group(0)\n    \n    # Extract the variable name (everything between the braces)\n    var_name = match[1:-1]\n    \n    # Check if there's a default value (after the colon)\n    if ':' in var_name:\n        var_name, default_value = var_name.split(':', 1)\n        default_value = default_value.strip()\n    else:\n        default_value = None\n    \n    # Get the environment variable value\n    env_value = os.environ.get(var_name)\n    \n    # If no value is found and no default is provided, raise an error\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nfrom typing import Optional\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = matcher.group(0)\n    if match.startswith('${'):\n        # Extract the variable name and optional default value\n        var_name_part = match[2:-1]\n        \n        # Check if it contains a default value (e.g., ${FOO:-bar})\n        if ':' in var_name_part:\n            var_name, default_value = var_name_part.split(':', 1)\n        else:\n            var_name = var_name_part\n            default_value = None\n            \n        # Get the environment variable value\n        env_value = os.environ.get(var_name)\n        \n        # If the variable is not defined and no default value is provided, raise an error\n        if env_value is None and default_value is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n            \n        # Return the value (either from environment or default)\n        return env_value if env_value is not None else default_value\n    else:\n        # This should not happen given the context\n        raise ValueError(f\"Invalid matcher format: {match}\")", "import os\nimport sys\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = matcher.group(0)\n    if match.startswith('${'):\n        # Extract the variable name and optional default value\n        var_name_part = match[2:-1]  # Remove the ${ and }\n        \n        # Check if there's a default value (e.g., ${FOO:-bar})\n        if ':' in var_name_part:\n            var_name, default = var_name_part.split(':', 1)\n        else:\n            var_name = var_name_part\n            default = None\n            \n        # Get the environment variable value\n        env_value = os.environ.get(var_name)\n        \n        # If the variable is not defined and no default value is provided, raise an error\n        if env_value is None and default is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n            \n        # Return the value (either from environment or default)\n        return env_value if env_value is not None else default\n    else:\n        # This should not happen as the matcher should be in the format ${VAR}\n        raise ValueError(f\"Invalid matcher format: {match}\")", "import os\nfrom typing import Optional\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO:-bar}\n    match = matcher.group(0)\n    if match.startswith('${'):\n        # Extract the variable name and optional default value\n        var_name_part = match[2:-1]\n        \n        # Check if it contains a default value (e.g., ${FOO:-bar})\n        if ':' in var_name_part:\n            var_name, default_value = var_name_part.split(':', 1)\n        else:\n            var_name = var_name_part\n            default_value = None\n            \n        # Get the environment variable value\n        env_value = os.environ.get(var_name)\n        \n        # If the variable is not defined and no default value is provided, raise an error\n        if env_value is None and default_value is None:\n            raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n            \n        # Return the value (either from environment or default)\n        return env_value if env_value is not None else default_value\n    else:\n        # Should not reach here given the context\n        raise ValueError(f\"Invalid matcher format: {match}\")"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69453, "num_retrieved": 138, "inference_time": 7.911143198609352, "generate_results": ["import urllib\nfrom urllib.parse import urlsplit, unquote\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed = urlsplit(image_href)\n    \n    # Validate the URL structure\n    if not parsed.netloc:\n        raise ValueError(f\"Invalid image href: {image_href} (missing netloc)\")\n    \n    # Extract the netloc (network location)\n    netloc = parsed.netloc\n    \n    # Determine if SSL is used (https vs http)\n    use_ssl = parsed.scheme == 'https'\n    \n    # Extract the path and get the image ID from the end of the path\n    path = parsed.path\n    if not path:\n        raise ValueError(f\"Invalid image href: {image_href} (empty path)\")\n    \n    # Remove leading slash and split by slashes\n    path_parts = path[1:].split('/')\n    \n    # The image ID should be the last part of the path\n    if not path_parts:\n        raise ValueError(f\"Invalid image href: {image_href} (path has no parts)\")\n    \n    image_id = path_parts[-1]\n    \n    # Validate that image_id is not empty\n    if not image_id:\n        raise ValueError(f\"Invalid image href: {image_href} (image_id is empty)\")\n    \n    # Validate that the image_id is a valid UUID or ID format\n    # This is a basic check - in production, you might want to validate UUID format\n    if not image_id or not image_id.strip():\n        raise ValueError(f\"Invalid image href: {image_href} (image_id is invalid)\")\n    \n    # Return the parsed components\n    return (image_id, netloc, use_ssl)", "import urllib\nfrom urllib.parse import urlsplit, unquote\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed = urlsplit(image_href)\n    \n    # Validate the URL structure\n    if not parsed.netloc:\n        raise ValueError(f\"Invalid image href: {image_href} (missing netloc)\")\n    \n    # Extract the netloc (domain and port)\n    netloc = parsed.netloc\n    \n    # Determine if SSL is used (https vs http)\n    use_ssl = parsed.scheme == 'https'\n    \n    # Extract the path and get the image ID from the end of the path\n    path = parsed.path\n    if not path:\n        raise ValueError(f\"Invalid image href: {image_href} (empty path)\")\n    \n    # Remove leading slash and split by slashes\n    path_parts = path[1:].split('/')\n    \n    # The image ID should be the last part of the path\n    if not path_parts:\n        raise ValueError(f\"Invalid image href: {image_href} (empty path parts)\")\n    \n    image_id = path_parts[-1]\n    \n    # Validate that the image ID is not empty\n    if not image_id:\n        raise ValueError(f\"Invalid image href: {image_href} (empty image ID)\")\n    \n    # Remove any query parameters or fragments\n    if parsed.query or parsed.fragment:\n        # If there are query parameters or fragments, they are not part of the image ID\n        # and we should not include them in our parsing\n        pass\n    \n    return (image_id, netloc, use_ssl)", "import urllib\nfrom typing import Tuple\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed_url = urllib.parse.urlsplit(image_href)\n    \n    # Validate the URL structure\n    if not parsed_url.netloc:\n        raise ValueError(f\"Invalid image href: {image_href} (missing netloc)\")\n    \n    # Extract the image_id from the path\n    path_parts = parsed_url.path.strip('/').split('/')\n    if len(path_parts) < 1:\n        raise ValueError(f\"Invalid image href: {image_href} (invalid path)\")\n    \n    # The image_id is the last part of the path\n    image_id = path_parts[-1]\n    \n    # Determine if SSL is used based on the scheme\n    use_ssl = parsed_url.scheme == 'https'\n    \n    # Return the parsed components\n    return (image_id, parsed_url.netloc, use_ssl)", "import urllib\nfrom urllib.parse import urlsplit, unquote\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    try:\n        parsed = urlsplit(image_href)\n        \n        # Validate the scheme\n        if not parsed.scheme:\n            raise ValueError(f\"Missing scheme in image href: {image_href}\")\n            \n        # Determine if SSL is used\n        use_ssl = parsed.scheme == 'https'\n        \n        # Extract the netloc (network location)\n        netloc = parsed.netloc\n        \n        # Extract the path and get the image ID\n        path = parsed.path\n        if not path:\n            raise ValueError(f\"Missing path in image href: {image_href}\")\n            \n        # Remove leading slash and split by '/'\n        path_parts = path[1:].split('/')\n        if len(path_parts) < 1:\n            raise ValueError(f\"Invalid path in image href: {image_href}\")\n            \n        # The last part of the path should be the image ID\n        image_id = path_parts[-1]\n        \n        # Validate that the image ID is not empty\n        if not image_id:\n            raise ValueError(f\"Missing image ID in image href: {image_href}\")\n            \n        # Validate that the image ID is a valid UUID or ID\n        # This is a basic check - in production, you might want to validate UUID format\n        if not image_id or not image_id.strip():\n            raise ValueError(f\"Invalid image ID in image href: {image_href}\")\n            \n        # Return the parsed components\n        return (image_id, netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Failed to parse image href: {image_href}. Error: {str(e)}\")", "import urllib\nfrom urllib.parse import urlsplit, unquote\n\ndef _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    try:\n        parsed = urlsplit(image_href)\n        \n        # Validate the scheme\n        if not parsed.scheme:\n            raise ValueError(f\"Missing scheme in image href: {image_href}\")\n        \n        # Determine if SSL is used\n        use_ssl = parsed.scheme == 'https'\n        \n        # Extract the netloc (network location)\n        netloc = parsed.netloc\n        \n        # Extract the path and get the image ID\n        path = parsed.path\n        if not path:\n            raise ValueError(f\"Missing path in image href: {image_href}\")\n            \n        # Remove leading slash and split by '/'\n        path_parts = path[1:].split('/')\n        if len(path_parts) < 1:\n            raise ValueError(f\"Invalid path in image href: {image_href}\")\n            \n        # The last part of the path should be the image ID\n        image_id = path_parts[-1]\n        \n        # Validate that the image ID is not empty\n        if not image_id:\n            raise ValueError(f\"Missing image ID in image href: {image_href}\")\n            \n        # Validate that the image ID is a valid UUID or ID\n        # This is a basic check - in production, you might want to validate UUID format\n        if not image_id or not image_id.strip():\n            raise ValueError(f\"Invalid image ID in image href: {image_href}\")\n            \n        # Return the parsed components\n        return (image_id, netloc, use_ssl)\n        \n    except Exception as e:\n        raise ValueError(f\"Failed to parse image href: {image_href}. Error: {str(e)}\")"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69306, "num_retrieved": 134, "inference_time": 7.911143198609352, "generate_results": ["import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item.endswith(os.sep):\n            result.append(item[:-1])\n        elif isinstance(item, str):\n            result.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '/'):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '/'):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '/'):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list of strings\")\n    \n    result = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1 and item[-1] in (os.sep, '/'):\n            result.append(item[:-1])\n        else:\n            result.append(item)\n    \n    return result"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 2440, "num_retrieved": 10, "inference_time": 7.911143198609352, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for safe matching\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words only\n    return r'\\b' + escaped_pattern + r'\\b'", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    return escaped_pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for safe matching\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words only\n    return r'\\b' + escaped_pattern + r'\\b'", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching whole words\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words only\n    return r'\\b' + escaped_pattern + r'\\b'", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching whole words\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words only\n    return r'\\b' + escaped_pattern + r'\\b'"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70120, "num_retrieved": 162, "inference_time": 7.911143198609352, "generate_results": ["import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Set the working directory if provided\n        if cwd is not None:\n            cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=not hide_stderr,\n            text=True,\n            env=env,\n            timeout=None\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return (0, result.stdout, result.stderr)\n        else:\n            return (result.returncode, result.stdout, result.stderr)\n            \n    except subprocess.TimeoutExpired:\n        return (-1, \"\", \"Command timed out\")\n    except FileNotFoundError:\n        return (-1, \"\", \"Command not found\")\n    except Exception as e:\n        return (-1, \"\", str(e))", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Set the working directory if provided\n        if cwd is not None:\n            cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=not hide_stderr,\n            text=True,\n            env=env,\n            shell=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return (0, result.stdout, result.stderr)\n        else:\n            return (result.returncode, result.stdout, result.stderr)\n            \n    except subprocess.SubprocessError as e:\n        return (e.returncode, \"\", str(e))\n    except Exception as e:\n        return (1, \"\", str(e))", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Set the working directory if provided\n        if cwd is not None:\n            cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=not hide_stderr,\n            text=True,\n            env=env,\n            shell=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return (0, result.stdout, result.stderr)\n        else:\n            return (result.returncode, result.stdout, result.stderr)\n            \n    except subprocess.SubprocessError as e:\n        return (e.returncode, \"\", str(e))", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Set the working directory if provided\n        if cwd is not None:\n            cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=not hide_stderr,\n            text=True,\n            env=env,\n            shell=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return (0, result.stdout, result.stderr)\n        else:\n            return (result.returncode, result.stdout, result.stderr)\n            \n    except subprocess.SubprocessError as e:\n        return (e.returncode, \"\", str(e))\n    except Exception as e:\n        return (1, \"\", str(e))", "import subprocess\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    try:\n        # Prepare the command list\n        cmd = commands\n        \n        # Set the working directory if provided\n        if cwd is not None:\n            cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n        \n        # Execute the command\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=not hide_stderr,\n            text=True,\n            env=env,\n            shell=False\n        )\n        \n        # Return the result\n        if result.returncode == 0:\n            return result.returncode, result.stdout, result.stderr\n        else:\n            return result.returncode, result.stdout, result.stderr\n            \n    except subprocess.SubprocessError as e:\n        return e.returncode, \"\", str(e)"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 7.911143198609352, "generate_results": ["import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69070, "num_retrieved": 140, "inference_time": 7.911143198609352, "generate_results": ["def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no object in the reverse direction)\n    # This means we're looking for nodes that are not pointed to by any other node\n    # via the given property in the reverse direction\n    \n    # Find all nodes that have no parent (i.e., no triple (parent, prop, child))\n    # by checking if any node has a triple (child, prop, parent)\n    # and then checking if that parent has no incoming triples\n    \n    # Get all objects of the property (these are the parents)\n    parents = set()\n    for s, p, o in graph.triples((None, prop, None)):\n        parents.add(o)\n    \n    # Get all subjects that are not in the parents set (these are the roots)\n    # These are nodes that have no parent in the hierarchy\n    all_subjects = set()\n    for s, p, o in graph.triples((None, None, None)):\n        all_subjects.add(s)\n    \n    # The roots are the subjects that are not parents\n    roots = all_subjects - parents\n    \n    # Recursively find roots by checking if any of the current roots have children\n    # that are not already in the roots set\n    new_roots = set()\n    for s in roots:\n        # Check if s has any children via the property\n        for child, _, _ in graph.triples((s, prop, None)):\n            # If the child is not already in roots, it's a root\n            if child not in roots:\n                new_roots.add(child)\n    \n    # If we found new roots, add them to the set and recurse\n    if new_roots:\n        roots.update(new_roots)\n        roots = find_roots(graph, prop, roots)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Find all nodes that have no parent (i.e., no node that points to them via the given property)\n    # These are the roots of the hierarchy\n    for subject, predicate, object in graph.triples((None, prop, None)):\n        # If this object has no incoming edges (no node points to it via prop), it's a root\n        # Check if there's any triple (s, prop, object)\n        has_parent = False\n        for s, p, o in graph.triples((None, prop, object)):\n            has_parent = True\n            break\n        \n        if not has_parent:\n            roots.add(object)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no object in the property)\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object has no parent, it's a root\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(prop, object_)):\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., they are not pointed to by prop)\n    # This means they are the roots of the hierarchy\n    for s, p, o in graph.triples((None, prop, None)):\n        # If this subject is not already in roots, add it\n        if s not in roots:\n            roots.add(s)\n    \n    # For each subject in roots, check if it has any children (i.e., if there's a triple (child, prop, s))\n    # If it does, then it's not a root, so we need to find its actual root\n    # We need to find the actual root by following the chain until we reach a node that has no parent\n    \n    # Instead, we can do this more efficiently by finding all nodes that have no parent\n    # We already have the set of roots, so we just need to make sure they are the actual roots\n    \n    # We can verify by checking if any of the roots have a parent\n    # If they do, then they are not roots, so we need to find the actual root\n    \n    # Actually, we can simplify: we want to find all nodes that have no parent\n    # So we just need to find all nodes that are not the object of any triple with predicate prop\n    \n    # We already have the roots set, so we just need to make sure they are the actual roots\n    # We can do this by checking if any of the roots have a parent\n    \n    # Instead, let's just find all nodes that have no parent\n    # This is equivalent to finding all nodes that are not the object of any triple with predicate prop\n    \n    # We already have the roots set, so we just need to make sure they are the actual roots\n    # We can do this by checking if any of", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no object in the property)\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If this subject has no parent (i.e., no object in the property), it's a root\n        if object_ is None:\n            roots.add(subject)\n    \n    # For each subject that has a parent, recursively check if it's a root\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        if object_ is not None:\n            # Recursively check if the object has any parent\n            # If it does, it's not a root\n            # If it doesn't, it's a root\n            if object_ not in roots:\n                # Check if the object has any parent\n                for obj_subject, obj_predicate, obj_object in graph.triples((None, prop, None)):\n                    if obj_subject == object_:\n                        # The object has a parent, so it's not a root\n                        break\n                else:\n                    # The object has no parent, so it's a root\n                    roots.add(object_)\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 16464, "num_retrieved": 45, "inference_time": 7.911143198609352, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False)", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper, default_flow_style=False).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 7074, "num_retrieved": 39, "inference_time": 7.911143198609352, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 20418, "num_retrieved": 67, "inference_time": 7.911143198609352, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69476, "num_retrieved": 151, "inference_time": 7.911143198609352, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70302, "num_retrieved": 180, "inference_time": 10.853728115558624, "generate_results": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Create a log record for any error encountered during parsing\n            logs.append(\n                logging.makeLogRecord(\n                    dict(\n                        levelno=logging.ERROR,\n                        levelname='ERROR',\n                        msg=f'Error parsing configuration file at {config_filename}: {str(error)}'\n                    )\n                )\n            )\n    \n    return configs, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Create a log record for any error encountered during parsing\n            logs.append(\n                logging.makeLogRecord(\n                    dict(\n                        levelno=logging.ERROR,\n                        levelname='ERROR',\n                        msg=f'Error parsing configuration file at {config_filename}: {str(error)}',\n                    )\n                )\n            )\n    \n    return configs, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Create a log record for any error encountered during parsing\n            logs.append(\n                logging.makeLogRecord(\n                    dict(\n                        levelno=logging.ERROR,\n                        levelname='ERROR',\n                        msg=f'Error parsing configuration file at {config_filename}: {str(error)}'\n                    )\n                )\n            )\n    \n    return configs, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Create a log record for any error encountered during parsing/validation\n            logs.append(\n                logging.makeLogRecord(\n                    dict(\n                        levelno=logging.ERROR,\n                        levelname='ERROR',\n                        msg=f'Error parsing configuration file at {config_filename}: {str(error)}'\n                    )\n                )\n            )\n    \n    return configs, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            # Create a log record for any error encountered during parsing\n            log_record = logging.makeLogRecord({\n                'levelname': 'ERROR',\n                'levelno': logging.ERROR,\n                'msg': f'Error parsing configuration file {config_filename}: {str(error)}'\n            })\n            logs.append(log_record)\n    \n    return configs, logs"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72647, "num_retrieved": 298, "inference_time": 10.853728115558624, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70698, "num_retrieved": 168, "inference_time": 10.853728115558624, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds = ticks // 1000000000\n    nanoseconds = ticks % 1000000000\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    remaining_seconds = seconds % 60\n    \n    # Create the time object\n    return Time(hours, minutes, remaining_seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not isinstance(ticks, int):\n        raise ValueError(\"ticks must be an integer\")\n    \n    if ticks < 0 or ticks >= 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 69614, "num_retrieved": 136, "inference_time": 10.853728115558624, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (3, 0):\n            handlers[protocol_version] = Bolt3\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # Return all available versions\n    handlers = {\n        (3, 0): Bolt3,\n        (4, 1): Bolt4x1,\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5, 2): Bolt5x2,\n        (5, 3): Bolt5x3\n    }\n    \n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 0):\n            handlers[protocol_version] = Bolt4x0\n        elif protocol_version == (3, 5):\n            handlers[protocol_version] = Bolt3\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # Return all available versions\n    handlers = {\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5, 2): Bolt5x2,\n        (5, 3): Bolt5x3,\n        (4, 1): Bolt4x1,\n        (4, 0): Bolt4x0,\n        (", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 0):\n            handlers[protocol_version] = Bolt4x0\n        elif protocol_version == (3, 5):\n            handlers[protocol_version] = Bolt3\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # Return all available versions\n    handlers = {\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5, 2): Bolt5x2,\n        (5, 3): Bolt5x3,\n        (4, 1): Bolt4x1,\n        (4, 0): Bolt4x0,\n        (", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 0):\n            handlers[protocol_version] = Bolt4x0\n        elif protocol_version == (3, 0):\n            handlers[protocol_version] = Bolt3\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # Return all available versions\n    handlers = {\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5, 2): Bolt5x2,\n        (5, 3): Bolt5x3,\n        (4, 1): Bolt4x1,\n        (4, 0): Bolt4x0,\n        (", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the specific version is supported\n        handlers = {}\n        if protocol_version == (4, 2):\n            handlers[protocol_version] = Bolt4x2\n        elif protocol_version == (4, 3):\n            handlers[protocol_version] = Bolt4x3\n        elif protocol_version == (4, 4):\n            handlers[protocol_version] = Bolt4x4\n        elif protocol_version == (5, 1):\n            handlers[protocol_version] = Bolt5x1\n        elif protocol_version == (5, 2):\n            handlers[protocol_version] = Bolt5x2\n        elif protocol_version == (5, 3):\n            handlers[protocol_version] = Bolt5x3\n        elif protocol_version == (4, 1):\n            handlers[protocol_version] = Bolt4x1\n        elif protocol_version == (4, 0):\n            handlers[protocol_version] = Bolt4x0\n        elif protocol_version == (3, 0):\n            handlers[protocol_version] = Bolt3\n        else:\n            handlers = {}\n            \n        return handlers\n    \n    # Return all available versions\n    handlers = {\n        (4, 2): Bolt4x2,\n        (4, 3): Bolt4x3,\n        (4, 4): Bolt4x4,\n        (5, 1): Bolt5x1,\n        (5, 2): Bolt5x2,\n        (5, 3): Bolt5x3,\n        (4, 1): Bolt4x1,\n        (4, 0): Bolt4x0,\n        ("]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 70554, "num_retrieved": 166, "inference_time": 10.853728115558624, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    return cls(list(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68655, "num_retrieved": 122, "inference_time": 10.853728115558624, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\n    Sequence is derived from *seq*\n    (or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.Source):\n        return seq, \"source\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        # Try to convert to a sequence\n        try:\n            seq = lena.core.Sequence(seq)\n            return seq, \"sequence\"\n        except lena.core.LenaTypeError:\n            raise lena.core.LenaTypeError(\n                \"unknown argument type. Must be one of \"\n                \"FillComputeSeq, FillRequestSeq or Source, \"\n                \"{} provided\".format(seq)\n            )"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68083, "num_retrieved": 102, "inference_time": 10.853728115558624, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # Only compute if not already computed\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram bins\n        # This would typically involve modifying the bin contents\n        # based on the new scale factor\n        # For now, we just store the new scale\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral()\n            else:\n                # Only compute if not already computed\n                if self._scale is None:\n                    self._scale = self.integral()\n        return self._scale\n    else:\n        # Rescale to the provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram to the new value\n        # This would typically involve scaling the bins by a factor\n        # that maintains the integral at the new value\n        scale_factor = other / self._scale\n        # Apply the scale factor to the bins\n        # This is a simplified representation - actual implementation\n        # would depend on the histogram's internal structure\n        self._scale = other\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral(self.bins, self.edges)\n            else:\n                # Only compute if not already computed\n                if self._scale is None:\n                    self._scale = self.integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to the provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"histogram with zero scale cannot be rescaled\")\n        # Rescale the histogram bins\n        # This would typically involve modifying the bin contents\n        # based on the new scale factor\n        # For now, we just store the new scale\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral()\n            else:\n                # If scale not computed and not recompute, raise error\n                raise lena.core.LenaValueError(\"scale not computed\")\n        return self._scale\n    else:\n        # Rescale to provided value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram to the provided value\n        # This would typically involve adjusting bin contents\n        # based on the current scale and the target scale\n        # The actual implementation would depend on the histogram's\n        # specific scaling mechanism\n        self._scale = other\n        # Update the histogram bins to reflect the new scale\n        # This is a simplified representation - actual implementation\n        # would depend on the histogram's internal structure\n        return self._scale", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral()\n            else:\n                # If scale not computed and not recompute, raise error\n                raise lena.core.LenaValueError(\"scale not computed\")\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"can't rescale histogram with zero scale\")\n        # Rescale the histogram to other value\n        # This would typically involve adjusting bin contents\n        # based on the current scale and target scale\n        # For now, we just store the new scale\n        self._scale = other"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68003, "num_retrieved": 100, "inference_time": 10.853728115558624, "generate_results": ["def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    if self._scale is None or self._scale == 0:\n        raise lena.core.LenaValueError(\n            \"cannot rescale graph with unknown or zero scale\"\n        )\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Rescale only the last coordinate and its corresponding errors\n    # For a 2D graph, rescale y; for a 3D graph, rescale z, etc.\n    last_coord_index = len(field_names) - 1\n    \n    # Rescale the last coordinate and its errors\n    coords = self.coords\n    new_coords = []\n    \n    for i, coord in enumerate(coords):\n        if i == last_coord_index:\n            # Rescale the last coordinate\n            new_coords.append([val * other for val in coord])\n        else:\n            new_coords.append(coord)\n    \n    # Rescale errors if they exist\n    if hasattr(self, 'errors') and self.errors:\n        error_coords = self.errors\n        new_error_coords = []\n        for i, error_coord in enumerate(error_coords):\n            if i == last_coord_index:\n                # Rescale the last error coordinate\n                new_error_coords.append([val * other for val in error_coord])\n            else:\n                new_error_coords.append(error_coord)\n        \n        # Update the graph with new coordinates and errors\n        self.coords = new_coords\n        self.errors = new_error_coords\n    \n    # Update the scale\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    if self._scale is None or self._scale == 0:\n        raise lena.core.LenaValueError(\n            \"cannot rescale graph with unknown or zero scale\"\n        )\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Rescale only the last coordinate and its corresponding errors\n    # For a 2D graph, scale the y coordinate\n    # For a 3D graph, scale the z coordinate\n    # All errors are scaled together with their coordinate\n    \n    # Assuming the graph has a method to get its coordinates\n    coords = self.coords\n    \n    # Scale the last coordinate\n    last_coord_index = len(field_names) - 1\n    scaled_coords = []\n    \n    for i, coord in enumerate(coords):\n        if i == last_coord_index:\n            # Scale the last coordinate\n            scaled_coords.append([val * other for val in coord])\n        else:\n            # Keep other coordinates unchanged\n            scaled_coords.append(coord)\n    \n    # Update the graph's coordinates\n    self.coords = scaled_coords\n    \n    # Scale errors if they exist\n    # Assuming errors are stored in the same order as coordinates\n    # and are stored in the same index as the coordinate\n    if hasattr(self, 'errors'):\n        error_coords = self.errors\n        scaled_errors = []\n        for i, error in enumerate(error_coords):\n            if i == last_coord_index:\n                # Scale the last coordinate's errors\n                scaled_errors.append([val * other for val in error])\n            else:\n                # Keep other errors unchanged\n                scaled_errors.append(error", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    if self._scale is None or self._scale == 0:\n        raise lena.core.LenaValueError(\n            \"graph has unknown or zero scale, cannot rescale\"\n        )\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Rescale only the last coordinate and its corresponding errors\n    # For a 2D graph with x, y coordinates, scale y\n    # For a 3D graph with x, y, z coordinates, scale z\n    last_coord_index = len(field_names) - 1\n    \n    # Scale the coordinates and errors\n    coords = self.coords\n    scaled_coords = []\n    \n    for i, coord in enumerate(coords):\n        if i == last_coord_index:\n            # Scale the last coordinate\n            scaled_coords.append([val * other for val in coord])\n        else:\n            # Keep other coordinates unchanged\n            scaled_coords.append(coord)\n    \n    # Scale errors if they exist\n    if hasattr(self, 'errors'):\n        error_coords = self.errors\n        scaled_errors = []\n        for i, error in enumerate(error_coords):\n            if i == last_coord_index:\n                # Scale the last error coordinate\n                scaled_errors.append([val * other for val in error])\n            else:\n                # Keep other error coordinates unchanged\n                scaled_errors.append(error)\n    \n    # Update the graph's coordinates and errors\n    self.coords = scaled_coords\n    if hasattr(self, 'errors'):\n        self.errors = scaled_errors\n    \n    # Update the scale\n    self._scale = other", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    if self._scale is None or self._scale == 0:\n        raise lena.core.LenaValueError(\n            \"cannot rescale graph with unknown or zero scale\"\n        )\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Rescale only the last coordinate and its corresponding errors\n    # For a 2D graph, rescale y; for a 3D graph, rescale z, etc.\n    last_coord_index = len(field_names) - 1\n    \n    # Get the coordinates and errors\n    coords = self.coords\n    error_coords = self.error_coords if hasattr(self, 'error_coords') else None\n    \n    # Rescale the last coordinate\n    new_coords = []\n    for i, coord in enumerate(coords):\n        if i == last_coord_index:\n            # Rescale the last coordinate\n            new_coords.append([val * other for val in coord])\n        else:\n            new_coords.append(coord)\n    \n    # Rescale errors if they exist\n    if error_coords is not None:\n        new_error_coords = []\n        for i, error_coord in enumerate(error_coords):\n            if i == last_coord_index:\n                # Rescale the last error coordinate\n                new_error_coords.append([val * other for val in error_coord])\n            else:\n                new_error_coords.append(error_coord)\n    \n    # Update the graph's coordinates and errors\n    self.coords = new_coords\n    if error_coords is not None:\n        self.error_coords = new_error_coords\n    \n    # Update the scale\n    self._scale = other", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    if self._scale is None or self._scale == 0:\n        raise lena.core.LenaValueError(\n            \"cannot rescale graph with unknown or zero scale\"\n        )\n    \n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Rescale only the last coordinate and its corresponding errors\n    # For a 2D graph with x, y coordinates, scale y\n    # For a 3D graph with x, y, z coordinates, scale z\n    if len(field_names) > 1:\n        last_field_index = len(field_names) - 1\n        last_field_name = field_names[last_field_index]\n        \n        # Scale the last coordinate and its errors\n        # This would typically be implemented in the underlying graph structure\n        # For now, we just return the scale value\n        self._scale = other\n        \n        # In a real implementation, this would modify the actual coordinates\n        # and errors in the graph data structure\n        return self._scale\n    \n    # For 1D graphs, scale the only coordinate\n    self._scale = other\n    return self._scale"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 68145, "num_retrieved": 104, "inference_time": 10.853728115558624, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Get the edges and bins from the histogram\n    edges = hist.edges\n    bins = hist.bins\n    \n    # Determine the coordinate based on get_coordinate\n    if get_coordinate == \"left\":\n        coordinate = lambda i: edges[0][i]\n    elif get_coordinate == \"right\":\n        coordinate = lambda i: edges[0][i + 1]\n    elif get_coordinate == \"middle\":\n        coordinate = lambda i: (edges[0][i] + edges[0][i + 1]) / 2\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle'; '{get_coordinate}' provided\")\n    \n    # Create points list\n    points = []\n    \n    # Process each bin\n    for i in range(len(edges[0]) - 1):\n        # Get the bin content\n        bin_content = bins[i]\n        \n        # Apply make_value to", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Get the edges and bins from the histogram\n    edges = hist.edges\n    bins = hist.bins\n    \n    # Determine the coordinate based on get_coordinate\n    if get_coordinate == \"left\":\n        coordinate = lambda i: edges[0][i]\n    elif get_coordinate == \"right\":\n        coordinate = lambda i: edges[0][i + 1]\n    elif get_coordinate == \"middle\":\n        coordinate = lambda i: (edges[0][i] + edges[0][i + 1]) / 2\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle'; '{get_coordinate}' provided\")\n    \n    # Create points list\n    points = []\n    \n    # Iterate through bins\n    for i in range(len(edges[0]) - 1):\n        # Get the bin content\n        bin_content = bins[i]\n        \n        # Apply make_value to", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Determine the coordinate based on get_coordinate\n    if get_coordinate == \"left\":\n        coordinate = lambda edges, bin_idx: edges[bin_idx]\n    elif get_coordinate == \"right\":\n        coordinate = lambda edges, bin_idx: edges[bin_idx + 1]\n    elif get_coordinate == \"middle\":\n        coordinate = lambda edges, bin_idx: (edges[bin_idx] + edges[bin_idx + 1]) / 2\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle'; '{get_coordinate}' provided\")\n    \n    # Create points list\n    points = []\n    \n    # Get edges and bins\n    edges = hist.edges\n    bins = hist.bins\n    \n    # Process each bin\n    for bin_idx in range(len(bins)):\n        # Get bin content\n        bin_content = bins[bin_idx]\n        \n        # Apply make_value to get the", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Get the edges and bins from the histogram\n    edges = hist.edges\n    bins = hist.bins\n    \n    # Determine the coordinate based on get_coordinate\n    if get_coordinate == \"left\":\n        coordinate = lambda i: edges[0][i]\n    elif get_coordinate == \"right\":\n        coordinate = lambda i: edges[0][i + 1]\n    elif get_coordinate == \"middle\":\n        coordinate = lambda i: (edges[0][i] + edges[0][i + 1]) / 2\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle'; '{get_coordinate}' provided\")\n    \n    # Create points list\n    points = []\n    \n    # Process each bin\n    for i in range(len(edges[0]) - 1):\n        # Get the bin content\n        bin_content = bins[i]\n        \n        # Apply make_value to", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_: bin_\n    \n    # Determine the coordinate based on get_coordinate\n    if get_coordinate == \"left\":\n        coordinate = lambda edges, ind: edges[ind][0]\n    elif get_coordinate == \"right\":\n        coordinate = lambda edges, ind: edges[ind][-1]\n    elif get_coordinate == \"middle\":\n        coordinate = lambda edges, ind: (edges[ind][0] + edges[ind][-1]) / 2\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle'; '{get_coordinate}' provided\")\n    \n    # Get the edges and bins from the histogram\n    edges = hist.edges\n    bins = hist.bins\n    \n    # Create points list\n    points = []\n    \n    # Process each bin\n    for ind in range(len(edges[0]) - 1):\n        # Get the bin content\n        bin_content = bins[ind]\n        \n        # Apply make_value to get the value"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71684, "num_retrieved": 220, "inference_time": 10.853728115558624, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(Invalid(\"Candidate does not have all required methods\"))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None:\n                # Check if method has correct signature\n                # This is a simplified check - in practice, would need to compare\n                # actual method signatures with interface requirements\n                pass\n    except AttributeError:\n        errors.append(Invalid(\"Candidate does not have all required methods\"))\n\n    # Step 4: Check if candidate defines all necessary attributes\n    try:\n        for name in iface.names(all=True):\n            attr = getattr(candidate, name, None)\n            if attr is None", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError as e:\n        errors.append(Invalid(\"Attribute error during method verification: %s\" % str(e)))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None and hasattr(method, 'getSignatureInfo'):\n                # Compare method signatures\n                method_sig = method.getSignatureInfo()\n                iface_sig = iface.getSignatureInfo()\n                # This is a simplified check - in practice, would need to compare\n                # actual method signatures more thoroughly\n                if name not in iface_sig:\n                    errors.append(Invalid(f\"Method {name} not found in interface {iface}\"))\n    except Exception as e:", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError as e:\n        errors.append(Invalid(\"Attribute error during method check: %s\" % str(e)))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None and hasattr(method, 'getSignatureInfo'):\n                # Compare method signatures\n                method_sig = method.getSignatureInfo()\n                iface_sig = iface.getSignatureInfo(name)\n                if method_sig != iface_sig:\n                    errors.append(Invalid(f\"Method '{name}' has incorrect signature\"))\n    except AttributeError as e:\n        errors.append(Invalid(\"Attribute error during signature check: %s\" % str(e)))\n\n    # Step 4: Check attributes", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError as e:\n        errors.append(Invalid(\"Attribute error during method check: %s\" % str(e)))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None and hasattr(method, 'getSignatureInfo'):\n                # Compare method signatures\n                method_sig = method.getSignatureInfo()\n                iface_sig = iface.getSignatureInfo()\n                # This is a simplified check - in practice, would need to compare\n                # more detailed signature information\n                if name not in iface_sig:\n                    errors.append(Invalid(f\"Method {name} not found in interface {iface}\"))\n    except AttributeError as e:", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(Invalid(\"Candidate does not have all required methods\"))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None:\n                # Check if method has correct signature\n                # This is a simplified check - actual signature validation would be more complex\n                pass\n    except AttributeError:\n        errors.append(Invalid(\"Candidate does not have all required methods\"))\n\n    # Step 4: Check if candidate defines all necessary attributes\n    try:\n        for name in iface.names(all=True):\n            attr = getattr(candidate, name, None)\n            if attr is None:\n                errors.append(BrokenImplementation(iface"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71712, "num_retrieved": 221, "inference_time": 10.853728115558624, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n            else:\n                # Check method signature\n                try:\n                    method_info = method.getSignatureInfo()\n                    # This is a simplified check - in practice, we'd compare\n                    # the actual method signature with the interface's definition\n                except AttributeError:\n                    errors.append(BrokenMethodImplementation(method, \"method has no signature info\"))\n    except AttributeError:\n        errors.append(Invalid(\"Candidate does not define all necessary methods\"))\n\n    # Step 3: Check all necessary attributes\n    try:\n        for name in iface.names(all=True):\n            attr = getattr(candidate, name, None)\n            if attr is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(Invalid(\"Candidate does not define all necessary attributes\"))\n\n    # Return True", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 3: Check if methods have correct signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None and hasattr(method, 'getSignatureInfo'):\n                # Check if method signature matches interface definition\n                # This is a simplified check - actual signature validation would be more complex\n                pass\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 4: Check if candidate defines all necessary attributes\n    try:\n        for name in iface.names(all=True):\n            attr = getattr(candidate, name, None)\n            if attr is None:\n                errors", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 3: Check if methods have correct signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None and hasattr(method, 'getSignatureInfo'):\n                # Compare method signatures\n                method_sig = method.getSignatureInfo()\n                iface_sig = None\n                for attr in iface.names(all=True):\n                    if attr == name:\n                        iface_sig = getattr(iface, attr, None)\n                        break\n                if iface_sig is not None and method_sig != iface_sig:\n                    errors.append(BrokenMethodImplementation(name, \"method signature mismatch\", method, iface))\n    except AttributeError:\n        pass", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n            else:\n                # Check method signature\n                try:\n                    method_info = method.getSignatureInfo()\n                    if method_info is None:\n                        errors.append(BrokenMethodImplementation(name, \"Method has no signature info\"))\n                except AttributeError:\n                    errors.append(BrokenMethodImplementation(name, \"Method has no signature info\"))\n    except AttributeError as e:\n        errors.append(Invalid(\"Missing method: %s\" % str(e)))\n\n    # Step 3: Check all necessary attributes\n    try:\n        for name in iface.names(all=True):\n            attr = getattr(candidate, name, None)\n            if attr is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError as e:\n        errors.append(Invalid(\"Missing attribute: %s\" % str(e)))", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 3: Check if methods have correct signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None and hasattr(method, 'getSignatureInfo'):\n                # Compare method signatures\n                method_sig = method.getSignatureInfo()\n                iface_sig = iface.getSignatureInfo(name)\n                if method_sig != iface_sig:\n                    errors.append(BrokenMethodImplementation(name, \"method signature mismatch\", method, iface))\n    except AttributeError:\n        pass\n\n    # Step 4: Check if candidate defines all necessary attributes\n    try:\n        for name in iface.names(all=True):\n            attr = getattr(candidate,"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73884, "num_retrieved": 293, "inference_time": 10.853728115558624, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate is a class\n    if not isinstance(candidate, type):\n        return False\n    \n    # Check if the candidate implements the interface\n    try:\n        from zope.interface import implementedBy\n        if not implementedBy(candidate).contains(iface):\n            return False\n    except ImportError:\n        # Fallback for environments without zope.interface\n        pass\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check if the candidate has a direct implementation\n        # of the interface or if it inherits from a class that implements the interface\n        try:\n            from zope.interface import implementedBy\n            spec = implementedBy(candidate)\n            for base in spec.declared:\n                if base is iface or iface.extends(base):\n                    return True\n        except ImportError:\n            pass\n    \n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate is a class\n    if not isinstance(candidate, type):\n        raise TypeError(\"Candidate must be a class\")\n    \n    # Check if the interface is a valid interface\n    if not hasattr(iface, '__bases__'):\n        raise TypeError(\"Interface must have __bases__ attribute\")\n    \n    # Check if the candidate class implements the interface\n    try:\n        # Get the interfaces implemented by the candidate class\n        implemented_interfaces = implementedBy(candidate)\n        \n        # Check if the interface is in the implemented interfaces\n        if not implemented_interfaces.extends(iface):\n            return False\n            \n        # If tentative is True, we allow for some flexibility\n        if tentative:\n            # Check if the interface is directly implemented or extended\n            return True\n            \n        # Otherwise, we require exact implementation\n        return True\n        \n    except Exception as e:\n        # If there's an error in the implementation, return False\n        return False", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if candidate is a class\n    if not isinstance(candidate, type):\n        raise TypeError(\"Candidate must be a class\")\n    \n    # Check if iface is an interface\n    if not hasattr(iface, '__bases__'):\n        raise TypeError(\"Interface must have __bases__ attribute\")\n    \n    # Verify that the candidate class implements the interface\n    # by checking if the interface is in the implementedBy specification\n    try:\n        implemented_interfaces = implementedBy(candidate)\n        if not implemented_interfaces.extends(iface):\n            return False\n    except Exception:\n        return False\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check if the interface is in the base classes\n        # or if it's directly implemented\n        try:\n            bases = candidate.__bases__\n            for base in bases:\n                if hasattr(base, '__implemented__'):\n                    implemented_interfaces = implementedBy(base)\n                    if implemented_interfaces.extends(iface):\n                        return True\n            return False\n        except Exception:\n            return False\n    \n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate is a class\n    if not isinstance(candidate, type):\n        return False\n    \n    # Check if the candidate implements the interface\n    try:\n        from zope.interface import implementedBy\n        if not implementedBy(candidate).contains(iface):\n            return False\n    except ImportError:\n        # Fallback if zope.interface is not available\n        return False\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check if the interface is in the class's bases\n        try:\n            from zope.interface import implementedBy\n            bases = implementedBy(candidate)\n            return any(iface in base for base in bases)\n        except ImportError:\n            return False\n    \n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate is a class\n    if not isinstance(candidate, type):\n        return False\n    \n    # Check if the candidate implements the interface\n    try:\n        from zope.interface import implementedBy\n        if not implementedBy(candidate).contains(iface):\n            return False\n    except ImportError:\n        # Fallback if zope.interface is not available\n        pass\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check if the candidate has a direct implementation\n        # of the interface, without strict inheritance checks\n        try:\n            from zope.interface import directlyProvides\n            # Check if the candidate directly provides the interface\n            if not directlyProvides(candidate, iface):\n                return False\n        except ImportError:\n            pass\n    \n    return True"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73034, "num_retrieved": 268, "inference_time": 10.853728115558624, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base class has a __metaclass__ attribute\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass (type)\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the metaclass of the first base\n    if bases:\n        return bases[0].__class__.__bases__[0] if bases[0].__class__.__bases__ else type\n    \n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 11633, "num_retrieved": 62, "inference_time": 10.853728115558624, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 9729, "num_retrieved": 47, "inference_time": 10.853728115558624, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 10.853728115558624, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 9.704619094729424, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 10009, "num_retrieved": 48, "inference_time": 9.704619094729424, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self.__data.pop(key)\n    del self.__size[key]\n    self.__currsize -= self.__size[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    # Remove the key-value pair\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    # Remove the key-value pair\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    # Remove the key and return the (key, value) pair\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 28011, "num_retrieved": 72, "inference_time": 9.704619094729424, "generate_results": ["def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a regular expression pattern to match in-style parameters\n    # This pattern should match the parameter style defined by the in_style\n    # attribute of the converter instance\n    pattern = self._in_style.param_regex\n    \n    # If the pattern is not provided, use a default pattern\n    if not pattern:\n        # Default pattern for named parameters\n        pattern = r'(?P<param>(?P<out_percent>\\%)(?P<escape>[^%]+))|(?P<param>[a-zA-Z_][a-zA-Z0-9_]*)'\n    \n    # Compile the pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a pattern that matches in-style parameters\n    # This pattern should match the parameter style defined by self._in_style\n    # It will match either named parameters (e.g., :param1:) or ordinal parameters (e.g., %s)\n    pattern = self._in_style.param_regex\n    \n    # If the pattern is not provided, create a default one\n    if not pattern:\n        # Default pattern for named parameters\n        pattern = r':(?P<param>[a-zA-Z_][a-zA-Z0-9_]*)'\n    \n    # Compile the pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a pattern that matches in-style parameters\n    # This pattern should match the parameter style defined by self._in_style\n    # It will match either named parameters or numeric parameters based on the style\n    \n    # Base pattern for parameter matching\n    pattern = self._in_style.param_regex\n    \n    # Add escape sequence handling\n    escape_pattern = self._in_style.escape_regex\n    \n    # Create the full pattern that matches parameters\n    # This pattern will match either named or numeric parameters\n    # and handle escape sequences\n    full_pattern = f\"({escape_pattern}|{pattern})\"\n    \n    # Compile the pattern\n    return re.compile(full_pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a regular expression that matches in-style parameters\n    # This pattern should match escape sequences, named parameters, and numeric parameters\n    # based on the in-style defined in the converter\n    pattern = self._in_style.param_regex\n    \n    # If escape_char is provided, we need to ensure that escape sequences are properly handled\n    # The pattern should match the escape sequence followed by the parameter\n    if self._in_style.escape_char:\n        # Escape the escape character in the regex to prevent it from being matched as a literal\n        escape_char = self._in_style.escape_char\n        # Create a pattern that matches escape sequences\n        escape_pattern = f\"(?P<escape>{escape_char}(?P<param>[^{escape_char}]+))\"\n        # Combine with the main parameter pattern\n        pattern = f\"{escape_pattern}|{pattern}\"\n    \n    # Compile the pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a regex pattern that matches in-style parameters\n    # This pattern should match the parameter style defined by self._in_style\n    # It will match named parameters, numeric parameters, and escape sequences\n    pattern = self._in_style.param_regex\n    \n    # If the pattern is not provided, use a default pattern\n    if not pattern:\n        # Default pattern for named parameters\n        pattern = r'(?P<param>(?P<out_percent>\\\\%)|(?P<escape>[^\\\\])\\\\(?P<escape_char>[^\\\\]+)|(?P<param>[a-zA-Z_][a-zA-Z0-9_]*)|(?P<param>\\d+))'\n    \n    # Compile the pattern\n    return re.compile(pattern)"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 27900, "num_retrieved": 71, "inference_time": 9.704619094729424, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the converter based on in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create appropriate converter based on style types\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NumericToNamedConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NumericToOrdinalConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter()\n    \n    # Default fallback\n    return _converting.Converter()", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the converter based on in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create appropriate converter based on style combinations\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NumericToNamedConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NumericToOrdinalConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter()\n    \n    # Default fallback\n    return _converting.Converter()", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the converter based on in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create appropriate converter based on style types\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NumericToNamedConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NumericToOrdinalConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter()\n    \n    # Default fallback\n    return _converting.Converter()", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the converter based on in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create appropriate converter based on style combinations\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NumericToNamedConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NumericToOrdinalConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter()\n    \n    # Default fallback\n    return _converting.Converter()", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Based on the context, this function should create a converter instance\n    # using the style parameters defined in the class\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 56503, "num_retrieved": 217, "inference_time": 9.704619094729424, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 50336, "num_retrieved": 190, "inference_time": 9.704619094729424, "generate_results": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 53147, "num_retrieved": 194, "inference_time": 9.704619094729424, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # The fold attribute indicates whether we're in the first (fold=0) \n        # or second (fold=1) occurrence of the ambiguous time\n        # We use the fold attribute to determine which one to return\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence, so we use fold=0\n        # If fold is 0, we're in the first occurrence, so we use fold=1\n        # This ensures we're always returning the correct fold\n        return enfold(dt, fold=0 if fold == 1 else 1)\n    \n    # If not ambiguous, simply convert to the local timezone\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is None:\n        return dt\n    \n    # Determine if the datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # If the datetime is ambiguous, we need to determine which fold to use\n    # The fold attribute indicates whether we're in the first (fold=0) or second (fold=1) occurrence\n    # For ambiguous times, we typically use the fold=0 (earlier) occurrence by default\n    # However, if the datetime is in a fold state, we need to adjust accordingly\n    \n    # Create a new datetime with the same time but in the local timezone\n    # We use the fold attribute to determine which occurrence to use\n    if is_ambiguous:\n        # For ambiguous times, we need to check if we're in a fold state\n        # If we are in a fold state, we use the fold value to determine which occurrence\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence (later), so we use fold=1\n        # If fold is 0, we're in the first occurrence (earlier), so we use fold=0\n        # We create a new datetime with the same time but in the local timezone\n        # and with the fold value set to the appropriate value\n        new_dt = enfold(dt, fold=fold)\n    else:\n        # For non-ambiguous times, we simply convert to the local timezone\n        new_dt = enfold(dt, fold=0)\n    \n    return new_dt", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # The fold attribute indicates whether we're in the first (fold=0) \n        # or second (fold=1) occurrence of the ambiguous time\n        # We use the fold attribute to determine which fold to use\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence, so we use fold=1\n        # If fold is 0, we're in the first occurrence, so we use fold=0\n        # We create a new datetime with the same time but in the target timezone\n        # and with the fold attribute set to the appropriate value\n        new_dt = enfold(dt, fold=fold)\n        return new_dt\n    \n    # If the datetime is not ambiguous, simply convert it to the target timezone\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is None:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # The fold attribute indicates whether we're in the first or second occurrence\n        # of the ambiguous time\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence (the one that comes after DST changes)\n        # Otherwise, we're in the first occurrence\n        if fold == 1:\n            # Use the fold=1 version of the datetime\n            return enfold(dt, fold=1)\n        else:\n            # Use the fold=0 version of the datetime\n            return enfold(dt, fold=0)\n    \n    # If not ambiguous, just return the datetime with the correct timezone\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # The fold attribute indicates whether we're in the first (fold=0) \n        # or second (fold=1) occurrence of the ambiguous time\n        # We use the fold value to determine which time to return\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 0, we return the first occurrence (standard time)\n        # If fold is 1, we return the second occurrence (daylight saving time)\n        # This is consistent with the behavior of the tzinfo implementation\n        return enfold(dt, fold=fold)\n    \n    # If not ambiguous, simply return the datetime with the correct offset\n    return dt.replace(tzinfo=self)"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 60209, "num_retrieved": 230, "inference_time": 9.704619094729424, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the ``tzinfo`` parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 19049, "num_retrieved": 67, "inference_time": 9.704619094729424, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72744, "num_retrieved": 242, "inference_time": 9.704619094729424, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return True\n    \n    # Check if the request has a private message (Diaspora private payload)\n    if request.headers.get('X-Diaspora-Private'):\n        return True\n    \n    # Check if the request has a legacy Diaspora signature\n    if request.headers.get('Signature'):\n        # Check if the signature is in the format expected by Diaspora\n        signature = request.headers.get('Signature')\n        if signature.startswith('diaspora'):\n            return True\n    \n    # Check if the request body contains a Diaspora-specific XML structure\n    body = request.body\n    if body and b'<XML' in body:\n        # Check for Diaspora-specific tags in the XML\n        if b'<status_message' in body or b'<profile' in body or b'<contact' in body:\n            return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return True\n    \n    # Check if the request has a private message (Diaspora private message)\n    if request.headers.get('X-Diaspora-Private'):\n        return True\n    \n    # Check if the request has a Diaspora-specific content type\n    content_type = request.headers.get('Content-Type', '')\n    if 'diaspora' in content_type.lower():\n        return True\n    \n    # Check if the request body contains a Diaspora-specific signature\n    body = request.body\n    if body and b'diaspora' in body:\n        return True\n    \n    # Check if the request has a Diaspora-specific header\n    if request.headers.get('Diaspora-Handle'):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('Content-Type') == 'application/diaspora+xml':\n        return True\n    \n    # Check if the request has a private message (Diaspora private payload)\n    if request.headers.get('Content-Type') == 'application/diaspora-private+xml':\n        return True\n    \n    # Check if the request has a legacy payload (Diaspora legacy format)\n    if request.body and b'<?xml' in request.body:\n        # Check for legacy XML structure\n        xml_start = request.body.find(b'<?xml')\n        if xml_start != -1:\n            xml_content = request.body[xml_start:]\n            if b'xmlns=\"http://salmon-protocol.org/ns/magic-env\"' in xml_content:\n                return True\n    \n    return False", "def identify_request(request: RequestType) -> str:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') == 'public':\n        return \"diaspora\"\n    \n    # Check if the request has a private message (private payload)\n    if request.headers.get('X-Diaspora-Message') == 'private':\n        return \"diaspora\"\n    \n    # Check for legacy payload indicators\n    # Legacy payloads often have specific headers or body patterns\n    if request.body and b\"magic-envelope\" in request.body:\n        return \"diaspora\"\n    \n    # Check for Diaspora-specific headers\n    if request.headers.get('X-Diaspora-Handle'):\n        return \"diaspora\"\n    \n    # Default to not being a Diaspora request\n    return \"\"", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return True\n    \n    # Check if the request has a private message (Diaspora private payload)\n    if request.headers.get('X-Diaspora-Private'):\n        return True\n    \n    # Check if the request has a Diaspora-specific content type\n    content_type = request.headers.get('Content-Type', '')\n    if 'application/diaspora+xml' in content_type or 'application/diaspora' in content_type:\n        return True\n    \n    # Check if the request has a Diaspora-specific signature\n    if request.headers.get('Signature') and 'diaspora' in request.headers.get('Signature', ''):\n        return True\n    \n    # Check if the request body contains a Diaspora-specific XML structure\n    body = request.body\n    if body and '<XML>' in body and '<post>' in body:\n        return True\n    \n    return False"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72730, "num_retrieved": 244, "inference_time": 9.704619094729424, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Check for common Matrix paths like /_matrix/client/r0\n    if '/_matrix/client/r0' in url:\n        return True\n    \n    # Check for Matrix-specific domains or subdomains\n    if 'matrix' in url.lower():\n        return True\n    \n    # Check if the request method is a Matrix-specific method\n    if request.method in ['POST', 'PUT', 'GET']:\n        # Matrix requests often use specific endpoints\n        if '/_matrix' in url or '/matrix' in url:\n            return True\n    \n    # Check headers for Matrix-specific headers\n    if request.headers.get('X-Matrix-Client-Id'):\n        return True\n    \n    # Check for Matrix-specific content types\n    content_type = request.headers.get('Content-Type', '')\n    if 'application/json' in content_type and '/_matrix' in url:\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Check for matrix-specific paths or domains\n    matrix_paths = [\n        '/_matrix/client/r0/',\n        '/_matrix/client/api/v1/',\n        '/_matrix/federation/v1/',\n        '/_matrix/media/r0/',\n        '/_matrix/client/r0/register',\n        '/_matrix/client/r0/rooms'\n    ]\n    \n    # Check if the URL contains any matrix paths\n    for path in matrix_paths:\n        if path in url:\n            return True\n    \n    # Check if the URL contains matrix domains or subdomains\n    matrix_domains = [\n        'matrix.',\n        'matrix.org',\n        'matrix.domain.tld'\n    ]\n    \n    for domain in matrix_domains:\n        if domain in url:\n            return True\n    \n    # Check if the request headers contain matrix-related information\n    if request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check if the request method is a matrix-specific method\n    if request.method in ['POST', 'GET']:\n        # Check for matrix-specific content types\n        content_type = request.headers.get('Content-Type', '')\n        if 'application/json' in content_type:\n            # Additional checks for matrix-specific patterns\n            if 'matrix' in content_type or 'm.' in content_type:\n                return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Check for common Matrix endpoints or domains\n    matrix_paths = [\n        '/_matrix/client/r0/',\n        '/_matrix/client/api/v1/',\n        '/_matrix/federation/v1/',\n        '/_matrix/media/r0/',\n        '/_matrix/client/r0/register',\n        '/_matrix/client/r0/login'\n    ]\n    \n    # Check if the URL contains any Matrix-specific paths\n    for path in matrix_paths:\n        if path in url:\n            return True\n    \n    # Check if the URL contains a matrix domain or subdomain\n    if 'matrix' in url.lower():\n        return True\n    \n    # Check if the request headers contain Matrix-specific information\n    if request.headers.get('X-Matrix-Server'):\n        return True\n    \n    # Check if the request method is one of the common Matrix API methods\n    if request.method in ['POST', 'GET', 'PUT']:\n        # Additional checks for Matrix-specific headers\n        if request.headers.get('Authorization') and 'Bearer' in request.headers.get('Authorization'):\n            # Check if the token is a Matrix appservice token\n            if 'appservice' in request.headers.get('Authorization', ''):\n                return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Matrix requests typically have specific paths or domains\n    # Check for common Matrix paths like /_matrix/client/r0\n    if '/_matrix/client/r0' in url or '/_matrix' in url:\n        return True\n    \n    # Check if the request method is a Matrix-specific method\n    if request.method in ['POST', 'PUT', 'GET']:\n        # Matrix requests often use specific endpoints\n        if '/_matrix/client/r0/register' in url or '/_matrix/client/r0/login' in url:\n            return True\n    \n    # Check if the request headers contain Matrix-specific headers\n    if request.headers.get('Authorization') and 'Bearer' in request.headers.get('Authorization'):\n        # Matrix often uses Bearer tokens\n        return True\n    \n    # Check if the request body contains Matrix-specific data\n    if request.body and 'access_token' in request.body.decode('utf-8', errors='ignore'):\n        return True\n    \n    # Check if the request URL contains a Matrix server domain\n    matrix_domains = ['matrix.org', 'matrix.dev', 'matrix.example.com']\n    for domain in matrix_domains:\n        if domain in url:\n            return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Check for common Matrix endpoints or paths\n    matrix_paths = [\n        '/_matrix', \n        '/_matrix/client', \n        '/_matrix/federation', \n        '/_matrix/client/api/v1/register',\n        '/_matrix/client/r0',\n        '/_matrix/media'\n    ]\n    \n    # Check if the URL contains any Matrix-specific paths\n    for path in matrix_paths:\n        if path in url:\n            return True\n    \n    # Check if the request headers contain Matrix-specific information\n    headers = request.headers\n    if headers.get('X-Matrix-Server') or headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check if the request method is one that Matrix typically uses\n    if request.method in ['POST', 'GET']:\n        # Check for Matrix-specific content types\n        content_type = headers.get('Content-Type', '')\n        if 'application/json' in content_type:\n            # Matrix often uses JSON with specific endpoints\n            return True\n    \n    return False"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72982, "num_retrieved": 251, "inference_time": 9.704619094729424, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if isinstance(dt, datetime.datetime):\n        return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")\n    elif isinstance(dt, datetime.date):\n        return dt.strftime(\"%Y-%m-%d\")\n    return \"\"", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if isinstance(dt, datetime.datetime):\n        return dt.strftime(\"%a, %d %b %Y %H:%M:%S GMT\")\n    elif isinstance(dt, datetime.date):\n        return dt.strftime(\"%Y-%m-%d\")\n    return \"\""]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72076, "num_retrieved": 218, "inference_time": 9.704619094729424, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result_text = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the code block and all following lines until the closing ``` \n            # We need to find the closing ``` \n            code_block_start = i\n            code_block_end = -1\n            for j in range(i + 1, len(lines)):\n                if lines[j].strip().startswith('```'):\n                    code_block_end = j\n                    break\n            # Skip all lines in the code block\n            if code_block_end != -1:\n                # Skip from code_block_start to code_block_end\n                continue\n                \n        # Find tags in the line\n        # Tags are in the format #tag\n        # We need to handle the case where tags are inside code blocks\n        # We'll use a simple regex to find tags\n        import re\n        tag_matches = re.finditer(r'#([a-zA-Z0-9_\\-]+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If a replacer is provided, replace the tag\n            if replacer:\n                result_text = result_text.replace(f'#{tag}', replacer(tag))\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    # Split text into lines\n    lines = text.splitlines()\n    result_lines = []\n    \n    for line in lines:\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Inside code block, skip processing\n            result_lines.append(line)\n            continue\n            \n        # Remove code block markers\n        line = line.strip()\n        \n        # Find all tags (words starting with #)\n        import re\n        tag_matches = re.finditer(r'#([a-zA-Z0-9_\\-]+)', line)\n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                replacement = replacer(tag)\n                line = line.replace(f'#{tag}', replacement)\n        \n        result_lines.append(line)\n    \n    # Join lines back into text\n    result_text = '\\n'.join(result_lines)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    # Pattern to match tags (e.g., #tag) that are not inside code blocks\n    # Code blocks are marked by ``` or ``` followed by language\n    code_block_pattern = r'```(?:\\w+)?\\s*'\n    \n    # Split the text into parts by code blocks\n    parts = []\n    current_part = \"\"\n    in_code_block = False\n    \n    i = 0\n    while i < len(text):\n        if not in_code_block:\n            # Check if we're starting a code block\n            if text[i:i+3] == '```':\n                in_code_block = True\n                current_part = \"\"\n                i += 3\n                # Skip the language if present\n                while i < len(text) and text[i] != '\\n':\n                    i += 1\n                continue\n            else:\n                # Add current character to part\n                current_part += text[i]\n                i += 1\n        else:\n            # Inside code block - skip until we see the closing ``` or end of text\n            if text[i:i+3] == '```':\n                in_code_block = False\n                parts.append(current_part)\n                current_part = \"\"\n                i += 3\n                continue\n            else:\n                current_part += text[i]\n                i += 1\n    \n    # Add the last part if not empty\n    if current_part:\n        parts.append(current_part)\n    \n    # Process each part to find tags\n    for part in parts:\n        # Find all tags in the part\n        tag_pattern = r'#([a-zA-Z0-9_]+)'\n        matches = re.finditer(tag_pattern, part, re.IGNORECASE)\n        for match in matches:\n            tag = match.group(1).lower()\n            tags.add(tag)\n            # Replace the tag with the replacer function if provided\n            if replacer:\n                replacement = replacer(tag)\n                result = result.replace(f\"#{tag}\", replacement)\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result_text = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip all lines until the closing code block\n            code_block_start = i\n            while i < len(lines) and not lines[i].strip().startswith('```'):\n                i += 1\n            continue\n            \n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip all lines until the closing code block\n            code_block_start = i\n            while i < len(lines) and not lines[i].strip().startswith('```'):\n                i += 1\n            continue\n            \n        # Process the line for tags\n        # Remove any code block markers\n        line = line.strip()\n        \n        # Find all tags (words starting with #)\n        import re\n        tag_matches = re.finditer(r'#([a-zA-Z0-9_\\-]+)', line)\n        \n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                replacement = replacer(tag)\n                result_text = result_text.replace(f'#{tag}', replacement)\n    \n    return tags, result_text", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the code block\n            continue\n            \n        # Check if line is inside a code block (starts with code block marker)\n        if line.strip().startswith('```'):\n            # Skip the code block\n            continue\n            \n        # Find tags in the line\n        # Use regex to find tags (e.g., #tag)\n        import re\n        tag_pattern = r'#(\\w+)(?=\\s|$)'\n        matches = re.finditer(tag_pattern, line)\n        \n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag\n            if replacer:\n                replacement = replacer(tag)\n                result = result.replace(f'#{tag}', replacement)\n    \n    return tags, result"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72487, "num_retrieved": 235, "inference_time": 9.704619094729424, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # Replace URLs in text with HTML links\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)]+)'  # Match URLs with protocol and domain\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace any other text that might be a link (e.g., plain text with no protocol)\n    # This is a fallback for cases where a link might be present without a protocol\n    plain_link_pattern = r'([a-zA-Z0-9][a-zA-Z0-9._-]*[a-zA-Z0-9])'\n    text = re.sub(plain_link_pattern, r'<a href=\"https://\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    return text.strip()", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs with HTML links\n    import re\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with HTML links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs in the text with HTML links\n    import re\n    # Match URLs (including http/https, www, and email addresses)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]]+|[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    \n    def replace_url(match):\n        url = match.group(0)\n        # Check if it's an email address\n        if '@' in url:\n            return f'<a href=\"mailto:{url}\" class=\"email\">{url}</a>'\n        else:\n            # It's a URL, add a target attribute for external links\n            return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\" class=\"url\">{url}</a>'\n    \n    # Replace URLs in the text\n    processed_text = re.sub(url_pattern, replace_url, text)\n    \n    return processed_text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs with HTML links\n    import re\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with HTML links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs with HTML links\n    import re\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with HTML links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    return text"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 74299, "num_retrieved": 288, "inference_time": 9.704619094729424, "generate_results": ["def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72647, "num_retrieved": 240, "inference_time": 9.704619094729424, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Tags should start with a letter or number and can contain letters, numbers, and underscores\n    # They should not contain spaces or special characters (except underscore)\n    if not tag[0].isalnum():\n        return False\n    \n    for char in tag[1:]:\n        if not char.isalnum() and char != '_':\n            return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should start with a letter or number and can contain letters, numbers, and underscores\n    # It should not start or end with a space or punctuation\n    # It should not contain consecutive spaces or punctuation\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Remove leading and trailing whitespace\n    tag = tag.strip()\n    \n    # Check if tag is empty after stripping\n    if not tag:\n        return False\n    \n    # Check if tag starts with a letter or number\n    if not (tag[0].isalpha() or tag[0].isdigit()):\n        return False\n    \n    # Check if tag ends with a letter or number\n    if not (tag[-1].isalpha() or tag[-1].isdigit()):\n        return False\n    \n    # Check for invalid characters (only allow letters, numbers, and underscores)\n    for char in tag:\n        if not (char.isalnum() or char == '_'):\n            return False\n    \n    # Check for consecutive spaces or punctuation\n    for i in range(len(tag) - 1):\n        if tag[i] in ' ' and tag[i + 1] in ' ':\n            return False\n        if tag[i] in '.,!?()[]{}\"\\'' and tag[i + 1] in '.,!?()[]{}\"\\'':\n            return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should start with a letter or number and can contain letters, numbers, and underscores\n    # It should not start or end with a space or punctuation\n    # It should not contain consecutive spaces or punctuation\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Remove leading and trailing whitespace\n    tag = tag.strip()\n    \n    # Check if tag is empty after stripping\n    if not tag:\n        return False\n    \n    # Check if tag starts with a letter or number\n    if not (tag[0].isalpha() or tag[0].isdigit()):\n        return False\n    \n    # Check if tag ends with a letter or number\n    if not (tag[-1].isalpha() or tag[-1].isdigit()):\n        return False\n    \n    # Check for valid characters (letters, numbers, underscores only)\n    for char in tag:\n        if not (char.isalnum() or char == '_'):\n            return False\n    \n    # Check for consecutive spaces or punctuation\n    for i in range(len(tag) - 1):\n        if tag[i] in ' ' and tag[i + 1] in ' ':\n            return False\n        if tag[i] in '.,!?()[]{}\"\\'' and tag[i + 1] in '.,!?()[]{}\"\\'':\n            return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # A tag is typically a word that starts with a hash symbol\n    if tag.startswith('#'):\n        # Remove the hash and check if the rest is valid\n        tag_content = tag[1:]\n        # Tag content should only contain alphanumeric characters and underscores\n        return tag_content.isalnum() or tag_content.replace('_', '').isalnum()\n    \n    # If no hash, it's not a valid tag\n    return False", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should start with a letter or number and can contain letters, numbers, and underscores\n    # It should not start or end with a space or punctuation\n    # It should not contain consecutive spaces or punctuation\n    if not tag or not isinstance(tag, str):\n        return False\n    \n    # Remove leading and trailing whitespace\n    tag = tag.strip()\n    \n    # Check if tag is empty after stripping\n    if not tag:\n        return False\n    \n    # Check if tag starts with a letter or number\n    if not (tag[0].isalpha() or tag[0].isdigit()):\n        return False\n    \n    # Check if tag ends with a letter or number\n    if not (tag[-1].isalpha() or tag[-1].isdigit()):\n        return False\n    \n    # Check for invalid characters (only allow letters, numbers, and underscores)\n    for char in tag:\n        if not (char.isalnum() or char == '_'):\n            return False\n    \n    # Check for consecutive spaces or punctuation\n    for i in range(len(tag) - 1):\n        if tag[i] in ' ' and tag[i + 1] in ' ':\n            return False\n        if tag[i] in '.,!?()[]{}\"\\'' and tag[i + 1] in '.,!?()[]{}\"\\'':\n            return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72622, "num_retrieved": 243, "inference_time": 10.915234610438347, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72290, "num_retrieved": 229, "inference_time": 10.915234610438347, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if not sender_handle or not entity_handle:\n        return False\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if not sender_handle or not entity_handle:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if not sender_handle or not entity_handle:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if not sender_handle or not entity_handle:\n        return False\n    return sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if not sender_handle or not entity_handle:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72597, "num_retrieved": 238, "inference_time": 10.915234610438347, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    doc = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/links\",\n                \"href\": f\"{url}{path}\"\n            }\n        ],\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Instance\",\n            \"adminAccount\": None\n        },\n        \"protocols\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": False,\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeHalfyear\": 0,\n                \"activeMonth\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        }\n    }\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"links\": [\n            {\n                \"rel\": \"self\",\n                \"href\": url + path\n            },\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/1.0\",\n                \"href\": url + path\n            }\n        ],\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Instance\",\n            \"adminAccount\": None\n        },\n        \"protocols\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": False,\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeHalfyear\": 0,\n                \"activeMonth\": 0,\n                \"activeWeek\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        }\n    }\n    \n    return document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    document_path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base NodeInfo document structure\n    nodeinfo_doc = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"metadata\": {\n            \"nodeName\": url.split(\"//\")[1].split(\"/\")[0],\n            \"adminAccount\": None\n        },\n        \"protocols\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": False,\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeHalfyear\": 0,\n                \"activeMonth\": 0,\n                \"activeWeek\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        },\n        \"links\": []\n    }\n    \n    # Add links to the document\n    base_url = url.rstrip(\"/\")\n    nodeinfo_doc[\"links\"].append({\n        \"rel\": \"self\",\n        \"href\": f\"{base_url}{document_path}\"\n    })\n    \n    # Add additional links if needed\n    if url.startswith(\"https\"):\n        nodeinfo_doc[\"links\"].append({\n            \"rel\": \"https\",\n            \"href\": f\"{base_url}/.well-known/https\"\n        })\n    \n    return nodeinfo_doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default document path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    document = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/links\",\n                \"href\": f\"{url}{path}\"\n            }\n        ],\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Instance\",\n            \"adminAccount\": None\n        },\n        \"protocols\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"services\": {\n            \"inbound\": [],\n            \"outbound\": []\n        },\n        \"openRegistrations\": False,\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeHalfyear\": 0,\n                \"activeMonth\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        }\n    }\n    \n    return document", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    doc = {\n        \"version\": \"1.0\",\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/links/1.0\",\n                \"href\": f\"{url}{path}\"\n            }\n        ],\n        \"metadata\": {\n            \"nodeName\": \"Diaspora Instance\",\n            \"adminAccount\": None\n        },\n        \"openRegistrations\": True,\n        \"protocols\": [\"activitypub\"],\n        \"services\": {\n            \"inbound\": [\"activitypub\"],\n            \"outbound\": [\"activitypub\"]\n        },\n        \"usage\": {\n            \"users\": {\n                \"total\": 0,\n                \"activeMonth\": 0,\n                \"activeWeek\": 0,\n                \"activeHalfyear\": 0\n            },\n            \"localPosts\": 0,\n            \"localComments\": 0\n        }\n    }\n    \n    return doc"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72563, "num_retrieved": 232, "inference_time": 10.915234610438347, "generate_results": ["def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise SignatureVerificationError(\"No data element found in the document\")\n            \n        data_text = data.text\n        if not data_text:\n            raise SignatureVerificationError(\"Data element is empty\")\n            \n        # Create the signature content\n        sig_contents = f\"{data_text}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Import the public key\n        key = RSA.importKey(public_key)\n        \n        # Verify the signature\n        cipher = PKCS1_v1_5.new(key)\n        sig = urlsafe_b64decode(signature)\n        \n        if not cipher.verify(sig_hash, sig):\n            raise SignatureVerificationError(\"Signature verification failed\")\n            \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise ValueError(\"Data element not found in the document\")\n            \n        data_text = data.text\n        if not data_text:\n            raise ValueError(\"Data element is empty\")\n            \n        # Create the signature content\n        sig_contents = f\"{data_text}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Import the public key\n        pk = RSA.import_key(public_key)\n        \n        # Verify the signature\n        cipher = PKCS1_v1_5.new(pk)\n        sig_bytes = urlsafe_b64decode(signature)\n        \n        try:\n            cipher.verify(sig_hash, sig_bytes)\n            return True\n        except (ValueError, TypeError):\n            return False\n            \n    except Exception as e:\n        logger.warning(f\"Signature verification failed: {e}\")\n        return False", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise ValueError(\"No data element found in the document\")\n            \n        data_text = data.text\n        if not data_text:\n            raise ValueError(\"Data element is empty\")\n            \n        # Create the signature content\n        sig_contents = f\"{data_text}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Import the public key\n        pk = RSA.importKey(public_key)\n        \n        # Verify the signature\n        cipher = PKCS1_v1_5.new(pk)\n        sig_bytes = urlsafe_b64decode(signature)\n        \n        if not cipher.verify(sig_hash, sig_bytes):\n            raise SignatureVerificationError(\"Signature cannot be verified using the given public key\")\n            \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise ValueError(\"Data element not found in the document\")\n            \n        data_text = data.text\n        \n        # Create the signature content\n        sig_contents = f\"{data_text}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Create a SHA256 hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Import the public key\n        key = RSA.importKey(public_key)\n        \n        # Create a PKCS1 v1.5 verifier\n        verifier = pkcs1_15.new(key)\n        \n        # Decode the signature\n        sig = urlsafe_b64decode(signature)\n        \n        # Verify the signature\n        verifier.verify(sig_hash, sig)\n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise ValueError(\"Data element not found in XML document\")\n            \n        data_text = data.text\n        if not data_text:\n            raise ValueError(\"Data element is empty\")\n            \n        # Create the signature content\n        sig_contents = f\"{data_text}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Import the public key\n        pk = RSA.importKey(public_key)\n        \n        # Verify the signature\n        cipher = PKCS1_v1_5.new(pk)\n        sig_bytes = urlsafe_b64decode(signature)\n        cipher.verify(sig_hash, sig_bytes)\n        \n        return True\n        \n    except (ValueError, InvalidSignature) as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72329, "num_retrieved": 229, "inference_time": 10.915234610438347, "generate_results": ["def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Check if it's a JSON format webfinger\n        if \"subject\" in doc and \"aliases\" in doc and \"links\" in doc:\n            return {\n                \"subject\": doc[\"subject\"],\n                \"aliases\": doc[\"aliases\"],\n                \"links\": doc[\"links\"]\n            }\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD\n    try:\n        from lxml import etree\n        root = etree.fromstring(document)\n        \n        # Extract subject\n        subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n        subject = None\n        if subject_element is not None:\n            subject = subject_element.text\n        \n        # Extract aliases\n        aliases = []\n        alias_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")\n        for alias in alias_elements:\n            aliases.append(alias.text)\n        \n        # Extract links\n        links = []\n        link_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\")\n        for link in link_elements:\n            link_dict = {\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            }\n            links.append(link_dict)\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except Exception:\n        pass\n    \n    # If parsing fails, return empty dict\n    return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Check if it's a JSON format webfinger\n        if \"subject\" in doc and \"aliases\" in doc and \"links\" in doc:\n            return {\n                \"subject\": doc[\"subject\"],\n                \"aliases\": doc[\"aliases\"],\n                \"links\": doc[\"links\"]\n            }\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD\n    try:\n        from lxml import etree\n        root = etree.fromstring(document)\n        \n        # Extract subject\n        subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n        subject = None\n        if subject_element is not None:\n            subject = subject_element.text\n        \n        # Extract aliases\n        aliases = []\n        alias_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")\n        for alias in alias_elements:\n            aliases.append(alias.text)\n        \n        # Extract links\n        links = []\n        link_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\")\n        for link in link_elements:\n            link_dict = {\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            }\n            links.append(link_dict)\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except Exception:\n        pass\n    \n    # If parsing fails, return empty dict\n    return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Check if it's a JSON format webfinger document\n        if \"subject\" in doc and \"aliases\" in doc and \"links\" in doc:\n            return {\n                \"subject\": doc[\"subject\"],\n                \"aliases\": doc[\"aliases\"],\n                \"links\": doc[\"links\"]\n            }\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD (old format)\n    try:\n        from lxml import etree\n        root = etree.fromstring(document)\n        \n        # Extract subject\n        subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n        subject = None\n        if subject_element is not None:\n            subject = subject_element.text\n        \n        # Extract aliases\n        aliases = []\n        alias_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")\n        for alias in alias_elements:\n            aliases.append(alias.text)\n        \n        # Extract links\n        links = []\n        link_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\")\n        for link in link_elements:\n            link_dict = {\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            }\n            links.append(link_dict)\n        \n        if subject is not None:\n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n    except Exception:\n        pass\n    \n    # If parsing fails, return empty dict\n    return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        # Try to parse as JSON first (new format)\n        data = json.loads(document)\n        if 'subject' in data:\n            return {\n                'subject': data['subject'],\n                'aliases': data.get('aliases', []),\n                'links': data.get('links', [])\n            }\n        elif 'Subject' in data:\n            # Handle XRD format (old format)\n            subject = data.get('Subject', {}).get('text', '')\n            aliases = []\n            links = []\n            \n            # Extract aliases from elements\n            for elem in data.get('elements', []):\n                if elem.tag == 'Alias':\n                    aliases.append(elem.text)\n            \n            # Extract links from links\n            for link in data.get('links', []):\n                if link.tag == 'Link':\n                    links.append({\n                        'rel': link.get('rel'),\n                        'type': link.get('type'),\n                        'href': link.get('href')\n                    })\n            \n            return {\n                'subject': subject,\n                'aliases': aliases,\n                'links': links\n            }\n    except json.JSONDecodeError:\n        pass\n    \n    # Fallback to parse as XRD (old format)\n    try:\n        from lxml import etree\n        root = etree.fromstring(document)\n        \n        subject = None\n        aliases = []\n        links = []\n        \n        # Extract subject\n        for elem in root.xpath(\"//Subject\"):\n            subject = elem.get('text')\n        \n        # Extract aliases\n        for elem in root.xpath(\"//Alias\"):\n            aliases.append(elem.get('text'))\n        \n        # Extract links\n        for elem in root.xpath(\"//Link\"):\n            links.append({\n                'rel': elem.get('rel'),\n                'type': elem.get('type'),\n                'href': elem.get('href')\n            })\n        \n        return {\n            'subject': subject,\n            'aliases': aliases,\n            'links': links\n        }\n    \n    except Exception:\n        # If parsing fails, return empty dict\n        pass\n    \n    return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        import json\n        doc = json.loads(document)\n        # Check if it's a JSON format webfinger document\n        if \"subject\" in doc and \"aliases\" in doc and \"links\" in doc:\n            return {\n                \"subject\": doc[\"subject\"],\n                \"aliases\": doc[\"aliases\"],\n                \"links\": doc[\"links\"]\n            }\n    except json.JSONDecodeError:\n        pass\n    \n    # If not JSON, try to parse as XRD (old format)\n    try:\n        from lxml import etree\n        root = etree.fromstring(document)\n        \n        # Extract subject\n        subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n        subject = None\n        if subject_element is not None:\n            subject = subject_element.text\n        \n        # Extract aliases\n        aliases = []\n        alias_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")\n        for alias in alias_elements:\n            aliases.append(alias.text)\n        \n        # Extract links\n        links = []\n        link_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\")\n        for link in link_elements:\n            link_dict = {\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            }\n            links.append(link_dict)\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except Exception:\n        pass\n    \n    # If parsing fails, return empty dict\n    return {}"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72758, "num_retrieved": 243, "inference_time": 10.915234610438347, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the provided handle\n        # The handle is expected to be in the format username@domain.tld\n        # The webfinger endpoint typically follows the pattern: https://domain.tld/.well-known/webfinger\n        domain = handle.split(\"@\")[1]\n        webfinger_url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document from the webfinger endpoint\n        document, status_code, exception = fetch_document(webfinger_url)\n        \n        # If document is retrieved successfully, return it\n        if document:\n            return document\n            \n        # If no document is found or an error occurs, return None\n        return None\n        \n    except Exception:\n        # Return None if any error occurs during retrieval\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the provided handle\n        # The handle is expected to be in the format username@domain.tld\n        # The webfinger endpoint is typically at https://domain.tld/.well-known/webfinger\n        domain = handle.split(\"@\")[1]\n        url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document using the network utility\n        document, status_code, exception = fetch_document(url)\n        \n        # If successful, return the document content\n        if document and status_code == 200:\n            return document\n            \n        # If the fetch fails or returns an error, return None\n        return None\n        \n    except Exception:\n        # Do not raise if the fetch fails - this is consistent with the docstring\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the provided handle\n        # The handle is expected to be in the format username@domain.tld\n        # We'll use the fetch_document function from the utils.network module\n        # to retrieve the document, and return the content if successful\n        document, status_code, exception = fetch_document(f\"https://federated-identity.org/.well-known/webfinger?resource=acct:{handle}\")\n        \n        # If we successfully retrieved a document, return it\n        if document:\n            return document\n            \n        # If we didn't get a document, return None\n        return None\n        \n    except Exception as e:\n        # If there's any error during the retrieval, return None without raising\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the provided handle\n        # The handle is expected to be in the format username@domain.tld\n        # We'll use the fetch_document function from the utils.network module\n        # to retrieve the document, and return the content if successful\n        document, status_code, exception = fetch_document(f\"https://federated-identity.org/.well-known/webfinger?resource={handle}\")\n        if document:\n            return document\n        return None\n    except Exception:\n        # Return None if any error occurs during retrieval\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the provided handle\n        # The handle is expected to be in the format username@domain.tld\n        # We'll use a network fetch to retrieve the document\n        from federation.utils.network import fetch_document\n        \n        # Construct the webfinger URL based on the handle\n        # RFC7033 webfinger typically uses the format: https://domain.tld/.well-known/webfinger\n        # But we need to determine the correct endpoint based on the handle\n        # For now, we'll use a generic approach to fetch the document\n        domain = handle.split(\"@\")[1]\n        webfinger_url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, status_code, exception = fetch_document(webfinger_url)\n        \n        # Return the document if successfully retrieved\n        if document:\n            return document\n            \n        # If no document is found, return None\n        return None\n        \n    except Exception as e:\n        # Do not raise if the fetch fails - this is consistent with the docstring\n        # Return None in case of any error\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72918, "num_retrieved": 246, "inference_time": 10.915234610438347, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Create a webfinger object with the given handle\n    webfinger = RFC7033Webfinger(\n        id=None,\n        handle=handle,\n        guid=None,\n        base_url=None,\n        profile_path=None,\n        hcard_path=None,\n        atom_path=None,\n        search_path=None\n    )\n    \n    # Generate the webfinger document\n    document = webfinger.render()\n    \n    # Parse the document as JSON\n    try:\n        return json.loads(document)\n    except json.JSONDecodeError:\n        return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Create a webfinger object with the given handle\n    webfinger = RFC7033Webfinger(\n        id=None,\n        handle=handle,\n        guid=None,\n        base_url=None,\n        profile_path=None,\n        hcard_path=None,\n        atom_path=None,\n        search_path=None\n    )\n    \n    # Generate the webfinger document\n    document = webfinger.render()\n    \n    # Parse the document as JSON\n    try:\n        return json.loads(document)\n    except json.JSONDecodeError:\n        return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Create a webfinger document using the provided handle\n    webfinger = RFC7033Webfinger(\n        id=None,\n        handle=handle,\n        guid=None,\n        base_url=None,\n        profile_path=None,\n        hcard_path=None,\n        atom_path=None,\n        search_path=None\n    )\n    \n    # Generate the webfinger document\n    webfinger_doc = webfinger.render()\n    \n    # Parse the webfinger document\n    try:\n        return json.loads(webfinger_doc)\n    except json.JSONDecodeError:\n        return None", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Create a webfinger object with the given handle\n    webfinger = RFC7033Webfinger(\n        id=None,\n        handle=handle,\n        guid=None,\n        base_url=None,\n        profile_path=None,\n        hcard_path=None,\n        atom_path=None,\n        search_path=None\n    )\n    \n    # Generate the webfinger document\n    webfinger_document = webfinger.render()\n    \n    # Parse the webfinger document\n    try:\n        document = json.loads(webfinger_document)\n    except json.JSONDecodeError:\n        return None\n    \n    return document", "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Create a webfinger document using the provided handle\n    webfinger = RFC7033Webfinger(\n        id=None,\n        handle=handle,\n        guid=None,\n        base_url=None,\n        profile_path=None,\n        hcard_path=None,\n        atom_path=None,\n        search_path=None\n    )\n    \n    # Generate the webfinger document\n    webfinger_doc = webfinger.render()\n    \n    # Parse the webfinger document as JSON\n    try:\n        return json.loads(webfinger_doc)\n    except json.JSONDecodeError:\n        return None"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 74898, "num_retrieved": 307, "inference_time": 10.915234610438347, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    template = \"diaspora\"\n    webfinger_host = host\n    kwargs = {\n        \"webfinger_host\": webfinger_host\n    }\n    return generate_host_meta(template=template, **kwargs)", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    template = \"diaspora\"\n    webfinger_host = host\n    kwargs = {\n        \"webfinger_host\": webfinger_host\n    }\n    return generate_host_meta(template=template, **kwargs)", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    template = \"diaspora\"\n    webfinger_host = host\n    kwargs = {\n        \"webfinger_host\": webfinger_host\n    }\n    return generate_host_meta(template=template, **kwargs)", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    template = \"diaspora\"\n    webfinger_host = host\n    kwargs = {\n        \"webfinger_host\": webfinger_host\n    }\n    return generate_host_meta(template=template, **kwargs)", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    template = \"diaspora\"\n    webfinger_host = host\n    kwargs = {\n        \"webfinger_host\": webfinger_host\n    }\n    return generate_host_meta(template=template, **kwargs)"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72334, "num_retrieved": 226, "inference_time": 10.915234610438347, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.request(\n            method=method,\n            url=url,\n            data=data,\n            timeout=timeout,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71721, "num_retrieved": 188, "inference_time": 10.915234610438347, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual components\n    components = [key] + list(keys)\n    \n    for i, component in enumerate(components):\n        if i == len(components) - 1:\n            # Last component - assign the value\n            current[component] = val\n        else:\n            # Intermediate component - create nested dictionary if it doesn't exist\n            if component not in current:\n                current[component] = {}\n            current = current[component]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Handle the first key\n    if key not in current:\n        current[key] = {}\n    current = current[key]\n    \n    # Handle the remaining keys\n    for k in keys:\n        if k not in current:\n            current[k] = {}\n        current = current[k]\n    \n    # Set the final value\n    current[val] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual components\n    components = [key] + list(keys)\n    \n    for i, component in enumerate(components):\n        if i == len(components) - 1:\n            # Last component - assign the value\n            current[component] = val\n        else:\n            # Intermediate component - create nested dictionary if it doesn't exist\n            if component not in current:\n                current[component] = {}\n            current = current[component]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into a list of keys\n    key_parts = [key] + list(keys)\n    \n    for i, k in enumerate(key_parts):\n        if i < len(key_parts) - 1:\n            # If not the last key, create a new dictionary if it doesn't exist\n            if k not in current:\n                current[k] = {}\n            current = current[k]\n        else:\n            # If it's the last key, set the value\n            current[k] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    # Split the key and keys into individual components\n    components = [key] + list(keys)\n    \n    for i, component in enumerate(components):\n        if i == len(components) - 1:\n            # Last component - assign the value\n            current[component] = val\n        else:\n            # Intermediate component - create nested dictionary if needed\n            if component not in current:\n                current[component] = {}\n            current = current[component]"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71527, "num_retrieved": 182, "inference_time": 10.915234610438347, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(arg_name=spec_option.get('name'), \n                          settings_dirs=settings_dirs, \n                          spec_option=spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(arg_name=spec_option.get('name'), \n                          settings_dirs=settings_dirs, \n                          spec_option=spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(arg_name=spec_option.get('name'), \n                          settings_dirs=settings_dirs, \n                          spec_option=spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(arg_name=spec_option.get('name'), \n                          settings_dirs=settings_dirs, \n                          spec_option=spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    return ListOfFileNames(arg_name=spec_option.get('name'), \n                          settings_dirs=settings_dirs, \n                          spec_option=spec_option)"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73837, "num_retrieved": 253, "inference_time": 10.915234610438347, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the class context and instance state\n    # In a real implementation, this would return an instance of AnsibleConfigManager\n    # that is properly initialized with the infrared home directory\n    return AnsibleConfigManager(cls.infrared_home)", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Assuming this is a class method that returns an instance of AnsibleConfigManager\n    # The actual implementation would depend on the class context and how the manager is instantiated\n    # Since the context does not provide a direct instantiation method, we return a placeholder\n    # In a real implementation, this would likely return a singleton or instance of AnsibleConfigManager\n    return AnsibleConfigManager(cls.infrared_home)", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the class context and instance state.\n    # In a real scenario, this would return an instance of AnsibleConfigManager.\n    return AnsibleConfigManager()", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the class context and the specific setup\n    # In a real scenario, this would return an instance of AnsibleConfigManager\n    # that is properly initialized with the infrared home directory\n    return AnsibleConfigManager(cls.infrared_home)", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the class context and the specific setup\n    # In a real scenario, this would return an instance of AnsibleConfigManager\n    # that is properly initialized with the infrared home directory\n    return AnsibleConfigManager(cls.infrared_home)"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73446, "num_retrieved": 243, "inference_time": 10.915234610438347, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or initialized once\n    # and stored in a class-level attribute, we return it\n    if not hasattr(cls, '_workspace_manager'):\n        # Initialize workspace manager with default workspaces base directory\n        # This is a simplified implementation based on context\n        from infrared.core.services import CoreServices\n        cls._workspace_manager = CoreServices.workspace_manager()\n    return cls._workspace_manager", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and state management\n    # In a real implementation, this would return an instance of WorkspaceManager\n    # that manages workspaces based on the application's configuration\n    return WorkspaceManager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton and should be initialized\n    # based on the context provided in the retrieved files\n    # The implementation would typically return an instance of WorkspaceManager\n    # that is already initialized in the CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and state management\n    # In a real implementation, this would return an instance of WorkspaceManager\n    # that manages workspaces based on the application's configuration\n    return WorkspaceManager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or cached instance\n    # based on the context provided, we return a placeholder implementation\n    # In a real scenario, this would return the actual workspace manager instance\n    # For now, we return a mock implementation that would be replaced with the actual manager\n    from infrared.core.services import CoreServices\n    return CoreServices.workspace_manager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 73911, "num_retrieved": 257, "inference_time": 10.915234610438347, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class and its context\n    # In the context of the retrieved code, this would typically return\n    # an instance of InfraredPluginManager or a similar class\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class and its context.\n    # In a real implementation, this would return an instance of the\n    # plugin manager class that is registered in the CoreServices.\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class and its context.\n    # In a real scenario, this would return an instance of the plugin manager.\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class and its context\n    # In a real scenario, this would return an instance of the plugin manager\n    # based on the class's internal state or configuration\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific class context and how the plugin manager\n    # is instantiated or retrieved in the system.\n    # In a real implementation, this would return an instance of the plugin manager.\n    return CoreServices.plugins_manager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71673, "num_retrieved": 186, "inference_time": 10.915234610438347, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise exceptions.IRValidatorException(\"Problem with loading spec file. %s\" % err)\n\n    # Validate required fields (example structure - adjust based on actual spec requirements)\n    required_fields = ['name', 'description', 'plugin_type', 'subparsers']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n    \n    # Additional validation can be added here based on specific requirements\n    # For example, validate subparsers structure, required options, etc.\n    \n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise exceptions.IRValidatorException(f\"Error parsing YAML content: {err}\")\n\n    # Validate required fields (example structure - adjust based on actual spec requirements)\n    required_fields = ['name', 'description', 'plugin_type', 'subparsers']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n    \n    # Additional validation can be added here based on specific requirements\n    # For example, validate subparsers structure, required options, etc.\n    \n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise cls.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise cls.IRValidatorException(f\"Error parsing YAML content: {err}\")\n\n    # Validate required fields - adjust based on actual required fields in spec\n    required_fields = ['subparsers', 'config', 'plugin_type']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise cls.IRValidatorException(f\"Missing required field: {field}\")\n    \n    # Additional validation can be added here based on specific requirements\n    # For example, validate that subparsers has at least one parser, etc.\n    \n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise exceptions.IRValidatorException(f\"Error parsing YAML content: {err}\")\n\n    # Validate required fields (example structure - adjust based on actual spec requirements)\n    required_fields = ['name', 'description', 'plugin_type', 'subparsers']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n    \n    # Additional validation can be added here based on specific requirements\n    # For example, validate subparsers structure, required options, etc.\n    \n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise exceptions.IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as err:\n        raise exceptions.IRValidatorException(\"Problem with loading spec file. %s\" % err)\n\n    # Validate required fields (example structure - adjust based on actual spec requirements)\n    required_fields = ['name', 'description', 'plugin_type', 'subparsers']\n    \n    for field in required_fields:\n        if field not in spec_data:\n            raise exceptions.IRValidatorException(f\"Missing required field: {field}\")\n    \n    # Additional validation can be added here based on specific requirements\n    # For example, validate subparsers structure, required options, etc.\n    \n    return spec_data"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72126, "num_retrieved": 198, "inference_time": 10.915234610438347, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise exceptions.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise exceptions.IRValidatorException(f\"File not found: {yaml_file}\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise exceptions.IRValidatorException(\"YAML file is empty or invalid\")\n            return data\n    except yaml.YAMLError as e:\n        raise exceptions.IRValidatorException(f\"Invalid YAML format in file {yaml_file}: {str(e)}\")\n    except Exception as e:\n        raise exceptions.IRValidatorException(f\"Error reading YAML file {yaml_file}: {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise cls.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise cls.IRValidatorException(f\"File {yaml_file} does not exist or is not a file\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise cls.IRValidatorException(\"YAML file is empty or contains invalid content\")\n            return data\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Error parsing YAML file: {str(e)}\")\n    except Exception as e:\n        raise cls.IRValidatorException(f\"Error loading YAML file: {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise ValueError(\"yaml_file parameter is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise exceptions.IRFileNotFoundException([yaml_file])\n\n    with open(yaml_file, 'r') as file:\n        try:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise exceptions.IRValidatorException(\"YAML file is empty\")\n            return data\n        except yaml.YAMLError as e:\n            raise exceptions.IRValidatorException(f\"Invalid YAML format in file: {yaml_file}. Error: {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise cls.IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise cls.IRValidatorException(f\"File {yaml_file} does not exist or is not a file\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise cls.IRValidatorException(f\"YAML file {yaml_file} is empty or contains invalid content\")\n            return data\n    except yaml.YAMLError as e:\n        raise cls.IRValidatorException(f\"Error parsing YAML file {yaml_file}: {str(e)}\")\n    except Exception as e:\n        raise cls.IRValidatorException(f\"Error loading YAML file {yaml_file}: {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise ValueError(\"yaml_file parameter is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise exceptions.IRFileNotFoundException([yaml_file])\n\n    with open(yaml_file, 'r') as file:\n        try:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise exceptions.IRValidatorException(\"YAML file is empty\")\n            return data\n        except yaml.YAMLError as e:\n            raise exceptions.IRValidatorException(f\"Invalid YAML format in file {yaml_file}: {str(e)}\")"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71741, "num_retrieved": 189, "inference_time": 10.1375852227211, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is intended to handle the inclusion of groups from spec files\n    # However, based on the provided context, there is no direct implementation\n    # of this specific function. The context shows functions related to shared groups\n    # and group management, but not the specific implementation of _include_groups.\n    \n    # Since the context does not provide the implementation for _include_groups,\n    # and given the function's purpose, we can infer that it likely processes\n    # group inclusion directives from the spec files and integrates them into\n    # the parser dictionary.\n    \n    # Placeholder implementation based on common patterns in such systems:\n    # 1. Extract group definitions from the parser_dict\n    # 2. Process any include directives (e.g., include: group_name)\n    # 3. Merge included groups into the existing parser_dict\n    \n    # Extract groups from the parser_dict if they exist\n    groups = parser_dict.get('groups', [])\n    \n    # Process any include directives (this is a placeholder - actual implementation\n    # would depend on the spec file format and how includes are defined)\n    for group in groups:\n        if 'include' in group:\n            # This would typically resolve the included group name and merge it\n            # into the parser_dict or create a new group\n            include_name = group['include']\n            # Placeholder: add the included group to the parser_dict\n            # In a real implementation, this would involve loading the group spec\n            # and merging its options into the current parser_dict\n            parser_dict.setdefault('included_groups', []).append(include_name)\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is intended to handle the inclusion of groups from spec files\n    # However, based on the provided context, there is no direct implementation\n    # of this specific method. The context shows various group-related functions\n    # like add_shared_groups, _add_groups, and group_list, but none of them\n    # directly implement _include_groups.\n    \n    # Since the context does not provide a clear implementation for this method,\n    # and given that the function is likely meant to process group inclusion\n    # directives in spec files, we implement a placeholder that would typically\n    # be used to merge or process included groups.\n    \n    # Placeholder implementation - in a real scenario, this would parse\n    # include directives and merge them with existing groups\n    included_groups = parser_dict.get('include', {})\n    \n    # If there are included groups, we would typically merge them with\n    # existing groups in the parser\n    if included_groups:\n        # Example: merge included groups with existing groups\n        # This would depend on the actual structure of the spec files\n        for group_name, group_config in included_groups.items():\n            # Add the group to the parser with its configuration\n            # This is a simplified version - actual implementation would\n            # depend on the specific requirements of the spec files\n            pass\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is intended to handle the inclusion of groups from spec files\n    # However, based on the provided context, there is no direct implementation\n    # of this specific function. The context shows functions related to shared groups\n    # and group management, but not the specific implementation of _include_groups.\n    \n    # Since the context does not provide the implementation for _include_groups,\n    # and assuming it should follow the pattern of other group-related functions,\n    # we can infer that it might be responsible for merging or expanding groups\n    # from a dictionary of groups into the parser dictionary.\n    \n    # Placeholder implementation based on similar functions in the context:\n    # The add_shared_groups function expands a list of groups into the spec_dict\n    # This function might perform a similar operation for included groups.\n    \n    # If there's an 'include' directive in the spec, we would process it here\n    # For now, we return the parser_dict as-is since no specific implementation\n    # is found in the context.\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is intended to handle the inclusion of groups from shared groups\n    # or other sources as defined in the spec files.\n    # Based on the context, particularly the add_shared_groups method which handles\n    # shared groups, we can infer that this function should process group inclusions.\n    \n    # Extract shared groups from the spec dictionary\n    shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n    \n    # Process any include directives in the parser_dict\n    # This would typically involve merging shared groups into the parser_dict\n    # or applying group inclusion rules as defined in the spec.\n    \n    # Since the exact implementation details are not provided in the context,\n    # we implement a placeholder that would integrate with the existing shared groups\n    # functionality.\n    \n    # Add shared groups to the parser_dict if they exist\n    if shared_groups:\n        for group in shared_groups:\n            # Assuming shared groups are added as a list of group names\n            # This would be expanded based on the actual spec format\n            if isinstance(group, dict):\n                # Handle dictionary-based group definitions\n                for key, value in group.items():\n                    parser_dict[key] = value\n            else:\n                # Handle simple group names\n                parser_dict[group] = True\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is intended to handle the inclusion of groups from spec files\n    # However, based on the provided context, there is no direct implementation\n    # of this specific function. The context shows functions related to shared groups\n    # and group management, but not the specific implementation of _include_groups.\n    \n    # Since the context does not provide the implementation for _include_groups,\n    # and based on the naming and typical behavior in such systems, we can infer\n    # that this function would process a dictionary of groups to be included\n    # and integrate them into the parser dictionary.\n    \n    # The implementation would likely involve:\n    # 1. Processing the include directive from the spec file\n    # 2. Resolving group references\n    # 3. Merging the included groups into the parser_dict\n    \n    # As no specific context is provided for this function, we return an empty\n    # implementation that could be extended based on the actual requirements.\n    \n    # Placeholder implementation - actual implementation would depend on the\n    # specific requirements and context not provided in the retrieved context.\n    pass"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72955, "num_retrieved": 226, "inference_time": 10.1375852227211, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the spec dictionary from the plugin specification\n    spec_dict = self.specification.spec_dict\n    \n    # Process the main parser options\n    for option_name, option_spec in spec_dict.get('options', {}).items():\n        # Resolve the value based on the option specification\n        if 'default' in option_spec:\n            result[option_name] = option_spec['default']\n    \n    # Process subparsers options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        for option_name, option_spec in subparser_dict.get('options', {}).items():\n            if 'default' in option_spec:\n                result[option_name] = option_spec['default']\n    \n    # Process shared groups options\n    for group in spec_dict.get('shared_groups', []):\n        for option_name, option_spec in group.get('options', {}).items():\n            if 'default' in option_spec:\n                result[option_name] = option_spec['default']\n    \n    return result", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary from the plugin\n    spec_dict = self.specification.spec_dict\n    \n    # Process the main parser options\n    for option_name, option_spec in spec_dict.get('options', {}).items():\n        # Get the default value from the option specification\n        if 'default' in option_spec:\n            result[option_name] = option_spec['default']\n    \n    # Process subparsers options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        for option_name, option_spec in subparser_dict.get('options', {}).items():\n            if 'default' in option_spec:\n                result[option_name] = option_spec['default']\n    \n    # Process shared groups options\n    for group in spec_dict.get('shared_groups', []):\n        for option_name, option_spec in group.get('options', {}).items():\n            if 'default' in option_spec:\n                result[option_name] = option_spec['default']\n    \n    # Resolve values using the plugin's settings\n    if hasattr(self, 'settings_dirs') and self.settings_dirs:\n        # Use settings directories to resolve values\n        for option_name, option_spec in result.items():\n            # Apply any value resolution logic based on settings\n            if 'type' in option_spec:\n                # Handle type-specific resolution\n                if option_spec['type'] == 'list':\n                    result[option_name] = option_spec.get('default', [])\n                elif option_spec['type'] == 'dict':\n                    result[option_name] = option_spec.get('default', {})\n    \n    return result", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary\n    spec_dict = self.specification.spec_dict\n    \n    # Process the main specification\n    for key, value in spec_dict.items():\n        if key == 'subparsers':\n            continue\n            \n        # Handle nested dictionaries and lists\n        if isinstance(value, dict):\n            # Process nested dictionaries\n            for sub_key, sub_value in value.items():\n                if isinstance(sub_value, dict):\n                    # Recursively process nested dictionaries\n                    result[sub_key] = self._resolve_nested_dict(sub_value)\n                else:\n                    result[sub_key] = sub_value\n        else:\n            result[key] = value\n    \n    # Add any default values from the plugin specification\n    if hasattr(self, 'plugin') and self.plugin:\n        plugin_spec = self.plugin.specification\n        if plugin_spec:\n            for key, value in plugin_spec.items():\n                if key not in result:\n                    result[key] = value\n    \n    return result\n\ndef _resolve_nested_dict(self, nested_dict):\n    \"\"\"Helper method to resolve nested dictionary values.\"\"\"\n    result = {}\n    for key, value in nested_dict.items():\n        if isinstance(value, dict):\n            result[key] = self._resolve_nested_dict(value)\n        else:\n            result[key] = value\n    return result", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary\n    spec_dict = self.specification.spec_dict\n    \n    # Process the main parser options\n    for option_name, option_spec in spec_dict.get('options', {}).items():\n        # Resolve the value based on the option specification\n        if 'default' in option_spec:\n            result[option_name] = option_spec['default']\n    \n    # Process subparsers options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        for option_name, option_spec in subparser_dict.get('options', {}).items():\n            if 'default' in option_spec:\n                result[option_name] = option_spec['default']\n    \n    # Process shared groups options\n    for group in spec_dict.get('shared_groups', []):\n        for option_name, option_spec in group.get('options', {}).items():\n            if 'default' in option_spec:\n                result[option_name] = option_spec['default']\n    \n    return result", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary\n    spec_dict = self.specification.spec_dict\n    \n    # Process the main specification\n    for key, value in spec_dict.items():\n        if key == 'subparsers':\n            continue\n            \n        # Handle nested values\n        if isinstance(value, dict):\n            # Process nested dictionaries\n            for nested_key, nested_value in value.items():\n                if isinstance(nested_value, dict):\n                    # Handle nested dictionaries\n                    result[nested_key] = nested_value\n                else:\n                    # Handle simple values\n                    result[nested_key] = nested_value\n        else:\n            # Handle simple values\n            result[key] = value\n    \n    # Add any default values from the plugin configuration\n    plugin_config = self.plugin.config\n    if plugin_config:\n        for key, value in plugin_config.items():\n            if key not in result:\n                result[key] = value\n    \n    # Add any default values from the plugin type\n    plugin_type = self.plugin.type\n    if plugin_type:\n        # Look for defaults in the plugin type configuration\n        # This is a placeholder - actual implementation would depend on the specific plugin type\n        pass\n    \n    return result"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72731, "num_retrieved": 219, "inference_time": 10.1375852227211, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72155, "num_retrieved": 203, "inference_time": 10.1375852227211, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # In the provided context, there is no implementation or reference to deprecated arguments\n    # or any specific validation logic for deprecated arguments.\n    # Therefore, this function would typically check for known deprecated arguments\n    # and issue warnings or errors if they are present.\n    \n    # Since no specific deprecated arguments are defined in the context,\n    # we simply return without any action.\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Since no specific deprecation rules or deprecated arguments are defined in the context,\n    # this function would typically check against a list of known deprecated arguments.\n    # For now, we'll implement a placeholder that checks for common deprecated arguments.\n    # In a real implementation, this would be populated with actual deprecation rules.\n    \n    deprecated_args = [\n        'old_option',\n        'legacy_flag',\n        'deprecated_param'\n    ]\n    \n    # Check for deprecated arguments in cli_args\n    deprecated_in_cli = []\n    for arg in deprecated_args:\n        if arg in cli_args:\n            deprecated_in_cli.append(arg)\n    \n    # Check for deprecated arguments in answer_file_args\n    deprecated_in_file = []\n    for arg in deprecated_args:\n        if arg in answer_file_args:\n            deprecated_in_file.append(arg)\n    \n    # Print warnings for deprecated arguments\n    if deprecated_in_cli or deprecated_in_file:\n        print(\"WARNING: The following arguments are deprecated and may be removed in future versions:\")\n        if deprecated_in_cli:\n            print(\"  In CLI arguments: \" + \", \".join(deprecated_in_cli))\n        if deprecated_in_file:\n            print(\"  In answer file arguments: \" + \", \".join(deprecated_in_file))\n    \n    # Return True if any deprecated arguments were found, False otherwise\n    return len(deprecated_in_cli) > 0 or len(deprecated_in_file) > 0", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific deprecation rules\n    deprecated_args = []\n    \n    # Example logic: check for known deprecated arguments\n    # This is a stub - actual implementation would need to be defined based on project requirements\n    \n    # Check if any deprecated arguments are present in cli_args\n    for arg in cli_args:\n        if arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(arg)\n    \n    # Check if any deprecated arguments are present in answer_file_args\n    for arg in answer_file_args:\n        if arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(arg)\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Warning: The following arguments are deprecated and will be removed in future versions:\")\n        for arg in deprecated_args:\n            print(f\"  --{arg}\")\n        print(\"Please update your configuration to use the new argument names.\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual logic would depend on specific deprecation rules\n    # This function would typically check for arguments that are no longer supported\n    # and print warnings or errors about their deprecation\n    \n    deprecated_args = []\n    \n    # Example logic - check for specific deprecated arguments\n    # This is a template and would need to be customized based on actual deprecation policy\n    \n    # Check if any deprecated arguments are present in cli_args\n    for arg in cli_args:\n        if arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(arg)\n    \n    # Check if any deprecated arguments are present in answer_file_args\n    for arg in answer_file_args:\n        if arg in ['old_option', 'legacy_flag', 'deprecated_param']:\n            deprecated_args.append(arg)\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Warning: The following arguments are deprecated and will be removed in a future version:\")\n        for arg in deprecated_args:\n            print(f\"  --{arg}\")\n        print(\"Please update your configuration to use the new arguments instead.\")\n    else:\n        print(\"No deprecated arguments found.\")", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Since no specific deprecated arguments are defined in the context,\n    # and there's no implementation details provided for deprecation checks,\n    # this function would typically check for known deprecated arguments\n    # and raise warnings or errors if found.\n    \n    # Placeholder implementation - in a real scenario, this would contain\n    # logic to identify and handle deprecated arguments.\n    deprecated_args = []\n    \n    # Example: if certain arguments are known to be deprecated\n    # Check for deprecated arguments in cli_args\n    for arg in cli_args:\n        if arg in ['old_option', 'legacy_flag']:\n            deprecated_args.append(arg)\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg in answer_file_args:\n        if arg in ['old_option', 'legacy_flag']:\n            deprecated_args.append(arg)\n    \n    # Print deprecated arguments if any found\n    if deprecated_args:\n        print(\"Warning: The following arguments are deprecated and may be removed in future versions:\")\n        for arg in deprecated_args:\n            print(f\"  --{arg}\")\n    \n    # Return the list of deprecated arguments for further processing\n    return deprecated_args"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71488, "num_retrieved": 182, "inference_time": 10.1375852227211, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71820, "num_retrieved": 192, "inference_time": 10.1375852227211, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser_name, parser_dict in self.spec_helper.iterate_parsers():\n        if parser_name == command_name:\n            for option_spec in self.spec_helper.get_parser_option_specs(parser_name):\n                if option_spec['name'] == argument_name:\n                    return option_spec\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser_name, parser_dict in self.spec_helper.iterate_parsers():\n        if parser_name == command_name:\n            for option_spec in self.spec_helper.get_parser_option_specs(parser_name):\n                if option_spec['name'] == argument_name:\n                    return option_spec\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser_name, parser_dict in self.spec_helper.iterate_parsers():\n        if parser_name == command_name:\n            for option_spec in self.spec_helper.get_parser_option_specs(parser_name):\n                if option_spec['name'] == argument_name:\n                    return option_spec\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser_name, parser_dict in self.spec_helper.iterate_parsers():\n        if parser_name == command_name:\n            for option_spec in self.spec_helper.get_parser_option_specs(parser_name):\n                if option_spec['name'] == argument_name:\n                    return option_spec\n    return None", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    for parser_name, parser_dict in self.spec_helper.iterate_parsers():\n        if parser_name == command_name:\n            for option_spec in self.spec_helper.get_parser_option_specs(parser_name):\n                if option_spec['name'] == argument_name:\n                    return option_spec\n    return None"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72298, "num_retrieved": 207, "inference_time": 10.1375852227211, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n    :param args: The received arguments.\n    :return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72217, "num_retrieved": 204, "inference_time": 10.1375852227211, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    parser = self.parser\n    required_args = parser._actions\n    missing_args = {}\n    \n    for action in required_args:\n        if action.option_strings and not action.required:\n            continue\n            \n        if action.dest not in args:\n            missing_args[action.dest] = action.option_strings\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    parser = self.parser\n    required_args = parser._actions\n    missing_args = {}\n    \n    for action in required_args:\n        if action.option_strings and not action.required:\n            continue\n            \n        if action.dest not in args:\n            missing_args[action.dest] = action.option_strings\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    parser = self.parser\n    required_args = {}\n    \n    # Get all required arguments from the parser\n    for action in parser._actions:\n        if action.option_strings or action.dest:\n            if action.required and action.dest not in required_args:\n                required_args[action.dest] = action\n    \n    # Check if all required arguments are present in args\n    missing_args = {}\n    for arg_name, action in required_args.items():\n        if arg_name not in args:\n            missing_args[arg_name] = action.option_strings\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    parser = self.parser\n    required_args = {}\n    \n    # Get all required arguments from the parser\n    for action in parser._actions:\n        if action.option_strings or action.dest:\n            if action.required and action.option_strings:\n                required_args[action.dest] = action.option_strings\n    \n    # Check if all required arguments are present in args\n    missing_args = {}\n    for arg_name, option_strings in required_args.items():\n        if arg_name not in args:\n            missing_args[arg_name] = option_strings\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    parser = self.parser\n    required_args = {}\n    \n    # Get all required arguments from the parser\n    for action in parser._actions:\n        if action.option_strings or action.dest:\n            if action.required:\n                required_args[action.dest] = action\n    \n    # Check if all required arguments are present in args\n    missing_args = {}\n    for arg_name, action in required_args.items():\n        if arg_name not in args:\n            missing_args[arg_name] = action.option_strings\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71761, "num_retrieved": 191, "inference_time": 10.1375852227211, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if the condition is met\n        condition_met = True\n        for part in condition_parts:\n            part = part.strip()\n            if '=' not in part:\n                # If no equals, check if the argument exists in args\n                arg_name = part\n                if arg_name not in args.get(command_name, {}):\n                    condition_met = False\n                    break\n            else:\n                arg_name, value = part.split('=', 1)\n                arg_value = args.get(command_name, {}).get(arg_name)\n                if arg_value is None or str(arg_value) != value:\n                    condition_met = False\n                    break\n        \n        # If condition is met, add the argument name to matched_args\n        if condition_met:\n            matched_args.append(option_spec['name'])\n    \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        condition_parts = required_when.split('==')\n        if len(condition_parts) != 2:\n            continue\n            \n        condition_var = condition_parts[0]\n        condition_value = condition_parts[1]\n        \n        # Check if the condition is met\n        condition_value = condition_value.strip()\n        \n        # Get the value of the condition variable from args\n        condition_value_from_args = args.get(command_name, {}).get(condition_var)\n        \n        # If the condition is met, add the argument to the matched list\n        if condition_value_from_args == condition_value:\n            matched_args.append(option_spec['name'])\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition = required_when\n        \n        # Check if the condition is met\n        condition_met = False\n        if condition.startswith('and'):\n            # Handle 'and' condition\n            conditions = condition[3:].split(',')\n            for cond in conditions:\n                cond_parts = cond.split('=', 1)\n                if len(cond_parts) != 2:\n                    continue\n                arg_name, arg_value = cond_parts\n                # Check if the argument exists and matches the value\n                if arg_name in args.get(command_name, {}) and \\\n                   str(args[command_name][arg_name]) == arg_value:\n                    condition_met = True\n                    break\n        elif condition.startswith('or'):\n            # Handle 'or' condition\n            conditions = condition[2:].split(',')\n            for cond in conditions:\n                cond_parts = cond.split('=', 1)\n                if len(cond_parts) != 2:\n                    continue\n                arg_name, arg_value = cond_parts\n                # Check if the argument exists and matches the value\n                if arg_name in args.get(command_name, {}) and \\\n                   str(args[command_name][arg_name]) == arg_value:\n                    condition_met = True\n                    break\n        else:\n            # Handle simple condition\n            cond_parts = condition.split('=', 1)\n            if len(cond_parts) != 2:\n                continue\n            arg_name, arg_value = cond_parts\n            # Check if the argument exists and matches the value\n            if arg_name in args.get(command_name, {}) and \\\n               str(args[command_name][arg_name]) == arg_value:\n                condition_met = True\n                \n        if condition_met:\n            matched_args.append(option_spec['name'])\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        # Format: \"arg1=value1,arg2=value2\" or \"arg1=value1\"\n        condition_parts = required_when.split(',')\n        \n        # Check if the condition is met\n        condition_met = True\n        for part in condition_parts:\n            part = part.strip()\n            if '=' in part:\n                condition_arg, condition_value = part.split('=', 1)\n            else:\n                condition_arg = part\n                condition_value = None\n                \n            # Get the value from args\n            arg_value = args.get(command_name, {}).get(condition_arg)\n            \n            # Check if the condition is met\n            if condition_value is not None:\n                if arg_value != condition_value:\n                    condition_met = False\n                    break\n            else:\n                # If no value specified, check if the argument is present\n                if arg_value is None:\n                    condition_met = False\n                    break\n                    \n        if condition_met:\n            matched_args.append(option_spec['name'])\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        required_when = option_spec.get('required_when')\n        if not required_when:\n            continue\n            \n        # Extract the condition from required_when\n        condition = required_when\n        \n        # Check if the condition is met\n        # For simplicity, we'll assume the condition is a string that can be evaluated\n        # In a real implementation, this would involve parsing the condition and evaluating it\n        # against the provided args\n        \n        # Check if the condition is satisfied\n        # This is a simplified implementation - in practice, you would need to parse and evaluate\n        # the condition expression properly\n        if condition in args.get(command_name, {}):\n            matched_args.append(option_spec['name'])\n            \n    return matched_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72572, "num_retrieved": 215, "inference_time": 10.1375852227211, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # If arg_value is a string, check its length\n            if len(arg_value) > 100:  # Assuming a reasonable maximum length of 100\n                raise IRInvalidLengthException([\n                    (arg_name, arg_value, 100)\n                ])\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # If the value is a string, check its length\n            if len(arg_value) > 1000:  # Assuming a reasonable maximum length\n                raise IRInvalidLengthException([\n                    (arg_name, arg_value, 1000)\n                ])\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # If the value is a string, check its length\n            if len(arg_value) > 100:  # Example length limit\n                raise IRInvalidLengthException([\n                    (arg_name, arg_value, 100)\n                ])\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # Check if the string length exceeds any specified limit\n            # This is a placeholder - actual implementation would depend on\n            # how length limits are defined in the context\n            if len(arg_value) > 100:  # Example limit\n                raise IRInvalidLengthException([\n                    (arg_name, arg_value, 100)\n                ])", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str):\n            # If the value is a string, check its length\n            if len(arg_value) > 100:  # Example length limit\n                raise IRInvalidLengthException([\n                    (arg_name, arg_value, 100)\n                ])\n    return True"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72307, "num_retrieved": 207, "inference_time": 10.1375852227211, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification for this option\n            option_spec = self.get_option_spec(command_name, option_name)\n            if not option_spec:\n                continue\n                \n            # Check if the option has a 'choices' field\n            if 'choices' not in option_spec:\n                continue\n                \n            # Check if the value is in the available choices\n            available_choices = option_spec['choices']\n            if option_value not in available_choices:\n                invalid_options.append((command_name, option_name, option_value, available_choices))\n    \n    # If there are invalid options, raise an exception\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification for this option\n            option_spec = self.get_option_spec(command_name, option_name)\n            if not option_spec:\n                continue\n                \n            # Check if the option has a 'choices' field\n            if 'choices' not in option_spec:\n                continue\n                \n            # Check if the value is in the available choices\n            available_choices = option_spec['choices']\n            if option_value not in available_choices:\n                invalid_options.append((command_name, option_name, option_value, available_choices))\n    \n    # If there are invalid options, raise an exception\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the available choices for this option\n            option_spec = self.get_option_spec(command_name, option_name)\n            if option_spec and 'choices' in option_spec:\n                available_choices = option_spec['choices']\n                if option_value not in available_choices:\n                    invalid_options.append((command_name, option_name, option_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification for this option\n            option_spec = self.get_option_spec(command_name, option_name)\n            if not option_spec:\n                continue\n                \n            # Check if the option has a 'choices' field\n            if 'choices' not in option_spec:\n                continue\n                \n            # Check if the value is in the available choices\n            available_choices = option_spec['choices']\n            if option_value not in available_choices:\n                invalid_options.append((command_name, option_name, option_value, available_choices))\n    \n    # If there are invalid options, raise an exception\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for option_name, option_value in command_args.items():\n            # Get the option specification for this option\n            option_spec = self.get_option_spec(command_name, option_name)\n            if option_spec and 'choices' in option_spec:\n                available_choices = option_spec['choices']\n                if option_value not in available_choices:\n                    invalid_options.append((command_name, option_name, option_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72730, "num_retrieved": 219, "inference_time": 10.1375852227211, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, (int, float)):\n            min_val = self._get_min_value(arg_name)\n            max_val = self._get_max_value(arg_name)\n            \n            if min_val is not None and arg_value < min_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is less than minimum value {min_val}\")\n                \n            if max_val is not None and arg_value > max_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is greater than maximum value {max_val}\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, (int, float)):\n            min_val = self._get_min_value(arg_name)\n            max_val = self._get_max_value(arg_name)\n            \n            if min_val is not None and arg_value < min_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is less than minimum value {min_val}\")\n            \n            if max_val is not None and arg_value > max_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is greater than maximum value {max_val}\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, (int, float)):\n            min_val = self._get_min_value(arg_name)\n            max_val = self._get_max_value(arg_name)\n            \n            if min_val is not None and arg_value < min_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is less than minimum value {min_val}\")\n                \n            if max_val is not None and arg_value > max_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is greater than maximum value {max_val}\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, (int, float)):\n            min_val = self._get_min_value(arg_name)\n            max_val = self._get_max_value(arg_name)\n            \n            if min_val is not None and arg_value < min_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is below minimum value {min_val}\")\n                \n            if max_val is not None and arg_value > max_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is above maximum value {max_val}\")\n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, (int, float)):\n            min_val = self._get_min_value(arg_name)\n            max_val = self._get_max_value(arg_name)\n            \n            if min_val is not None and arg_value < min_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is less than minimum value {min_val}\")\n                \n            if max_val is not None and arg_value > max_val:\n                raise UtilError(f\"Argument '{arg_name}' value {arg_value} is greater than maximum value {max_val}\")\n    return True"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72063, "num_retrieved": 198, "inference_time": 10.1375852227211, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Extract settings directories from the spec option if available\n    settings_dirs = []\n    if 'settings_dirs' in spec_option:\n        settings_dirs = spec_option['settings_dirs']\n    \n    # Create the complex type instance with the provided parameters\n    complex_type = None\n    \n    # Determine the type based on the type_name\n    if type_name == 'value':\n        complex_type = Value(option_name, settings_dirs, subcommand, spec_option)\n    elif type_name == 'bool':\n        complex_type = Bool(option_name, settings_dirs, subcommand, spec_option)\n    elif type_name == 'list':\n        complex_type = ListOfFileNames(option_name, settings_dirs, subcommand, spec_option)\n    elif type_name == 'nested_dict':\n        complex_type = NestedDict(option_name, settings_dirs, subcommand, spec_option)\n    elif type_name == 'nested_list':\n        complex_type = NestedList(option_name, settings_dirs, subcommand, spec_option)\n    elif type_name == 'file':\n        complex_type = FileType(option_name, settings_dirs, subcommand, spec_option)\n    elif type_name == 'inventory':\n        complex_type = Inventory(option_name, settings_dirs, subcommand, spec_option)\n    elif type_name == 'additional_args':\n        complex_type = AdditionalOptionsType(option_name, settings_dirs, subcommand, spec_option)\n    else:\n        # Default to Value type if type is not recognized\n        complex_type = Value(option_name, settings_dirs, subcommand, spec_option)\n    \n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Extract settings directories from the spec option if available\n    settings_dirs = []\n    if 'settings_dirs' in spec_option:\n        settings_dirs = spec_option['settings_dirs']\n    \n    # Create the complex type instance with the provided parameters\n    complex_type = None\n    \n    # Determine the appropriate complex type based on the type_name\n    if type_name == 'value':\n        complex_type = Value(arg_name=option_name, \n                           settings_dirs=settings_dirs, \n                           sub_command_name=subcommand, \n                           spec_option=spec_option)\n    elif type_name == 'bool':\n        complex_type = Bool(arg_name=option_name, \n                           settings_dirs=settings_dirs, \n                           sub_command_name=subcommand, \n                           spec_option=spec_option)\n    elif type_name == 'list':\n        complex_type = ListOfFileNames(arg_name=option_name, \n                                       settings_dirs=settings_dirs, \n                                       sub_command_name=subcommand, \n                                       spec_option=spec_option)\n    elif type_name == 'nested_dict':\n        complex_type = NestedDict(arg_name=option_name, \n                                  settings_dirs=settings_dirs, \n                                  sub_command_name=subcommand, \n                                  spec_option=spec_option)\n    elif type_name == 'nested_list':\n        complex_type = NestedList(arg_name=option_name, \n                                  settings_dirs=settings_dirs, \n                                  sub_command_name=subcommand, \n                                  spec_option=spec_option)\n    elif type_name == 'additional_args':\n        complex_type = AdditionalOptionsType(arg_name=option_name, \n                                             settings_dirs=settings_dirs, \n                                             sub_command_name=subcommand, \n                                             spec_option=spec_option)\n    else:\n        # Default to Value type if type_name is not recognized\n        complex_type = Value(arg_name=option_name, \n                           settings_dirs=settings_dirs, \n                           sub_command_name=subcommand, \n                           spec_option=spec_option)\n    \n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Extract settings directories from the spec option if available\n    settings_dirs = []\n    if 'settings_dirs' in spec_option:\n        settings_dirs = spec_option['settings_dirs']\n    \n    # Create the complex type instance with the provided parameters\n    complex_type = None\n    \n    # Determine the appropriate complex type based on the type_name\n    if type_name == 'value':\n        complex_type = Value(arg_name=option_name, \n                           settings_dirs=settings_dirs,\n                           sub_command_name=subcommand,\n                           spec_option=spec_option)\n    elif type_name == 'bool':\n        complex_type = Bool(arg_name=option_name, \n                           settings_dirs=settings_dirs,\n                           sub_command_name=subcommand,\n                           spec_option=spec_option)\n    elif type_name == 'list':\n        complex_type = ListOfFileNames(arg_name=option_name, \n                                       settings_dirs=settings_dirs,\n                                       sub_command_name=subcommand,\n                                       spec_option=spec_option)\n    elif type_name == 'nested_dict':\n        complex_type = NestedDict(arg_name=option_name, \n                                  settings_dirs=settings_dirs,\n                                  sub_command_name=subcommand,\n                                  spec_option=spec_option)\n    elif type_name == 'nested_list':\n        complex_type = NestedList(arg_name=option_name, \n                                  settings_dirs=settings_dirs,\n                                  sub_command_name=subcommand,\n                                  spec_option=spec_option)\n    elif type_name == 'additional_args':\n        complex_type = AdditionalOptionsType(arg_name=option_name, \n                                             settings_dirs=settings_dirs,\n                                             sub_command_name=subcommand,\n                                             spec_option=spec_option)\n    else:\n        # Default to Value type if type_name is not recognized\n        complex_type = Value(arg_name=option_name, \n                           settings_dirs=settings_dirs,\n                           sub_command_name=subcommand,\n                           spec_option=spec_option)\n    \n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Extract settings directories from spec_option if available\n    settings_dirs = []\n    if 'settings_dirs' in spec_option:\n        settings_dirs = spec_option['settings_dirs']\n    \n    # Create the complex type instance with the provided parameters\n    complex_type = None\n    \n    # Determine the specific complex type based on type_name\n    if type_name == 'value':\n        complex_type = Value(arg_name=option_name, \n                           settings_dirs=settings_dirs,\n                           sub_command_name=subcommand,\n                           spec_option=spec_option)\n    elif type_name == 'bool':\n        complex_type = Bool(arg_name=option_name, \n                           settings_dirs=settings_dirs,\n                           sub_command_name=subcommand,\n                           spec_option=spec_option)\n    elif type_name == 'list':\n        complex_type = ListOfFileNames(arg_name=option_name, \n                                       settings_dirs=settings_dirs,\n                                       sub_command_name=subcommand,\n                                       spec_option=spec_option)\n    elif type_name == 'nested_dict':\n        complex_type = NestedDict(arg_name=option_name, \n                                  settings_dirs=settings_dirs,\n                                  sub_command_name=subcommand,\n                                  spec_option=spec_option)\n    elif type_name == 'nested_list':\n        complex_type = NestedList(arg_name=option_name, \n                                  settings_dirs=settings_dirs,\n                                  sub_command_name=subcommand,\n                                  spec_option=spec_option)\n    elif type_name == 'additional_args':\n        complex_type = AdditionalOptionsType(arg_name=option_name, \n                                             settings_dirs=settings_dirs,\n                                             sub_command_name=subcommand,\n                                             spec_option=spec_option)\n    else:\n        # Default to Value if type is not recognized\n        complex_type = Value(arg_name=option_name, \n                           settings_dirs=settings_dirs,\n                           sub_command_name=subcommand,\n                           spec_option=spec_option)\n    \n    return complex_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    # Extract settings directories from the spec option if available\n    settings_dirs = []\n    if 'settings_dirs' in spec_option:\n        settings_dirs = spec_option['settings_dirs']\n    \n    # Create the complex type instance with the provided parameters\n    complex_type = None\n    \n    # Determine the appropriate complex type based on the type_name\n    if type_name == 'value':\n        complex_type = Value(arg_name=option_name, \n                           settings_dirs=settings_dirs, \n                           sub_command_name=subcommand, \n                           spec_option=spec_option)\n    elif type_name == 'bool':\n        complex_type = Bool(arg_name=option_name, \n                           settings_dirs=settings_dirs, \n                           sub_command_name=subcommand, \n                           spec_option=spec_option)\n    elif type_name == 'list':\n        complex_type = ListOfFileNames(arg_name=option_name, \n                                       settings_dirs=settings_dirs, \n                                       sub_command_name=subcommand, \n                                       spec_option=spec_option)\n    elif type_name == 'nested_dict':\n        complex_type = NestedDict(arg_name=option_name, \n                                  settings_dirs=settings_dirs, \n                                  sub_command_name=subcommand, \n                                  spec_option=spec_option)\n    elif type_name == 'nested_list':\n        complex_type = NestedList(arg_name=option_name, \n                                  settings_dirs=settings_dirs, \n                                  sub_command_name=subcommand, \n                                  spec_option=spec_option)\n    elif type_name == 'additional_args':\n        complex_type = AdditionalOptionsType(arg_name=option_name, \n                                             settings_dirs=settings_dirs, \n                                             sub_command_name=subcommand, \n                                             spec_option=spec_option)\n    elif type_name == 'inventory':\n        complex_type = Inventory(arg_name=option_name, \n                                 settings_dirs=settings_dirs, \n                                 sub_command_name=subcommand, \n                                 spec_option=spec_option)\n    else:\n        # Default to Value type if type_name is not recognized\n        complex_type = Value(arg_name=option_name, \n                           settings_dirs=settings_dirs, \n                           sub_command_name=subcommand, \n                           spec_option=spec_option)\n    \n    return complex_type"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71802, "num_retrieved": 191, "inference_time": 10.1375852227211, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process arguments to separate control and nested\n    for arg_key, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These typically include: dry-run, output, verbose, debug, etc.\n        if arg_key in ['dry-run', 'output', 'verbose', 'debug', 'generate-answers-file']:\n            control_args[arg_key] = arg_value\n        elif arg_key.startswith('extra-vars'):\n            # Handle extra-vars which are custom ansible variables\n            # These are processed as custom arguments\n            custom_args = {}\n            if isinstance(arg_value, str):\n                # Parse extra-vars string if it's a string\n                for var in arg_value.split(','):\n                    if '=' in var:\n                        key, value = var.split('=', 1)\n                        custom_args[key] = value\n            elif isinstance(arg_value, (list, tuple)):\n                for var in arg_value:\n                    if '=' in var:\n                        key, value = var.split('=', 1)\n                        custom_args[key] = value\n            if custom_args:\n                nested_args[arg_key] = custom_args\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by Ansible playbooks\n            nested_args[arg_key] = arg_value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process arguments to separate control and nested\n    for arg_name, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These typically include: dry-run, output, verbose, debug, etc.\n        if arg_name in ['dry-run', 'output', 'verbose', 'debug', 'generate-answers-file']:\n            control_args[arg_name] = arg_value\n        # Check if the argument is a nested argument (used by Ansible playbooks)\n        # These are typically passed to Ansible as extra-vars\n        elif arg_name.startswith('extra-vars'):\n            # Handle extra-vars as custom arguments\n            custom_args = {}\n            if isinstance(arg_value, str):\n                # Parse extra-vars string\n                for var in arg_value.split(';'):\n                    if '=' in var:\n                        key, value = var.split('=', 1)\n                        custom_args[key] = value\n            nested_args[arg_name] = custom_args\n        else:\n            # Default to nested arguments\n            nested_args[arg_name] = arg_value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process arguments to separate control and nested\n    for arg_name, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These typically include verbosity, dry-run, output, etc.\n        if arg_name in ['verbose', 'dry-run', 'output', 'debug', 'generate-answers-file']:\n            control_args[arg_name] = arg_value\n        # Check if the argument is a nested argument (used by Ansible playbooks)\n        # These are typically passed to the playbook as vars\n        elif arg_name.startswith('extra-vars') or arg_name.startswith('ansible-args'):\n            # Handle extra-vars and ansible-args as nested/custom\n            if arg_name.startswith('extra-vars'):\n                # Extra-vars are custom ansible variables\n                nested_args[arg_name] = arg_value\n            else:\n                # ansible-args are passed directly to Ansible\n                nested_args[arg_name] = arg_value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by the Ansible playbooks\n            nested_args[arg_name] = arg_value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process arguments to separate control and nested\n    for arg_key, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These are typically flags or options that control IR behavior\n        if arg_key in ['debug', 'dry-run', 'output', 'verbose', 'extra-vars', 'ansible-args']:\n            control_args[arg_key] = arg_value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by Ansible playbooks and stored in the spec yml\n            nested_args[arg_key] = arg_value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process arguments to separate control and nested\n    for arg_name, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These are typically flags or settings that control IR behavior\n        if arg_name in ['debug', 'dry-run', 'output', 'verbose', 'extra-vars', 'ansible-args']:\n            control_args[arg_name] = arg_value\n        elif arg_name.startswith('ansible-'):\n            # Handle ansible-specific arguments that might be custom variables\n            # These are typically passed directly to Ansible\n            control_args[arg_name] = arg_value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by Ansible playbooks\n            nested_args[arg_name] = arg_value\n    \n    return control_args, nested_args"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71495, "num_retrieved": 184, "inference_time": 10.1375852227211, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if '=' in extra_var:\n            key, value = extra_var.split('=', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This might be a placeholder or error condition\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if '=' in extra_var:\n            key, value = extra_var.split('=', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This might be a placeholder or error case\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if '=' in extra_var:\n            key, value = extra_var.split('=', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This might be a placeholder or error case\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if '=' in extra_var:\n            key, value = extra_var.split('=', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This might be a placeholder or error case\n            pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if '=' in extra_var:\n            key, value = extra_var.split('=', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This might be a placeholder or error case\n            pass\n    \n    return vars_dict"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72117, "num_retrieved": 202, "inference_time": 10.1375852227211, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    ansible_cmd = ['ansible-playbook']\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', str(extra_vars)])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        # Convert dict to list of arguments\n        for key, value in ansible_args.items():\n            ansible_cmd.extend(['--' + key, str(value)])\n    \n    # Add playbook path\n    ansible_cmd.append(playbook_path)\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except Exception as e:\n        return 1", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    ansible_cmd = ['ansible-playbook']\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', yaml.safe_dump(extra_vars, default_flow_style=False)])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            ansible_cmd.extend(['--' + arg, str(value)])\n    \n    # Add playbook path\n    ansible_cmd.append(playbook_path)\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        \n        # Return the exit code\n        return result.returncode\n        \n    except subprocess.CalledProcessError as e:\n        # Return the exit code from the failed command\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    ansible_cmd = ['ansible-playbook']\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', ','.join(f'{k}={v}' for k, v in extra_vars.items())])\n    \n    # Add additional arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            ansible_cmd.extend(['--' + arg, str(value)])\n    \n    # Add playbook path\n    ansible_cmd.append(playbook_path)\n    \n    # Execute the command using subprocess\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        \n        # Return the exit code\n        return result.returncode\n        \n    except subprocess.CalledProcessError as e:\n        # Return the exit code from the failed command\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    ansible_cmd = ['ansible-playbook']\n    \n    # Add verbosity if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', yaml.safe_dump(extra_vars, default_flow_style=False)])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            ansible_cmd.extend(['--' + arg, str(value)])\n    \n    # Add playbook path\n    ansible_cmd.append(playbook_path)\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            cwd=ir_workspace.path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Return the exit code\n        return result.returncode\n        \n    except subprocess.CalledProcessError as e:\n        # Return the exit code from the failed command\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n                         directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    ansible_cmd = ['ansible-playbook']\n    \n    # Add verbose argument if provided\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', yaml.safe_dump(extra_vars, default_flow_style=False)])\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            ansible_cmd.extend(['--' + arg, str(value)])\n    \n    # Add playbook path\n    ansible_cmd.append(playbook_path)\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71639, "num_retrieved": 190, "inference_time": 9.101134697596232, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract ansible arguments from cli_args\n    ansible_args = []\n    for arg in cli_args:\n        if arg.startswith('--'):\n            ansible_args.append(arg)\n    \n    # Prepare the playbook path\n    playbook_path = ir_plugin.playbook\n    \n    # Execute the ansible playbook with the provided variables and arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,  # Default to no verbose output unless specified\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract ansible arguments from cli_args\n    ansible_args = []\n    for arg in cli_args:\n        if arg.startswith('--'):\n            ansible_args.append(arg)\n    \n    # Prepare the playbook path\n    playbook_path = ir_plugin.playbook\n    \n    # Execute the ansible playbook with the provided variables and arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,  # Default to no verbose output unless specified\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract ansible args from cli_args\n    ansible_args = []\n    for arg in cli_args:\n        if arg.startswith('--'):\n            ansible_args.append(arg)\n    \n    # Prepare the playbook path\n    playbook_path = ir_plugin.playbook\n    \n    # Execute the ansible playbook with the provided variables and arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,  # Default to no verbose output unless specified\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract ansible args from cli_args if present\n    ansible_args = cli_args.get('ansible-args')\n    \n    # Prepare the playbook path from the plugin\n    playbook_path = ir_plugin.playbook\n    \n    # Execute the ansible playbook with the provided variables and arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=ansible_args.get('verbose', None),\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract ansible arguments from cli_args\n    ansible_args = []\n    for arg in cli_args:\n        if arg.startswith('--'):\n            ansible_args.append(arg)\n    \n    # Prepare the playbook path\n    playbook_path = ir_plugin.playbook\n    \n    # Execute the ansible playbook with the provided variables and arguments\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,  # Default to no verbose output unless specified\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71740, "num_retrieved": 189, "inference_time": 9.101134697596232, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for non-CLI arguments\n    # This function should be implemented based on specific type conversion rules\n    # for each parser_name and argument\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for non-CLI arguments\n    # This would be extended with specific type converters based on parser_name\n    # and argument specifications\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # TODO: Implement this function\n    pass"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 71530, "num_retrieved": 186, "inference_time": 9.101134697596232, "generate_results": ["def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.isfile(plugin_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {plugin_path}\")\n    \n    with open(plugin_path, 'r') as spec_file:\n        spec_dict = yaml.safe_load(spec_file) or {}\n    \n    # Flatten the dictionary by removing nested structures\n    # Extract required fields from the spec\n    plugin_spec = {}\n    \n    # Extract basic plugin properties\n    plugin_type = spec_dict.get(\"config\", {}).get(\"plugin_type\") or spec_dict.get(\"plugin_type\")\n    plugin_spec[\"plugin_type\"] = plugin_type\n    \n    entry_point = spec_dict.get(\"config\", {}).get(\"entry_point\") or spec_dict.get(\"entry_point\")\n    plugin_spec[\"entry_point\"] = entry_point\n    \n    roles_path = spec_dict.get(\"config\", {}).get(\"roles_path\") or spec_dict.get(\"roles_path\")\n    plugin_spec[\"roles_path\"] = roles_path\n    \n    description = spec_dict.get(\"description\")\n    plugin_spec[\"description\"] = description or \"\"\n    \n    # Extract subparsers (if any)\n    subparsers = spec_dict.get(\"subparsers\", {})\n    if subparsers:\n        subparsers_keys = list(subparsers.keys())\n        if subparsers_keys:\n            plugin_spec[\"subparsers\"] = subparsers_keys[0]\n    \n    return plugin_spec", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.exists(plugin_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {plugin_path}\")\n    \n    with open(plugin_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    if not spec_content:\n        return {}\n    \n    # Extract plugin type and description\n    plugin_type = spec_content.get('config', {}).get('plugin_type', spec_content.get('plugin_type'))\n    description = spec_content.get('description', '')\n    \n    # Extract subparsers (first key)\n    subparsers = spec_content.get('subparsers', {})\n    subparser_name = next(iter(subparsers.keys())) if subparsers else None\n    subparser_description = subparsers.get(subparser_name, {}).get('description', '')\n    \n    # Extract entry point\n    entry_point = spec_content.get('config', {}).get('entry_point', spec_content.get('entry_point', ''))\n    \n    # Extract roles path\n    roles_path = spec_content.get('config', {}).get('roles_path', spec_content.get('roles_path', ''))\n    \n    # Extract playbook path (assumed to be in plugin dir)\n    playbook_path = os.path.join(plugin_dir, entry_point)\n    \n    # Create flat dictionary\n    flat_dict = {\n        'plugin_type': plugin_type,\n        'description': description,\n        'subparser_name': subparser_name,\n        'subparser_description': subparser_description,\n        'entry_point': entry_point,\n        'roles_path': roles_path,\n        'playbook_path': playbook_path\n    }\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.exists(plugin_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {plugin_path}\")\n    \n    with open(plugin_path, 'r') as spec_file:\n        spec_content = spec_file.read()\n    \n    spec_dict = yaml.safe_load(spec_content)\n    \n    if not spec_dict:\n        return {}\n    \n    # Flatten the dictionary by removing nested structures\n    # Extract required fields from the spec\n    result = {}\n    \n    # Extract basic plugin properties\n    if 'description' in spec_dict:\n        result['description'] = spec_dict['description']\n    \n    # Extract config section\n    if 'config' in spec_dict:\n        config = spec_dict['config']\n        result['plugin_type'] = config.get('plugin_type', '')\n        result['entry_point'] = config.get('entry_point', '')\n        result['roles_path'] = config.get('roles_path', '')\n    \n    # Extract subparsers section\n    if 'subparsers' in spec_dict:\n        subparsers = spec_dict['subparsers']\n        subparsers_keys = list(subparsers.keys())\n        if subparsers_keys:\n            result['subparser_name'] = subparsers_keys[0]\n    \n    return result", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.isfile(plugin_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {plugin_path}\")\n    \n    with open(plugin_path, 'r') as spec_file:\n        spec_content = spec_file.read()\n    \n    spec_dict = yaml.safe_load(spec_content)\n    \n    if not isinstance(spec_dict, dict):\n        raise ValueError(f\"Invalid plugin spec file: {spec_content}\")\n    \n    # Flatten the dictionary by removing nested structures\n    # Extract required fields from the spec\n    flattened_dict = {}\n    \n    # Extract basic plugin properties\n    if 'description' in spec_dict:\n        flattened_dict['description'] = spec_dict['description']\n    \n    # Extract config section\n    if 'config' in spec_dict:\n        config = spec_dict['config']\n        if 'plugin_type' in config:\n            flattened_dict['plugin_type'] = config['plugin_type']\n        if 'entry_point' in config:\n            flattened_dict['entry_point'] = config['entry_point']\n        if 'roles_path' in config:\n            flattened_dict['roles_path'] = config['roles_path']\n    \n    # Extract subparsers (get the first subparser)\n    if 'subparsers' in spec_dict:\n        subparsers = spec_dict['subparsers']\n        if subparsers:\n            first_subparser_name = list(subparsers.keys())[0]\n            first_subparser = subparsers[first_subparser_name]\n            if 'description' in first_subparser:\n                flattened_dict['subparser_description'] = first_subparser['description']\n    \n    return flattened_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_path = os.path.join(plugin_dir, 'plugin.spec')\n    \n    if not os.path.isfile(plugin_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {plugin_path}\")\n    \n    with open(plugin_path, 'r') as spec_file:\n        spec_dict = yaml.safe_load(spec_file) or {}\n    \n    # Flatten the dictionary by removing nested structures\n    # Extract required fields from the spec\n    plugin_spec = {}\n    \n    # Extract basic plugin properties\n    plugin_type = spec_dict.get(\"config\", {}).get(\"plugin_type\") or spec_dict.get(\"plugin_type\")\n    plugin_spec[\"plugin_type\"] = plugin_type\n    \n    entry_point = spec_dict.get(\"config\", {}).get(\"entry_point\") or spec_dict.get(\"entry_point\")\n    plugin_spec[\"entry_point\"] = entry_point\n    \n    roles_path = spec_dict.get(\"config\", {}).get(\"roles_path\") or spec_dict.get(\"roles_path\")\n    plugin_spec[\"roles_path\"] = roles_path\n    \n    description = spec_dict.get(\"description\")\n    plugin_spec[\"description\"] = description or \"\"\n    \n    # Extract subparsers (if any)\n    subparsers = spec_dict.get(\"subparsers\", {})\n    if subparsers:\n        subparsers_keys = list(subparsers.keys())\n        if subparsers_keys:\n            plugin_spec[\"subparsers\"] = subparsers_keys[0]\n    \n    return plugin_spec"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72221, "num_retrieved": 204, "inference_time": 9.101134697596232, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        config_path = os.path.join(self.infrared_home, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        config_path = os.path.join(self.infrared_home, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        config_path = os.path.join(self.infrared_home, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        config_path = os.path.join(self.infrared_home, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    config_path = os.environ.get('ANSIBLE_CONFIG')\n    if not config_path:\n        config_path = os.path.join(self.infrared_home, 'ansible.cfg')\n        os.environ['ANSIBLE_CONFIG'] = config_path"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 72708, "num_retrieved": 217, "inference_time": 9.101134697596232, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"\n    Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific spec and its options\n    # The function should extend the subparser with the options defined in the spec\n    # For example, it might create new arguments, subparsers, or groups\n    # based on the spec definition\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"\n    Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would involve parsing the spec and adding\n    # appropriate CLI options to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"\n    Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a subparser for the plugin\n    plugin_subparser = root_subparsers.add_parser(\n        self.name,\n        help=self.get_name(),\n        description=self.get_name()\n    )\n    \n    # Extend the subparser with options from the spec\n    spec_dict = self.spec_helper.spec_dict\n    for option_name, option_dict in spec_dict.get('options', {}).items():\n        # Add option to the subparser\n        if option_dict.get('action') == 'store_true':\n            attr = option_dict.get('dest', option_name)\n            plugin_subparser.add_argument(\n                '--' + attr,\n                action='store_true',\n                help=option_dict.get('help', '')\n            )\n        else:\n            attr = option_dict.get('dest', option_name)\n            plugin_subparser.add_argument(\n                '--' + attr,\n                type=option_dict.get('type', str),\n                help=option_dict.get('help', '')\n            )\n    \n    # Add groups if defined\n    if 'groups' in spec_dict:\n        for group_dict in spec_dict['groups']:\n            group = plugin_subparser.add_argument_group(group_dict['title'])\n            for opt_name, opt_dict in group_dict.get('options', {}).items():\n                if opt_dict.get('action') == 'store_true':\n                    attr = opt_dict.get('dest', opt_name)\n                    group.add_argument(\n                        '--' + attr,\n                        action='store_true',\n                        help=opt_dict.get('help', '')\n                    )\n                else:\n                    attr = opt_dict.get('dest', opt_name)\n                    group.add_argument(\n                        '--' + attr,\n                        type=opt_dict.get('type', str),\n                        help=opt_dict.get('help', '')\n                    )", "def extend_cli(self, root_subparsers):\n    \"\"\"\n    Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a parser for the current spec\n    spec_parser = self.parser\n    spec_dict = spec_parser.spec_helper.spec_dict\n    \n    # Process subparsers\n    subparsers = spec_dict.get('subparsers', {})\n    if subparsers:\n        dest_path = 'subcommand'\n        \n        for subparser_name, subparser_dict in subparsers.items():\n            cmd_parser = root_subparsers.add_parser(\n                subparser_name,\n                help=subparser_dict.get('help', subparser_dict.get('description', '')),\n                description=subparser_dict.get('description', subparser_dict.get('help', '')),\n                formatter_class=subparser_dict.get('formatter_class', argparse.RawTextHelpFormatter)\n            )\n            \n            # Add groups to the parser\n            self._add_groups(self, cmd_parser, subparser_name, subparser_dict, path_prefix=dest_path)", "def extend_cli(self, root_subparsers):\n    \"\"\"\n    Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # Create a parser for the current spec\n    parser_dict = self.spec_helper.spec_dict\n    \n    # Process subparsers\n    subparsers = parser_dict.get('subparsers', {})\n    if subparsers:\n        dest_path = 'subcommand'\n        \n        for subparser_name, subparser_dict in parser_dict.get('subparsers', {}).items():\n            cmd_parser = root_subparsers.add_parser(\n                subparser_name,\n                help=subparser_dict.get('help', subparser_dict.get('description', '')),\n                description=subparser_dict.get('description', subparser_dict.get('help', '')),\n                formatter_class=parser_dict.get('formatter_class', argparse.RawTextHelpFormatter)\n            )\n            \n            # Add groups to the parser\n            self._add_groups(self, cmd_parser, subparser_name, subparser_dict, path_prefix=dest_path)"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 65536, "actual_tokens": 49368, "num_retrieved": 127, "inference_time": 9.101134697596232, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")"]}
