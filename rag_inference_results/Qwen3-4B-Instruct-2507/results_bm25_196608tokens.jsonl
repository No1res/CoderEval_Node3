{"_id": "62e60f43d76274f8a4026e28", "name": "hydrate_time", "project": "neo4j/neo4j-python-driver", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 172064, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")", "def hydrate_time(nanoseconds, tz=None):\n    \"\"\"\n    Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds: nanoseconds since midnight\n    :param tz: optional timezone\n    :return: Time\n    \"\"\"\n    if not isinstance(nanoseconds, int):\n        raise TypeError(\"Ticks must be int\")\n    if 0 <= nanoseconds < 86400000000000:\n        second, nanosecond = divmod(nanoseconds, NANO_SECONDS)\n        minute, second = divmod(second, 60)\n        hour, minute = divmod(minute, 60)\n        return Time(hour, minute, second, nanosecond, tz)\n    raise ValueError(\"Ticks out of range (0..86400000000000)\")"]}
{"_id": "62e60f3bd76274f8a4026e10", "name": "dehydrate_timedelta", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 179389, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :return: dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    # Convert to integer seconds and nanoseconds\n    seconds_int = int(seconds)\n    nanoseconds = int((seconds - seconds_int) * 1000000000)\n    return Structure(b\"t\", seconds_int, nanoseconds)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :return: dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    return Structure(b\"t\", int(seconds))", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :return: dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    days, seconds = divmod(int(seconds), 86400)\n    return Structure(b\"I\", days, seconds)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :return: dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    days, seconds = divmod(int(seconds), 86400)\n    return Structure(b\"I\", days, seconds)", "def dehydrate_timedelta(value):\n    \"\"\"\n    Dehydrator for `timedelta` values.\n\n    :param value: timedelta value to dehydrate\n    :type value: timedelta\n    :return: dehydrated representation of the timedelta\n    \"\"\"\n    seconds = value.total_seconds()\n    days, seconds = divmod(int(seconds), 86400)\n    return Structure(b\"I\", days, seconds)"]}
{"_id": "62e60f37d76274f8a4026dfd", "name": "dehydrate_time", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 168133, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)", "def dehydrate_time(value):\n    \"\"\" Dehydrator for Time data.\n\n    :param value:\n    :type value: Time\n    :returns:\n    \"\"\"\n    return _hydration.dehydrate_time(value)"]}
{"_id": "62e60f33d76274f8a4026de9", "name": "dehydrate_point", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 166464, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)", "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :returns:\n    \"\"\"\n    srid = value.srid\n    coordinates = list(value)\n    return Structure(b\"X\", srid, *coordinates)"]}
{"_id": "62e60ed4d76274f8a4026da0", "name": "keys", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 165640, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)", "def keys(self):\n    \"\"\"\n    Return the keys of the record.\n\n    :return: list of key names\n    \"\"\"\n    return list(self._keys)"]}
{"_id": "62e60ecfd76274f8a4026d6a", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 199424, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["@classmethod\n    def protocol_handlers(cls, protocol_version=None):\n        \"\"\"\n        Return a dictionary of available Bolt protocol handlers,\n        keyed by version tuple. If an explicit protocol version is\n        provided, the dictionary will contain either zero or one items,\n        depending on whether that version is supported. If no protocol\n        version is provided, all available versions will be returned.\n\n        :param protocol_version: tuple identifying a specific protocol\n            version (e.g. (3, 5)) or None\n        :return: dictionary of version tuple to handler class for all\n            relevant and supported protocol versions\n        :raise TypeError: if protocol version is not passed in a tuple\n        \"\"\"\n        if protocol_version is not None and not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n\n        handlers = {\n            Version(3, 0): Bolt3,\n            Version(3, 1): Bolt3,\n            Version(3, 2): Bolt3,\n            Version(3, 3): Bolt3,\n            Version(3, 4): Bolt3,\n            Version(4, 0): Bolt4x0,\n            Version(4, 1): Bolt4x1,\n            Version(4, 2): Bolt4x2,\n            Version(4, 3): Bolt4x3,\n            Version(4, 4): Bolt4x4,\n            Version(5, 0): Bolt5x0,\n            Version(5, 1): Bolt5x1,\n            Version(5, 2): Bolt5x2,\n            Version(5, 3): Bolt5x3,\n        }\n\n        if protocol_version is None:\n            return handlers\n\n        return {k: v for k, v in handlers.items() if k == protocol_version}", "@classmethod\n    def protocol_handlers(cls, protocol_version=None):\n        \"\"\"\n        Return a dictionary of available Bolt protocol handlers,\n        keyed by version tuple. If an explicit protocol version is\n        provided, the dictionary will contain either zero or one items,\n        depending on whether that version is supported. If no protocol\n        version is provided, all available versions will be returned.\n\n        :param protocol_version: tuple identifying a specific protocol\n            version (e.g. (3, 5)) or None\n        :return: dictionary of version tuple to handler class for all\n            relevant and supported protocol versions\n        :raise TypeError: if protocol version is not passed in a tuple\n        \"\"\"\n        if protocol_version is not None and not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n\n        handlers = {}\n\n        # Define all available protocol handlers from the context\n        # Bolt 3.x handlers\n        handlers[Version(3, 0)] = Bolt3\n        handlers[Version(3, 1)] = Bolt3\n        handlers[Version(3, 2)] = Bolt3\n        handlers[Version(3, 3)] = Bolt3\n        handlers[Version(3, 4)] = Bolt3\n        \n        # Bolt 4.x handlers\n        handlers[Version(4, 0)] = Bolt4x0\n        handlers[Version(4, 1)] = Bolt4x1\n        handlers[Version(4, 2)] = Bolt4x2\n        handlers[Version(4, 3)] = Bolt4x3\n        handlers[Version(4, 4)] = Bolt4x4\n        \n        # Bolt 5.x handlers\n        handlers[Version(5, 0)] = Bolt5x0\n        handlers[Version(5, 1)] = Bolt5x1\n        handlers[Version(5, 2)] = Bolt5x2\n        handlers[Version(5, 3)] = Bolt5x3\n\n        # If a specific protocol version is provided, return only that version\n        if protocol_version is not None:\n            return {protocol_version: handlers.get(protocol_version)}\n            \n        return handlers", "@classmethod\n    def protocol_handlers(cls, protocol_version=None):\n        \"\"\"\n        Return a dictionary of available Bolt protocol handlers,\n        keyed by version tuple. If an explicit protocol version is\n        provided, the dictionary will contain either zero or one items,\n        depending on whether that version is supported. If no protocol\n        version is provided, all available versions will be returned.\n\n        :param protocol_version: tuple identifying a specific protocol\n            version (e.g. (3, 5)) or None\n        :return: dictionary of version tuple to handler class for all\n            relevant and supported protocol versions\n        :raise TypeError: if protocol version is not passed in a tuple\n        \"\"\"\n        if protocol_version is not None and not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n\n        handlers = {}\n\n        # Add all protocol handlers from the context\n        # Bolt 3.x handlers\n        if (3, 0) in [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]:\n            handlers[(3, 0)] = Bolt3\n        if (3, 5) in [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]:\n            handlers[(3, 5)] = Bolt3\n\n        # Bolt 4.x handlers\n        if (4, 0) in [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]:\n            handlers[(4, 0)] = Bolt4x0\n        if (4, 1) in [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]:\n            handlers[(4, 1)] = Bolt4x1\n        if (4, 2) in [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]:\n            handlers[(4, 2)] = Bolt4x2\n        if (4, 3) in [(4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]:\n            handlers[(4,", "@classmethod\n    def protocol_handlers(cls, protocol_version=None):\n        \"\"\"\n        Return a dictionary of available Bolt protocol handlers,\n        keyed by version tuple. If an explicit protocol version is\n        provided, the dictionary will contain either zero or one items,\n        depending on whether that version is supported. If no protocol\n        version is provided, all available versions will be returned.\n\n        :param protocol_version: tuple identifying a specific protocol\n            version (e.g. (3, 5)) or None\n        :return: dictionary of version tuple to handler class for all\n            relevant and supported protocol versions\n        :raise TypeError: if protocol version is not passed in a tuple\n        \"\"\"\n        if protocol_version is not None and not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n\n        handlers = {\n            Version(3, 0): Bolt3,\n            Version(3, 1): Bolt3,\n            Version(3, 2): Bolt3,\n            Version(3, 3): Bolt3,\n            Version(3, 4): Bolt3,\n            Version(4, 0): Bolt4x0,\n            Version(4, 1): Bolt4x1,\n            Version(4, 2): Bolt4x2,\n            Version(4, 3): Bolt4x3,\n            Version(4, 4): Bolt4x4,\n            Version(5, 0): Bolt5x0,\n            Version(5, 1): Bolt5x1,\n            Version(5, 2): Bolt5x2,\n            Version(5, 3): Bolt5x3,\n        }\n\n        if protocol_version is None:\n            return handlers\n\n        return {k: v for k, v in handlers.items() if k == protocol_version}", "@classmethod\n    def protocol_handlers(cls, protocol_version=None):\n        \"\"\"\n        Return a dictionary of available Bolt protocol handlers,\n        keyed by version tuple. If an explicit protocol version is\n        provided, the dictionary will contain either zero or one items,\n        depending on whether that version is supported. If no protocol\n        version is provided, all available versions will be returned.\n\n        :param protocol_version: tuple identifying a specific protocol\n            version (e.g. (3, 5)) or None\n        :return: dictionary of version tuple to handler class for all\n            relevant and supported protocol versions\n        :raise TypeError: if protocol version is not passed in a tuple\n        \"\"\"\n        if protocol_version is not None and not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n\n        handlers = {\n            Version(3, 0): Bolt3,\n            Version(3, 1): Bolt3,\n            Version(3, 2): Bolt3,\n            Version(3, 3): Bolt3,\n            Version(3, 4): Bolt3,\n            Version(4, 0): Bolt4x0,\n            Version(4, 1): Bolt4x1,\n            Version(4, 2): Bolt4x2,\n            Version(4, 3): Bolt4x3,\n            Version(4, 4): Bolt4x4,\n            Version(5, 0): Bolt5x0,\n            Version(5, 1): Bolt5x1,\n            Version(5, 2): Bolt5x2,\n            Version(5, 3): Bolt5x3,\n        }\n\n        if protocol_version is None:\n            return handlers\n\n        return {k: v for k, v in handlers.items() if k == protocol_version}"]}
{"_id": "62e60e49d76274f8a4026d25", "name": "unit_of_work", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 202962, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with the provided metadata and timeout\n            # These will be passed to the transaction when it is executed\n            if metadata is not None:\n                # Attach metadata to the transaction\n                pass\n            if timeout is not None:\n                # Set the timeout for the transaction\n                pass\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with the provided metadata and timeout\n            # These will be passed to the transaction when it is executed\n            if metadata is not None:\n                # Attach metadata to the transaction\n                pass\n            if timeout is not None:\n                # Set the timeout for the transaction\n                pass\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with the provided metadata and timeout\n            # These will be passed to the transaction when it is executed\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with the provided metadata and timeout\n            # These will be passed to the transaction when it is executed\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator", "def unit_of_work(metadata=None, timeout=None):\n    \"\"\"\n    This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`\n    \"\"\"\n    def decorator(func):\n        def wrapper(tx, *args, **kwargs):\n            # Create a new transaction function with the provided metadata and timeout\n            # These will be passed to the transaction when it is executed\n            return func(tx, *args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e60e05d76274f8a4026cfd", "name": "index", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 171711, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self._keys.index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self.index(key)\n    except AttributeError:\n        # If self doesn't have an index method, we need to implement it\n        # This is a fallback implementation for a collection\n        for i, item in enumerate(self):\n            if item == key:\n                return i\n        return -1\n\nHowever, since the context doesn't specify what type of collection this function is operating on, and given that the context shows functions like `value` and `values` that operate on result records, I believe this function might be intended to work with a list of records or values.\n\nThe implementation above attempts to find the index of the given key in the collection. If the collection doesn't have an index method, it falls back to a manual search. If the key is not found, it returns -1.", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self.keys().index(key)\n    except ValueError:\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    try:\n        return self.index(key)\n    except AttributeError:\n        # If the object doesn't have an index method, try to find the key in a list\n        # This is a fallback implementation for collections\n        for i, item in enumerate(self):\n            if item == key:\n                return i\n        return -1", "def index(self, key):\n    \"\"\"\n    Return the index of the given item.\n\n    :param key: a key\n    :return: index\n    :rtype: int\n    \"\"\"\n    keys = self.keys()\n    try:\n        return keys.index(key)\n    except ValueError:\n        return -1"]}
{"_id": "62e60da4d76274f8a4026cf1", "name": "values", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 193316, "num_retrieved": 497, "inference_time": 46.91491948068142, "generate_results": ["def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[key] for key in self._keys]\n    return [self._values[key] for key in keys]", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[key] for key in self._keys]\n    return [self._values[key] for key in keys]", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[i] for i in range(len(self._values))]\n    \n    values = []\n    for key in keys:\n        if isinstance(key, int):\n            values.append(self._values[key])\n        else:\n            values.append(self._values.get(key))\n    return values", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[key] for key in self._keys]\n    return [self._values[key] for key in keys if key in self._values]", "def values(self, *keys):\n    \"\"\"\n    Return the values of the record, optionally filtering to\n    include only certain values by index or key.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: list of values\n    :rtype: list\n    \"\"\"\n    if not keys:\n        return [self._values[key] for key in self._keys]\n    return [self._values[key] for key in keys]"]}
{"_id": "62e60b10d76274f8a4026ccd", "name": "data", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 200692, "num_retrieved": 498, "inference_time": 46.91491948068142, "generate_results": ["def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    record_dict = {}\n    record_keys = self.keys()\n    \n    # If no keys are provided, return all values\n    if not keys:\n        for key in record_keys:\n            record_dict[key] = self.value(key)\n        return record_dict\n    \n    # Process each key provided\n    for key in keys:\n        if isinstance(key, int):\n            # Handle index-based access\n            if key < 0 or key >= len(record_keys):\n                raise IndexError(f\"Index {key} is out of bounds for record with {len(record_keys)} fields\")\n            record_dict[record_keys[key]] = self.value(key)\n        else:\n            # Handle key-based access\n            record_dict[key] = self.value(key)\n    \n    # Fill in missing keys with None\n    for key in record_keys:\n        if key not in record_dict:\n            record_dict[key] = None\n    \n    return record_dict", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(zip(self.keys(), self.values()))\n    \n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(self.values()):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            result[self.keys()[key]] = self.values()[key]\n        else:\n            result[key] = self.values(key)\n    \n    return result", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    # Create a dictionary to hold the result\n    result = {}\n    \n    # Get the keys and values from the record\n    record_keys = self.keys()\n    record_values = list(self.values())\n    \n    # If no keys are provided, return all values\n    if not keys:\n        return dict(zip(record_keys, record_values))\n    \n    # Process each key\n    for key in keys:\n        # If key is an integer, treat it as an index\n        if isinstance(key, int):\n            if key < 0 or key >= len(record_values):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            result[record_keys[key]] = record_values[key]\n        # If key is a string, treat it as a field name\n        else:\n            # Find the index of the key in the record keys\n            try:\n                index = record_keys.index(key)\n                result[key] = record_values[index]\n            except ValueError:\n                # Key not found in record, insert with None\n                result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    result = {}\n    record_keys = self.keys()\n    \n    # If no keys are provided, return all values\n    if not keys:\n        for key in record_keys:\n            result[key] = self.value(key)\n        return result\n    \n    # Process each key\n    for key in keys:\n        if isinstance(key, int):\n            # Handle index\n            if key < 0 or key >= len(record_keys):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            result[record_keys[key]] = self.value(key)\n        else:\n            # Handle key name\n            result[key] = self.value(key)\n    \n    # Add any keys not in the provided keys list with None values\n    for key in record_keys:\n        if key not in result:\n            result[key] = None\n    \n    return result", "def data(self, *keys):\n    \"\"\"\n    Return the keys and values of this record as a dictionary,\n    optionally including only certain values by index or key. Keys\n    provided in the items that are not in the record will be\n    inserted with a value of :const:`None`; indexes provided\n    that are out of bounds will trigger an :exc:`IndexError`.\n\n    :param keys: indexes or keys of the items to include; if none\n                 are provided, all values will be included\n    :return: dictionary of values, keyed by field name\n    :raises: :exc:`IndexError` if an out-of-bounds index is specified\n    \"\"\"\n    if not keys:\n        return dict(zip(self.keys(), self.values()))\n    \n    result = {}\n    for key in keys:\n        if isinstance(key, int):\n            if key < 0 or key >= len(self.values()):\n                raise IndexError(f\"Index {key} is out of bounds\")\n            result[self.keys()[key]] = self.values()[key]\n        else:\n            result[key] = self.values().get(key)\n    \n    return result"]}
{"_id": "62e6087bd76274f8a4026bfa", "name": "pop_u16", "project": "neo4j/neo4j-python-driver", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 174290, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 2 bytes\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 2 bytes\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 2 bytes\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to read 2 bytes\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value", "def pop_u16(self):\n    \"\"\"\n    Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.\n    \"\"\"\n    if len(self.data) < 2:\n        raise ValueError(\"Not enough data to pop 2 bytes\")\n    value = (self.data[-2] << 8) | self.data[-1]\n    self.data = self.data[:-2]\n    return value"]}
{"_id": "62e6087ad76274f8a4026bf2", "name": "discard", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 191800, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3D\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n            hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a DISCARD message to the output queue.\n\n    :param n: number of records to discard, default = -1 (ALL)\n    :param qid: query ID to discard for, default = -1 (last query)\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    \"\"\"\n    extra = {\"n\": n}\n    if qid != -1:\n        extra[\"qid\"] = qid\n    log.debug(\"[#%04X]  C: DISCARD %r\", self.local_port, extra)\n    self._append(b\"\\x3E\", (extra,),\n                 Response(self, \"discard\", hydration_hooks, **handlers),\n                 dehydration_hooks=dehydration_hooks)"]}
{"_id": "62e60879d76274f8a4026bec", "name": "begin", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 192872, "num_retrieved": 486, "inference_time": 46.91491948068142, "generate_results": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, fields)\n    self._append(b\"\\x11\", fields,", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, fields)\n    self._append(b\"\\x11\", fields,", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, fields)\n    self._append(b\"\\x03\", fields,", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, fields)\n    self._append(b\"\\x11\", fields,", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n          db=None, imp_user=None, dehydration_hooks=None,\n          hydration_hooks=None, **handlers):\n    \"\"\"\n    Appends a BEGIN message to the output queue.\n\n    :param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n    :param bookmarks: iterable of bookmark values after which this transaction should begin\n    :param metadata: custom metadata dictionary to attach to the transaction\n    :param timeout: timeout for transaction execution (seconds)\n    :param db: name of the database against which to begin the transaction\n        Requires Bolt 4.0+.\n    :param imp_user: the user to impersonate\n        Requires Bolt 4.4+\n    :param dehydration_hooks:\n        Hooks to dehydrate types (dict from type (class) to dehydration\n        function). Dehydration functions receive the value and returns an\n        object of type understood by packstream.\n    :param hydration_hooks:\n        Hooks to hydrate types (mapping from type (class) to\n        dehydration function). Dehydration functions receive the value of\n        type understood by packstream and are free to return anything.\n    :param handlers: handler functions passed into the returned Response object\n    :return: Response object\n    \"\"\"\n    if not parameters:\n        parameters = {}\n    extra = {}\n    if mode in (READ_ACCESS, \"r\"):\n        # It will default to mode \"w\" if nothing is specified\n        extra[\"mode\"] = \"r\"\n    if db:\n        extra[\"db\"] = db\n    if (\n        self._client_state_manager.state\n        != self.bolt_states.TX_READY_OR_TX_STREAMING\n    ):\n        self.last_database = db\n    if imp_user:\n        extra[\"imp_user\"] = imp_user\n    if bookmarks:\n        try:\n            extra[\"bookmarks\"] = list(bookmarks)\n        except TypeError:\n            raise TypeError(\"Bookmarks must be provided as iterable\")\n    if metadata:\n        try:\n            extra[\"tx_metadata\"] = dict(metadata)\n        except TypeError:\n            raise TypeError(\"Metadata must be coercible to a dict\")\n    if timeout is not None:\n        extra[\"tx_timeout\"] = tx_timeout_as_ms(timeout)\n    fields = (mode, extra)\n    log.debug(\"[#%04X]  C: BEGIN %s\", self.local_port, fields)\n    self._append(b\"\\x11\", fields,"]}
{"_id": "62e60723d76274f8a4026b75", "name": "round_half_to_even", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 181502, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def round_half_to_even(n):\n    \"\"\"\n    Rounds a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the fractional part\n    fractional = n - int(n)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional < 0.5:\n        return int(n)\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional > 0.5:\n        return int(n) + 1\n    \n    # If fractional part is exactly 0.5, round to the nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        integer_part = int(n)\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounding to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the fractional part\n    fractional = n - int(n)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional < 0.5:\n        return int(n)\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional > 0.5:\n        return int(n) + 1\n    \n    # If fractional part is exactly 0.5, round to the nearest even integer\n    else:\n        # Get the last digit of the integer part\n        last_digit = int(n) % 2\n        \n        # If the last digit is even, round down (to keep it even)\n        # If the last digit is odd, round up (to make it even)\n        if last_digit == 0:\n            return int(n)\n        else:\n            return int(n) + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the fractional part\n    fractional = n - int(n)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional < 0.5:\n        return int(n)\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional > 0.5:\n        return int(n) + 1\n    \n    # If fractional part is exactly 0.5, round to nearest even integer\n    else:\n        # Get the integer part\n        integer_part = int(n)\n        # Check if the integer part is even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the fractional part\n    fractional = n - int(n)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional < 0.5:\n        return int(n)\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional > 0.5:\n        return int(n) + 1\n    \n    # If fractional part is exactly 0.5, round to the nearest even integer\n    else:\n        # Get the integer part\n        integer_part = int(n)\n        # Check if the integer part is even\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1", "def round_half_to_even(n):\n    \"\"\"\n    Round a number to the nearest integer, with ties rounded to the nearest even integer.\n    \n    :param n: The number to round\n    :return: The rounded integer\n    \"\"\"\n    # Get the fractional part\n    fractional = n - int(n)\n    \n    # If fractional part is less than 0.5, round down\n    if fractional < 0.5:\n        return int(n)\n    \n    # If fractional part is greater than 0.5, round up\n    elif fractional > 0.5:\n        return int(n) + 1\n    \n    # If fractional part is exactly 0.5, round to the nearest even integer\n    else:\n        # Check if the integer part is even or odd\n        integer_part = int(n)\n        if integer_part % 2 == 0:\n            return integer_part\n        else:\n            return integer_part + 1"]}
{"_id": "62e60707d76274f8a4026b69", "name": "point_type", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 183378, "num_retrieved": 472, "inference_time": 46.91491948068142, "generate_results": ["def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            # Validate coordinates\n            if len(coordinates) != len(fields):\n                raise ValueError(f\"Expected {len(fields)} coordinates, got {len(coordinates)}\")\n            \n            # Create instance with coordinates\n            instance = super().__new__(cls)\n            instance._coordinates = tuple(coordinates)\n            return instance\n        \n        def __repr__(self):\n            return f\"{self.__class__.__name__}({self._coordinates})\"\n        \n        def __len__(self):\n            return len(self._coordinates)\n        \n        def __getitem__(self, index):\n            return self._coordinates[index]\n        \n        def __iter__(self):\n            return iter(self._coordinates)\n        \n        def __eq__(self, other):\n            if not isinstance(other, PointSubclass):\n                return False\n            return self._coordinates == other._coordinates\n        \n        def __hash__(self):\n            return hash(self._coordinates)\n        \n        def __str__(self):\n            return f\"{self.__class__.__name__}({self._coordinates})\"\n    \n    # Set up the srid mapping for this point type\n    srid_table[name] = (PointSubclass, len(fields))\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            # Use the hydrate_point function to create a Point instance\n            # with the given coordinates and SRID\n            if len(coordinates) == 0:\n                raise ValueError(\"Point requires at least one coordinate\")\n            \n            # Determine the SRID based on the coordinates\n            srid = srid_map.get(fields)\n            if srid is None:\n                # If no SRID is specified, use the default SRID\n                srid = 4326\n            \n            # Create the Point instance using hydrate_point\n            return hydrate_point(srid, *coordinates)\n        \n        def __repr__(self):\n            return f\"{self.__class__.__name__}({self.coordinates}, srid={self.srid})\"\n    \n    # Set the class name and return it\n    PointSubclass.__name__ = name\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            # Use the hydrate_point function to create the point instance\n            # with the given coordinates and srid\n            return hydrate_point(srid_map.get(name, 0), *coordinates)\n    \n    # Set the class name and return it\n    PointSubclass.__name__ = name\n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            # Use the hydrate_point function to create the point instance\n            # with the given coordinates and srid\n            return hydrate_point(srid_map.get(name, 0), *coordinates)\n    \n    # Set the class attributes to match the fields\n    PointSubclass.__name__ = name\n    PointSubclass.__doc__ = f\"Point subclass for {name} with {fields} fields\"\n    \n    return PointSubclass", "def point_type(name, fields, srid_map):\n    \"\"\"\n    Dynamically create a Point subclass.\n    \"\"\"\n    # Create a new class with the given name that inherits from Point\n    class PointSubclass(Point):\n        def __new__(cls, *coordinates):\n            # Use the hydrate_point function to create the point instance\n            # with the given SRID and coordinates\n            srid = srid_map.get(fields[0])\n            if srid is None:\n                # If no SRID is found, use the default Point class\n                return super().__new__(cls, *coordinates)\n            else:\n                # Use the hydrate_point function to create the point instance\n                return hydrate_point(srid, *coordinates)\n    \n    # Set the class name and return it\n    PointSubclass.__name__ = name\n    return PointSubclass"]}
{"_id": "62e5dc9ed76274f8a4026b5b", "name": "deprecated", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 178374, "num_retrieved": 500, "inference_time": 46.91491948068142, "generate_results": ["def deprecated(message):\n    \"\"\"Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n::\n\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n::\n\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "def deprecated(message):\n    \"\"\"\n    Decorator for deprecating functions and methods.\n\n:: \n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            import warnings\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator"]}
{"_id": "62e4fc3c85ea98643089041e", "name": "_inline_r_setup", "project": "pre-commit/pre-commit", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 52035, "num_retrieved": 224, "inference_time": 3.536793813109398, "generate_results": ["def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options that cannot be set via environment variables\n    # These options are set directly in the R code to ensure they are applied\n    # when R starts up\n    return f'options(stringsAsFactors = FALSE, warn = 1)\\n{code}'", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    # Common R options might include setting working directory, memory limits, or other runtime settings\n    r_options = [\n        # Example options that might be needed\n        'options(warn=1)',  # Enable warnings\n        'options(digits=5)',  # Set number of significant digits\n        'options(scipen=999)',  # Prevent scientific notation\n        'options(stringsAsFactors=FALSE)',  # Prevent string factors\n    ]\n    \n    # Insert R options at the beginning of the code\n    r_options_code = '\\n'.join(r_options)\n    return f'{r_options_code}\\n{code}'", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    r_options = [\n        '--vanilla',  # Use vanilla R without user-specific settings\n        '--no-save',  # Don't save the workspace\n        '--no-restore',  # Don't restore the workspace\n        '--no-readline',  # Disable readline for better compatibility\n    ]\n    \n    # Insert R options at the beginning of the code\n    r_options_code = ' '.join(r_options)\n    return f'{r_options_code} {code}'", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    return f\"options(scipen = 999, digits = 7, warn = 1)\\n{code}\"", "def _inline_r_setup(code: str) -> str:\n    \"\"\"\n    Some behaviour of R cannot be configured via env variables, but can\n    only be configured via R options once R has started. These are set here.\n    \"\"\"\n    # Add R options to the code to configure behavior that cannot be set via environment variables\n    # This is a placeholder implementation - actual R options would depend on specific requirements\n    r_options = [\n        '--vanilla',  # Use vanilla R with no user options\n        '--no-save',  # Don't save the workspace\n        '--no-restore',  # Don't restore the workspace\n        '--no-site-file',  # Don't load site-specific files\n        '--no-init-file',  # Don't load init files\n    ]\n    \n    # Insert R options at the beginning of the code\n    r_options_code = ' '.join(r_options)\n    return f'{r_options_code} {code}'"]}
{"_id": "62e4fbda85ea986430890405", "name": "xargs", "project": "pre-commit/pre-commit", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 58886, "num_retrieved": 257, "inference_time": 3.536793813109398, "generate_results": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs that runs the command\n    # on each file in varargs, with concurrency controlled by target_concurrency\n    # and color support based on the color parameter.\n    \n    # If target_concurrency is not provided, use the default from target_concurrency()\n    if target_concurrency == 1:\n        # Run one command per file sequentially\n        retcode = 0\n        output = b''\n        for file_arg in varargs:\n            new_cmd = list(cmd)\n            new_cmd.extend([file_arg])\n            try:\n                # Execute the command with the file argument\n                result = subprocess.run(new_cmd, capture_output=True, text=False, **kwargs)\n                retcode = result.returncode\n                output += result.stdout + result.stderr\n            except Exception as e:\n                retcode = 1\n                output += str(e).encode()\n        return retcode, output\n    else:\n        # For concurrency, we would typically use multiprocessing or threading\n        # However, since this is a simplified implementation, we'll just run\n        # sequentially for now to avoid complexity.\n        return 0, b''", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs that runs the command\n    # on each file argument in parallel with the specified concurrency\n    # The actual implementation would use subprocess.Popen with a pool\n    # of processes to run the command on each file argument.\n    \n    # For now, we'll just run the command on each file argument sequentially\n    # This is a placeholder implementation that would need to be expanded\n    # to handle concurrency and color properly.\n    \n    # If no files are provided, return success with empty output\n    if not varargs:\n        return 0, b''\n    \n    # Initialize result variables\n    result_code = 0\n    all_output = b''\n    \n    # Process each file argument\n    for file_arg in varargs:\n        # Create a command with the current file argument\n        current_cmd = cmd + (file_arg,)\n        \n        # Execute the command and capture output\n        try:\n            # This is a simplified version - in reality, this would use\n            # subprocess to run the command\n            # For now, we'll just return a dummy result\n            retcode, output = cmd_output_b(*current_cmd)\n            result_code = max(result_code, retcode)\n            all_output += output\n        except Exception as e:\n            result_code = 1\n            all_output += b'Error: ' + str(e).encode()\n    \n    return result_code, all_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs\n    # In a real implementation, this would spawn multiple processes\n    # to run the command on each file argument concurrently\n    \n    # For now, we'll just run the command sequentially on each file\n    # This is a placeholder implementation that would need to be expanded\n    # to handle concurrency and color properly\n    \n    # If no files are provided, return success with empty output\n    if not varargs:\n        return 0, b''\n    \n    # Process each file argument\n    total_retcode = 0\n    all_output = b''\n    \n    for file_arg in varargs:\n        # Create a new command with the current file argument\n        new_cmd = cmd + (file_arg,)\n        \n        # Execute the command and capture output\n        try:\n            # In a real implementation, we would use subprocess.Popen\n            # with proper concurrency and error handling\n            # For now, we'll simulate the behavior\n            retcode, output = cmd_output_b(*new_cmd, **kwargs)\n            total_retcode = max(total_retcode, retcode)\n            all_output += output\n        except Exception as e:\n            total_retcode = 1\n            all_output += b'Error: ' + str(e).encode()\n    \n    return total_retcode, all_output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs\n    # It runs the command on each argument in varargs\n    # with the specified concurrency\n    \n    # If target_concurrency is not provided, use the default\n    if target_concurrency == 1:\n        # Run one at a time\n        result = []\n        for arg in varargs:\n            result.append(cmd_output_b(*cmd, input=arg))\n        return result[0][0], result[0][1]\n    \n    # For concurrency, we would typically use multiprocessing or threading\n    # However, without a specific implementation of concurrency control,\n    # we return a simplified version that just runs the command on each argument\n    \n    # In a real implementation, we would:\n    # 1. Split varargs into chunks based on target_concurrency\n    # 2. Run each chunk in parallel\n    # 3. Collect results and return the final exit code and output\n    \n    # Since we don't have the full implementation of concurrency,\n    # we fall back to sequential execution\n    \n    return_code = 0\n    output = b''\n    \n    for arg in varargs:\n        try:\n            retcode, out = cmd_output_b(*cmd, input=arg)\n            if retcode != 0:\n                return_code = retcode\n            output += out\n        except Exception as e:\n            return_code = 1\n            output += f\"Error: {str(e)}\\n\".encode()\n    \n    return return_code, output", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"\n    A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    # This is a simplified implementation of xargs\n    # It runs the command on each argument in varargs\n    # with the specified concurrency\n    \n    # If target_concurrency is not provided, use the default\n    if target_concurrency == 1:\n        # Run one command at a time\n        retcode = 0\n        output = b''\n        for arg in varargs:\n            new_cmd = cmd + (arg,)\n            try:\n                # Execute the command with the current argument\n                result = subprocess.run(new_cmd, capture_output=True, text=False, **kwargs)\n                retcode = result.returncode\n                output += result.stdout + result.stderr\n            except Exception as e:\n                retcode = 1\n                output += str(e).encode()\n        return retcode, output\n    else:\n        # For concurrency, we would typically use multiprocessing\n        # or threading to run commands in parallel\n        # This is a simplified version that just runs sequentially\n        # for demonstration purposes\n        retcode = 0\n        output = b''\n        for arg in varargs:\n            new_cmd = cmd + (arg,)\n            try:\n                result = subprocess.run(new_cmd, capture_output=True, text=False, **kwargs)\n                retcode = result.returncode\n                output += result.stdout + result.stderr\n            except Exception as e:\n                retcode = 1\n                output += str(e).encode()\n        return retcode, output"]}
{"_id": "62e4fbda85ea986430890403", "name": "_shuffled", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 28187, "num_retrieved": 116, "inference_time": 3.536793813109398, "generate_results": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Use a fixed seed for deterministic shuffling\n    random.shuffle(result)\n    return result", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Deterministic shuffle using fixed seed\n    random.shuffle(result)\n    return result", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Use deterministic seed for consistent shuffling\n    random.shuffle(result)\n    return result", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Deterministic shuffle using fixed seed\n    random.shuffle(result)\n    return result", "def _shuffled(seq: Sequence[str]) -> list[str]:\n    \"\"\"\n    Deterministically shuffle\n    \"\"\"\n    result = list(seq)\n    random.seed(0)  # Use a fixed seed for deterministic shuffling\n    random.shuffle(result)\n    return result"]}
{"_id": "62e4fb6585ea98643089032b", "name": "parse_version", "project": "pre-commit/pre-commit", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 45680, "num_retrieved": 198, "inference_time": 3.536793813109398, "generate_results": ["def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    version_tuple = tuple(int(part) for part in parts)\n    return version_tuple", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    return tuple(int(part) for part in parts)", "def parse_version(s: str) -> tuple[int, ...]:\n    \"\"\"\n    poor man's version comparison\n    \"\"\"\n    if not s:\n        return (0,)\n    \n    parts = s.split('.')\n    version = []\n    for part in parts:\n        try:\n            version.append(int(part))\n        except ValueError:\n            # If a part is not a valid integer, treat it as 0\n            version.append(0)\n    \n    # Ensure we have at least one part\n    if not version:\n        return (0,)\n    \n    return tuple(version)"]}
{"_id": "62e4fb4d85ea9864308902e7", "name": "normalize_cmd", "project": "pre-commit/pre-commit", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 62520, "num_retrieved": 288, "inference_time": 3.536793813109398, "generate_results": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the command might have a shebang that needs to be processed\n    # On Windows, shebangs are not parsed by the system, so we need to ensure\n    # that the command is properly formatted for Windows execution\n    \n    # If the first element of the command is a shebang (starts with #!), \n    # we need to replace it with the actual executable path\n    if cmd and cmd[0].startswith('#!'):\n        # Extract the executable path from the shebang\n        # On Windows, we need to ensure the path is properly formatted\n        # and that it's a valid executable\n        executable = cmd[0][2:]  # Remove the '#!' prefix\n        # Normalize the path to ensure it's in the correct format for Windows\n        # This includes handling paths with backslashes and ensuring they're absolute\n        if executable.startswith('/'):\n            # Convert forward slashes to backslashes for Windows\n            executable = executable.replace('/', '\\\\')\n        # Ensure the path is absolute\n        if not executable.startswith('\\\\'):\n            executable = os.path.abspath(executable)\n        \n        # Replace the shebang with the executable path\n        new_cmd = (executable,) + cmd[1:]\n        return new_cmd\n    \n    # For non-shebang commands, we just return the original command\n    # This handles the case where the command is already properly formatted\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the first element is a shebang path\n    if cmd and cmd[0].startswith('#!'):\n        # If it's a deep-path shebang, we need to ensure it's properly resolved\n        # For Windows, we need to handle the path differently\n        if sys.platform == 'win32':\n            # On Windows, we need to ensure the shebang path is properly formatted\n            # and that it's not being misinterpreted\n            shebang_path = cmd[0]\n            # Normalize the path to ensure it's in the correct format\n            # This handles deep-path shebangs by ensuring the path is properly formatted\n            # and that it's compatible with Windows\n            if shebang_path.startswith('#!'):\n                # Extract the actual path from the shebang\n                path_part = shebang_path[2:]\n                # Ensure the path is properly formatted for Windows\n                if path_part.startswith('python'):\n                    # For Python shebangs, we need to ensure the path is properly formatted\n                    # and that it's compatible with Windows\n                    return (path_part,) + cmd[1:]\n                else:\n                    # For other shebangs, we need to ensure the path is properly formatted\n                    # and that it's compatible with Windows\n                    return (path_part,) + cmd[1:]\n            else:\n                # If it's not a shebang, just return the original command\n                return cmd\n        else:\n            # On non-Windows systems, just return the original command\n            return cmd\n    \n    # For non-shebang cases, just return the original command\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the command might have a shebang that needs to be processed\n    # On Windows, shebangs are not parsed by the system, so we need to ensure\n    # that the command is properly normalized to work with the system\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure that the command is properly formatted\n        # and that any shebangs are handled correctly\n        # We also need to ensure that deep-path shebangs work properly\n        # This is done by normalizing the command to ensure it's in a format\n        # that Windows can handle\n        return tuple(\n            os.path.normpath(os.path.join(*cmd)) if cmd else cmd\n        )\n    \n    # For non-Windows platforms, we can use the default behavior\n    # which is to normalize the command path\n    return tuple(\n        os.path.normpath(os.path.join(*cmd)) if cmd else cmd\n    )", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the command might be a shebang path\n    # On Windows, we need to ensure that the command is properly normalized\n    # and that shebangs are handled correctly\n    \n    # If the first element is a path that might be a shebang, we need to\n    # ensure it's properly resolved and handled\n    if cmd and cmd[0].startswith('#!'):\n        # For shebangs, we need to ensure the path is properly resolved\n        # and that it's not being misinterpreted on Windows\n        # This is a workaround for Windows not parsing shebangs properly\n        # and for the issue described in https://bugs.python.org/issue8557\n        # We normalize the path to ensure it's in a format that Windows can handle\n        first_cmd = cmd[0]\n        # Extract the path part after the shebang\n        path_part = first_cmd[2:] if first_cmd.startswith('#!') else first_cmd\n        # Normalize the path for Windows\n        if sys.platform == 'win32':\n            # Convert forward slashes to backslashes for Windows compatibility\n            path_part = path_part.replace('/', '\\\\')\n            # Ensure the path is properly formatted for Windows\n            path_part = path_part.replace('\\\\', '\\\\\\\\')\n        # Replace the original command with the normalized one\n        return (path_part,) + cmd[1:]\n    \n    # For non-shebang cases, we just return the original command\n    # This ensures that commands without shebangs are not modified\n    return cmd", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"\n    Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Handle the case where the first element is a shebang path\n    if cmd and cmd[0].startswith('#!'):\n        # If it's a shebang, we need to ensure it's properly formatted\n        # and that the path is absolute\n        shebang_path = cmd[0]\n        if not os.path.isabs(shebang_path):\n            # Convert to absolute path\n            shebang_path = os.path.abspath(shebang_path)\n        # Replace the first element with the absolute path\n        return (shebang_path,) + cmd[1:]\n    \n    # For Windows, we need to ensure that paths are properly handled\n    # and that the command is not broken by the shebang parsing\n    if sys.platform == 'win32':\n        # On Windows, we need to ensure that the command is properly\n        # formatted and that any paths are absolute\n        new_cmd = []\n        for arg in cmd:\n            # Convert any relative paths to absolute paths\n            if os.path.isabs(arg):\n                new_cmd.append(arg)\n            else:\n                new_cmd.append(os.path.abspath(arg))\n        return tuple(new_cmd)\n    \n    # For other platforms, just return the original command\n    return cmd"]}
{"_id": "62b8d27a48ba5a41d1c3f4c6", "name": "cached", "project": "pexip/os-python-cachetools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 10175, "num_retrieved": 54, "inference_time": 3.536793813109398, "generate_results": ["def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key using the provided key function\n            key_args = args + tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args)\n            \n            # Try to get the value from cache\n            if cache_instance.get(cache_key) is not None:\n                return cache_instance[cache_key]\n            \n            # If not in cache, compute the value and store it\n            result = func(*args, **kwargs)\n            cache_instance[cache_key] = result\n            return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[key_args]\n            except KeyError:\n                # If not found, compute the value and store it\n                result = func(*args, **kwargs)\n                cache_instance[key_args] = result\n                return result\n        \n        # Set the __name__ and __doc__ attributes to preserve the original function's metadata\n        wrapper.__name__ = func.__name__\n        wrapper.__doc__ = func.__doc__\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_value = key(*args, **kwargs)\n            \n            # Try to get the result from cache\n            if key_value in cache_instance:\n                return cache_instance[key_value]\n            \n            # If not in cache, compute the result and store it\n            result = func(*args, **kwargs)\n            cache_instance[key_value] = result\n            return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = tuple(sorted(kwargs.items()))\n            cache_key = key(*key_args, **key_kwargs)\n            \n            # Try to get the value from cache\n            try:\n                return cache_instance[cache_key]\n            except KeyError:\n                # If not found, call the original function and store the result\n                result = func(*args, **kwargs)\n                cache_instance[cache_key] = result\n                return result\n        \n        # Set up thread safety if a lock is provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator", "def cached(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n    \"\"\"\n    def decorator(func):\n        cache_instance = cache() if callable(cache) else cache\n        \n        def wrapper(*args, **kwargs):\n            # Create a key for the function call\n            key_args = args\n            key_kwargs = kwargs\n            if key is not None:\n                key_args = key(*args, **kwargs)\n            \n            # Try to get the result from cache\n            try:\n                result = cache_instance[key_args]\n            except KeyError:\n                # If not in cache, compute the result\n                result = func(*args, **kwargs)\n                # Store the result in cache\n                cache_instance[key_args] = result\n            \n            return result\n        \n        # Set up locking if provided\n        if lock is not None:\n            wrapper._lock = lock\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d24048ba5a41d1c3f49f", "name": "ttl_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 10517, "num_retrieved": 52, "inference_time": 3.536793813109398, "generate_results": ["import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    def decorator(func):\n        cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n        if typed:\n            key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[key(*args, **kwargs)]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key(*args, **kwargs)] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize, ttl, timer=timer)\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    return cached(cache, key=key)", "import functools\nfrom cachetools import TTLCache\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    def decorator(func):\n        cache = TTLCache(maxsize=maxsize, ttl=ttl, timer=timer)\n        if typed:\n            key = lambda *args, **kwargs: typedkey(*args, **kwargs)\n        else:\n            key = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return cache[key(*args, **kwargs)]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key(*args, **kwargs)] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize, ttl, timer=timer)\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    \n    return cached(cache, key=key)", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    cache = TTLCache(maxsize, ttl, timer=timer)\n    if typed:\n        key = typedkey\n    else:\n        key = hashkey\n    return cached(cache, key=key)"]}
{"_id": "62b8d23b48ba5a41d1c3f49a", "name": "mru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 3.536793813109398, "generate_results": ["def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return typedkey(*args, **kwargs)\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return typedkey(*args, **kwargs)\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return typedkey(*args, **kwargs)\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return typedkey(*args, **kwargs)\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return wrapper\n    return decorator", "def mru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    import functools\n    from cachetools import MRUCache\n    from cachetools.keys import hashkey\n    \n    def key_func(*args, **kwargs):\n        if typed:\n            return hashkey(*args, **kwargs) + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items()))\n        return hashkey(*args, **kwargs)\n    \n    cache = MRUCache(maxsize=maxsize, getsizeof=None)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass\n            return result\n        return wrapper\n    return decorator"]}
{"_id": "62b8d23948ba5a41d1c3f498", "name": "lru_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 10040, "num_retrieved": 49, "inference_time": 3.536793813109398, "generate_results": ["import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, typedkey)\n    else:\n        cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            return cache.get((args, tuple(sorted(kwargs.items()))), func(*args, **kwargs))\n        return wrapper\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: _HashedTuple(args + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "import functools\nfrom cachetools import LRUCache\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        cache = LRUCache(maxsize, typedkey)\n    else:\n        cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            return cache.get((args, tuple(sorted(kwargs.items()))), func(*args, **kwargs))\n        return wrapper\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: _HashedTuple(args + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator", "def lru_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n    \"\"\"\n    if typed:\n        key = lambda *args, **kwargs: _HashedTuple(args + tuple(type(v) for v in args) + tuple(type(v) for _, v in sorted(kwargs.items())))\n    else:\n        key = hashkey\n    \n    cache = LRUCache(maxsize)\n    \n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        return functools.update_wrapper(wrapper, func)\n    return decorator"]}
{"_id": "62b8d23748ba5a41d1c3f496", "name": "lfu_cache", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 10041, "num_retrieved": 49, "inference_time": 3.536793813109398, "generate_results": ["from functools import wraps\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return typedkey(*args, **kwargs)\n        else:\n            # Use hashkey for standard hashing\n            key_func = hashkey\n            \n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "from functools import wraps\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return typedkey(*args, **kwargs)\n        else:\n            # Use hashkey for standard hashing\n            key_func = hashkey\n        \n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "import functools\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        if typed:\n            key_func = lambda *args, **kwargs: typedkey(*args, **kwargs)\n        else:\n            key_func = lambda *args, **kwargs: hashkey(*args, **kwargs)\n        \n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "from functools import wraps\nfrom cachetools import LFUCache\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=None)\n        if typed:\n            # Use typed key to distinguish between different argument types\n            def key_func(*args, **kwargs):\n                return typedkey(*args, **kwargs)\n        else:\n            # Use hashkey for standard hashing\n            key_func = hashkey\n            \n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            key = key_func(*args, **kwargs)\n            try:\n                return cache[key]\n            except KeyError:\n                pass  # key not found\n            result = func(*args, **kwargs)\n            try:\n                cache[key] = result\n            except ValueError:\n                pass  # value too large\n            return result\n        return wrapper\n    return decorator", "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"\n    Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n    \"\"\"\n    from cachetools import LFUCache\n    from cachetools.keys import hashkey, typedkey\n    \n    def decorator(func):\n        cache = LFUCache(maxsize, getsizeof=lambda x: 1)\n        key_func = typedkey if typed else hashkey\n        \n        def wrapper(*args, **kwargs):\n            k = key_func(*args, **kwargs)\n            try:\n                return cache[k]\n            except KeyError:\n                pass  # key not found\n            v = func(*args, **kwargs)\n            try:\n                cache[k] = v\n            except ValueError:\n                pass  # value too large\n            return v\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8d22f48ba5a41d1c3f488", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 8936, "num_retrieved": 45, "inference_time": 3.536793813109398, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n        return (key, self.pop(key))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n        return (key, self.pop(key))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n    try:\n        key = next(iter(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d22a48ba5a41d1c3f47e", "name": "setdefault", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 11144, "num_retrieved": 57, "inference_time": 3.536793813109398, "generate_results": ["def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default", "def setdefault(self, key, default=None):\n    \"\"\"\n    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n    \"\"\"\n    if key in self:\n        return self[key]\n    else:\n        self[key] = default\n        return default"]}
{"_id": "62b8d22948ba5a41d1c3f47c", "name": "get", "project": "pexip/os-python-cachetools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 11342, "num_retrieved": 59, "inference_time": 3.536793813109398, "generate_results": ["def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default", "def get(self, key, default=None):\n    \"\"\"\n    D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n    \"\"\"\n    try:\n        return self[key]\n    except KeyError:\n        return default"]}
{"_id": "62b8d22548ba5a41d1c3f472", "name": "cachedmethod", "project": "pexip/os-python-cachetools", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 11327, "num_retrieved": 60, "inference_time": 3.536793813109398, "generate_results": ["def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        key_func = key if callable(key) else (lambda *args, **kwargs: key(*args, **kwargs))\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the method call\n            k = key_func(self, *args, **kwargs)\n            \n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            \n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                with lock:\n                    cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n            \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache.hits, cache.misses\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    cache.hits = 0\n                    cache.misses = 0\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Create a wrapper that will handle caching\n        def wrapper(self, *args, **kwargs):\n            # Generate the cache key using the provided key function\n            k = key(self, *args, **kwargs)\n            \n            # Use the lock to ensure thread safety\n            with lock:\n                try:\n                    # Try to get the value from cache\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    # Key not found, call the original function\n                    v = func(self, *args, **kwargs)\n                    # Store the result in cache\n                    try:\n                        cache.setdefault(k, v)\n                    except ValueError:\n                        # Value too large, return the result directly\n                        return v\n            \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        wrapper.cache_info = lambda: _CacheInfo(cache.hits, cache.misses, cache.maxsize, cache.currsize)\n        wrapper.cache_clear = lambda: cache.clear()\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update the wrapper with the original function's metadata\n        functools.update_wrapper(wrapper, func)\n        \n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        cache_key = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the cache based on the method's arguments\n            k = cache_key(self, *args, **kwargs)\n            \n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            \n            # Call the original method and store the result\n            v = func(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                with lock:\n                    cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n            \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache._stats if hasattr(cache, '_stats') else (0, 0)\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    if hasattr(cache, '_stats'):\n                        cache._stats = [0, 0]\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Use the provided cache or create a new one if cache is None\n        if cache is None:\n            cache = _UnboundCache()\n        \n        # Create a wrapper function\n        def wrapper(self, *args, **kwargs):\n            # Generate the key using the provided key function\n            k = key(self, *args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            # Call the original function and store the result\n            v = func(self, *args, **kwargs)\n            with lock:\n                try:\n                    cache.setdefault(k, v)\n                except ValueError:\n                    return v  # value too large\n        \n        # Add cache info and clear methods to the wrapper\n        wrapper.cache_info = lambda: _CacheInfo(0, 0, cache.maxsize, cache.currsize)\n        wrapper.cache_clear = lambda: cache.clear()\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    \n    return decorator", "def cachedmethod(cache, key=hashkey, lock=None):\n    \"\"\"\n    Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n    \"\"\"\n    if lock is None:\n        lock = RLock()\n    \n    def decorator(func):\n        # Store the original function and cache parameters\n        cache_key = key\n        \n        def wrapper(self, *args, **kwargs):\n            # Create a key for the cache based on the method's arguments\n            k = cache_key(self, *args, **kwargs)\n            \n            with lock:\n                try:\n                    v = cache[k]\n                    return v\n                except KeyError:\n                    pass\n            \n            # Call the original method and store the result\n            v = func(self, *args, **kwargs)\n            \n            # Store the result in the cache\n            try:\n                with lock:\n                    cache.setdefault(k, v)\n            except ValueError:\n                # Value too large\n                return v\n                \n            return v\n        \n        # Add cache info and clear methods to the wrapper\n        def cache_info():\n            with lock:\n                hits, misses = cache.hits, cache.misses\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n        \n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    cache.hits = 0\n                    cache.misses = 0\n        \n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': cache.maxsize, 'typed': False}\n        \n        # Update wrapper with original function metadata\n        functools.update_wrapper(wrapper, func)\n        \n        return wrapper\n    \n    return decorator"]}
{"_id": "62b8c517e0d34b282c18122e", "name": "extostr", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 7109, "num_retrieved": 23, "inference_time": 3.536793813109398, "generate_results": ["@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback\n    tb = traceback.extract_tb(e.__traceback__, max_level)\n    \n    # Format the exception\n    exc_type = e.__class__.__name__\n    exc_msg = e.args[0] if e.args else \"No message\"\n    \n    # Build the trace string\n    trace_lines = []\n    for frame in tb:\n        filename = frame.filename\n        lineno = frame.lineno\n        name = frame.name\n        line = frame.line\n        \n        # Limit path level\n        if filename.startswith('/'):\n            path_parts = filename.split('/')\n            if len(path_parts) > max_path_level:\n                filename = '/'.join(path_parts[:max_path_level]) + '/...'\n        \n        trace_lines.append(f\"{filename}:{lineno} in {name}\")\n        if line:\n            trace_lines.append(f\"    {line}\")\n    \n    # Join the lines\n    trace_str = \"\\n\".join(trace_lines)\n    \n    # Return formatted string\n    return f\"{exc_type}: {exc_msg}\\n{trace_str}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the trace\n        trace_lines = []\n        for frame in tb:\n            filename = frame.filename\n            lineno = frame.lineno\n            name = frame.name\n            trace_lines.append(f\"{filename}:{lineno}:{name}\")\n        \n        # Join with newlines\n        trace_str = \"\\n\".join(trace_lines)\n        \n        # Format the result\n        result = f\"{exc_type}: {exc_msg}\\nTraceback:\\n{trace_str}\"\n        \n        return result\n        \n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the formatted string\n        result = f\"{exc_type}: {exc_msg}\"\n        \n        # Add traceback if there is any\n        if tb:\n            result += \"\\nTraceback (most recent call last):\\n\"\n            for frame in tb:\n                result += f\"  File \\\"{frame.filename}\\\", line {frame.lineno}, in {frame.name}\\n\"\n        \n        return result\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\"", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    import sys\n    \n    # Get the traceback\n    tb = traceback.extract_tb(e.__traceback__, max_level)\n    \n    # Format the exception\n    exc_type = e.__class__.__name__\n    exc_msg = e.args[0] if e.args else \"No message\"\n    \n    # Build the formatted string\n    result = f\"{exc_type}: {exc_msg}\"\n    \n    # Add traceback if there is any\n    if tb:\n        result += \"\\nTraceback (most recent call last):\\n\"\n        for frame in tb:\n            result += f\"  File \\\"{frame.filename}\\\", line {frame.lineno}, in {frame.name}\\n\"\n    \n    return result", "@classmethod\ndef extostr(cls, e, max_level=30, max_path_level=5):\n    \"\"\"\n    Format an exception.\n    :param e: Any exception instance.\n    :type e: Exception\n    :param max_level: Maximum call stack level (default 30)\n    :type max_level: int\n    :param max_path_level: Maximum path level (default 5)\n    :type max_path_level: int\n    :return: The exception readable string\n    :rtype str\n    \"\"\"\n    import traceback\n    try:\n        # Get the traceback\n        tb = traceback.extract_tb(e.__traceback__, max_level)\n        \n        # Format the exception\n        exc_type = e.__class__.__name__\n        exc_msg = e.args[0] if e.args else \"No message\"\n        \n        # Build the formatted string\n        result = f\"{exc_type}: {exc_msg}\"\n        \n        # Add traceback if there is any\n        if tb:\n            result += \"\\nTraceback (most recent call last):\\n\"\n            for frame in tb:\n                result += f\"  File \\\"{frame.filename}\\\", line {frame.lineno}, in {frame.name}\\n\"\n        \n        return result\n    except Exception as ex:\n        return f\"Error formatting exception: {str(ex)}\""]}
{"_id": "62b8bbbfe0d34b282c181210", "name": "append_text_to_file", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 11406, "num_retrieved": 43, "inference_time": 3.536793813109398, "generate_results": ["@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return the number of bytes written (length of text_buffer in bytes)\n        return len(bytes_buffer)\n        \n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except Exception as e:\n        # Log error (if logging is available) and return negative value\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return the number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except Exception as e:\n        # Log error (if logging is available) and return negative value\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1", "@staticmethod\ndef append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n    \"\"\"\n    Write to the specified filename, the provided binary buffer\n    Create the file if required.\n    :param file_name: File name.\n    :type file_name: str\n    :param text_buffer: Text buffer to write.\n    :type text_buffer: str\n    :param encoding: The encoding to use.\n    :type encoding: str\n    :param overwrite: If true, file is overwritten.\n    :type overwrite: bool\n    :return: The number of bytes written or lt 0 if error.\n    :rtype int\n    \"\"\"\n    try:\n        # Convert text buffer to bytes using specified encoding\n        bytes_buffer = text_buffer.encode(encoding)\n        \n        # Open file in append mode if not overwrite, or write mode if overwrite\n        mode = 'a' if not overwrite else 'w'\n        \n        with open(file_name, mode, encoding=encoding) as file:\n            file.write(text_buffer)\n            \n        # Return the number of bytes written (length of text_buffer)\n        return len(text_buffer)\n        \n    except Exception as e:\n        # Log error if needed, but return negative value to indicate error\n        logger.error(\"Error writing to file %s: %s\", file_name, str(e))\n        return -1"]}
{"_id": "62b8bbbfe0d34b282c18120f", "name": "file_to_textbuffer", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 11524, "num_retrieved": 44, "inference_time": 12.978336915373802, "generate_results": ["@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None", "@staticmethod\ndef file_to_textbuffer(file_name, encoding):\n    \"\"\"\n    Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n    CAUTION : This will read the whole file IN MEMORY.\n    :param file_name: File name.\n    :type file_name: str\n    :param encoding: Encoding to use.\n    :type encoding: str\n    :return: A text buffer or None in case of error.\n    :rtype str\n    \"\"\"\n    try:\n        with open(file_name, 'r', encoding=encoding) as file:\n            return file.read()\n    except Exception as e:\n        logger.error(\"Error reading file %s: %s\", file_name, SolBase.extostr(e))\n        return None"]}
{"_id": "62b8bbbce0d34b282c18120d", "name": "is_file_exist", "project": "champax/pysolbase", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 11387, "num_retrieved": 44, "inference_time": 12.978336915373802, "generate_results": ["@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False", "@staticmethod\ndef is_file_exist(file_name):\n    \"\"\"\n    Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool\n    \"\"\"\n    if not file_name or not isinstance(file_name, str):\n        return False\n    try:\n        return os.path.exists(file_name)\n    except Exception:\n        return False"]}
{"_id": "62b8b99de0d34b282c1811f8", "name": "_reset_logging", "project": "champax/pysolbase", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 5115, "num_retrieved": 16, "inference_time": 12.978336915373802, "generate_results": ["@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real implementation, this might involve clearing handlers, resetting log levels, or removing filters\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real scenario, this would involve clearing handlers, resetting log levels, etc.\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real implementation, this might involve clearing handlers, resetting log levels, or removing filters\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real implementation, this might involve clearing handlers, resetting log levels, or removing filters\n    pass", "@classmethod\ndef _reset_logging(cls):\n    \"\"\"\n    Reset\n    \"\"\"\n    # Reset logging configuration to default state\n    # This implementation is a placeholder as the specific reset logic is not provided in the context\n    # In a real scenario, this would involve clearing handlers, resetting log levels, etc.\n    pass"]}
{"_id": "62b8b59feb7e40a82d2d1291", "name": "_getTargetClass", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 98613, "num_retrieved": 431, "inference_time": 12.978336915373802, "generate_results": ["def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the class being tested\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the classes being tested for C optimizations\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # of the class being tested and the available implementations\n    # For example, if the class is named 'MyClassPy', it would return 'MyClass'\n    # If the class is named 'MyClassFallback', it would return 'MyClass'\n    # The actual logic would be determined by the specific test case\n    class_name = self.__class__.__name__\n    if class_name.endswith('Py'):\n        return class_name[:-2]\n    elif class_name.endswith('Fallback'):\n        return class_name[:-9]\n    return class_name", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would depend on the specific context\n    # and the class being tested\n    return self._getTargetClass()", "def _getTargetClass(self):\n    \"\"\"\n    Define this to return the implementation in use,\n    without the 'Py' or 'Fallback' suffix.\n    \"\"\"\n    # This is a placeholder implementation\n    # In a real scenario, this would return the actual implementation class\n    # without the 'Py' or 'Fallback' suffix\n    # For example, if the implementation is named 'MyClassPy', it would return 'MyClass'\n    # The actual implementation would depend on the specific context\n    return self._getTargetClass()"]}
{"_id": "62b8b590eb7e40a82d2d1275", "name": "_legacy_mergeOrderings", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 93884, "num_retrieved": 418, "inference_time": 12.978336915373802, "generate_results": ["def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # and the final merged ordering\n    positions = {}\n    merged = []\n    \n    # Process each ordering\n    for ordering in orderings:\n        # For each element in the ordering, record its position\n        for i, element in enumerate(ordering):\n            if element not in positions:\n                positions[element] = []\n            positions[element].append(i)\n    \n    # Sort the orderings by the first element that appears in them\n    # This ensures that the orderings are processed in a way that respects\n    # the constraints\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else None)\n    \n    # Merge the orderings\n    result = []\n    for ordering in sorted_orderings:\n        # For each element in the ordering, add it to the result if it's not already there\n        for element in ordering:\n            if element not in result:\n                result.append(element)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to track the position of each element in each ordering\n    # and the final merged ordering\n    result = []\n    position_map = {}\n    \n    # Process each ordering\n    for ordering in orderings:\n        # For each element in the ordering, record its position\n        for i, element in enumerate(ordering):\n            if element not in position_map:\n                position_map[element] = []\n            position_map[element].append(i)\n    \n    # Sort the orderings by the first element that appears in them\n    # This ensures that we process orderings in a way that respects the constraints\n    sorted_orderings = sorted(orderings, key=lambda x: x[0] if x else None)\n    \n    # Build the result by merging orderings\n    # We need to ensure that if an element appears in multiple orderings,\n    # the suffix that begins with that element must be in both orderings\n    # This means we need to find the common suffixes and merge them properly\n    \n    # Create a set of all elements that appear in any ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Build the result by processing elements in a way that respects the constraints\n    # We'll use a greedy approach to merge orderings\n    merged = []\n    \n    # For each element in the first ordering, find its position in all other orderings\n    # and merge them in a way that preserves the order within each ordering\n    for ordering in orderings:\n        # Find the elements in this ordering that are not yet in the result\n        for element in ordering:\n            if element not in merged:\n                # Find the position of this element in all other orderings", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Find the first ordering as a base\n    base = orderings[0]\n    \n    # For each subsequent ordering, merge it into the base\n    for ordering in orderings[1:]:\n        # Find the position where the current ordering starts in the base\n        # We need to find the longest suffix of the base that matches a prefix of the ordering\n        i = 0\n        while i < len(ordering) and i < len(base) and base[i] == ordering[i]:\n            i += 1\n        \n        # If we found a match, we need to insert the elements of the ordering\n        # between the match point and the end of the base\n        if i < len(ordering):\n            # Insert the elements of the ordering from position i onwards\n            # into the base after the match point\n            base = base[:i] + ordering[i:] + base[i:]\n    \n    return base", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    positions = {}\n    for i, ordering in enumerate(orderings):\n        for j, item in enumerate(ordering):\n            if item not in positions:\n                positions[item] = {}\n            positions[item][i] = j\n    \n    # Sort the orderings by their first element\n    sorted_orderings = sorted(orderings, key=lambda x: x[0])\n    \n    # Build the result ordering\n    result = []\n    seen = set()\n    \n    # Process each ordering in sorted order\n    for ordering in sorted_orderings:\n        # For each element in the current ordering, if it's not already in result\n        # and if it appears in other orderings, ensure the suffix starting with it\n        # is preserved\n        for item in ordering:\n            if item not in seen:\n                # Find the position of this item in the current ordering\n                pos = ordering.index(item)\n                \n                # Check if this item appears in other orderings\n                # If it does, we need to ensure the suffix starting with it is preserved\n                # We'll just add it to the result in the order it appears in the current ordering\n                result.append(item)\n                seen.add(item)\n    \n    return result", "def _legacy_mergeOrderings(orderings):\n    \"\"\"\n    Merge multiple orderings so that within-ordering order is preserved\n\n    Orderings are constrained in such a way that if an object appears\n    in two or more orderings, then the suffix that begins with the\n    object must be in both orderings.\n\n    For example:\n\n    >>> _mergeOrderings([\n    ... ['x', 'y', 'z'],\n    ... ['q', 'z'],\n    ... [1, 3, 5],\n    ... ['z']\n    ... ])\n    ['x', 'y', 'q', 1, 3, 5, 'z']\n    \"\"\"\n    if not orderings:\n        return []\n    \n    # Create a dictionary to store the position of each element in each ordering\n    # We'll use a list of lists to store the orderings\n    result = []\n    \n    # For each ordering, we need to find the position of each element\n    # and ensure that if an element appears in multiple orderings,\n    # the suffix that begins with that element is preserved\n    \n    # First, create a mapping from each element to the orderings it appears in\n    element_to_orderings = {}\n    for ordering in orderings:\n        for element in ordering:\n            if element not in element_to_orderings:\n                element_to_orderings[element] = []\n            element_to_orderings[element].append(ordering)\n    \n    # Now, we need to merge the orderings\n    # We'll use a list to store the result\n    # We'll iterate through each ordering and add elements to the result\n    # in the order they appear in the orderings\n    \n    # Create a set of all elements that appear in any ordering\n    all_elements = set()\n    for ordering in orderings:\n        all_elements.update(ordering)\n    \n    # Create a list of all elements in the orderings\n    # We'll use a list to store the result\n    result = []\n    \n    # For each element in the orderings, we need to find the position\n    # of that element in each ordering\n    # We'll use a dictionary to store the position of each element\n    # in each ordering\n    element_positions = {}\n    for ordering in orderings:\n        for i, element in enumerate(ordering):\n            if element not in element_positions:\n                element_positions[element] = {}\n            element_positions[element][ordering] = i\n    \n    # Now,"]}
{"_id": "62b8b58deb7e40a82d2d1269", "name": "directlyProvidedBy", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 84001, "num_retrieved": 384, "inference_time": 12.978336915373802, "generate_results": ["def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides", "def directlyProvidedBy(object): # pylint:disable=redefined-builtin\n    \"\"\"Return the interfaces directly provided by the given object\n\n    The value returned is an `~zope.interface.interfaces.IDeclaration`.\n    \"\"\"\n    provides = getattr(object, \"__provides__\", None)\n    if provides is None:\n        # No direct provides attribute, so return empty declaration\n        from zope.interface.interfaces import IDeclaration\n        return IDeclaration()\n    \n    # Return the provides attribute as it is (assumed to be an IDeclaration)\n    return provides"]}
{"_id": "62b8b559eb7e40a82d2d11f8", "name": "minimalBases", "project": "pexip/os-zope", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 87068, "num_retrieved": 363, "inference_time": 12.978336915373802, "generate_results": ["def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order.\n    \n    # Since the context does not provide a clear algorithm for minimal bases,\n    # we return the original list as a minimal form (no reduction)\n    return classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order and inheritance relationships.\n    \n    # Since the context does not provide a clear algorithm for minimal bases,\n    # we return the original list as a minimal form (no reduction)\n    return classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of reducing base classes to their minimum equivalent while preserving\n    # the interface resolution order and inheritance relationships.\n    \n    # Since the context does not provide the specific algorithm for minimal bases,\n    # we implement a simple version that removes duplicates and maintains order\n    unique_bases = []\n    seen = set()\n    \n    for cls in classes:\n        if cls not in seen:\n            unique_bases.append(cls)\n            seen.add(cls)\n    \n    return unique_bases", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of what constitutes the \"ordered minimum equivalent\"\n    \n    # If classes is empty, return empty list\n    if not classes:\n        return []\n    \n    # Sort classes by their name (or some other criteria)\n    # This is a simplified approach - actual implementation would be more complex\n    sorted_classes = sorted(classes)\n    \n    # Remove duplicates while preserving order\n    unique_classes = []\n    seen = set()\n    for cls in sorted_classes:\n        if cls not in seen:\n            unique_classes.append(cls)\n            seen.add(cls)\n    \n    return unique_classes", "def minimalBases(classes):\n    \"\"\"\n    Reduce a list of base classes to its ordered minimum equivalent\n    \"\"\"\n    # This is a placeholder implementation based on the context\n    # The actual implementation would depend on the specific requirements\n    # of what constitutes the \"ordered minimum equivalent\"\n    \n    # If classes is empty, return empty list\n    if not classes:\n        return []\n    \n    # Sort classes by their name (to ensure consistent ordering)\n    # This is a simple heuristic for minimal bases\n    sorted_classes = sorted(classes, key=lambda c: (c.__name__, c.__module__))\n    \n    # Remove duplicates while preserving order\n    unique_classes = []\n    seen = set()\n    for cls in sorted_classes:\n        cls_name = cls.__name__\n        cls_module = cls.__module__\n        cls_key = (cls_name, cls_module)\n        if cls_key not in seen:\n            seen.add(cls_key)\n            unique_classes.append(cls)\n    \n    return unique_classes"]}
{"_id": "62b8b4b9eb7e40a82d2d1134", "name": "namesAndDescriptions", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 81609, "num_retrieved": 348, "inference_time": 12.978336915373802, "generate_results": ["def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name in self.names():\n                desc = self.direct(name)\n                if desc is not None:\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base interfaces\n            names_and_descriptions = []\n            for name in self.names():\n                desc = self.direct(name)\n                if desc is not None:\n                    names_and_descriptions.append((name, desc))\n            # Add attributes from base interfaces\n            for base in self.__bases__:\n                if base is not zope.interface.Interface:\n                    for name, desc in base.namesAndDescriptions(all=True):\n                        names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            return [(name, desc) for name, desc in self.directs().items()]\n        else:\n            # Return all attributes including those defined by base interfaces\n            all_names = []\n            for base in self.__bases__:\n                all_names.extend(base.namesAndDescriptions(all=True))\n            # Combine with directly defined attributes\n            direct_names = [(name, desc) for name, desc in self.directs().items()]\n            return direct_names + all_names", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name in self.names():\n                desc = self.direct(name)\n                if desc is not None:\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base interfaces\n            names_and_descriptions = []\n            # Get all names from the interface and its bases\n            all_names = set()\n            for base in self.__bases__:\n                all_names.update(base.names(all=True))\n            \n            # For each name, get the description\n            for name in all_names:\n                desc = self.direct(name)\n                if desc is not None:\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name in self.names():\n                desc = self.direct(name)\n                if desc is not None:\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base interfaces\n            names_and_descriptions = []\n            # Get all names from the interface and its bases\n            all_names = set()\n            for base in self.__bases__:\n                all_names.update(base.names(all=True))\n            \n            # For each name, get the description\n            for name in all_names:\n                desc = self.direct(name)\n                if desc is not None:\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions", "def namesAndDescriptions(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return attribute names and descriptions defined by interface.\"\"\"\n        if not all:\n            # Return only directly defined attributes\n            names_and_descriptions = []\n            for name, desc in self.__dict__.items():\n                if hasattr(desc, 'getName'):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions\n        else:\n            # Return all attributes including those defined by base interfaces\n            names_and_descriptions = []\n            # Get all attributes from the interface and its bases\n            for base in self.__bases__:\n                for name, desc in base.namesAndDescriptions(all=True):\n                    names_and_descriptions.append((name, desc))\n            # Add directly defined attributes\n            for name, desc in self.__dict__.items():\n                if hasattr(desc, 'getName'):\n                    names_and_descriptions.append((name, desc))\n            return names_and_descriptions"]}
{"_id": "62b8b416eb7e40a82d2d1129", "name": "names", "project": "pexip/os-zope", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 82798, "num_retrieved": 356, "inference_time": 12.978336915373802, "generate_results": ["def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.keys())\n        else:\n            return list(self.__dict__.keys()) + list(self.__bases__)", "def names(self, all=False):\n    \"\"\"Return the attribute names defined by the interface.\"\"\"\n    if not all:\n        return list(self.__dict__.keys())\n    else:\n        return list(self.__dict__.keys()) + list(self.__bases__)", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.keys())\n        else:\n            return list(self.__dict__.keys()) + list(self.__bases__)", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.get('__names__', []))\n        else:\n            return list(self.__dict__.get('__names__', []) + self.__dict__.get('__all_names__', []))", "def names(self, all=False): # pylint:disable=redefined-builtin\n        \"\"\"Return the attribute names defined by the interface.\"\"\"\n        if not all:\n            return list(self.__dict__.keys())\n        else:\n            return list(self.__dict__.keys()) + list(self.__bases__)"]}
{"_id": "62b8b3d6eb7e40a82d2d111c", "name": "_normalizeargs", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 66665, "num_retrieved": 273, "inference_time": 12.978336915373802, "generate_results": ["def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence.flattened())\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, IDeclaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, IDeclaration):\n        output.append(sequence)\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence.flattened())\n    else:\n        output.append(sequence)\n    \n    return output", "def _normalizeargs(sequence, output=None):\n    \"\"\"\n    Normalize declaration arguments\n\n    Normalization arguments might contain Declarions, tuples, or single\n    interfaces.\n\n    Anything but individial interfaces or implements specs will be expanded.\n    \"\"\"\n    if output is None:\n        output = []\n    \n    if isinstance(sequence, (tuple, list)):\n        for item in sequence:\n            _normalizeargs(item, output)\n    elif isinstance(sequence, Declaration):\n        output.extend(sequence.flattened())\n    else:\n        output.append(sequence)\n    \n    return output"]}
{"_id": "62b8b3d5eb7e40a82d2d1110", "name": "_c_optimizations_available", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 97698, "num_retrieved": 422, "inference_time": 12.978336915373802, "generate_results": ["def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _use_c_impl\n        return _use_c_impl\n    except ImportError:\n        return False", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _use_c_impl\n        return _use_c_impl\n    except ImportError:\n        return False", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _use_c_impl\n        return _use_c_impl\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _use_c_impl\n        return _use_c_impl\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")", "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    try:\n        from zope.interface._compat import _use_c_impl\n        return _use_c_impl\n    except ImportError:\n        raise ImportError(\"C optimizations are not available\")"]}
{"_id": "62b8b3d4eb7e40a82d2d110f", "name": "_should_attempt_c_optimizations", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 98931, "num_retrieved": 437, "inference_time": 12.978336915373802, "generate_results": ["def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # TODO: Implement this function\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # TODO: Implement this function\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # TODO: Implement this function\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # TODO: Implement this function\n    return _use_c_impl()", "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    # TODO: Implement this function\n    return _use_c_impl()"]}
{"_id": "62b8b3d4eb7e40a82d2d110e", "name": "_c_optimizations_ignored", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 89434, "num_retrieved": 361, "inference_time": 12.978336915373802, "generate_results": ["def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()", "def _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    return not _should_attempt_c_optimizations()"]}
{"_id": "62b8b3d4eb7e40a82d2d110d", "name": "_c_optimizations_required", "project": "pexip/os-zope", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 98905, "num_retrieved": 437, "inference_time": 12.978336915373802, "generate_results": ["def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()", "def _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    return not _use_c_impl()"]}
{"_id": "62b87b989a0c4fa8b80b35ee", "name": "reset", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 115047, "num_retrieved": 337, "inference_time": 12.978336915373802, "generate_results": ["def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n\"\"\"\n    self._data = []\n    self._cur_context = {}", "def reset(self):\n    \"\"\"\n    Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._data = []\n    self._cur_context = {}", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n\"\"\"\n    self._data = []\n    self._cur_context = {}", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n\"\"\"\n    self._data = []\n    self._cur_context = {}", "def reset(self):\n    \"\"\"Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).\n    \"\"\"\n    self._data = []\n    self._cur_context = {}"]}
{"_id": "62b87b859a0c4fa8b80b35d7", "name": "to_csv", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 115760, "num_retrieved": 378, "inference_time": 12.978336915373802, "generate_results": ["def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This method is deprecated and not used in Lena 0.5+\n    # The actual implementation is in the run method of ToCSV class\n    # which handles histograms and iterables\n    pass", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # The actual conversion is handled by ToCSV class\n    # which converts histograms and iterables to CSV\n    # For a graph, we would need to convert each point\n    # to CSV format with coordinates and values\n    \n    # Since this is deprecated, we'll just return a placeholder\n    # that indicates the function is not used anymore\n    \n    if header is not None:\n        csv_lines = [header]\n    else:\n        csv_lines = []\n    \n    # For a graph, we would iterate through points and format them\n    # But since this is deprecated, we don't implement the full conversion\n    # Instead, we rely on ToCSV class for actual conversion\n    \n    return \"\\n\".join(csv_lines)", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and replaced by ToCSV\n    # The actual implementation is in the ToCSV class\n    # which handles histograms and iterables to CSV\n    # For now, we return a placeholder that indicates the deprecation\n    warnings.warn(\n        \"to_csv is deprecated in Lena 0.5. Use ToCSV instead.\",\n        DeprecationWarning\n    )\n    # The actual CSV conversion is handled by the ToCSV class\n    # which processes data based on its type (histogram, iterable, etc.)\n    # This function is a placeholder for the deprecation notice\n    return \"\"", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5\n    # The actual CSV conversion is handled by ToCSV class\n    # which handles histograms and iterables\n    \n    # For a graph, we need to convert each point to CSV\n    # First, get the graph data\n    graph, context = lena.flow.get_data_context(self)\n    \n    # If the graph has no points, return empty\n    if not hasattr(graph, 'coords') or len(graph.coords) == 0:\n        return \"\"\n    \n    # Create the CSV output\n    csv_lines = []\n    \n    # Add header if provided\n    if header is not None:\n        csv_lines.append(header)\n    \n    # Convert each point to CSV format\n    for", "def to_csv(self, separator=\",\", header=None):\n    \"\"\"\n    .. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\n    Convert graph's points to CSV.\n\n    *separator* delimits values, the default is comma.\n\n    *header*, if not ``None``, is the first string of the output\n    (new line is added automatically).\n\n    Since a graph can be multidimensional,\n    for each point first its coordinate is converted to string\n    separated by *separator*, then each part of its value.\n\n    To convert :class:`Graph` to CSV inside a Lena sequence,\n    use :class:`lena.output.ToCSV`.\n    \"\"\"\n    # This function is deprecated and not used in Lena 0.5+\n    # The actual CSV conversion is handled by other functions\n    # like hist1d_to_csv, hist2d_to_csv, and iterable_to_table\n    # in the to_csv.py file\n    \n    # For compatibility with the docstring, we'll return a placeholder\n    # that indicates the function is deprecated and should be replaced\n    # with the proper ToCSV element\n    \n    if header is not None:\n        yield header\n    \n    # Since this function is deprecated and not used,\n    # we don't implement the actual CSV conversion logic\n    # The proper implementation is in the ToCSV class\n    # which handles histograms and iterables to CSV\n    pass"]}
{"_id": "62b87b839a0c4fa8b80b35cb", "name": "_get_err_indices", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 107439, "num_retrieved": 298, "inference_time": 39.00348663330078, "generate_results": ["def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Assuming this function is part of a class that manages error indices\n    # for a graph or histogram, we need to find the index of the coordinate\n    # in the field names and return the corresponding error index.\n    \n    # This is a placeholder implementation based on typical behavior\n    # in similar functions. In a real implementation, this would look\n    # at the field names and error indices stored in the class.\n    \n    # Find the index of coord_name in field_names\n    field_names = self.field_names\n    try:\n        coord_idx = field_names.index(coord_name)\n        # Return the error index for this coordinate\n        # This would typically be stored in a separate attribute\n        # or computed based on the error field names\n        return self._error_indices[coord_idx] if hasattr(self, '_error_indices') else -1\n    except ValueError:\n        # Coordinate not found in field names\n        return -1", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Find the error index for the given coordinate name\n    for i, coord in enumerate(self._field_names):\n        if coord == coord_name:\n            return i\n    # If coordinate name not found, return -1\n    return -1", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Assuming this function is part of a class that has a field for error indices\n    # and it needs to find the index of a coordinate in the field names\n    # For example, if field_names is (\"x\", \"y\", \"z\") and coord_name is \"y\"\n    # it should return the index of \"y\" in field_names\n    \n    # This is a placeholder implementation based on the context\n    # In a real implementation, this would look up the coordinate name\n    # in the field names and return its index\n    \n    # Since the exact implementation details are not provided in the context,\n    # this is a generic implementation that would work in a typical scenario\n    \n    field_names = getattr(self, \"field_names\", None)\n    if field_names is None:\n        return -1\n        \n    try:\n        return field_names.index(coord_name)\n    except ValueError:\n        return -1", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Assuming this function is part of a class that manages error indices\n    # for a graph, we need to find the index of the error field for the given coordinate\n    # This is a placeholder implementation based on typical behavior\n    \n    # In a real implementation, this would look up the error index in a mapping\n    # or within the graph's error structure\n    \n    # For example, if we have a mapping of coordinate names to error indices:\n    error_indices = {\n        \"x\": self._error_x_ind,\n        \"y\": self._error_y_ind\n    }\n    \n    # Return the error index for the given coordinate name\n    if coord_name in error_indices:\n        return error_indices[coord_name]\n    else:\n        # If coordinate name is not found, return -1 or raise an error\n        return -1", "def _get_err_indices(self, coord_name):\n    \"\"\"\n    Get error indices corresponding to a coordinate.\n    \"\"\"\n    # Assuming this function is part of a class that manages error indices\n    # for a graph, and coord_name is the name of the coordinate (e.g., \"x\" or \"y\")\n    # We need to find the index of this coordinate in the field names\n    # and return the corresponding error index from the internal error indices\n    \n    # This is a placeholder implementation based on typical behavior\n    # In a real implementation, this would look up the error index\n    # from a stored mapping or internal state\n    \n    # Example: if the coordinate is \"x\", return the index of x in field_names\n    # and use that to get the error index from internal storage\n    \n    # Since the actual implementation details are not provided in the context,\n    # this is a generic implementation that would need to be adapted\n    # to the specific class and data structure\n    \n    # Find the index of coord_name in field_names\n    field_names = self.field_names\n    try:\n        coord_index = field_names.index(coord_name)\n    except ValueError:\n        # If coordinate not found, return -1 or raise an error\n        return -1\n    \n    # Return the error index for this coordinate\n    # This would typically be stored in an internal attribute\n    # For example, if self._error_x_ind and self._error_y_ind are stored\n    # then we would return self._error_x_ind for \"x\" and self._error_y_ind for \"y\"\n    \n    # This is a simplified version - in reality, the error indices\n    # would be stored in a more complex structure\n    if coord_name == \"x\":\n        return self._error_x_ind\n    elif coord_name == \"y\":\n        return self._error_y_ind\n    else:\n        return -1"]}
{"_id": "62b87b7e9a0c4fa8b80b35bc", "name": "_update_context", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 115255, "num_retrieved": 385, "inference_time": 39.00348663330078, "generate_results": ["def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error indices from the graph\n    error_x_ind = self._error_x_ind\n    error_y_ind = self._error_y_ind\n    \n    # Create error subcontext\n    error_context = {}\n    \n    # Add x error index if it exists\n    if error_x_ind >= 0:\n        error_context[\"x\"] = {\"index\": error_x_ind}\n    \n    # Add y error index if it exists\n    if error_y_ind >= 0:\n        error_context[\"y\"] = {\"index\": error_y_ind}\n    \n    # Add to context if there are errors\n    if error_context:\n        context.update({\"error\": error_context})", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error indices from the graph\n    error_x_ind = self._error_x_ind\n    error_y_ind = self._error_y_ind\n    \n    # Create error subcontext\n    error_context = {}\n    \n    # Add x error index if it exists\n    if error_x_ind >= 0:\n        error_context[\"x\"] = {\"index\": error_x_ind}\n    \n    # Add y error index if it exists\n    if error_y_ind >= 0:\n        error_context[\"y\"] = {\"index\": error_y_ind}\n    \n    # Add to context if there are errors\n    if error_context:\n        context[\"error\"] = error_context", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error indices from the graph\n    error_x_ind = self._error_x_ind\n    error_y_ind = self._error_y_ind\n    \n    # Create error subcontext\n    error_context = {}\n    \n    # Add x error index if it exists\n    if error_x_ind >= 0:\n        error_context[\"x\"] = {\"index\": error_x_ind}\n    \n    # Add y error index if it exists\n    if error_y_ind >= 0:\n        error_context[\"y\"] = {\"index\": error_y_ind}\n    \n    # Append to context.error if it exists, otherwise create it\n    if \"error\" in context:\n        # If error already has a subcontext, merge with new one\n        if isinstance(context[\"error\"], dict):\n            context[\"error\"].update(error_context)\n        else:\n            context[\"error\"] = error_context\n    else:\n        context[\"error\"] = error_context", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error indices from the graph\n    error_x_ind = self._error_x_ind\n    error_y_ind = self._error_y_ind\n    \n    # Create error subcontext\n    error_context = {}\n    \n    # Add x error index if it exists\n    if error_x_ind >= 0:\n        error_context[\"x\"] = {\"index\": error_x_ind}\n    \n    # Add y error index if it exists\n    if error_y_ind >= 0:\n        error_context[\"y\"] = {\"index\": error_y_ind}\n    \n    # Add to context if there are errors\n    if error_context:\n        context[\"error\"] = error_context", "def _update_context(self, context):\n    \"\"\"\n    Update *context* with the properties of this graph.\n\n    *context.error* is appended with indices of errors.\n    Example subcontext for a graph with fields \"E,t,error_E_low\":\n    {\"error\": {\"x_low\": {\"index\": 2}}}.\n    Note that error names are called \"x\", \"y\" and \"z\"\n    (this corresponds to first three coordinates,\n    if they are present), which allows to simplify plotting.\n    Existing values are not removed\n    from *context.value* and its subcontexts.\n\n    Called on \"destruction\" of the graph (for example,\n    in :class:`.ToCSV`). By destruction we mean conversion\n    to another structure (like text) in the flow.\n    The graph object is not really destroyed in this process.\n    \"\"\"\n    # Get error indices from the graph\n    error_x_ind = self._error_x_ind\n    error_y_ind = self._error_y_ind\n    \n    # Create error subcontext\n    error_context = {}\n    \n    # Add x error index if exists\n    if error_x_ind >= 0:\n        error_context[\"x\"] = {\"index\": error_x_ind}\n    \n    # Add y error index if exists\n    if error_y_ind >= 0:\n        error_context[\"y\"] = {\"index\": error_y_ind}\n    \n    # Add to context if there are errors\n    if error_context:\n        context[\"error\"] = error_context"]}
{"_id": "62b87b4f9a0c4fa8b80b3580", "name": "integral", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 115695, "num_retrieved": 372, "inference_time": 39.00348663330078, "generate_results": ["def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify the 1D and multidimensional edges format\n    edges = hf.unify_1_md(edges)\n    \n    # Compute the integral by summing up bin values multiplied by bin widths\n    total_integral = 0.0\n    \n    # Iterate over each dimension\n    for dim in range(len(edges)):\n        # Get the edges for this dimension\n        dim_edges = edges[dim]\n        \n        # Compute the bin widths for this dimension\n        bin_widths = [dim_edges[i+1] - dim_edges[i] for i in range(len(dim_edges) - 1)]\n        \n        # For each bin in this dimension, multiply the bin value by its width\n        # and add to the total integral\n        for i in range(len(bins)):\n            # Get the bin value at position i\n            bin_value = bins[i]\n            \n            # Multiply by the bin width (assuming 1D case for simplicity)\n            # In multidimensional case, we need to consider the product of all bin widths\n            # For now, assuming 1D case as per the context\n            if len(edges) == 1:\n                total_integral += bin_value * bin_widths[i]\n            else:\n                # For multidimensional case, we need to compute the volume of each bin\n                # This is a simplified implementation assuming the bin values are already\n                # the product of all bin widths\n                total_integral += bin_value\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify the 1D and multidimensional edges format\n    edges = hf.unify_1_md(edges)\n    \n    # Compute the integral by summing over all bins\n    # For each bin, the width is the difference between consecutive edges\n    # The integral is the sum of (bin_value * bin_width)\n    total_integral = 0.0\n    \n    # Iterate over each dimension\n    for dim in range(len(edges)):\n        # For each dimension, compute the bin widths\n        dim_edges = edges[dim]\n        dim_bins = bins[dim] if isinstance(bins, list) and len(bins) > dim else bins\n        \n        # If bins is a nested list, we need to access the correct dimension\n        if isinstance(bins, list) and len(bins) > dim:\n            dim_bins = bins[dim]\n        \n        # Compute the integral for this dimension\n        for i in range(len(dim_edges) - 1):\n            bin_width = dim_edges[i + 1] - dim_edges[i]\n            bin_value = dim_bins[i] if isinstance(dim_bins, list) and len(dim_bins) > i else dim_bins[i]\n            total_integral += bin_value * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify the 1D and multidimensional edges format\n    edges = hf.unify_1_md(edges)\n    \n    # Compute the integral by summing over all bins\n    # For each bin, the width is the difference between consecutive edges\n    # The integral is the sum of (bin_value * bin_width)\n    total_integral = 0.0\n    \n    # Iterate over each dimension\n    for dim in range(len(edges)):\n        # For each dimension, compute the bin widths\n        dim_edges = edges[dim]\n        dim_bins = bins[dim] if isinstance(bins, list) and len(bins) > dim else bins\n        \n        # If bins is a nested list, we need to access the correct dimension\n        if isinstance(bins, list) and len(bins) > dim:\n            dim_bins = bins[dim]\n        \n        # Compute the integral for this dimension\n        for i in range(len(dim_edges) - 1):\n            bin_width = dim_edges[i + 1] - dim_edges[i]\n            bin_value = dim_bins[i] if isinstance(dim_bins, list) and len(dim_bins) > i else dim_bins[i]\n            total_integral += bin_value * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify the 1D and multidimensional edges format\n    edges = hf.unify_1_md(edges)\n    \n    # Compute the integral by summing over all bins\n    # For each bin, the width is the difference between consecutive edges\n    total_integral = 0.0\n    \n    # Iterate over each dimension\n    for dim in range(len(edges)):\n        # For each dimension, compute the bin widths\n        dim_edges = edges[dim]\n        dim_bins = bins[dim] if isinstance(bins, list) and len(bins) > dim else bins\n        \n        # Calculate the width of each bin\n        for i in range(len(dim_edges) - 1):\n            bin_width = dim_edges[i + 1] - dim_edges[i]\n            bin_value = dim_bins[i] if isinstance(dim_bins, list) and len(dim_bins) > i else dim_bins\n            \n            # Add the contribution of this bin to the total integral\n            total_integral += bin_value * bin_width\n    \n    return total_integral", "def integral(bins, edges):\n    \"\"\"\n    Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    # Unify the 1D and multidimensional edges format\n    edges = hf.unify_1_md(edges)\n    \n    # Compute the integral by summing over all bins\n    # For each bin, the width is the difference between consecutive edges\n    total_integral = 0.0\n    \n    # Iterate over each dimension\n    for dim in range(len(edges)):\n        # For each dimension, compute the bin widths\n        dim_edges = edges[dim]\n        dim_bins = bins[dim] if isinstance(bins, list) and len(bins) > dim else bins\n        \n        # Compute the width of each bin\n        bin_widths = [dim_edges[i+1] - dim_edges[i] for i in range(len(dim_edges) - 1)]\n        \n        # Sum the product of bin values and bin widths\n        for i in range(len(dim_bins)):\n            if isinstance(dim_bins, list) and len(dim_bins) > i:\n                bin_value = dim_bins[i]\n                if bin_value is not None:\n                    total_integral += bin_value * bin_widths[i]\n    \n    return total_integral"]}
{"_id": "62b87b199a0c4fa8b80b354e", "name": "is_fill_request_seq", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 119542, "num_retrieved": 357, "inference_time": 39.00348663330078, "generate_results": ["def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for elem in seq._seq:\n            if isinstance(elem, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for elem in seq._seq:\n            if isinstance(elem, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for elem in seq._seq:\n            if isinstance(elem, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for elem in seq._seq:\n            if isinstance(elem, fill_request_seq.FillRequestSeq):\n                return True\n    return False", "def is_fill_request_seq(seq):\n    \"\"\"\n    Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if isinstance(seq, fill_request_seq.FillRequestSeq):\n        return True\n    if isinstance(seq, sequence.Sequence):\n        for elem in seq._seq:\n            if isinstance(elem, fill_request_seq.FillRequestSeq):\n                return True\n    return False"]}
{"_id": "62b87b099a0c4fa8b80b3538", "name": "is_fill_request_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 115801, "num_retrieved": 326, "inference_time": 39.00348663330078, "generate_results": ["def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)", "def is_fill_request_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'request'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'request') and callable(obj.request)"]}
{"_id": "62b87af99a0c4fa8b80b3524", "name": "is_run_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 114340, "num_retrieved": 313, "inference_time": 39.00348663330078, "generate_results": ["def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run'))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run'))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run'))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, \"run\") and callable(getattr(obj, \"run\"))", "def is_run_el(obj):\n    \"\"\"\n    Object contains executable method 'run'.\n    \"\"\"\n    return hasattr(obj, 'run') and callable(getattr(obj, 'run'))"]}
{"_id": "62b87af69a0c4fa8b80b351a", "name": "is_fill_compute_el", "project": "ynikitenko/lena", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 115801, "num_retrieved": 326, "inference_time": 39.00348663330078, "generate_results": ["def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)", "def is_fill_compute_el(obj):\n    \"\"\"\n    Object contains executable methods 'fill' and 'compute'.\n    \"\"\"\n    return hasattr(obj, 'fill') and callable(obj.fill) and hasattr(obj, 'compute') and callable(obj.compute)"]}
{"_id": "62b87af19a0c4fa8b80b34f7", "name": "difference", "project": "ynikitenko/lena", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 114499, "num_retrieved": 385, "inference_time": 39.00348663330078, "generate_results": ["def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion: return all differences at any level\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    \n    if level == 0:\n        # Level 1: only check for different values at the same key\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    \n    # Level > 1: check for keys in d1 not in d2, and recursively process nested dictionaries\n    result = {}\n    for k in d1:\n        if k not in d2:\n            result[k] = d1[k]\n        elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n            result[k] = difference(d1[k], d2[k], level-1)\n        elif d1[k] != d2[k]:\n            result[k] = d1[k]\n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    else:\n        # Level > 1 - recursive check\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level-1)\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - return all differences\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    \n    if level == 0:\n        # Level 0 - only keys in d1 not in d2\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n        return result\n    \n    # Level > 0 - recursive difference\n    result = {}\n    for k in d1:\n        if k not in d2:\n            result[k] = d1[k]\n        elif level > 0 and isinstance(d1[k], dict) and isinstance(d2[k], dict):\n            result[k] = difference(d1[k], d2[k], level - 1)\n        elif d1[k] != d2[k]:\n            result[k] = d1[k]\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - return all differences\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif d1[k] != d2[k]:\n                result[k] = d1[k]\n        return result\n    \n    # Level 1: only check direct keys and values\n    result = {}\n    for k in d1:\n        if k not in d2:\n            result[k] = d1[k]\n        elif d1[k] != d2[k]:\n            result[k] = d1[k]\n    \n    # For nested dictionaries, recursively process if level > 1\n    if level > 1:\n        for k in d1:\n            if k in d2:\n                if isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                    nested_diff = difference(d1[k], d2[k], level - 1)\n                    if nested_diff:\n                        result[k] = nested_diff\n    \n    return result", "def difference(d1, d2, level=-1):\n    \"\"\"\n    Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.\n    \"\"\"\n    if level == -1:\n        # Infinite recursion - check all levels\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=-1)\n            else:\n                if d1[k] != d2[k]:\n                    result[k] = d1[k]\n        return result\n    elif level == 0:\n        # Level 1 - only check direct keys\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level=0)\n            else:\n                if d1[k] != d2[k]:\n                    result[k] = d1[k]\n        return result\n    else:\n        # Level > 1 - recursive check\n        result = {}\n        for k in d1:\n            if k not in d2:\n                result[k] = d1[k]\n            elif isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                result[k] = difference(d1[k], d2[k], level-1)\n            else:\n                if d1[k] != d2[k]:\n                    result[k] = d1[k]\n        return result"]}
{"_id": "62b87af09a0c4fa8b80b34f1", "name": "fill", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 110010, "num_retrieved": 303, "inference_time": 39.00348663330078, "generate_results": ["def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the data context for the current value\n    data, context = lena.flow.get_data_context(coord)\n    \n    # If data is not a histogram, return early\n    if not isinstance(data, lena.structures.histogram):\n        return\n    \n    # Get the edges and bins from the histogram\n    edges = data.edges\n    bins = data.bins\n    \n    # Check if the coordinate is within the histogram edges\n    # For 1D case, check if coord is within the range\n    if len(edges) == 1:\n        if not (edges[0][0] <= coord <= edges[0][1]):\n            return\n    \n    # For multidimensional case, check each dimension\n    else:\n        # Convert coord to a tuple if it's not already\n        if not isinstance(coord, (tuple, list)):\n            coord = (coord,)\n        \n        # Check if each coordinate is within its corresponding edge range\n        for i, coord_val in enumerate(coord):\n            if not (edges[i][0] <= coord_val <= edges[i][1]):\n                return\n    \n    # Find the bin index for the coordinate\n    # Use get_bin_on_value to find the bin index\n    bin_indices = get_bin_on_value(coord, edges)\n    \n    # If bin indices are found, update the bin content\n    if bin_indices is not None:\n        # Get the bin content at the calculated indices\n        bin_content = lena.structures.get_bin_on_index(bin_indices, bins)\n        \n        # Update the bin content with the weight\n        bin_content += weight\n        \n        # Update the histogram with the new bin content\n        # This would typically be done by modifying the internal data structure\n        # However, since we're working with a numpy histogram, we would typically\n        # use numpy.histogram to update the bins\n        pass", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the data context for the current value\n    data, context = lena.flow.get_data_context(coord)\n    \n    # If data is not a histogram, return early\n    if not isinstance(data, lena.structures.histogram):\n        return\n    \n    # Get the edges and bins from the histogram\n    edges = data.edges\n    bins = data.bins\n    \n    # Check if the coordinate is within the histogram edges\n    # For 1D histogram, check if coord is within the range\n    if len(edges) == 1:\n        # For 1D histogram, check if coord is within the range\n        if not (edges[0][0] <= coord <= edges[0][1]):\n            return\n    \n    # For multidimensional histogram, check if coord is within all edges\n    else:\n        # Check if coord is within all edges\n        for i, edge in enumerate(edges):\n            if not (edge[0] <= coord[i] <= edge[1]):\n                return\n    \n    # Find the bin index for the coordinate\n    # Use get_bin_on_value to find the bin index\n    bin_indices = get_bin_on_value(coord, edges)\n    \n    # If bin indices are found, update the bin content\n    if bin_indices:\n        # Get the bin content at the indices\n        bin_content = bins[bin_indices]\n        \n        # Update the bin content with the weight\n        bin_content += weight\n        \n        # Update the histogram with the new bin content\n        # This is a simplified implementation - in practice,\n        # the actual histogram update would depend on the specific\n        # histogram implementation\n        # For now, we just update the bin content\n        data.bins[bin_indices] = bin_content", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the data context for the current value\n    data, context = lena.flow.get_data_context(coord)\n    \n    # Check if data is a valid histogram\n    if not isinstance(data, lena.structures.histogram):\n        return\n    \n    # Get the edges and bins from the histogram\n    edges = data.edges\n    bins = data.bins\n    \n    # Convert coord to a tuple of coordinates for multidimensional case\n    if isinstance(coord, (list, tuple)):\n        coord = tuple(coord)\n    else:\n        coord = (coord,)\n    \n    # Determine the dimension of the histogram\n    dim = len(edges)\n    \n    # Check if the coordinate is within the valid range for each dimension\n    valid = True\n    for i in range(dim):\n        if isinstance(edges[i], (list, tuple)):\n            # For multidimensional edges, check if coord[i] is within bounds\n            if coord[i] < edges[i][0] or coord[i] >= edges[i][-1]:\n                valid = False\n                break\n        else:\n            # For 1D edges, check if coord is within bounds\n            if coord[i] < edges[i][0] or coord[i] >= edges[i][-1]:\n                valid = False\n                break\n    \n    # If coordinate is outside valid range, ignore it\n    if not valid:\n        return\n    \n    # Find the bin index for the coordinate\n    bin_indices = []\n    for i in range(dim):\n        if isinstance(edges[i], (list, tuple)):\n            # For multidimensional edges, find bin index\n            bin_idx = get_bin_on_value_1d(coord[i], edges[i])\n            bin_indices.append(bin_idx)\n        else:\n            # For 1D edges, find bin index\n            bin_idx = get_bin_on_value_1d(coord[i], edges[i])\n            bin_indices.append(bin_idx)\n    \n    # Get the bin content\n    bin_content = lena.structures.get_bin_on_index(bin_indices, bins)\n    \n    # Update the bin content with the weight\n    if bin_content is not None:\n        # Add the weight to the bin content\n        if isinstance(bin_content, (int, float)):\n            bin_content += weight\n        else:\n            # If bin_content is", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the data context for the current value\n    data, context = lena.flow.get_data_context(coord)\n    \n    # If data is not a histogram, return early\n    if not isinstance(data, lena.structures.histogram):\n        return\n    \n    # Get the edges and bins from the histogram\n    edges = data.edges\n    bins = data.bins\n    \n    # Check if the coordinate is within the histogram edges\n    # For 1D case, check if coord is within the range\n    if len(edges) == 1:\n        if not (edges[0][0] <= coord <= edges[0][1]):\n            return\n    \n    # For multidimensional case, check each dimension\n    else:\n        # Check if coord is within the range for each dimension\n        for i, coord_val in enumerate(coord):\n            if not (edges[i][0] <= coord_val <= edges[i][1]):\n                return\n    \n    # Find the bin index for the coordinate\n    # For 1D case\n    if len(edges) == 1:\n        bin_idx = get_bin_on_value_1d(coord, edges[0])\n    else:\n        # For multidimensional case\n        bin_idx = get_bin_on_value(coord, edges)\n    \n    # Add the weight to the bin\n    if bin_idx is not None:\n        bins[bin_idx] += weight", "def fill(self, coord, weight=1):\n    \"\"\"\n    Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n    \"\"\"\n    import numpy as np\n    \n    # Get the data context for the current value\n    data, context = lena.flow.get_data_context(coord)\n    \n    # Check if the coordinate is within the histogram edges\n    edges = self.edges\n    if not isinstance(edges, (list, tuple)):\n        # Handle 1D case\n        if not hasattr(edges[0], '__iter__'):\n            # 1D edges\n            if coord < edges[0] or coord >= edges[-1]:\n                return\n            # Find the bin index using linear search\n            bin_idx = get_bin_on_value_1d(coord, edges)\n            if bin_idx == -1:\n                return\n            # Update the bin content\n            self._data[bin_idx] += weight\n        else:\n            # Handle multidimensional case\n            # Convert coord to a tuple if it's not already\n            if not isinstance(coord, (tuple, list)):\n                coord = (coord,)\n            # Find the bin indices for each dimension\n            bin_indices = []\n            for i, edge in enumerate(edges):\n                if hasattr(edge, '__iter__'):\n                    bin_idx = get_bin_on_value_1d(coord[i], edge)\n                    if bin_idx == -1:\n                        return\n                    bin_indices.append(bin_idx)\n                else:\n                    # 1D case\n                    bin_idx = get_bin_on_value_1d(coord[i], edge)\n                    if bin_idx == -1:\n                        return\n                    bin_indices.append(bin_idx)\n            # Update the bin content\n            bin_content = self.bins\n            # Navigate to the bin using the indices\n            current = bin_content\n            for idx in bin_indices:\n                current = current[idx]\n            current += weight\n    else:\n        # Handle multidimensional case with nested edges\n        # Convert coord to a tuple if it's not already\n        if not isinstance(coord, (tuple, list)):\n            coord = (coord,)\n        # Find the bin indices for each dimension\n        bin_indices = []\n        for i, edge in enumerate(edges):\n            if hasattr(edge, '__iter__'):\n                bin_idx = get_bin_on_value_1d(coord[i], edge)\n                if bin_idx == -1:\n                    return\n                bin_indices.append(bin_idx)\n            else:"]}
{"_id": "62b86aa3b4d922cb0e688d36", "name": "_validate_labels", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 210323, "num_retrieved": 469, "inference_time": 39.00348663330078, "generate_results": ["def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    \n    # Validate keys (label names)\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the regex pattern for label keys\n        # (assuming a regex pattern is defined in the context, e.g., only alphanumeric and hyphens)\n        # This is a placeholder - actual regex would be defined in the context\n        if not key.isalnum() and not any(c in key for c in \"-_\"):\n            errors.append({str(key): \"Label key does not match the regex pattern\"})\n    \n    # Validate values (label values)\n    for key, value in labels.items():\n        # Check if value is a string or list of strings\n        if isinstance(value, list):\n            for item in value:\n                if not isinstance(item, str):\n                    errors.append({str(item): \"Label value must be a string\"})\n        elif not isinstance(value, str):\n            errors.append({str(value): \"Label value must be a string\"})\n    \n    if errors:\n        raise ValidationError(errors)", "import re\nfrom typing import Dict, Any, List, Tuple\nfrom . import ValidationError\n\ndef _validate_labels(labels: Dict[str, Any]) -> None:\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regex patterns for labels\n    # Keys should be alphanumeric with hyphens only\n    key_pattern = r'^[a-z][a-z0-9_-]*[a-z0-9]$'\n    # Values should be alphanumeric with hyphens, dots, and underscores only\n    value_pattern = r'^[a-z][a-z0-9_.-]*[a-z0-9]$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({\"key\": str(key), \"value\": \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the pattern\n        if not re.match(key_pattern, key):\n            errors.append({\"key\": key, \"value\": f\"Label key '{key}' does not match the regex pattern\"})\n            \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string", "import re\nfrom typing import Dict, Any, List, Tuple\n\ndef _validate_labels(labels: Dict[str, Any]) -> None:\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define regex patterns for labels\n    # Keys should be valid identifiers (alphanumeric, underscore, hyphen)\n    key_pattern = r'^[a-zA-Z_][a-zA-Z0-9_]*$'\n    # Values should be valid strings (alphanumeric, underscore, hyphen, dot)\n    value_pattern = r'^[a-zA-Z0-9_\\-\\.]*$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the pattern\n        if not re.match(key_pattern, key):\n            errors.append({key: f\"Label key '{key}' does not match the regex pattern\"})\n            \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, str):\n            # If value is a list, check each element\n            if isinstance(value, list):\n                for i,", "def _validate_labels(labels):\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({str(key): \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the regex pattern (assuming a pattern is defined elsewhere)\n        # This is a placeholder - in a real implementation, we'd have a regex pattern\n        # to validate against\n        try:\n            # Validate key format (example: only alphanumeric and hyphens)\n            if not re.match(r'^[a-zA-Z0-9_-]+$', key):\n                errors.append({key: \"Label key does not match the regex pattern\"})\n        except Exception:\n            errors.append({key: \"Label key does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, str):\n            # If value is a list, check each element\n            if isinstance(value, list):\n                for i, item in enumerate(value):\n                    if not isinstance(item, str):\n                        errors.append({f\"{key}[{i}]\": \"", "import re\nfrom typing import Dict, Any, List, Tuple\nfrom . import ValidationError\n\ndef _validate_labels(labels: Dict[str, Any]) -> None:\n    \"\"\"\n    Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n    \"\"\"\n    # Define the regex pattern for valid label keys (alphanumeric, hyphens, underscores)\n    key_pattern = r'^[a-z][a-z0-9._-]*[a-z0-9]$'\n    \n    errors = []\n    \n    # Validate keys\n    for key, value in labels.items():\n        # Check if key is a string\n        if not isinstance(key, str):\n            errors.append({\"key\": str(key), \"value\": \"Label key must be a string\"})\n            continue\n            \n        # Check if key matches the pattern\n        if not re.match(key_pattern, key):\n            errors.append({\"key\": key, \"value\": f\"Label key '{key}' does not match the regex pattern\"})\n    \n    # Validate values\n    for key, value in labels.items():\n        # Check if value is a string\n        if not isinstance(value, str):\n            errors.append({\"key\": key, \"value\": \"Label value must be a string\"})\n            continue\n    \n    # If any errors were found,"]}
{"_id": "62b86a9eb4d922cb0e688d25", "name": "_get_resource_name_regex", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 211096, "num_retrieved": 488, "inference_time": 39.00348663330078, "generate_results": ["def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n    return pattern", "def _get_resource_name_regex():\n    \"\"\"\n    Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    # Resource names must start with a letter or number\n    # Followed by letters, numbers, hyphens, or underscores\n    # Must be at least 1 character long\n    pattern = re.compile(r\"^[a-zA-Z0-9][a-zA-Z0-9_-]*$\")\n    return pattern"]}
{"_id": "62b86a4fb4d922cb0e688cf8", "name": "validate_value", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 211331, "num_retrieved": 500, "inference_time": 39.00348663330078, "generate_results": ["def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The context does not provide the specific regular expression to validate against\n    # Based on the function name and typical usage, this likely validates against a pattern\n    # such as alphanumeric strings, email addresses, etc.\n    # Since no specific pattern is provided in the context, we'll implement a basic validation\n    # that checks if the value is a non-empty string and contains only alphanumeric characters\n    # and common special characters (like hyphens, underscores, dots)\n    \n    if not value:\n        raise ValidationError(\"Value cannot be empty\")\n    \n    # Basic pattern: alphanumeric, hyphens, underscores, dots, and spaces\n    import re\n    pattern = r'^[a-zA-Z0-9._-]+$'\n    \n    if not re.match(pattern, value):\n        raise ValidationError(f\"Value '{value}' does not conform to the expected pattern\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic is not provided in the context\n    # Based on the context, this function is likely used to validate values\n    # against a regular expression pattern, but the specific pattern is not defined\n    # in the provided context.\n    # Since no specific pattern or validation rules are given, \n    # we assume it's a placeholder for a validation function that would\n    # check against a pattern defined elsewhere.\n    pass", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic is not provided in the context\n    # Based on the context, this function is likely used to validate values\n    # against a regular expression pattern, but the specific pattern is not defined\n    # in the provided context.\n    # Since no specific pattern is given, we assume a basic validation\n    # that checks if the value is a non-empty string.\n    if not isinstance(value, str) or not value.strip():\n        raise ValidationError(f\"Value must be a non-empty string\")", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The context does not provide the specific regular expression to validate against\n    # Therefore, we cannot implement the actual validation logic\n    # This function is likely intended to be used with a specific regex pattern\n    # that is defined elsewhere in the codebase\n    pass", "def validate_value(value):\n    \"\"\"\n    Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    # The actual validation logic is not provided in the context\n    # Based on the context, this function is likely used to validate values\n    # against a regular expression pattern, but the specific pattern is not defined\n    # in the provided context.\n    # Since no specific pattern or validation rules are given, we assume\n    # a basic validation that checks if the value is not None and is a string.\n    if value is None:\n        raise ValidationError(\"Value cannot be None\")\n    if not isinstance(value, str):\n        raise ValidationError(\"Value must be a string\")\n    # If more specific validation rules were provided in the context, they would be implemented here\n    # For now, we only validate that the value is a string and not None"]}
{"_id": "62b86a4fb4d922cb0e688cf7", "name": "validate_key", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 210694, "num_retrieved": 479, "inference_time": 39.00348663330078, "generate_results": ["def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Assuming the regular expression is defined elsewhere in the context\n    # For example, a common key validation might be alphanumeric with hyphens and underscores\n    import re\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Assuming the regular expression is defined elsewhere in the context\n    # For example, a common key validation might be for alphanumeric with hyphens and underscores\n    import re\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Assuming the regular expression is defined elsewhere in the context\n    # For example, a common key validation might be for alphanumeric with hyphens and underscores\n    import re\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Regular expression pattern for valid keys (example pattern based on common key validation)\n    # This pattern typically allows alphanumeric characters, hyphens, and underscores\n    # and ensures it starts and ends with an alphanumeric character\n    import re\n    pattern = r'^[a-zA-Z0-9][a-zA-Z0-9_-]*[a-zA-Z0-9]$'\n    \n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")", "def validate_key(key):\n    \"\"\"\n    Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    # Assuming the regular expression is defined elsewhere in the context\n    # For example, a common key validation might be alphanumeric with hyphens and underscores\n    import re\n    pattern = r'^[a-zA-Z_][a-zA-Z0-9_\\-]*$'\n    if not re.match(pattern, key):\n        raise ValidationError(f\"Invalid key format: {key!r}\")"]}
{"_id": "62b86a01b4d922cb0e688ccc", "name": "generate_default_observer_schema_dict", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 204667, "num_retrieved": 500, "inference_time": 39.00348663330078, "generate_results": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # Copy all keys and values from manifest_dict\n    for key, value in manifest_dict.items():\n        # If the value is a dict or list, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level=False)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level=False)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n    \n    # In case of first_level dictionary, copy the identifying fields from the manifest\n    if first_level:\n        try:\n            observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        except KeyError:\n            pass\n            \n        try:\n            observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n        except KeyError:\n            pass\n            \n        try:\n            metadata = manifest_dict[\"metadata\"]\n            observer_schema[\"metadata\"] = {\n                \"name\": metadata[\"name\"],\n                \"namespace\": metadata.get(\"namespace\")\n            }\n        except (KeyError, AssertionError):\n            pass\n    \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # Copy all keys from the manifest dictionary\n    for key, value in manifest_dict.items():\n        # If the value is a dict or list, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n    \n    # In case of first_level dictionary, copy the identifying fields from the manifest\n    if first_level:\n        try:\n            # Copy apiVersion and kind from the manifest\n            if \"apiVersion\" in manifest_dict:\n                observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n            if \"kind\" in manifest_dict:\n                observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n            \n            # Copy metadata fields if present\n            if \"metadata\" in manifest_dict:\n                metadata = manifest_dict[\"metadata\"]\n                if isinstance(metadata, dict):\n                    # Copy name from metadata\n                    if \"name\" in metadata:\n                        observer_schema[\"metadata\"] = {\n                            \"name\": metadata[\"name\"]\n                        }\n                    else:\n                        observer_schema[\"metadata\"] = {}\n        except KeyError:\n            # If any of the required fields are missing, just return the default structure\n            pass\n    \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # Copy all keys and values from manifest_dict\n    for key, value in manifest_dict.items():\n        # If the value is a dict or list, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n    \n    # In case of first_level dictionary, copy the identifying fields from the manifest\n    if first_level:\n        try:\n            # Copy apiVersion and kind from the manifest\n            if \"apiVersion\" in manifest_dict:\n                observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n            if \"kind\" in manifest_dict:\n                observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n            if \"metadata\" in manifest_dict:\n                metadata = manifest_dict[\"metadata\"]\n                if isinstance(metadata, dict):\n                    # Copy name from metadata\n                    if \"name\" in metadata:\n                        observer_schema[\"metadata\"] = {\n                            \"name\": metadata[\"name\"]\n                        }\n        except KeyError:\n            # If any of the required fields are missing, just return the default schema\n            pass\n    \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # Copy all keys from the manifest dictionary\n    for key, value in manifest_dict.items():\n        # If the value is a dict or list, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n    \n    # In case of first_level dictionary, copy the identifying fields from the manifest\n    if first_level:\n        try:\n            # Copy apiVersion and kind from the manifest\n            if \"apiVersion\" in manifest_dict:\n                observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n            if \"kind\" in manifest_dict:\n                observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n            \n            # Copy metadata fields if they exist\n            if \"metadata\" in manifest_dict:\n                metadata = manifest_dict[\"metadata\"]\n                if isinstance(metadata, dict):\n                    # Copy name from metadata\n                    if \"name\" in metadata:\n                        observer_schema[\"metadata\"] = {\n                            \"name\": metadata[\"name\"]\n                        }\n                    else:\n                        observer_schema[\"metadata\"] = {}\n        except KeyError:\n            # If any of the required fields are missing, just return the default structure\n            pass\n    \n    return observer_schema", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"\n    Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n    \"\"\"\n    # Create a new dictionary to store the generated observer schema\n    observer_schema = {}\n    \n    # Copy all keys and values from manifest_dict\n    for key, value in manifest_dict.items():\n        # If the value is a dict or list, recursively process it\n        if isinstance(value, dict):\n            observer_schema[key] = generate_default_observer_schema_dict(value, first_level)\n        elif isinstance(value, list):\n            observer_schema[key] = generate_default_observer_schema_list(value, first_level)\n        else:\n            # Replace all non-list and non-dict values with None\n            observer_schema[key] = None\n    \n    # In case of first_level dictionary, copy the identifying fields from the manifest\n    if first_level:\n        try:\n            # Copy apiVersion and kind from manifest\n            if \"apiVersion\" in manifest_dict:\n                observer_schema[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n            if \"kind\" in manifest_dict:\n                observer_schema[\"kind\"] = manifest_dict[\"kind\"]\n            \n            # Copy metadata fields if present\n            if \"metadata\" in manifest_dict:\n                metadata = manifest_dict[\"metadata\"]\n                if isinstance(metadata, dict):\n                    # Copy name field\n                    if \"name\" in metadata:\n                        observer_schema[\"metadata\"] = {\n                            \"name\": metadata[\"name\"]\n                        }\n                    else:\n                        observer_schema[\"metadata\"] = {}\n        except KeyError:\n            # If any of the required fields are missing, just return the default structure\n            pass\n    \n    return observer_schema"]}
{"_id": "62b869ebb4d922cb0e688cc6", "name": "update_last_applied_manifest_list_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 211011, "num_retrieved": 493, "inference_time": 39.00348663330078, "generate_results": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        # Check if the response element is present in the last_applied_manifest\n        if idx >= len(last_applied_manifest):\n            # If the response element is not present in the last_applied_manifest,\n            # add it with the response value\n            last_applied_manifest.append(response[idx])\n        elif idx < len(last_applied_manifest):\n            # If the response element is present in the last_applied_manifest,\n            # update it with the response value\n            last_applied_manifest[idx] = response[idx]\n        else:\n            # If the response element is not present in the last_applied_manifest,\n            # add it with the response value\n            last_applied_manifest.append(response[idx])", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        # Check if the response element exists\n        if idx >= len(response):\n            # If response is missing, add a placeholder\n            last_applied_manifest.append(None)\n            continue\n            \n        # Check if the response element exists in the last_applied_manifest\n        if idx >= len(last_applied_manifest):\n            # If last_applied_manifest is missing, add a placeholder\n            last_applied_manifest.append(None)\n            continue\n            \n        # If the response element exists, update the last_applied_manifest\n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n        else:\n            # For primitive values, just assign the response value\n            last_applied_manifest[idx] = response[idx]", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        # Check if the response element exists at the same index\n        if idx >= len(response):\n            # If the response is missing an element, we need to create a new entry\n            # with the same structure as the observer schema\n            new_entry = deepcopy(value)\n            last_applied_manifest.append(new_entry)\n        elif idx < len(response):\n            # If both exist, we need to update the value in last_applied_manifest\n            # with the value from the response\n            if isinstance(value, dict):\n                # For dictionary values, recursively update the dictionary\n                update_last_applied_manifest_dict_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            elif isinstance(value, list):\n                # For list values, recursively update the list\n                update_last_applied_manifest_list_from_resp(\n                    last_applied_manifest[idx], value, response[idx]\n                )\n            else:\n                # For primitive values, just assign the response value\n                last_applied_manifest[idx] = response[idx]\n        else:\n            # If response is shorter than observer_schema, we need to handle the\n            # missing elements\n            new_entry = deepcopy(value)\n            last_applied_manifest.append(new_entry)", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        # Check if the response element exists at the same index\n        if idx >= len(response):\n            # If response is missing at this index, we need to add a new element\n            # to last_applied_manifest with the value from observer_schema\n            last_applied_manifest.append(value)\n        elif idx < len(response):\n            # If both exist, we need to update the value in last_applied_manifest\n            # with the value from response\n            last_applied_manifest[idx] = response[idx]\n        else:\n            # If response is longer than observer_schema, we need to handle the\n            # extra elements in response\n            # This case should not happen as per the context, but we handle it\n            # by updating the last_applied_manifest with the response value\n            last_applied_manifest[idx] = response[idx]", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for idx, value in enumerate(observer_schema[:-1]):\n        # Check if the response element is present in the last_applied_manifest\n        if idx >= len(last_applied_manifest):\n            # If the response element is not present in the last_applied_manifest,\n            # add it with the value from the response\n            last_applied_manifest.append(response[idx])\n        elif idx < len(last_applied_manifest):\n            # If the response element is present in the last_applied_manifest,\n            # update its value with the value from the response\n            last_applied_manifest[idx] = response[idx]\n        else:\n            # If the response element is not present in the last_applied_manifest,\n            # add it with the value from the response\n            last_applied_manifest.append(response[idx])"]}
{"_id": "62b869eab4d922cb0e688cc5", "name": "update_last_applied_manifest_dict_from_resp", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 210571, "num_retrieved": 480, "inference_time": 39.00348663330078, "generate_results": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest.get(key, {}), value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest.get(key, []), value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = value", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest.get(key, {}), value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest.get(key, []), value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = value", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest.get(key, {}), value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest.get(key, []), value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = value", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest.get(key, {}), value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest.get(key, []), value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = value", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"\n    Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n    \"\"\"\n    for key, value in observer_schema.items():\n        if key not in response:\n            raise KeyError(f\"Observed field '{key}' not present in Kubernetes response\")\n        \n        if isinstance(value, dict):\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest.get(key, {}), value, response[key]\n            )\n        elif isinstance(value, list):\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest.get(key, []), value, response[key]\n            )\n        else:\n            last_applied_manifest[key] = value"]}
{"_id": "62b869eab4d922cb0e688cbf", "name": "generate_default_observer_schema", "project": "rak-n-rok/Krake", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 197071, "num_retrieved": 500, "inference_time": 15.261328995227814, "generate_results": ["def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Get the manifest from the application spec\n    manifest = app.spec.manifest\n    \n    # If no manifest is provided, return immediately\n    if not manifest:\n        return\n    \n    # Initialize the observer schema list\n    observer_schema = []\n    \n    # Iterate over each resource in the manifest\n    for resource in manifest:\n        # Create a default observer schema for each resource\n        # The schema includes the resource's API version, kind, and name\n        observer_schema_dict = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the observer schema to the list\n        observer_schema.append(observer_schema_dict)\n    \n    # Set the observer schema on the application\n    app.spec.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Get the manifest from the application spec\n    manifest = app.spec.manifest\n    \n    # If no manifest is provided, return early\n    if not manifest:\n        return\n    \n    # Initialize the observer schema list\n    observer_schema = []\n    \n    # Iterate over each resource in the manifest\n    for resource in manifest:\n        # Create a default observer schema for each resource\n        # The schema includes the resource's apiVersion, kind, and metadata.name\n        observer_schema_item = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the observer schema item to the list\n        observer_schema.append(observer_schema_item)\n    \n    # Set the observer schema on the application\n    app.spec.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Get the manifest from the application spec\n    manifest = app.spec.manifest\n    \n    # If no manifest is provided, return early\n    if not manifest:\n        return\n    \n    # Initialize the observer schema list\n    observer_schema = []\n    \n    # Iterate over each resource in the manifest\n    for resource in manifest:\n        # Create a new observer schema entry for each resource\n        observer_schema_entry = {\n            \"apiVersion\": resource[\"apiVersion\"],\n            \"kind\": resource[\"kind\"],\n            \"metadata\": {\n                \"name\": resource[\"metadata\"][\"name\"]\n            }\n        }\n        \n        # Add the observer schema entry to the list\n        observer_schema.append(observer_schema_entry)\n    \n    # Set the observer schema on the application\n    app.spec.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Get the manifest from the application spec\n    manifest = app.spec.manifest\n    \n    # If no manifest is provided, return early\n    if not manifest:\n        return\n    \n    # Initialize the observer schema list\n    observer_schema = []\n    \n    # Iterate over each resource in the manifest\n    for resource in manifest:\n        # Extract the resource's API version, kind, and name\n        api_version = resource.get(\"apiVersion\")\n        kind = resource.get(\"kind\")\n        name = resource.get(\"metadata\", {}).get(\"name\")\n        \n        # Skip if any required field is missing\n        if not api_version or not kind or not name:\n            continue\n            \n        # Create a default observer schema dictionary for this resource\n        observer_schema_dict = {\n            \"apiVersion\": api_version,\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name\n            }\n        }\n        \n        # Add the observer schema to the list\n        observer_schema.append(observer_schema_dict)\n    \n    # Set the observer schema on the application\n    app.spec.observer_schema = observer_schema", "def generate_default_observer_schema(app):\n    \"\"\"\n    Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n    # Get the manifest from the application spec\n    manifest = app.spec.manifest\n    \n    # If no manifest is provided, return immediately\n    if not manifest:\n        return\n    \n    # Initialize the observer schema list\n    observer_schema = []\n    \n    # Iterate over each resource in the manifest\n    for resource in manifest:\n        # Extract the resource's API version, kind, and name\n        api_version = resource.get(\"apiVersion\")\n        kind = resource.get(\"kind\")\n        name = resource.get(\"metadata\", {}).get(\"name\")\n        \n        # Skip if any required field is missing\n        if not api_version or not kind or not name:\n            continue\n            \n        # Create a default observer schema dictionary for this resource\n        observer_schema_dict = {\n            \"apiVersion\": api_version,\n            \"kind\": kind,\n            \"metadata\": {\n                \"name\": name\n            }\n        }\n        \n        # Add the observer schema to the list\n        observer_schema.append(observer_schema_dict)\n    \n    # Set the observer schema on the application\n    app.spec.observer_schema = observer_schema"]}
{"_id": "62b43427903eeb48555d3ea5", "name": "format", "project": "cpburnz/python-sql-parameters", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 28364, "num_retrieved": 74, "inference_time": 15.261328995227814, "generate_results": ["def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)", "def format(\n    self,\n    sql: AnyStr,\n    params: Union[Dict[Union[str, int], Any], Sequence[Any]],\n) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of\n    the in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n    contains the set of in-style parameters. It maps each parameter\n    (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n    is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n    If :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n    *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       The set of converted out-style parameters (:class:`dict` or\n            :class:`list`).\n    \"\"\"\n    if isinstance(params, Mapping):\n        # For named parameters, use NamedConverter\n        converter = self._named_converter\n    else:\n        # For ordinal parameters, use OrdinalConverter\n        converter = self._ordinal_converter\n    \n    return converter.convert(sql, params)"]}
{"_id": "62b43428903eeb48555d3eaa", "name": "formatmany", "project": "cpburnz/python-sql-parameters", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 28438, "num_retrieved": 74, "inference_time": 15.261328995227814, "generate_results": ["def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)", "def formatmany(\n    self,\n    sql: AnyStr,\n    many_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n    \"\"\"\n    Convert the SQL query to use the out-style parameters instead of the\n    in-style parameters.\n\n    *sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n    *many_params* (:class:`~collections.abc.Iterable`) contains each set\n    of in-style parameters (*params*).\n\n    -       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\n    Returns a :class:`tuple` containing:\n\n    -       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n    -       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).\n    \"\"\"\n    return self.convert_many(sql, many_params)"]}
{"_id": "62b45df05108cfac7f2109ce", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 41738, "num_retrieved": 112, "inference_time": 15.261328995227814, "generate_results": ["def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if path is not None:\n            validator.obj_fs = open_fs(path, create=False)\n        return validator.validate(path)\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if self.root_fs is not None:\n            # If we have a root filesystem, open the path in it\n            obj_fs = self.root_fs.opendir(path)\n        else:\n            # Otherwise, open the path directly\n            obj_fs = open_fs(path)\n        \n        # Validate the object\n        if validator.validate(obj_fs):\n            return True\n        else:\n            return False\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if path is None:\n            # Validate root filesystem\n            validator.validate(ocfl_opendir(self.root_fs, '/'))\n        else:\n            # Validate object at path\n            validator.validate(ocfl_opendir(self.root_fs, path))\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False\n    \n    # Check if any errors were found\n    return validator.log.num_errors == 0", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if path is None:\n            # Validate root filesystem\n            validator.validate(ocfl_opendir(self.root_fs, '/'))\n        else:\n            # Validate object at path\n            validator.validate(ocfl_opendir(self.root_fs, path))\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False\n    \n    # Check if there are any errors\n    return validator.log.num_errors == 0", "def validate(self, path):\n    \"\"\"Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.\"\"\"\n    validator = Validator(show_warnings=True)\n    try:\n        if path is None:\n            # Validate root filesystem\n            validator.validate(ocfl_opendir(self.root_fs, '/'))\n        else:\n            # Validate object at path\n            validator.validate(ocfl_opendir(self.root_fs, path))\n    except Exception as e:\n        logging.error(\"Validation failed for path %s: %s\", path, str(e))\n        return False\n    \n    # Check if there are any errors\n    return validator.log.num_errors == 0"]}
{"_id": "62b45df15108cfac7f2109dc", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 50103, "num_retrieved": 122, "inference_time": 15.261328995227814, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"VALID\" if self.log.num_errors == 0 else \"INVALID\")\n    return \" | \".join(status)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"Status: \" + (\"VALID\" if self.log.num_errors == 0 else \"INVALID\"))\n    if self.log.num_warnings > 0:\n        status.append(f\"Warnings: {self.log.num_warnings}\")\n    if self.log.num_errors > 0:\n        status.append(f\"Errors: {self.log.num_errors}\")\n    return \" | \".join(status)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"VALID\" if self.log.num_errors == 0 else \"INVALID\")\n    return \" | \".join(status)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"VALID\" if self.log.num_errors == 0 else \"INVALID\")\n    return \" | \".join(status)", "def status_str(self, prefix=''):\n    \"\"\"Return string of validator status, with optional prefix.\"\"\"\n    status = []\n    if prefix:\n        status.append(prefix)\n    status.append(\"Status: \" + (\"VALID\" if self.log.num_errors == 0 else \"INVALID\"))\n    if self.log.num_warnings > 0:\n        status.append(f\"Warnings: {self.log.num_warnings}\")\n    if self.log.num_errors > 0:\n        status.append(f\"Errors: {self.log.num_errors}\")\n    return \" | \".join(status)"]}
{"_id": "62b45df15108cfac7f2109dd", "name": "status_str", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 43124, "num_retrieved": 108, "inference_time": 15.261328995227814, "generate_results": ["def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]", "def status_str(self, prefix=''):\n    \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n    s = ''\n    for message in sorted(self.messages):\n        s += prefix + message + '\\n'\n    return s[:-1]"]}
{"_id": "62b45e135108cfac7f2109f4", "name": "is_valid", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 49205, "num_retrieved": 120, "inference_time": 15.261328995227814, "generate_results": ["def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True", "def is_valid(self, identifier):  # pylint: disable=unused-argument\n    \"\"\"Return True if identifier is valid, always True in this base implementation.\"\"\"\n    return True"]}
{"_id": "62b45e145108cfac7f210a07", "name": "validate", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 41818, "num_retrieved": 110, "inference_time": 15.261328995227814, "generate_results": ["def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from type value\n            if 'type' in inventory and inventory['type']:\n                # Extract version from type URL\n                m = re.search(r'/(\\d+\\.\\d+)', inventory['type'])\n                if m:\n                    spec_version = m.group(1)\n                    inv_validator.spec_version = spec_version\n                    self.spec_version = spec_version\n            else:\n                # Use self.spec_version as fallback\n                pass\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='standalone', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the type value in inventory\n            if 'type' in inventory:\n                type_value = inventory['type']\n                # Extract version from type URL (e.g., https://ocfl.io/1.0/spec/#inventory)\n                match = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n                if match:\n                    spec_version = match.group(1)\n                    inv_validator.spec_version = spec_version\n                else:\n                    # If type value doesn't match expected format, use self.spec_version\n                    pass\n            # If no type value or invalid type value, use self.spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the type value in inventory\n            type_value = inventory.get('type')\n            if type_value:\n                # Extract version from type value (e.g., https://ocfl.io/1.0/spec/#inventory)\n                match = re.search(r'/(\\d+\\.\\d+)', type_value)\n                if match:\n                    spec_version = match.group(1)\n                    inv_validator.spec_version = spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inv_validator\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='standalone',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the type value in the inventory\n            if 'type' in inventory:\n                type_value = inventory['type']\n                # Extract version from type URL (e.g., https://ocfl.io/1.0/spec/#inventory)\n                match = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n                if match:\n                    spec_version = match.group(1)\n                    inv_validator.spec_version = spec_version\n                else:\n                    # If type value doesn't match expected format, use self.spec_version\n                    pass\n            # If no type value or invalid type value, use self.spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='standalone', explanation=str(e))\n        raise ValidatorAbortException", "def validate(self, inventory, extract_spec_version=False):\n    \"\"\"Validate a given inventory.\n\n    If extract_spec_version is True then will look at the type value to determine\n    the specification version. In the case that there is no type value or it isn't\n    valid, then other tests will be based on the version given in self.spec_version.\n    \"\"\"\n    try:\n        inv_validator = InventoryValidator(log=self.log, where='root',\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        if extract_spec_version:\n            # Try to extract spec_version from the type value in inventory\n            if 'type' in inventory:\n                type_value = inventory['type']\n                # Extract version from type URL (e.g., https://ocfl.io/1.0/spec/#inventory)\n                m = re.match(r'https://ocfl.io/(\\d+\\.\\d+)/spec/#inventory', type_value)\n                if m:\n                    inv_validator.spec_version = m.group(1)\n                else:\n                    # If type value doesn't match expected format, use self.spec_version\n                    pass\n            # If no type value or invalid type value, use self.spec_version\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return True\n    except Exception as e:\n        self.log.error('E033', where='root', explanation=str(e))\n        raise ValidatorAbortException"]}
{"_id": "62b45e145108cfac7f210a09", "name": "check_digests_present_and_used", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 40674, "num_retrieved": 99, "inference_time": 15.261328995227814, "generate_results": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used[filepath]:\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used.get(filepath, []):\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used[filepath]:\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used[filepath]:\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)", "def check_digests_present_and_used(self, manifest_files, digests_used):\n    \"\"\"\n    Check all digests in manifest that are needed are present and used.\n    \"\"\"\n    for filepath in manifest_files:\n        for digest in digests_used[filepath]:\n            if digest not in manifest_files[filepath]:\n                self.log.error('E092b', where='root', content_path=filepath)"]}
{"_id": "62b45e165108cfac7f210a16", "name": "validate_as_prior_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 41982, "num_retrieved": 113, "inference_time": 15.261328995227814, "generate_results": ["def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version's spec version is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version's spec version is less than or equal to the current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Ensure the prior version's spec version is less than or equal to current version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)", "def validate_as_prior_version(self, prior):\n    \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n    The input variable prior is also expected to be an InventoryValidator object\n    and both self and prior inventories are assumed to have been checked for\n    internal consistency.\n    \"\"\"\n    # Check that the spec version of prior is less than or equal to the current spec version\n    if prior.spec_version > self.spec_version:\n        self.log.error('E103', where=prior.where, this_spec_version=prior.spec_version,\n                       prev_version_dir=prior.where, prev_spec_version=self.spec_version)"]}
{"_id": "62b45e165108cfac7f210a17", "name": "get_logical_path_map", "project": "zimeon/ocfl-py", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 42172, "num_retrieved": 118, "inference_time": 15.261328995227814, "generate_results": ["def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                for content_file in inventory['manifest'][digest]:\n                    if logical_file not in logical_path_map:\n                        logical_path_map[logical_file] = set()\n                    logical_path_map[logical_file].add(content_file)\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map", "def get_logical_path_map(inventory, version):\n    \"\"\"\n    Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    logical_path_map = {}\n    \n    for digest, logical_files in state.items():\n        for logical_file in logical_files:\n            # For each logical file, we need to find the corresponding content file\n            # The content file is stored in the manifest under the same digest\n            if digest in inventory['manifest']:\n                content_files = set()\n                for content_file in inventory['manifest'][digest]:\n                    content_files.add(content_file)\n                logical_path_map[logical_file] = content_files\n    \n    return logical_path_map"]}
{"_id": "62b45e175108cfac7f210a19", "name": "validate_fixity", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 41181, "num_retrieved": 104, "inference_time": 15.261328995227814, "generate_results": ["def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that fixity block has the correct structure\n    for digest_algorithm in fixity:\n        if not isinstance(fixity[digest_algorithm], dict):\n            self.log.error('E094', where='root', digest_algorithm=digest_algorithm)\n            return\n            \n        for digest in fixity[digest_algorithm]:\n            if not isinstance(fixity[digest_algorithm][digest], list):\n                self.log.error('E095', where='root', digest_algorithm=digest_algorithm, digest=digest)\n                return\n                \n            # Check that all files in fixity are present in manifest\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E096', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that fixity block has the correct structure\n    for digest_algorithm in fixity:\n        if not isinstance(fixity[digest_algorithm], dict):\n            self.log.error('E091', where='root', digest_algorithm=digest_algorithm)\n            return\n            \n        for digest in fixity[digest_algorithm]:\n            if not isinstance(fixity[digest_algorithm][digest], list):\n                self.log.error('E091', where='root', digest_algorithm=digest_algorithm, digest=digest)\n                return\n                \n            # Check that all files in fixity are present in manifest\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E094', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)\n                    return", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"Validate fixity block in inventory.\n\n    Check the structure of the fixity block and makes sure that only files\n    listed in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Validate that fixity is a dictionary with digest algorithms as keys\n    for digest_algorithm in fixity:\n        if not isinstance(digest_algorithm, str):\n            self.log.error('E094', where='root', digest_algorithm=digest_algorithm)\n            continue\n            \n        # Validate that each digest in the algorithm has a dictionary of filepaths\n        if not isinstance(fixity[digest_algorithm], dict):\n            self.log.error('E095', where='root', digest_algorithm=digest_algorithm)\n            continue\n            \n        # Check that all files referenced in fixity are in the manifest\n        for digest in fixity[digest_algorithm]:\n            if not isinstance(digest, str):\n                self.log.error('E096', where='root', digest_algorithm=digest_algorithm, digest=digest)\n                continue\n                \n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E097', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all fixity entries reference files that are in the manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)", "def validate_fixity(self, fixity, manifest_files):\n    \"\"\"\n    Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.\n    \"\"\"\n    if not fixity:\n        return\n    \n    # Check that all fixity entries reference files in the manifest\n    for digest_algorithm in fixity:\n        for digest in fixity[digest_algorithm]:\n            for filepath in fixity[digest_algorithm][digest]:\n                if filepath not in manifest_files:\n                    self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, \n                                  digest=digest, content_path=filepath)"]}
{"_id": "62b463153879012d19481498", "name": "files_list", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 130104, "num_retrieved": 500, "inference_time": 15.261328995227814, "generate_results": ["def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list_from_folder(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list_from_folder(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list_from_folder(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list_from_folder(path)\n    else:\n        return []", "def files_list(path):\n    \"\"\"\n    Return the files in `path`\n    \"\"\"\n    if file_utils.is_zipfile(path):\n        return file_utils.files_list_from_zipfile(path)\n    elif file_utils.is_folder(path):\n        return file_utils.files_list_from_folder(path)\n    else:\n        return []"]}
{"_id": "62b463153879012d1948149a", "name": "_group_files_by_xml_filename", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 144664, "num_retrieved": 500, "inference_time": 15.261328995227814, "generate_results": ["def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_filename in xmls:\n        filename_root, _ = os.path.splitext(xml_filename)\n        grouped_files[filename_root] = []\n    \n    for file in files:\n        filename_root, file_ext = os.path.splitext(file)\n        if file_ext.lower() in ['.xml', '.xml']:\n            # Check if the file matches any XML filename\n            for xml_filename in xmls:\n                xml_root, _ = os.path.splitext(xml_filename)\n                if filename_root == xml_root:\n                    grouped_files[xml_root].append(file)\n                    break\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for file in files:\n        # Extract the basename of the file\n        file_basename = os.path.basename(file)\n        \n        # Check if the file is an XML file\n        if file_basename in xmls:\n            xml_filename = file_basename\n            \n            # Group files by their XML basename\n            if xml_filename not in grouped_files:\n                grouped_files[xml_filename] = []\n            \n            # Add the file to the group\n            grouped_files[xml_filename].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for file in files:\n        # Extract the filename without extension\n        file_name = os.path.basename(file)\n        \n        # Check if the file is an XML file\n        if file_name.endswith(\".xml\"):\n            # Get the XML filename without extension\n            xml_base_name = file_name[:-4]\n            \n            # Check if this XML file is in the list of XML filenames\n            if xml_base_name in xmls:\n                # If not already in the dictionary, initialize an empty list\n                if xml_base_name not in grouped_files:\n                    grouped_files[xml_base_name] = []\n                \n                # Add the file to the list for this XML filename\n                grouped_files[xml_base_name].append(file)\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_filename in xmls:\n        basename = os.path.splitext(xml_filename)[0]\n        grouped_files[basename] = []\n    \n    for file in files:\n        basename = os.path.splitext(file)[0]\n        for xml_filename in xmls:\n            if basename == os.path.splitext(xml_filename)[0]:\n                grouped_files[basename].append(file)\n                break\n    \n    return grouped_files", "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    grouped_files = {}\n    \n    for xml_filename in xmls:\n        basename = os.path.splitext(xml_filename)[0]\n        grouped_files[basename] = []\n    \n    for file in files:\n        basename = os.path.splitext(file)[0]\n        for xml_filename in xmls:\n            if basename == os.path.splitext(xml_filename)[0]:\n                grouped_files[basename].append(file)\n                break\n    \n    return grouped_files"]}
{"_id": "62b463153879012d1948149b", "name": "match_file_by_prefix", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 139251, "num_retrieved": 500, "inference_time": 15.261328995227814, "generate_results": ["def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix", "def match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorno `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    return file == prefix"]}
{"_id": "62b463153879012d1948149c", "name": "select_filenames_by_prefix", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 135961, "num_retrieved": 500, "inference_time": 15.261328995227814, "generate_results": ["def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]", "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [f for f in files if f.startswith(prefix)]"]}
{"_id": "62b463153879012d1948149d", "name": "_explore_folder", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 142041, "num_retrieved": 500, "inference_time": 24.79959660768509, "generate_results": ["def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    package_data = defaultdict(list)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Check if the file is a zip file\n        if filename.lower().endswith('.zip'):\n            # Extract the XML filename from the zip file name\n            xml_filename = filename.replace('.zip', '.xml')\n            package_data[xml_filename].append(file_path)\n        elif filename.lower().endswith('.xml'):\n            # Direct XML file\n            package_data[filename].append(file_path)\n    \n    # Convert defaultdict to regular dict\n    return dict(package_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to store packages data\n    packages_data = defaultdict(dict)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        filepath = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(filepath):\n            continue\n            \n        # Check if file has XML extension\n        if not filename.lower().endswith('.xml'):\n            continue\n            \n        # Extract basename (without extension) from XML file\n        basename = os.path.splitext(filename)[0]\n        \n        # Get the full path of the XML file\n        xml_path = filepath\n        \n        # Look for associated files (renditions, assets) with the same basename\n        # by checking if they exist in the folder\n        renditions = []\n        assets = []\n        \n        for item in os.listdir(folder):\n            item_path = os.path.join(folder, item)\n            if not os.path.isfile(item_path):\n                continue\n                \n            # Skip if it's the XML file itself\n            if item == filename:\n                continue\n                \n            # Check if the file has the same basename\n            if basename in item:\n                # Check if it's a rendition (typically ends with .pdf, .tiff, etc.)\n                if item.lower().endswith(('.pdf', '.tiff', '.tif', '.jpg', '.jpeg', '.png')):\n                    renditions.append(item_path)\n                # Check if it's an asset (typically ends with .tiff, .jpg, etc.)\n                elif item.lower().endswith(('.tiff', '.tif', '.jpg', '.jpeg', '.png')):\n                    assets.append(item_path)\n        \n        # Store the data in the packages_data dictionary\n        packages_data[basename]['xml'] = xml_path\n        packages_data[basename]['renditions'] = renditions\n        packages_data[basename]['assets'] = assets\n    \n    return dict(packages_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    package_data = defaultdict(list)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Check if the file is a zip file\n        if filename.endswith('.zip'):\n            # Extract the XML basename from the zip filename\n            xml_basename = filename.replace('.zip', '')\n            package_data[xml_basename].append(file_path)\n        elif filename.endswith('.xml'):\n            # For XML files, use the filename as the basename\n            xml_basename = filename\n            package_data[xml_basename].append(file_path)\n    \n    # Convert defaultdict to regular dict\n    return dict(package_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to store packages data\n    packages_data = defaultdict(dict)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Check if the file is a zip file\n        if filename.endswith('.zip'):\n            # Extract the XML basename from the zip filename\n            xml_basename = filename.replace('.zip', '')\n            \n            # Get the content of the zip file\n            with ZipFile(file_path, 'r') as zip_ref:\n                # Get all XML files in the zip\n                xml_files = [f for f in zip_ref.namelist() if f.endswith('.xml')]\n                \n                # If there are XML files, add them to the packages data\n                if xml_files:\n                    # Get the first XML file's content\n                    xml_file = xml_files[0]\n                    xml_content = zip_ref.read(xml_file)\n                    \n                    # Store the XML content in the packages data\n                    packages_data[xml_basename]['xml'] = xml_content\n                    \n                    # Get the other files in the zip\n                    other_files = [f for f in zip_ref.namelist() if f not in xml_files]\n                    packages_data[xml_basename]['other_files'] = other_files\n    \n    return dict(packages_data)", "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from collections import defaultdict\n    \n    # Dictionary to store packages data\n    packages_data = defaultdict(dict)\n    \n    # Iterate through all files in the folder\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        \n        # Skip if it's not a file\n        if not os.path.isfile(file_path):\n            continue\n            \n        # Get the file extension\n        _, ext = os.path.splitext(filename)\n        \n        # Only process XML files\n        if ext.lower() != '.xml':\n            continue\n            \n        # Extract the basename (without extension) of the XML file\n        xml_basename = os.path.splitext(filename)[0]\n        \n        # Get the full path of the XML file\n        xml_path = file_path\n        \n        # Look for associated files (renditions, assets) with the same basename\n        # Check for files that match the pattern: <basename>.pdf, <basename>.jpg, etc.\n        for file_name in os.listdir(folder):\n            file_path = os.path.join(folder, file_name)\n            if not os.path.isfile(file_path):\n                continue\n                \n            # Skip if it's the XML file itself\n            if file_name == filename:\n                continue\n                \n            # Check if the file name starts with the XML basename\n            if file_name.startswith(xml_basename):\n                # Determine the file type based on extension\n                file_ext = os.path.splitext(file_name)[1].lower()\n                \n                # Add to the packages data\n                if xml_basename not in packages_data:\n                    packages_data[xml_basename] = {}\n                    \n                # Group by file type\n                if file_ext not in packages_data[xml_basename]:\n                    packages_data[xml_basename][file_ext] = []\n                    \n                packages_data[xml_basename][file_ext].append(file_path)\n    \n    # Convert defaultdict to regular dict\n    return dict(packages_data)"]}
{"_id": "62b463153879012d1948149f", "name": "_eval_file", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 141904, "num_retrieved": 500, "inference_time": 24.79959660768509, "generate_results": ["def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in get_pdf_langs():\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in get_pdf_langs():\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in get_pdf_langs():\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'rendition', 'path': file_path}\n        for lang in get_pdf_langs():\n            if file == f'{prefix}-{lang}':\n                return {'type': 'rendition', 'path': file_path}\n    return {'type': 'asset', 'path': file_path}", "def _eval_file(prefix, file_path, file_folder=None):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereo do arquivo em anlise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extenso\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    file, ext = os.path.splitext(file_path)\n    \n    if ext == '.xml':\n        return {'type': 'xml', 'path': file_path}\n    elif ext == '.pdf':\n        if file == prefix:\n            return {'type': 'renditions', 'path': file_path}\n        for lang in get_pdf_langs():\n            if file == f'{prefix}-{lang}':\n                return {'type': 'renditions', 'path': file_path}\n    return {'type': 'assets', 'path': file_path}"]}
{"_id": "62b463153879012d194814a1", "name": "add_rendition", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 56190, "num_retrieved": 181, "inference_time": 24.79959660768509, "generate_results": ["def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies do pacote.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF da rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome original e o nome da rendio em um idioma especfico\n    \"\"\"\n    # Cria um dicionrio com o nome original e o nome da rendio\n    rendition_dict = {\n        'original': os.path.basename(file_path),\n        lang: f\"{os.path.splitext(file_path)[0]}-{lang}.pdf\"\n    }\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies com base na lngua e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Cdigo da lngua do PDF (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com a chave da lngua e o valor sendo o nome do arquivo de rendio\n        Exemplo: {\"pt\": \"artigo02.pdf\", \"en\": \"artigo02-en.pdf\"}\n    \"\"\"\n    # Extrai o nome do arquivo sem a extenso\n    filename = os.path.basename(file_path)\n    name, ext = os.path.splitext(filename)\n    \n    # Cria o nome da rendio com base na lngua\n    if lang == 'en':\n        rendition_name = f\"{name}-en{ext}\"\n    else:\n        rendition_name = f\"{name}{ext}\"\n    \n    return {lang: rendition_name}", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies com base na linguagem e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com a rendio, onde a chave  o idioma e o valor  o nome do arquivo\n        Exemplo: {\"pt\": \"artigo02.pdf\", \"en\": \"artigo02-en.pdf\"}\n    \"\"\"\n    # Extrai o nome do arquivo sem a extenso\n    filename = os.path.basename(file_path)\n    name, ext = os.path.splitext(filename)\n    \n    # Cria o nome da rendio com base na linguagem\n    if lang == 'en':\n        rendition_name = f\"{name}-en{ext}\"\n    else:\n        rendition_name = f\"{name}{ext}\"\n    \n    return {lang: rendition_name}", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies do pacote.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho do arquivo PDF da rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome original e o nome da rendio em um idioma especfico\n    \"\"\"\n    # Cria um dicionrio com o nome original e o nome da rendio\n    rendition_dict = {\n        'original': os.path.basename(file_path),\n        lang: f\"{os.path.splitext(file_path)[0]}-{lang}.pdf\"\n    }\n    return rendition_dict", "def add_rendition(self, lang, file_path):\n    \"\"\"\n    Adiciona um PDF de rendio ao dicionrio de rendies com base na linguagem e no caminho do arquivo.\n\n    Parameters\n    ----------\n    lang : str\n        Idioma da rendio (ex: 'pt', 'en')\n    file_path : str\n        Caminho completo do arquivo PDF de rendio\n\n    Returns\n    -------\n    dict\n        Dicionrio com o nome original e o nome da rendio na linguagem especificada\n        Exemplo:\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\"\n        }\n    \"\"\"\n    # Extrai o nome do arquivo original sem a extenso\n    base_name = os.path.splitext(os.path.basename(file_path))[0]\n    \n    # Cria o nome da rendio com base na linguagem\n    if lang == 'en':\n        rendition_name = f\"{base_name}-en.pdf\"\n    else:\n        rendition_name = f\"{base_name}.pdf\"\n    \n    return {\n        \"original\": file_path,\n        \"en\": rendition_name if lang == 'en' else None\n    }"]}
{"_id": "62b463163879012d194814a2", "name": "add_asset", "project": "scieloorg/packtools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 54083, "num_retrieved": 224, "inference_time": 24.79959660768509, "generate_results": ["def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(basename)[1].lower()\n    if ext in ['.tiff', '.tif']:\n        self._assets[basename] = file_path\n    elif ext in ['.jpg', '.jpeg']:\n        # Create a jpg version with the same basename\n        jpg_basename = basename.rsplit('.', 1)[0] + '.jpg'\n        self._assets[jpg_basename] = file_path\n    elif ext in ['.png']:\n        # Create a png version with the same basename\n        png_basename = basename.rsplit('.', 1)[0] + '.png'\n        self._assets[png_basename] = file_path\n    else:\n        # For other extensions, just add the original basename\n        self._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets dictionary with its basename and file path.\"\"\"\n    # Create a dictionary entry with the basename as key and file path as value\n    self.assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(basename)[1].lower()\n    if ext in ['.tiff', '.tif']:\n        # For TIFF files, keep the original extension\n        self._assets[basename] = file_path\n    elif ext in ['.jpg', '.jpeg']:\n        # For JPEG files, create a thumbnail and add both original and thumbnail\n        thumbnail_path = os.path.splitext(file_path)[0] + \".thumbnail.jpg\"\n        # Assuming thumbnail generation is handled elsewhere, just store the original\n        self._assets[basename] = file_path\n    elif ext in ['.png']:\n        # For PNG files, create a PNG version and add it\n        png_path = os.path.splitext(file_path)[0] + \".png\"\n        self._assets[basename] = file_path\n    else:\n        # For other extensions, just store the original\n        self._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"\n    \"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n    }\n    \"\"\"\n    ext = os.path.splitext(basename)[1].lower()\n    if ext in ['.tif', '.tiff']:\n        # Keep the original file extension for TIFF\n        self._assets[basename] = file_path\n    elif ext in ['.jpg', '.jpeg']:\n        # Create a JPEG version with the same basename\n        new_basename = basename.rsplit('.', 1)[0] + '.jpg'\n        self._assets[new_basename] = file_path\n    elif ext in ['.png']:\n        # Keep the PNG file as is\n        self._assets[basename] = file_path\n    else:\n        # For any other extension, add the original file\n        self._assets[basename] = file_path", "def add_asset(self, basename, file_path):\n    \"\"\"Add an asset to the assets dictionary with the basename as key and file_path as value.\"\"\"\n    self.assets[basename] = file_path"]}
{"_id": "62b463163879012d194814a4", "name": "_explore_zipfile", "project": "scieloorg/packtools", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 141270, "num_retrieved": 500, "inference_time": 24.79959660768509, "generate_results": ["def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path, 'r') as zip_file:\n        file_list = zip_file.namelist()\n        for file_name in file_list:\n            if file_name.endswith(\".xml\"):\n                basename = os.path.splitext(file_name)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append(file_name)\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path) as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append({\n                    \"filename\": filename,\n                    \"xml_with_pre\": get_xml_with_pre(zf.read(filename).decode(\"utf-8\"))\n                })\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path) as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append({\n                    \"filename\": filename,\n                    \"xml_with_pre\": get_xml_with_pre(zf.read(filename).decode(\"utf-8\"))\n                })\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path) as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append({\n                    \"filename\": filename,\n                    \"xml_with_pre\": get_xml_with_pre(zf.read(filename).decode(\"utf-8\"))\n                })\n    return data", "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict\n    \"\"\"\n    import os\n    from zipfile import ZipFile\n    \n    data = {}\n    with ZipFile(zip_path) as zf:\n        filenames = zf.namelist()\n        for filename in filenames:\n            if filename.endswith(\".xml\"):\n                basename = os.path.splitext(filename)[0]\n                if basename not in data:\n                    data[basename] = []\n                data[basename].append({\n                    \"filename\": filename,\n                    \"xml_with_pre\": get_xml_with_pre(zf.read(filename).decode(\"utf-8\"))\n                })\n    return data"]}
{"_id": "62b463163879012d194814a6", "name": "files_list_from_zipfile", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 135325, "num_retrieved": 500, "inference_time": 24.79959660768509, "generate_results": ["def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path) as zip_file:\n        return [member.filename for member in zip_file.infolist() if member.file_size > 0]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path) as zip_file:\n        return [member.filename for member in zip_file.infolist() if member.file_size > 0]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path) as zip_file:\n        return [member.filename for member in zip_file.infolist() if member.file_size > 0]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    with ZipFile(zip_path, 'r') as zip_file:\n        return [file_info.filename for file_info in zip_file.infolist() if file_info.file_size > 0]", "def files_list_from_zipfile(zip_path):\n    \"\"\"\n    Return the files in `zip_path`\n\nExample:\n\n\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n\n    \"\"\"\n    files = []\n    with ZipFile(zip_path, 'r') as zip_file:\n        for file_info in zip_file.infolist():\n            if file_info.filename:\n                files.append(file_info.filename)\n    return files"]}
{"_id": "62b4631b3879012d194814dd", "name": "fix_namespace_prefix_w", "project": "scieloorg/packtools", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 134895, "num_retrieved": 484, "inference_time": 24.79959660768509, "generate_results": ["def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    replacement = r'w-st=\"'\\1\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    return re.sub(r'w:st=\"', 'w-st=\"', content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"'\n    replacement = r'w-st=\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    pattern = r'w:st=\"([^\"]*)\"'\n    replacement = r'w-st=\"'\\1\"'\n    return re.sub(pattern, replacement, content)", "def fix_namespace_prefix_w(content):\n    \"\"\"\n    Convert text that defaults to 'w:st=\"' to 'w-st=\"'\n    \"\"\"\n    import re\n    return re.sub(r'w:st=\"', 'w-st=\"', content)"]}
{"_id": "62b463283879012d1948153d", "name": "match_pubdate", "project": "scieloorg/packtools", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 143363, "num_retrieved": 500, "inference_time": 24.79959660768509, "generate_results": ["def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None", "def match_pubdate(node, pubdate_xpaths):\n    \"\"\"\n    Returns the first match in the pubdate_xpaths list\n    \"\"\"\n    for xpath in pubdate_xpaths:\n        match = node.xpath(xpath)\n        if match:\n            return match[0]\n    return None"]}
{"_id": "62b463303879012d19481579", "name": "_extract_number_and_supplment_from_issue_element", "project": "scieloorg/packtools", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 130764, "num_retrieved": 461, "inference_time": 24.79959660768509, "generate_results": ["def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Remove any non-numeric characters and split by spaces\n    cleaned_issue = ''.join(c for c in issue if c.isdigit() or c == ' ')\n    parts = cleaned_issue.split()\n    \n    # Extract the number part (first number)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # Extract the supplement part (if present)\n    for part in parts:\n        if part.isdigit() and part.startswith('s'):\n            supplement = part[1:]\n            break\n    \n    # If no number found, return 0\n    if number is None:\n        number = 0\n    \n    # If supplement is not found, return empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Remove any non-printable characters\n    issue = issue.strip()\n    \n    # Split the issue string by spaces and remove empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Extract number and supplement\n    number = \"\"\n    supplement = \"\"\n    \n    # Try to find the number (usually a number, sometimes with a letter)\n    for part in parts:\n        # Check if the part is a number or a number followed by a letter\n        if part.isdigit():\n            number = part\n            break\n        elif part.replace('-', '').isdigit():\n            number = part\n            break\n    \n    # Check for supplement (usually a letter or a number followed by 's')\n    for part in parts:\n        if part.lower() in ['s', 'suppl', 'suppl.', 's.']:\n            supplement = 's'\n            break\n        elif part.lower() in ['s0', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9']:\n            supplement = part\n            break\n    \n    # If no number was found, return 0\n    if not number:\n        number = \"0\"\n    \n    # If no supplement was found, return empty string\n    if not supplement:\n        supplement = \"\"\n    \n    return int(number), supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if not issue:\n        return 0, \"\"\n    \n    # Remove any non-numeric characters and split by spaces\n    cleaned_issue = ''.join(c for c in issue if c.isdigit() or c == ' ')\n    parts = cleaned_issue.split()\n    \n    # Extract the number part (first number)\n    number = None\n    supplement = None\n    \n    for part in parts:\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # Extract the supplement part (if any)\n    for part in parts:\n        if part.isdigit() and part.startswith('s'):\n            supplement = part[1:]\n            break\n    \n    # If no number found, return 0\n    if number is None:\n        number = 0\n    \n    # If no supplement found, return empty string\n    if supplement is None:\n        supplement = \"\"\n    \n    return number, supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Remove any non-printable characters\n    issue = issue.strip()\n    \n    # Split the issue string by spaces and remove empty strings\n    parts = [part.strip() for part in issue.split() if part.strip()]\n    \n    # Extract number and supplement\n    number = \"\"\n    supplement = \"\"\n    \n    # Check if the issue contains a number and/or supplement\n    for part in parts:\n        # Check if the part is a number (digits only)\n        if part.isdigit():\n            number = part\n            break\n        # Check if the part is a supplement (starts with 's' or 'spe')\n        elif part.startswith('s') or part.startswith('spe'):\n            supplement = part\n            break\n    \n    # If no number is found, return 0\n    if not number:\n        number = \"0\"\n    \n    # If no supplement is found, return empty string\n    if not supplement:\n        supplement = \"\"\n    \n    return int(number), supplement", "def _extract_number_and_supplment_from_issue_element(issue):\n    \"\"\"\n    Extract the possible values of number and suppl from the contents of issue.\n    \"\"\"\n    if issue is None:\n        return 0, \"\"\n    \n    # Remove any non-numeric characters and split by spaces\n    cleaned_issue = ''.join(c for c in issue if c.isdigit() or c == ' ')\n    parts = cleaned_issue.split()\n    \n    # Extract the number part (first numeric value)\n    number = 0\n    for part in parts:\n        if part.isdigit():\n            number = int(part)\n            break\n    \n    # Extract the supplement part (if present)\n    supplement = \"\"\n    for part in parts:\n        if part.isdigit():\n            # If we already have a number, the next digit is likely the supplement\n            if number == 0:\n                number = int(part)\n            else:\n                supplement = part\n            break\n    \n    return number, supplement"]}
{"_id": "62b46740d2f69a53b466171a", "name": "pretty", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 178, "num_retrieved": 1, "inference_time": 24.79959660768509, "generate_results": ["def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}, {debug}) called on {self}\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.operator == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return \"NOT \" + self.children[0].pretty(indent + 1, debug)\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.op == NOT:\n            return f\"NOT {self.expr.pretty(indent + 1, debug)}\"\n        elif self.op == TOKEN_LPAR:\n            return f\"({self.expr.pretty(indent + 1, debug)})\"\n        elif self.op == TOKEN_RPAR:\n            return \"\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.operator == OR:\n            return f\"({self.left.pretty(indent + 1, debug)} OR {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == AND:\n            return f\"({self.left.pretty(indent + 1, debug)} AND {self.right.pretty(indent + 1, debug)})\"\n        elif self.operator == NOT:\n            return f\"NOT ({self.expression.pretty(indent + 1, debug)})\"\n    \n    return \"\"", "def pretty(self, indent=0, debug=False):\n    \"\"\"\n    Return a pretty formatted representation of self.\n    \"\"\"\n    if debug:\n        print(f\"pretty({indent}= {indent}, debug={debug})\")\n    \n    if isinstance(self, Symbol):\n        return self.name\n    \n    if isinstance(self, Expression):\n        if self.op == AND:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == OR:\n            return \" \".join([child.pretty(indent + 1, debug) for child in self.children])\n        elif self.op == NOT:\n            return f\"NOT {self.children[0].pretty(indent + 1, debug)}\"\n    \n    return \"\""]}
{"_id": "62b46746d2f69a53b4661722", "name": "absorb", "project": "bastikr/boolean", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 253, "num_retrieved": 1, "inference_time": 24.79959660768509, "generate_results": ["def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            simplified = expr.simplify()\n            result.append(simplified)\n        else:\n            result.append(expr)\n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For absorption: A & (A | B) = A, A | (A & B) = A\n        # For negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Check if expression is of form A & (A | B) or A | (A & B)\n        # This is a simplified implementation - actual logic would depend on expression structure\n        if expr.is_and() and len(expr.args) == 2:\n            left, right = expr.args\n            if left == right or (left.is_or() and right.is_or()):\n                # Simplify based on absorption rules\n                result.append(left)\n            elif left.is_not() and right.is_or():\n                # Handle negative absorption\n                if left.negate() == right.args[0]:\n                    result.append(left.args[0])\n                elif left.negate() == right.args[1]:\n                    result.append(left.args[1])\n            else:\n                result.append(expr)\n        elif expr.is_or() and len(expr.args) == 2:\n            left, right = expr.args\n            if left == right or (left.is_and() and right.is_and()):\n                # Simplify based on absorption rules\n                result.append(left)\n            elif left.is_not() and right.is_and():\n                # Handle negative absorption\n                if left.negate() == right.args[0]:\n                    result.append(left.args[0])\n                elif left.negate() == right.args[1]:\n                    result.append(left.args[1])\n            else:\n                result.append(expr)\n        else:\n            result.append(expr)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # These simplifications are typically handled by the Expression class\n            # We assume the Expression class has methods to handle these simplifications\n            simplified = expr.simplify()\n            if simplified is not None:\n                result.append(simplified)\n            else:\n                result.append(expr)\n        else:\n            result.append(expr)\n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        simplified = expr\n        # Check for absorption: A & (A | B) = A, A | (A & B) = A\n        # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n        \n        # Simplify using absorption laws\n        if isinstance(simplified, Expression):\n            # Check if it's of the form A & (A | B) or A | (A & B)\n            # This is a simplified version - actual implementation would require\n            # parsing the expression tree to detect these patterns\n            # For now, we just return the original expression\n            pass\n        \n        result.append(simplified)\n    \n    return result", "def absorb(self, args):\n    \"\"\"\n    Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B\n    \"\"\"\n    result = []\n    for expr in args:\n        # Apply absorption and negative absorption rules\n        # For each expression, check if it can be simplified\n        if isinstance(expr, Expression):\n            # Check for absorption: A & (A | B) = A, A | (A & B) = A\n            # Check for negative absorption: A & (~A | B) = A & B, A | (~A & B) = A | B\n            # Simplify based on the structure of the expression\n            # This is a simplified implementation that assumes the expression\n            # is already in a form that can be checked for these laws\n            # In a full implementation, we would need to parse the expression tree\n            # and apply the rules recursively\n            result.append(expr)\n        else:\n            result.append(expr)\n    return result"]}
{"_id": "62b86707b4d922cb0e688c2a", "name": "on", "project": "rak-n-rok/Krake", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 204769, "num_retrieved": 500, "inference_time": 24.79959660768509, "generate_results": ["def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook] = handler\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook] = handler\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"Decorator function to add a new handler to the registry.\n\n    Args:\n        hook (HookType): Hook attribute for which to register the handler.\n\n    Returns:\n        callable: Decorator for registering listeners for the specified hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook] = handler\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook] = handler\n        return handler\n    return decorator", "def on(self, hook):\n    \"\"\"\n    Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.\n    \"\"\"\n    def decorator(handler):\n        self.hooks[hook] = handler\n        return handler\n    return decorator"]}
{"_id": "62b86729b4d922cb0e688c2f", "name": "base_config", "project": "rak-n-rok/Krake", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 203657, "num_retrieved": 500, "inference_time": 24.79959660768509, "generate_results": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    return {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }\n    return config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n    \"\"\"\n    Creates a configuration with some simple parameters, which have a default value\n    that can be set.\n\n    Args:\n        user (str): the name of the user for the static authentication\n        etcd_host (str): the host for the database.\n        etcd_port (int): the port for the database.\n\n    Returns:\n        dict: the created configuration.\n    \"\"\"\n    config = {\n        \"authentication\": {\n            \"strategy\": {\n                \"static\": {\n                    \"enabled\": True,\n                    \"name\": user\n                }\n            }\n        },\n        \"etcd\": {\n            \"host\": etcd_host,\n            \"port\": etcd_port,\n            \"retry_transactions\": 1\n        }\n    }\n    return config"]}
{"_id": "62b8a4a4755ee91dce50a3d2", "name": "_fromutc", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 53581, "num_retrieved": 195, "inference_time": 24.79959660768509, "generate_results": ["def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For UTC, return the original dt\n    if isinstance(self, tzutc):\n        return dt\n    \n    # For other timezones, we need to determine if the datetime is ambiguous\n    # and in a fold state\n    \n    # First, check if the timezone has an is_ambiguous method\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            # If the timezone defines its own is_ambiguous function, use it\n            if is_ambiguous_fn(dt):\n                # The datetime is ambiguous, so we need to determine the fold\n                # state\n                fold = self._fold(dt)\n                if fold is not None:\n                    # Return the datetime with the fold state\n                    return enfold(dt, fold=int(fold))\n                else:\n                    # No fold state, return the original datetime\n                    return dt\n            else:\n                # Not ambiguous, return the original datetime\n                return dt\n        except Exception:\n            # If there's an error with the is_ambiguous function, fall back\n            # to the default behavior\n            pass\n    \n    # If no is_ambiguous function is defined, check if the datetime is ambiguous\n    # by comparing the offsets of the fold states\n    dt_replaced = dt.replace(tzinfo=self)\n    wall_0 = enfold(dt_replaced, fold=0)\n    wall_1 = enfold(dt_replaced, fold=1)\n    \n    # If the offsets are the same, then the datetime is not ambiguous\n    if wall_0.utcoffset() == wall_1.utcoffset():\n        return dt\n    \n    # If the offsets are different, then the datetime is ambiguous\n    # We need to determine which fold state to use\n    # The fold state is determined by the timezone's rules\n    fold = self._fold(dt)\n    if", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Create a timezone-aware datetime with the current timezone\n    dt_with_tz = dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt_with_tz)\n    \n    # If it's ambiguous, determine which fold to use\n    if is_ambiguous:\n        # Use the fold value to determine which occurrence to use\n        # The fold value is 0 for the earlier occurrence, 1 for the later\n        fold = 0 if self._fold(dt_with_tz) else 1\n        return enfold(dt_with_tz, fold=int(fold))\n    else:\n        # If not ambiguous, just return the datetime\n        return dt_with_tz", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For timezone-aware datetimes, we need to convert from UTC to local time\n    # First, we need to determine if the datetime is ambiguous\n    # We'll use the tzinfo's is_ambiguous method if available\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    \n    # If the timezone has an is_ambiguous method, use it\n    if is_ambiguous_fn is not None:\n        try:\n            # Check if the datetime is ambiguous\n            is_ambiguous = is_ambiguous_fn(dt)\n            # If ambiguous, we need to determine which fold to use\n            # The fold attribute will be set to 0 or 1\n            fold = 0 if not is_ambiguous else 1\n            # Create the new datetime with the appropriate fold\n            return enfold(dt, fold=int(fold))\n        except Exception:\n            # If there's an error with the is_ambiguous method, fall back to standard behavior\n            pass\n    \n    # If no is_ambiguous method, or if it fails, use the standard fromutc behavior\n    # This is a fallback to the standard implementation\n    # We need to find the transition that the datetime falls into\n    idx = self._find_last_transition(dt, in_utc=True)\n    tti = self._get_ttinfo(idx)\n    \n    # Calculate the offset from UTC to local time\n    dt_out = dt + datetime.timedelta(seconds=tti.offset)\n    \n    # Determine if the datetime is in a fold state\n    fold = self.is_ambiguous(dt_out, idx=idx)\n    \n    return enfold(dt_out, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # For UTC, return the original dt\n    if isinstance(self, tzutc):\n        return dt\n    \n    # For other timezones, we need to determine if the datetime is ambiguous\n    # and in a fold state\n    \n    # First, check if the timezone has an is_ambiguous method\n    is_ambiguous_fn = getattr(self, 'is_ambiguous', None)\n    if is_ambiguous_fn is not None:\n        try:\n            # If the timezone defines its own is_ambiguous function, use it\n            is_ambiguous = is_ambiguous_fn(dt)\n        except Exception:\n            is_ambiguous = False\n    else:\n        # If no custom is_ambiguous function, check if the datetime is ambiguous\n        # by comparing the offsets of the fold states\n        dt_replaced = dt.replace(tzinfo=self)\n        wall_0 = enfold(dt_replaced, fold=0)\n        wall_1 = enfold(dt_replaced, fold=1)\n        \n        same_offset = wall_0.utcoffset() == wall_1.utcoffset()\n        same_dst = wall_0.dst() == wall_1.dst()\n        \n        is_ambiguous = not (same_offset and same_dst)\n    \n    # Determine the fold state\n    fold = 0\n    if is_ambiguous:\n        # Check if the original dt is in the fold state\n        # This is a simplified check - in practice, we'd need to determine\n        # which fold state the original dt corresponds to\n        fold = 1\n    \n    # Return the datetime in the new timezone\n    return enfold(dt, fold=int(fold))", "def _fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Create a timezone-aware datetime with the current timezone\n    dt_with_tz = dt.replace(tzinfo=self)\n    \n    # Check if the datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt_with_tz)\n    \n    # If it's ambiguous, determine which fold to use\n    if is_ambiguous:\n        # Use the fold value to determine which occurrence to use\n        fold = self._fold(dt_with_tz)\n        # Create the enfolded datetime with the appropriate fold\n        return enfold(dt_with_tz, fold=int(fold))\n    else:\n        # If not ambiguous, return the datetime as is\n        return dt_with_tz"]}
{"_id": "62b8982f755ee91dce50a241", "name": "normalized", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 53378, "num_retrieved": 203, "inference_time": 24.79959660768509, "generate_results": ["def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through each attribute and convert fractional values to integers\n    # by rounding to the nearest integer\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days':\n                # Handle days with fractional components\n                if value != int(value):\n                    # Add the fractional part to the next day\n                    days = int(value) + 1\n                    # Adjust hours to account for the extra day\n                    hours = int(getattr(self, 'hours', 0))\n                    if hours > 23:\n                        hours = hours - 24\n                        days += 1\n                else:\n                    days = int(value)\n            elif attr == 'hours':\n                # Handle hours with fractional components\n                if value != int(value):\n                    # Add the fractional part to the next hour\n                    hours = int(value) + 1\n                    # Adjust minutes to account for the extra hour\n                    minutes = int(getattr(self, 'minutes', 0))\n                    if minutes > 59:\n                        minutes = minutes - 60\n                        hours += 1\n                else:\n                    hours = int(value)\n            else:\n                # For other attributes, just convert to integer\n                if value != int(value):\n                    value = int(value) + 1\n                else:\n                    value = int(value)\n            \n            # Set the attribute on the result object\n            setattr(result, attr, value)\n    \n    # Handle the case where we have fractional days that need to be converted\n    # to hours and minutes\n    if hasattr(self, 'days') and self.days != int(self.days):\n        days = int(self.days)\n        fractional_days = self.days - days\n        hours = int(fractional_days * 24)\n        minutes = int((fractional_days *", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    # by rounding to the nearest integer, then adjusting the values accordingly\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days':\n                days = int(round(value))\n                # Adjust hours to account for the fractional days\n                hours = int(round((value - days) * 24))\n                # Add the adjusted hours to the hours attribute\n                if hours != 0:\n                    setattr(result, 'hours', hours)\n            elif attr == 'hours':\n                hours = int(round(value))\n                # Adjust minutes to account for the fractional hours\n                minutes = int(round((value - hours) * 60))\n                # Add the adjusted minutes to the minutes attribute\n                if minutes != 0:\n                    setattr(result, 'minutes', minutes)\n            elif attr == 'minutes':\n                minutes = int(round(value))\n                # Adjust seconds to account for the fractional minutes\n                seconds = int(round((value - minutes) * 60))\n                # Add the adjusted seconds to the seconds attribute\n                if seconds != 0:\n                    setattr(result, 'seconds', seconds)\n            elif attr == 'seconds':\n                seconds = int(round(value))\n                # Adjust the seconds to the nearest integer\n                if seconds != 0:\n                    setattr(result, 'seconds', seconds)\n            else:\n                # For other attributes, just set the integer value\n                setattr(result, attr, int(round(value)))\n    \n    # Return the normalized object\n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    # by rounding to the nearest integer, then adjust the values accordingly\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days':\n                days = int(round(value))\n                # Adjust hours to account for the fractional days\n                hours = int(round((value - days) * 24))\n                # Add the adjusted hours to the hours attribute\n                if hours != 0:\n                    setattr(result, 'hours', hours)\n            elif attr == 'hours':\n                hours = int(round(value))\n                # Adjust minutes to account for the fractional hours\n                minutes = int(round((value - hours) * 60))\n                # Add the adjusted minutes to the minutes attribute\n                if minutes != 0:\n                    setattr(result, 'minutes', minutes)\n            elif attr == 'minutes':\n                minutes = int(round(value))\n                # Adjust seconds to account for the fractional minutes\n                seconds = int(round((value - minutes) * 60))\n                # Add the adjusted seconds to the seconds attribute\n                if seconds != 0:\n                    setattr(result, 'seconds', seconds)\n            elif attr == 'seconds':\n                seconds = int(round(value))\n                # Adjust the seconds attribute\n                if seconds != 0:\n                    setattr(result, 'seconds', seconds)\n            elif attr == 'weeks':\n                weeks = int(round(value))\n                # Adjust days to account for the fractional weeks\n                days = int(round((value - weeks) * 7))\n                # Add the adjusted days to the days attribute\n                if days != 0:\n                    setattr(result, 'days', days)\n            elif attr == 'months':\n                months = int(round(value))\n                # Adjust years to account for the fractional months", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    # by rounding to the nearest integer and adjusting the value accordingly\n    for attr in ['days', 'hours', 'minutes', 'seconds', 'weeks', 'months', 'years']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if attr == 'days':\n                days = int(round(value))\n                # Adjust the hours to account for the fractional days\n                hours = int(round((value - days) * 24))\n                result.days = days\n                result.hours = hours\n            elif attr == 'hours':\n                result.hours = int(round(value))\n            elif attr == 'minutes':\n                result.minutes = int(round(value))\n            elif attr == 'seconds':\n                result.seconds = int(round(value))\n            elif attr == 'weeks':\n                result.weeks = int(round(value))\n            elif attr == 'months':\n                result.months = int(round(value))\n            elif attr == 'years':\n                result.years = int(round(value))\n    \n    # Ensure that the values are properly normalized\n    # For example, if days is 1.5, we want to convert it to days=1, hours=14\n    if hasattr(self, 'days') and self.days != 0:\n        days = int(self.days)\n        hours = int((self.days - days) * 24)\n        result.days = days\n        result.hours = hours\n    \n    return result", "def normalized(self):\n    \"\"\"\n    Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.\n    \"\"\"\n    # Create a copy of the current relativedelta object\n    result = self.__class__()\n    \n    # Iterate through all the attributes and convert fractional values to integers\n    # by rounding to the nearest integer\n    for attr in ['years', 'months', 'days', 'hours', 'minutes', 'seconds', 'weeks']:\n        value = getattr(self, attr, 0)\n        if value != 0:\n            # Convert fractional values to integers\n            if isinstance(value, float):\n                # Round to nearest integer\n                rounded_value = round(value)\n            else:\n                rounded_value = int(value)\n            \n            # Set the attribute on the result object\n            setattr(result, attr, rounded_value)\n    \n    # Handle the case where days and hours might need to be adjusted due to overflow\n    # This is a simplified version - in a real implementation, we'd need to handle\n    # the complex interactions between days, hours, minutes, and seconds\n    if hasattr(self, 'days') and hasattr(self, 'hours'):\n        # Adjust days and hours to ensure they are valid\n        days = getattr(result, 'days', 0)\n        hours = getattr(result, 'hours', 0)\n        \n        # Convert fractional hours to days and adjust days\n        if hours > 0:\n            # Convert hours to days\n            hours_in_days = hours / 24\n            # Add to days\n            days += hours_in_days\n            # Convert back to hours\n            hours = int(hours % 24)\n            \n            # Adjust days to be an integer\n            result.days = int(days)\n    \n    return result"]}
{"_id": "62b89640755ee91dce50a114", "name": "tzname_in_python2", "project": "pexip/os-python-dateutil", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 51772, "num_retrieved": 185, "inference_time": 24.79959660768509, "generate_results": ["def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('utf-8')\n            return result\n        return wrapper\n    else:\n        return namefunc", "def tzname_in_python2(namefunc):\n    \"\"\"\n    Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    import sys\n    if sys.version_info[0] == 2:\n        def wrapper(self, dt):\n            result = namefunc(self, dt)\n            if isinstance(result, unicode):\n                return result.encode('ascii')\n            return result\n        return wrapper\n    else:\n        return namefunc"]}
{"_id": "62b87d24d292efb640a55670", "name": "get_versions", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 16408, "num_retrieved": 60, "inference_time": 8.868048772215843, "generate_results": ["def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]", "def get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]"]}
{"_id": "62b87d24d292efb640a5566f", "name": "render", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 13626, "num_retrieved": 43, "inference_time": 8.868048772215843, "generate_results": ["def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")", "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if style == \"pep440\":\n        return render_pep440(pieces)\n    elif style == \"pep440-post\":\n        return render_pep440_post(pieces)\n    elif style == \"pep440-branch\":\n        return render_pep440_branch(pieces)\n    elif style == \"pep440-post-branch\":\n        return render_pep440_post_branch(pieces)\n    elif style == \"git-describe\":\n        return render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        return render_git_describe_long(pieces)\n    elif style == \"pep440-old\":\n        return render_pep440_old(pieces)\n    else:\n        raise ValueError(f\"Unknown style: {style}\")"]}
{"_id": "62b87d24d292efb640a5566d", "name": "plus_or_dot", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 16253, "num_retrieved": 58, "inference_time": 8.868048772215843, "generate_results": ["def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"\n    return \"\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"\n    return \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"\n    return \"\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"+\" not in pieces[\"closest-tag\"] else \".\"\n    else:\n        return \"+\"\n    return \".\"", "def plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            return \"+\" if \"plus\" not in pieces else \".\"\n    return \"+\" if not pieces[\"closest-tag\"] else \".\""]}
{"_id": "62b87d23d292efb640a5566b", "name": "run_command", "project": "eykd/prestoplot", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 10252, "num_retrieved": 29, "inference_time": 8.868048772215843, "generate_results": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        raise", "import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    # Construct the full command list\n    full_command = commands + args\n    \n    # Set up environment\n    env = env or os.environ.copy()\n    \n    # Set up cwd\n    if cwd is None:\n        cwd = os.getcwd()\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            full_command,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT if hide_stderr else subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose is True\n        if verbose and result.stdout:\n            print(result.stdout)\n            \n        # Return the result\n        return {\n            'returncode': result.returncode,\n            'stdout': result.stdout,\n            'stderr': result.stderr if not hide_stderr else None\n        }\n        \n    except subprocess.SubprocessError as e:\n        return {\n            'returncode': -1,\n            'stdout': '',\n            'stderr': str(e)\n        }", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(f\"Error running command: {e}\", file=sys.stderr)\n        raise", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \"\"\"\n    import subprocess\n    import sys\n    \n    # Prepare the command\n    cmd = commands + args\n    \n    # Set up environment\n    env = env or {}\n    \n    # Run the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        \n        # Print output if verbose and not hiding stderr\n        if verbose:\n            if result.stdout:\n                print(result.stdout)\n            if not hide_stderr and result.stderr:\n                print(result.stderr, file=sys.stderr)\n                \n        return result\n        \n    except subprocess.CalledProcessError as e:\n        if verbose:\n            print(e.stderr, file=sys.stderr)\n        raise\n    except Exception as e:\n        if verbose:\n            print(str(e), file=sys.stderr)\n        raise"]}
{"_id": "62b87d23d292efb640a55668", "name": "get_config", "project": "eykd/prestoplot", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 16221, "num_retrieved": 59, "inference_time": 8.868048772215843, "generate_results": ["def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg", "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return None\n    return cfg"]}
{"_id": "62b87d23d292efb640a55667", "name": "register_vcs_handler", "project": "eykd/prestoplot", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 12606, "num_retrieved": 37, "inference_time": 8.868048772215843, "generate_results": ["def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate", "def register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        if not hasattr(f, '_vcs_handlers'):\n            f._vcs_handlers = {}\n        f._vcs_handlers[vcs] = method\n        return f\n    return decorate"]}
{"_id": "62b45e945108cfac7f210a4a", "name": "validate_hierarchy", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 46559, "num_retrieved": 104, "inference_time": 8.868048772215843, "generate_results": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n    \"\"\"\n    Validate storage root hierarchy.\n\n    Returns:\n        num_objects - number of objects checked\n        good_objects - number of objects checked that were found to be valid\n    \"\"\"\n    num_objects = 0\n    good_objects = 0\n    \n    # Walk through the storage root to find all OCFL objects\n    for (dirpath, dirs, files) in ocfl_walk(self.obj_fs, dir='/'):\n        # Check if this directory is an OCFL object root\n        if self.is_object_root(dirpath):\n            num_objects += 1\n            \n            # Validate the object if requested\n            if validate_objects:\n                validator = Validator(\n                    show_warnings=show_warnings,\n                    show_errors=True,\n                    check_digests=check_digests,\n                    lax_digests=self.lax_digests\n                )\n                try:\n                    valid = validator.validate(dirpath)\n                    if valid:\n                        good_objects += 1\n                except Exception as e:\n                    self.log.error('E002', error=str(e))\n    \n    return num_objects, good_objects"]}
{"_id": "62b45e515108cfac7f210a3c", "name": "initialize", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 45289, "num_retrieved": 92, "inference_time": 8.868048772215843, "generate_results": ["def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = 'unknown'\n    if self.spec_version is None:\n        self.spec_version = '1.0'\n    if self.digest_algorithm is None:\n        self.digest_algorithm = 'sha512'\n    if self.content_directory is None:\n        self.content_directory = 'content'\n    \n    # Initialize the inventory and other required components\n    self.inventory = {}\n    self.all_versions = []\n    self.manifest_files = None\n    self.unnormalized_digests = None\n    self.head = 'UNKNOWN'\n    \n    # Set up the root inventory structure\n    self.inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        self.inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        self.inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            self.inventory['fixity'][fixity_type] = {}\n    \n    # Initialize the root inventory digest file\n    self.inventory_digest_files = {}", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = 'unknown'\n    if self.spec_version is None:\n        self.spec_version = '1.1'\n    if self.digest_algorithm is None:\n        self.digest_algorithm = 'sha512'\n    if self.content_directory is None:\n        self.content_directory = 'content'\n    \n    # Initialize the inventory and other required components\n    self.inventory = {}\n    self.manifest_files = None\n    self.unnormalized_digests = None\n    self.head = 'UNKNOWN'\n    self.all_versions = []\n    \n    # Set up the root inventory structure\n    self.inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        self.inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        self.inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            self.inventory['fixity'][fixity_type] = {}\n    \n    # Initialize the extensions directory if not already set\n    if not hasattr(self, 'extensions_dir'):\n        self.extensions_dir = 'extensions'\n    \n    # Initialize the object state\n    self.state = {}", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object with default values if not already set\n    if self.id is None:\n        self.id = 'unknown'\n    if self.spec_version is None:\n        self.spec_version = '1.0'\n    if self.digest_algorithm is None:\n        self.digest_algorithm = 'sha512'\n    if self.content_directory is None:\n        self.content_directory = 'content'\n    \n    # Initialize the inventory and other required components\n    self.inventory = {}\n    self.all_versions = []\n    self.manifest_files = None\n    self.unnormalized_digests = None\n    self.head = 'UNKNOWN'\n    \n    # Set up the root inventory structure\n    self.inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        self.inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        self.inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            self.inventory['fixity'][fixity_type] = {}\n    \n    # Initialize the extensions directory if not already set\n    if not hasattr(self, 'extensions_dir'):\n        self.extensions_dir = 'extensions'", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object's filesystem if not already set\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        # This will be handled by the object creation process\n        pass\n    \n    # Set up the initial inventory structure\n    self.id = None\n    self.spec_version = None\n    self.digest_algorithm = None\n    self.content_directory = None\n    self.inventory_digest_files = None\n    self.root_inv_validator = None\n    \n    # Initialize the object's state\n    self.obj_fs = self.obj_fs or open_fs('')  # Use empty path to create a new root\n    \n    # Create the initial inventory structure\n    self.start_inventory()\n    \n    # Set default values for the storage root\n    self.content_directory = 'content'\n    self.digest_algorithm = 'sha512'\n    self.spec_version = '1.0'\n    \n    # Initialize the inventory digest files\n    self.inventory_digest_files = {}\n    \n    # Initialize the root inventory validator\n    self.root_inv_validator = None", "def initialize(self):\n    \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n    # Initialize the object's filesystem if not already done\n    if self.obj_fs is None:\n        # Create a new filesystem for the storage root\n        self.obj_fs = open_fs(self.path, create=True)\n    \n    # Initialize the object's metadata\n    self.id = self.id or self._get_id_from_inventory()\n    \n    # Set up the initial inventory structure\n    self.spec_version = self.spec_version or '1.1'\n    \n    # Create the necessary directories\n    self.obj_fs.makedir('extensions')\n    self.obj_fs.makedir('content')\n    \n    # Initialize the inventory with default values\n    self.inventory = {\n        'id': self.id,\n        'type': 'https://ocfl.io/' + self.spec_version + '/spec/#inventory',\n        'digestAlgorithm': self.digest_algorithm,\n        'versions': {},\n        'manifest': {}\n    }\n    \n    # Add contentDirectory if not 'content'\n    if self.content_directory != 'content':\n        self.inventory['contentDirectory'] = self.content_directory\n    \n    # Add fixity section if requested\n    if self.fixity is not None and len(self.fixity) > 0:\n        self.inventory['fixity'] = {}\n        for fixity_type in self.fixity:\n            self.inventory['fixity'][fixity_type] = {}\n    \n    # Create the initial object declaration\n    self.object_declaration_object().write(pyfs=self.obj_fs, dir='')\n    \n    # Create the initial inventory file\n    inventory_path = 'inventory.json'\n    with self.obj_fs.open(inventory_path, 'w') as fh:\n        json.dump(self.inventory, fh, sort_keys=True, indent=2)\n    \n    # Create the initial inventory digest file\n    digest_path = inventory_path + '.' + self.digest_algorithm\n    digest = file_digest(inventory_path, digest_type=self.digest_algorithm, pyfs=self.obj_fs)\n    with self.obj_fs.open(digest_path, 'w') as fh:\n        fh.write(digest + ' ' + inventory_path)"]}
{"_id": "62b45e2eb89c9fd354170232", "name": "next_version", "project": "zimeon/ocfl-py", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 49588, "num_retrieved": 114, "inference_time": 8.868048772215843, "generate_results": ["def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if the version is zero-prefixed (has leading zeros)\n    # If it's zero-prefixed, we need to find the padding size\n    # by checking if there's a version with a different padding size\n    \n    # First, check if we have a zero-prefixed version\n    # We'll assume the pattern is consistent and just increment the number\n    next_num = version_num + 1\n    \n    # Format the next version with the same padding as the current version\n    # We need to determine the padding size from the current version\n    padding_size = len(str(version_num))\n    \n    # Format the next version with the same padding\n    next_version_str = f'v{next_num:0{padding_size}d}'\n    \n    # If the next version has a different padding size, we need to adjust\n    # For example, if we go from v1 to v10, the padding size changes\n    # But since we're just incrementing, we need to ensure the padding is consistent\n    \n    # Check if the next version number has a different number of digits\n    # If so, we need to adjust the padding size\n    if len(str(next_num)) != padding_size:\n        # The padding size has changed, so we need to use the new padding size\n        new_padding_size = len(str(next_num))\n        next_version_str = f'v{next_num:0{new_padding_size}d}'\n    \n    return next_version_str", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if version is zero-prefixed (has leading zeros)\n    # If it's zero-prefixed, we need to find the padding size\n    # by checking if there's a version with a different padding\n    # We'll assume the padding size is the maximum number of digits\n    # in any version directory name\n    \n    # Find the maximum padding size by checking all version directories\n    # We need to determine the pattern of zero-padding\n    \n    # If the version is not zero-prefixed, just increment\n    if version_num < 999999:\n        return 'v' + str(version_num + 1).zfill(len(str(version_num)))\n    \n    # If we reach here, the version is at the maximum limit\n    # This should not happen in normal use cases\n    raise ValueError(\"Version number exceeds maximum limit\")", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if we have a zero-prefixed version pattern\n    # Look for the maximum number of digits in the version number\n    max_digits = 0\n    for v in ['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10']:\n        if v in ['v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9']:\n            max_digits = max(max_digits, len(v))\n    \n    # If we have a zero-prefixed version, use the same pattern\n    # Otherwise, just increment the number\n    if version_num < 10:\n        return 'v' + str(version_num + 1).zfill(max_digits)\n    else:\n        # For versions with more than 1 digit, just increment\n        return 'v' + str(version_num + 1)", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if version is zero-prefixed (e.g., v001, v002)\n    if version_num < 10:\n        # For versions 1-9, we need to determine the padding\n        # If it's v1, v2, ..., v9, then next is v10\n        # If it's v01, v02, ..., v09, then next is v10\n        # But we need to find the pattern of zero-padding\n        # The pattern is determined by the maximum number of zeros\n        # We'll check if the version has leading zeros\n        if version_num == 1:\n            return 'v2'\n        else:\n            # For v2 to v9, next is v10\n            return 'v10'\n    \n    # For versions >= 10, we just increment\n    # But we need to maintain the zero-padding pattern\n    # The zero-padding is determined by the maximum number of zeros\n    # in the version string\n    max_zeros = len(version) - len(str(version_num))\n    \n    # Create the next version with the same zero-padding\n    next_num = version_num + 1\n    next_version_str = 'v' + str(next_num).zfill(max_zeros)\n    \n    return next_version_str", "def next_version(version):\n    \"\"\"\n    Next version identifier following existing pattern.\n    \n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    # Handle v1 case\n    if version == 'v1':\n        return 'v2'\n    \n    # Extract the version number\n    version_num = int(version[1:])\n    \n    # Check if it's zero-prefixed (e.g., v001, v002)\n    if version_num < 10:\n        # For versions 1-9, we need to determine if they are zero-prefixed\n        # If the version is v1, v2, ..., v9, then next is v10\n        if version_num == 1:\n            return 'v2'\n        else:\n            # For v2 to v9, next is v10\n            return 'v10'\n    \n    # For versions >= 10, we just increment\n    next_num = version_num + 1\n    \n    # Determine the padding size based on the original version\n    # If the original version was v10, v100, v1000, etc., we maintain the same padding\n    # We can infer padding from the original version string\n    padding_size = len(str(version_num)) if version_num >= 10 else 1\n    \n    # Format the next version with the same padding\n    return f'v{next_num:0{padding_size}d}'"]}
{"_id": "62b45e23e0d4551b0392c90a", "name": "validate_version_inventories", "project": "zimeon/ocfl-py", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 44686, "num_retrieved": 124, "inference_time": 8.868048772215843, "generate_results": ["def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root.\n\n    Each version directory should have an inventory file that represents the state up to that version.\n    Also records any content digests that differ from the root inventory for later validation.\n\n    Args:\n        version_dirs: List of version directory names in sequence (e.g., ['v1', 'v2', 'v3'])\n    \"\"\"\n    # Validate that version_dirs is a non-empty list\n    if not version_dirs:\n        self.error(\"E045\")\n        return\n    \n    # Initialize a set to track digests that differ from root\n    differing_digests = set()\n    \n    # Process each version directory in sequence\n    for vdir in version_dirs:\n        # Check if inventory file exists in the version directory\n        inventory_path = os.path.join(vdir, 'inventory.json')\n        if not os.path.exists(inventory_path):\n            self.error(\"E046\", version=vdir)\n            return\n            \n        # Load the inventory for this version\n        try:\n            with open(inventory_path, 'r') as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E047\", version=vdir, error=str(e))\n            return\n            \n        # Validate the inventory structure (basic checks)\n        if 'manifest' not in version_inventory:\n            self.error(\"E041a\", version=vdir)\n            return\n            \n        # Get the root inventory (first version)\n        if version_dirs.index(vdir) == 0:\n            # This is the first version, so we use it as the root\n            root_manifest = version_inventory['manifest']\n        else:\n            # For subsequent versions, check that all digests in their manifest\n            # are present in the root manifest or are new\n            version_manifest = version_inventory['manifest']\n            for digest in version_manifest:\n                if digest not in root_manifest:\n                    differing_digests.add(digest)\n                    \n    # Return the set of differing digests\n    return differing_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root inventory.\"\"\"\n    # Validate that version_dirs is a non-empty list\n    if not version_dirs:\n        self.error(\"E044\")\n        return\n    \n    # Initialize a set to track digests that are different from root inventory\n    different_digests = set()\n    \n    # Assume we have a root inventory with a manifest that we'll compare against\n    # For each version directory, validate that it has an inventory and check for different digests\n    for vdir in version_dirs:\n        # Check if the version directory exists and has an inventory\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E045\", version=vdir)\n            return\n            \n        # Read the inventory for this version\n        inventory = self.parse_inventory(vdir)\n        if inventory is None:\n            self.error(\"E046\", version=vdir)\n            return\n            \n        # Extract the manifest from the inventory\n        manifest = inventory.get('manifest', {})\n        \n        # Compare the manifest with the root inventory manifest to find different digests\n        # This is a simplified version - in a real implementation, we would compare with the root manifest\n        # For now, we'll just assume we have access to the root manifest\n        root_manifest = self.parse_inventory().get('manifest', {})\n        \n        # Find digests that are in the current version but not in the root\n        for digest, paths in manifest.items():\n            if digest not in root_manifest:\n                different_digests.add(digest)\n    \n    # Return the set of different digests\n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root.\n\n    Each version directory should have an inventory file that represents the state up to that version.\n    Also records any content digests that differ from the root inventory for later validation.\n\n    Args:\n        version_dirs: List of version directory names in sequence (e.g., ['v1', 'v2', 'v3'])\n    \"\"\"\n    # Initialize a set to track digests that differ from root\n    differing_digests = set()\n    \n    # Validate each version directory\n    for vdir in version_dirs:\n        # Check if inventory file exists in version directory\n        inventory_path = fs.path.join(vdir, 'inventory.json')\n        if not self.obj_fs.exists(inventory_path):\n            self.error(\"E043\", version=vdir)\n            continue\n            \n        # Load the inventory for this version\n        try:\n            with self.obj_fs.open(inventory_path) as fh:\n                version_inventory = json.load(fh)\n        except Exception as e:\n            self.error(\"E044\", version=vdir, error=str(e))\n            continue\n            \n        # Validate the inventory structure (basic checks)\n        if 'id' not in version_inventory:\n            self.error(\"E036a\", version=vdir)\n            continue\n            \n        if 'type' not in version_inventory:\n            self.error(\"E036b\", version=vdir)\n            continue\n            \n        if 'digestAlgorithm' not in version_inventory:\n            self.error(\"E036c\", version=vdir)\n            continue\n            \n        # Compare digests with root inventory to find differences\n        # This assumes we have access to root inventory and its manifest\n        if hasattr(self, 'root_manifest') and self.root_manifest:\n            # Extract digests from current version's manifest\n            current_manifest = version_inventory.get('manifest', {})\n            current_digests = set(current_manifest.keys())\n            \n            # Extract digests from root inventory's manifest\n            root_digests = set(self.root_manifest.keys())\n            \n            # Find digests that are in current version but not in root\n            new_digests = current_digests - root_digests\n            differing_digests.update(new_digests)\n            \n            # Find digests that are in root but not in current version\n            missing_digests = root_digests - current_digests\n            # These are not", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point and keep track of content digests different from root.\n\n    Each version SHOULD have an inventory up to that point.\n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in version sequence (1, 2, 3...).\n    \"\"\"\n    # Initialize a set to track digests that are different from the root inventory\n    different_digests = set()\n    \n    # Validate each version directory in sequence\n    for vdir in version_dirs:\n        # Check if the version directory exists\n        if not self.obj_fs.exists(vdir):\n            self.error(\"E043\", version=vdir)\n            continue\n            \n        # Read the inventory for this version\n        inventory = self.parse_inventory()\n        \n        # Validate the inventory structure\n        if 'manifest' not in inventory:\n            self.error(\"E041a\", version=vdir)\n            continue\n            \n        # Validate the manifest block\n        (manifest_files, manifest_files_correct_format, unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n        \n        # Check if the manifest files are valid\n        if not manifest_files:\n            self.error(\"E041c\", version=vdir)\n            continue\n            \n        # Validate the versions block\n        if 'versions' not in inventory:\n            self.error(\"E041b\", version=vdir)\n            continue\n            \n        # Extract the root inventory's manifest digests\n        root_manifest_digests = set(manifest_files.values())\n        \n        # Extract the current version's manifest digests\n        current_manifest_digests = set(manifest_files.values())\n        \n        # Find digests that are different from the root inventory\n        for digest in current_manifest_digests:\n            if digest not in root_manifest_digests:\n                different_digests.add(digest)\n    \n    # Return the set of different digests\n    return different_digests", "def validate_version_inventories(self, version_dirs):\n    \"\"\"Validate that each version has an inventory up to that point.\n    \n    Also keep a record of any content digests different from those in the root inventory\n    so that we can also check them when validating the content.\n    \n    version_dirs is an array of version directory names and is assumed to be in\n    version sequence (1, 2, 3...).\n    \"\"\"\n    # Validate that version_dirs is a non-empty list\n    if not version_dirs:\n        self.error(\"E008\")\n        return\n    \n    # Initialize a set to track digests that are different from the root inventory\n    different_digests = set()\n    \n    # Process each version directory in sequence\n    for vdir in version_dirs:\n        # Validate that the version directory exists and has a valid inventory\n        # This would typically involve checking the presence of the inventory file\n        # and validating its structure, but the specific implementation details\n        # are not provided in the context\n        \n        # For now, we'll assume the existence of the inventory is validated elsewhere\n        # and focus on tracking digests that differ from the root\n        \n        # If we had access to the root inventory and the current version's inventory,\n        # we would compare their manifest digests to identify differences\n        # Since this information isn't available in the context, we'll just\n        # record that we've processed the version\n        \n        # In a complete implementation, we would:\n        # 1. Parse the current version's inventory\n        # 2. Compare its manifest digests with the root inventory's manifest digests\n        # 3. Add any digests that are present in the current version but not in the root\n        #    to the different_digests set\n        \n        # Placeholder for actual digest comparison logic\n        pass\n    \n    # Return the set of digests that are different from the root inventory\n    return different_digests"]}
{"_id": "62b45e21e0d4551b0392c8ed", "name": "find_path_type", "project": "zimeon/ocfl-py", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 50776, "num_retrieved": 127, "inference_time": 8.868048772215843, "generate_results": ["def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs, limit=1)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* files found, likely a file\n        \n        # Check the filename of the first namaste file\n        namaste_file = namastes[0].filename\n        if namaste_file.startswith('0=ocfl_object_'):\n            # This is an OCFL object\n            return 'object'\n        elif namaste_file.startswith('0=ocfl_'):\n            # This is an OCFL storage root\n            return 'root'\n        else:\n            # Unknown namaste file type\n            return 'file'\n            \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* files found, likely a file\n        \n        # Check the filename of the first namaste file\n        first_namaste = namastes[0]\n        filename = first_namaste.filename\n        \n        # Check if it's a root (has a version directory structure)\n        if filename.startswith('0=ocfl_object_'):\n            # Check if there are any version directories\n            version_dirs = []\n            for entry in fs.scandir(''):\n                if entry.is_dir and re.match(r'v\\d+$', entry.name):\n                    version_dirs.append(entry.name)\n            \n            if len(version_dirs) > 0:\n                return 'root'\n            else:\n                return 'object'\n        \n        # If we get here, it's likely a file\n        return 'file'\n        \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* files found, likely a file\n        \n        # Check the filename of the first namaste file\n        first_namaste = namastes[0]\n        filename = first_namaste.filename\n        \n        # Check if it's a root (has a version directory or extensions)\n        if filename.startswith('0=ocfl_object_'):\n            # Check if there are version directories or extensions\n            version_dirs = []\n            extensions_dir = None\n            \n            # Look for version directories (v1, v2, etc.)\n            for entry in fs.scandir(''):\n                if entry.is_dir and re.match(r'v\\d+$', entry.name):\n                    version_dirs.append(entry.name)\n            \n            # Look for extensions directory\n            if fs.exists('extensions'):\n                extensions_dir = 'extensions'\n            \n            # If we have version directories, it's a root\n            if version_dirs:\n                return 'root'\n            \n            # If we have extensions directory, it's a root\n            if extensions_dir:\n                return 'root'\n            \n            # Otherwise, it's an object\n            return 'object'\n        \n        # If it's not a 0=ocfl_object_ file, it's likely a file\n        return 'file'\n        \n    except Exception as e:\n        return f\"error: {str(e)}\"", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs, limit=1)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* file found, likely a file\n        \n        # Check if the path is a directory\n        if not fs.exists(path):\n            return 'file'  # Path doesn't exist\n        \n        # Check if it's a directory with a 0=* file\n        if not fs.isdir(path):\n            return 'file'  # Path is not a directory\n        \n        # Check if the 0=* file is in the root directory\n        # If it's in the root, it's likely an OCFL Storage Root\n        # If it's in a subdirectory, it's likely an OCFL Object\n        namaste_filename = namastes[0].filename\n        if namaste_filename.startswith('0=ocfl_object_'):\n            # This is an object declaration\n            return 'object'\n        elif namaste_filename == '0=ocfl_object_1.0':\n            # This is a root declaration\n            return 'root'\n        else:\n            # Unknown 0=* file\n            return 'file'\n            \n    except Exception as e:\n        return 'error: ' + str(e)", "def find_path_type(path):\n    \"\"\"\n    Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        # Try to open the filesystem at the path\n        fs = open_fs(path)\n        \n        # Look for 0=* Namaste files\n        namastes = find_namastes(0, dir=path, pyfs=fs)\n        \n        if len(namastes) == 0:\n            return 'file'  # No 0=* files found, likely a file\n        \n        # Check the filename of the first namaste file\n        first_namaste = namastes[0]\n        filename = first_namaste.filename\n        \n        # Check if it's a root (has a version directory structure)\n        if filename.startswith('0=ocfl_object_'):\n            # Check if there are any version directories\n            version_dirs = []\n            for entry in fs.scandir(''):\n                if entry.is_dir and re.match(r'v\\d+$', entry.name):\n                    version_dirs.append(entry.name)\n            \n            if len(version_dirs) > 0:\n                return 'root'\n            else:\n                return 'object'\n        \n        # If we get here, it's not a standard OCFL object or root\n        return 'file'\n        \n    except Exception as e:\n        return f\"Error accessing path {path}: {str(e)}\""]}
{"_id": "62b45b396decaeff903e1001", "name": "amend_bzparams", "project": "mozilla/relman-auto-nag", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 162180, "num_retrieved": 500, "inference_time": 8.868048772215843, "generate_results": ["def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is not implemented in the provided context\n    # It appears to be a placeholder or a method that should be implemented\n    # based on the specific requirements of the class it belongs to\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is not implemented in the provided context\n    # It appears to be a placeholder or method that would be implemented\n    # based on specific requirements not provided in the context\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # No implementation provided in the context\n    # This function is likely intended to be implemented based on the context\n    # However, no specific implementation details are available in the provided context\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # No implementation provided in the context\n    # This function is likely intended to be implemented based on the context\n    # However, no specific implementation details were found in the provided context\n    pass", "def amend_bzparams(self, params, bug_ids):\n    \"\"\"\n    Amend the Bugzilla params\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the context, it appears to be a method that modifies Bugzilla query parameters\n    # However, no specific implementation details are provided in the retrieved context\n    # The function signature suggests it takes self, params, and bug_ids as arguments\n    # Without additional context or implementation details, we cannot provide a concrete implementation\n    pass"]}
{"_id": "62b4567ed7d32e5b55cc83d9", "name": "deep_merge_nodes", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 63340, "num_retrieved": 192, "inference_time": 8.868048772215843, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b4567ad7d32e5b55cc83af", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 62185, "num_retrieved": 198, "inference_time": 8.868048772215843, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this function would create and configure an ArgumentParser instance\n    # and parse the provided arguments accordingly.\n    parser = ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--log-file', help='Log file path')\n    parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Syslog verbosity level')\n    parser.add_argument('--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Log file verbosity level')\n    parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Monitoring verbosity level')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Example subparsers (actual implementation would have more detailed subparsers)\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Archive name')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    \n    return parser.parse_args(arguments)", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace object or similar structure\n    \n    # Since the context does not contain the actual implementation of parse_arguments,\n    # we return a dummy implementation that would be used in the actual codebase\n    \n    parser = ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--config-paths', nargs='+', help='Paths to configuration files')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Syslog verbosity level')\n    parser.add_argument('--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Log file verbosity level')\n    parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Monitoring verbosity level')\n    parser.add_argument('--log-file', help='Path to log file')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (actual implementation would have more specific subparsers)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories')\n    \n    prune_parser = subparsers.add_parser('prune', help='Prune old archives')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Keep weekly archives", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this function would create and configure an ArgumentParser instance\n    # and parse the provided arguments accordingly.\n    parser = ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--log-file', help='Log file path')\n    parser.add_argument('--syslog-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Syslog verbosity level')\n    parser.add_argument('--log-file-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Log file verbosity level')\n    parser.add_argument('--monitoring-verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Monitoring verbosity level')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Example subparser for 'create'\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source', help='Source directories')\n    create_parser.add_argument('--exclude', help='Exclude patterns')\n    \n    # Example subparser for 'prune'\n    prune_parser = subparsers.add_parser('prune', help='Prune old archives')\n    prune_parser.add_argument('--keep-daily', type=int, help='Keep daily archives')\n    prune_parser.add_argument('--keep-weekly', type=int, help='Keep weekly archives')\n    \n    # Example subparser for 'check", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace object or similar structure\n    parser = ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--repository', help='Repository path')\n    parser.add_argument('--archive', help='Archive name')\n    parser.add_argument('--prefix', help='Prefix for archives')\n    parser.add_argument('--paths', nargs='+', help='Paths to restore')\n    parser.add_argument('--source-repository', help='Source repository path')\n    parser.add_argument('--glob-archives', help='Glob pattern for archives')\n    parser.add_argument('--lock-wait', type=int, help='Lock wait time in seconds')\n    parser.add_argument('--umask', type=int, help='UMask for file creation')\n    parser.add_argument('--info', action='store_true', help='Show info output')\n    parser.add_argument('--debug', action='store_true', help='Enable debug output')\n    parser.add_argument('--json', action='store_true', help='Output in JSON format')\n    parser.add_argument('--progress', action='store_true', help='Show progress output')\n    parser.add_argument('--strip-components', type=int, help='Number of path components to strip')\n    parser.add_argument('--extract-to-stdout', action='store_true', help='Extract to stdout')\n    parser.add_argument('--extract-to', help='Destination path for extraction')\n    parser.add_argument('--create', action='store_true', help='Create a new archive')\n    parser.add_argument('--compact', action='store_true', help='Compact repository segments')\n    parser.add_argument('--pr", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the arguments\n    parser = ArgumentParser()\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--config', help='Configuration file path')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    parser.add_argument('--verbosity', choices=['error', 'warning', 'some', 'lots'], default='some', help='Verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    parser.add_argument('--help', action='help', help='Show this help message and exit')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Example subparsers (actual implementation would have more detailed subparsers)\n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Archive name')\n    list_parser.add_argument('--paths', nargs='+', help='Paths to list')\n    \n    return parser.parse_args(arguments)"]}
{"_id": "62b45679d7d32e5b55cc83a9", "name": "parser_flags", "project": "witten/atticmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 62481, "num_retrieved": 189, "inference_time": 8.868048772215843, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(sorted(flags))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            for option_string in action.option_strings:\n                flags.append(option_string)\n    return ' '.join(sorted(set(flags)))"]}
{"_id": "62b45665d7d32e5b55cc8365", "name": "parse_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 61648, "num_retrieved": 204, "inference_time": 8.868048772215843, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration overrides in the form section.option=value')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parsers for each subcommand\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Exclude patterns from files')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude patterns if present')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')\n    create_parser.add_argument('--patterns-from', nargs='+', help='Patterns file to use')\n    create", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration overrides in format section.option=value')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parsers for each subcommand\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Files to exclude from backup')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')\n    create_parser.add_argument('--patterns-from', nargs='+', help='File containing patterns to use')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parsers for each action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Exclude patterns from files')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files that have these patterns')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files that are not to be dumped')\n    create_parser.add_argument('--patterns-from', nargs='+', help='Patterns file to use')\n    \n    extract_parser = subparsers.add_parser('extract', help='Extract a backup')\n    extract_parser.add_argument('--repository', help='Repository path')\n    extract_parser.add_argument('--archive', help='Archive name", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration overrides in format section.option=value')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create archives')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name or pattern')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='File containing exclude patterns')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--keep-exclude-tags', action='store_true', help='Keep exclude tags')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--config-paths', nargs='+', default=['/etc/borgmatic/config.yaml'], help='Paths to configuration files')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration overrides in the form section.option=value')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    global_parser.add_argument('--verbosity', type=int, default=0, choices=[0, 1, 2, 3], help='Verbosity level (0=error, 1=warning, 2=info, 3=debug)')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different commands\n    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n    \n    # Create parser for create command\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--repository', help='Repository path')\n    create_parser.add_argument('--archive', help='Archive name')\n    create_parser.add_argument('--source-directories', nargs='+', help='Source directories to back up')\n    create_parser.add_argument('--exclude-from', nargs='+', help='Files to exclude from backup')\n    create_parser.add_argument('--exclude-if-present', nargs='+', help='Exclude files if present')\n    create_parser.add_argument('--exclude-caches', action='store_true', help='Exclude cache directories')\n    create_parser.add_argument('--exclude-nodump', action='store_true', help='Exclude files marked as nodump')\n    create_parser.add_argument('--keep-daily', type=int, help='Keep daily archives')\n    create"]}
{"_id": "62b45665d7d32e5b55cc8364", "name": "parse_subparser_arguments", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 61825, "num_retrieved": 205, "inference_time": 21.850803151726723, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Parse the remaining arguments with the current subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser with the remaining arguments\n        parser = subparser\n        \n        # Parse the arguments using the subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If the parser raises a SystemExit, it means the arguments are not valid for this subparser\n            # So we skip this subparser and continue with the next one\n            continue\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Parse the arguments using the subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser\n        parser = subparser\n        \n        # Parse the remaining arguments with the current subparser\n        try:\n            parsed_args = parser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from the remaining list\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('action', 0):]\n        except SystemExit:\n            # If the subparser raises SystemExit, it means the arguments were not valid\n            # We need to continue with the next subparser\n            continue\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Create a parser for the current subparser with the remaining arguments\n        parser = subparser\n        parsed_args = parser.parse_args(remaining_arguments, namespace=argparse.Namespace())\n        \n        # If the subparser successfully parsed arguments, add them to the result\n        if parsed_args:\n            arguments[subparser_name] = parsed_args\n            \n            # Remove the parsed arguments from the remaining arguments\n            remaining_arguments = [arg for arg in remaining_arguments if arg not in parsed_args.__dict__.keys()]\n    \n    # Return the parsed arguments and remaining arguments\n    return arguments, remaining_arguments"]}
{"_id": "62b45665d7d32e5b55cc8363", "name": "make_parsers", "project": "witten/atticmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 60165, "num_retrieved": 180, "inference_time": 21.850803151726723, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool for managing Borg backups',\n        prog='borgmatic'\n    )\n    \n    # Add global flags\n    global_flags = top_level_parser.add_argument_group('global')\n    global_flags.add_argument(\n        '--version', \n        action='version', \n        version='borgmatic {}'.format(pkg_resources.require('borgmatic')[0].version)\n    )\n    global_flags.add_argument(\n        '--bash-completion', \n        action='store_true', \n        help='Print bash completion script'\n    )\n    global_flags.add_argument(\n        '--config-paths', \n        nargs='+', \n        default=[], \n        help='Paths to configuration files'\n    )\n    global_flags.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode'\n    )\n    global_flags.add_argument(\n        '--verbosity', \n        choices=['error', 'warning', 'some', 'lots'], \n        default='some', \n        help='Verbosity level'\n    )\n    global_flags.add_argument(\n        '--no-color', \n        action='store_true', \n        help='Disable color output'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument(\n        '--source-directories', \n        nargs='+', \n        help='Source directories to back up'\n    )\n    create_parser.add_argument(\n        '--exclude-from', \n        nargs='+', \n        help='Exclude patterns from file'\n    )\n    create_parser.add_argument(\n        '--exclude-if-present', \n        nargs='+', \n        help='Exclude patterns if present'\n    )\n    create_parser.add_argument(\n        '--exclude-caches', \n        action='store_true', \n        help='Exclude cache directories'\n    )\n    create_parser.add_argument(\n        '--exclude-nodump', \n        action='store_true', \n        help='Exclude directories that are not to be dumped'\n    )\n    create_parser.add_argument(\n        '--patterns-from', \n        nargs='+', \n        help='Patterns file to use'", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='borgmatic - A tool for managing Borg backups',\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    # Add global arguments\n    global_args = top_level_parser.add_argument_group('global')\n    global_args.add_argument(\n        '--version', \n        action='version', \n        version='borgmatic {}'.format(pkg_resources.require('borgmatic')[0].version)\n    )\n    global_args.add_argument(\n        '--bash-completion', \n        action='store_true', \n        help='Print bash completion script'\n    )\n    global_args.add_argument(\n        '--config-paths', \n        nargs='+', \n        default=[], \n        help='Paths to configuration files'\n    )\n    global_args.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode'\n    )\n    global_args.add_argument(\n        '--verbosity', \n        choices=['error', 'warning', 'some', 'lots'], \n        default='some', \n        help='Verbosity level'\n    )\n    global_args.add_argument(\n        '--no-color', \n        action='store_true', \n        help='Disable color output'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument(\n        '--source-directories', \n        nargs='+', \n        help='Source directories to back up'\n    )\n    create_parser.add_argument(\n        '--exclude-from', \n        nargs='+', \n        help='Exclude patterns from files'\n    )\n    create_parser.add_argument(\n        '--exclude-if-present', \n        nargs='+', \n        help='Exclude files if present'\n    )\n    create_parser.add_argument(\n        '--exclude-caches', \n        action='store_true', \n        help='Exclude cache directories'\n    )\n    create_parser.add_argument(\n        '--exclude-nodump', \n        action='store_true', \n        help='Exclude files marked as nodump'\n    )\n    create_parser.add_argument(\n        '--keep-exclude-tags', \n        action='store_true', \n        help='", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='borgmatic - Backup and restore Borg repositories',\n        prog='borgmatic'\n    )\n    \n    # Add global flags\n    global_flags = top_level_parser.add_argument_group('global')\n    global_flags.add_argument(\n        '--version', \n        action='version', \n        version='borgmatic {}'.format(pkg_resources.require('borgmatic')[0].version)\n    )\n    global_flags.add_argument(\n        '--bash-completion', \n        action='store_true', \n        help='Print bash completion script'\n    )\n    global_flags.add_argument(\n        '--config-paths', \n        nargs='+', \n        default=[],\n        help='Paths to configuration files'\n    )\n    global_flags.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode'\n    )\n    global_flags.add_argument(\n        '--verbosity', \n        choices=['error', 'warning', 'some', 'lots'], \n        default='some', \n        help='Verbosity level'\n    )\n    global_flags.add_argument(\n        '--no-color', \n        action='store_true', \n        help='Disable color output'\n    )\n    \n    # Add subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Create parser for create action\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument(\n        '--source-directories', \n        nargs='+', \n        help='Source directories to back up'\n    )\n    create_parser.add_argument(\n        '--exclude-from', \n        nargs='+', \n        help='Exclude patterns from files'\n    )\n    create_parser.add_argument(\n        '--exclude-if-present', \n        nargs='+', \n        help='Exclude files if present'\n    )\n    create_parser.add_argument(\n        '--exclude-caches', \n        action='store_true', \n        help='Exclude cache directories'\n    )\n    create_parser.add_argument(\n        '--exclude-nodump', \n        action='store_true', \n        help='Exclude files that are not to be dumped'\n    )\n    create_parser.add_argument(\n        '--keep-exclude-tags', \n        action='store_true', \n        help='Keep exclude tags'", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(\n        description='Borgmatic - a tool for managing Borg repositories',\n        prog='borgmatic'\n    )\n    \n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    # Note: The actual implementation would depend on the specific actions defined in the codebase\n    # This is a simplified version based on typical structure\n    \n    # Example subparsers (these would be defined in the actual code)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('--source', help='Source directories to back up')\n    create_parser.add_argument('--archive', help='Archive name')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('--archive', help='Specific archive to list')\n    \n    info_parser = subparsers.add_parser('info', help='Get information about archives')\n    info_parser.add_argument('--archive', help='Specific archive to get info about')\n    \n    # Add more subparsers as needed\n    \n    return parser, subparsers", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    top_level_parser = argparse.ArgumentParser(\n        description='Borgmatic - A tool for managing Borg backups',\n        add_help=False,\n    )\n    \n    # Add global flags\n    global_flags = top_level_parser.add_argument_group('global')\n    global_flags.add_argument(\n        '--version', \n        action='version', \n        version='borgmatic %(prog)s ' + pkg_resources.require('borgmatic')[0].version\n    )\n    global_flags.add_argument(\n        '--bash-completion', \n        action='store_true', \n        help='Print bash completion script'\n    )\n    global_flags.add_argument(\n        '--config-paths', \n        nargs='+', \n        default=[], \n        help='Paths to configuration files'\n    )\n    global_flags.add_argument(\n        '--dry-run', \n        action='store_true', \n        help='Run in dry-run mode'\n    )\n    global_flags.add_argument(\n        '--verbosity', \n        choices=['error', 'warning', 'some', 'lots'], \n        default='some', \n        help='Verbosity level'\n    )\n    global_flags.add_argument(\n        '--no-color', \n        action='store_true', \n        help='Disable color output'\n    )\n    \n    # Create subparsers\n    subparsers = top_level_parser.add_subparsers(\n        dest='action',\n        help='Available actions'\n    )\n    \n    # Add subparsers for each action\n    # Note: The actual implementation would depend on the specific actions defined in the codebase\n    # For now, we'll create placeholder subparsers\n    \n    # Example subparsers (these would be expanded with actual action definitions)\n    create_parser = subparsers.add_parser('create', help='Create a backup')\n    create_parser.add_argument('source', help='Source directories to back up')\n    create_parser.add_argument('destination', help='Destination repository')\n    \n    list_parser = subparsers.add_parser('list', help='List archives')\n    list_parser.add_argument('repository', help='Repository path')\n    \n    info_parser = subparsers.add_parser('info', help='Get information about a repository')\n    info_parser.add_argument('repository', help='Repository path')\n    \n    return top_level_parser, subparsers"]}
{"_id": "62b438ba66fea644fe22cca2", "name": "deep_merge_nodes", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 90309, "num_retrieved": 286, "inference_time": 21.850803151726723, "generate_results": ["def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b", "def deep_merge_nodes(nodes):\n    \"\"\"\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one b"]}
{"_id": "62b438b666fea644fe22cc78", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 92602, "num_retrieved": 291, "inference_time": 21.850803151726723, "generate_results": ["def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace containing the parsed arguments\n    \n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be fully defined in the actual code)\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', default=DEFAULT_BORGMATIC_SOURCE_DIRECTORY, help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('destination_filename', help='Destination filename')\n    generate_parser.add_argument('source_filename', help='Source filename', nargs='?', default=None)\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file if it exists')\n    \n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('validate', help='Configuration to validate')\n    \n    # Add more subparsers as needed\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    # Create a namespace with the parsed arguments\n    namespace = argparse.Namespace()\n    \n    # Set the global arguments\n    namespace.global = argparse.Namespace(\n        version=False,\n        bash_completion=False,\n        fish_completion=False,\n        dry", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this function would use argparse to parse the command-line arguments\n    # and return a namespace object containing the parsed arguments\n    \n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    parser.add_argument('--dry-run', action='store_true', help='Run in dry-run mode')\n    parser.add_argument('--log-file', type=str, help='Log to file')\n    parser.add_argument('--log-file-format', type=str, help='Log file format')\n    parser.add_argument('--verbosity', type=int, choices=[0, 1, 2, 3, 4], default=0, help='Verbosity level')\n    parser.add_argument('--syslog-verbosity', type=int, choices=[0, 1, 2, 3, 4], default=0, help='Syslog verbosity level')\n    parser.add_argument('--log-file-verbosity', type=int, choices=[0, 1, 2, 3, 4], default=0, help='Log file verbosity level')\n    parser.add_argument('--monitoring-verbosity', type=int, choices=[0, 1, 2, 3, 4], default=0, help='Monitoring verbosity level')\n    parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('--repository', type=str, required=True, help='Repository path')\n    bootstrap_parser.add_argument('--archive', type=str, required=True, help='Archive name')\n    bootstrap_parser.add_argument('--destination', type=str, default='.', help='Destination directory')\n    bootstrap_parser.add_argument('--strip", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a Namespace object containing the parsed arguments\n    \n    import argparse\n    \n    # Create the parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', default=DEFAULT_BORGMATIC_SOURCE_DIRECTORY, help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', help='Destination path for configuration files')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('destination_filename', help='Destination filename for the configuration')\n    generate_parser.add_argument('source_filename', help='Source filename to merge with the generated configuration')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file if it exists')\n    \n    # Add validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('validate', help='Configuration to validate')\n    \n    # Add other actions as needed\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n        return args\n    except SystemExit:\n        # If parsing fails, re-raise the exception\n        raise", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this would use argparse to parse the command-line arguments\n    # and return a namespace with the parsed values\n    \n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--borgmatic-source-directory', default=DEFAULT_BORGMATIC_SOURCE_DIRECTORY, help='Borgmatic source directory')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a configuration file')\n    generate_parser.add_argument('destination_filename', help='Destination filename')\n    generate_parser.add_argument('source_filename', help='Source filename', nargs='?', default=None)\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file if it exists')\n    \n    # Add validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('validate', help='Configuration to validate')\n    \n    # Add other actions as needed\n    \n    # Parse the arguments\n    args = parser.parse_args(arguments)\n    \n    # Create a namespace with the parsed arguments\n    namespace = argparse.Namespace()\n    \n    # Set the global arguments\n    namespace.global = argparse.Namespace(\n        version=False,\n        bash_completion=False,\n        fish_completion=False,", "def parse_arguments(*arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as an ArgumentParser instance.\n    \"\"\"\n    # This is a placeholder implementation as the actual parsing logic is not provided in the context\n    # In a real implementation, this function would use argparse to parse the command-line arguments\n    # and return a namespace containing the parsed arguments.\n    \n    # Since the actual implementation details are not available in the provided context,\n    # we return a dummy implementation that would be replaced with the actual parsing logic.\n    \n    import argparse\n    \n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    parser.add_argument('--version', action='store_true', help='Show version information')\n    parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Example subparsers (these would be filled with actual action parsers)\n    # bootstrap, generate, validate, create, extract, etc.\n    \n    # Parse the arguments\n    try:\n        args = parser.parse_args(arguments)\n    except SystemExit:\n        # If parsing fails, re-raise to allow proper error handling\n        raise\n        \n    return args"]}
{"_id": "62b438b666fea644fe22cc72", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 91951, "num_retrieved": 284, "inference_time": 21.850803151726723, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            flags.append(f\"--{action.option_strings[0].lstrip('-')}\")\n    return \" \".join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            flags.append(f\"--{action.option_strings[0].lstrip('-')}\")\n    return \" \".join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62b438b566fea644fe22cc70", "name": "bash_completion", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 91946, "num_retrieved": 290, "inference_time": 21.850803151726723, "generate_results": ["def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _init_completion() {{\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _borgmatic() {{\n                local cur prev words cword\n                COMPREPLY=()\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n                words=(\"${{COMP_WORDS[@]}}\")\n                cword=${{COMP_CWORD}}\n\n                # Check if any of the given arguments are the last on the command line before the cursor\n                if [[ ${cword} -lt 2 ]]; then\n                    return 1\n                fi\n\n                for arg in \"${{words[@]}}\"; do\n                    if [[ \"${arg}\" == \"${{words[-1]}\" }\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_completion() {{\n                local cur\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                COMPREPLY=()\n                local action_parser_condition=\"not __fish_seen_subcommand_from {all_action_parsers}\"\n                local exact_option_condition=\"not __borgmatic_current_arg {' '.join(exact_option_args)}\"\n                complete -o default -F _complete_borgmatic -W \"{all_action_parsers}\" borgmatic\n            }}\n\n            _global_flags_completion() {{\n                local cur\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                COMPREPLY=()\n                local exact_option_condition=\"", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _init_completion() {{\n                if [[ $(basename \"$0\") == \"borgmatic\" ]]; then\n                    COMPREPLY=()\n                    local cur prev words cword\n                    _init_completion\n                fi\n            }}\n\n            _init_completion() {{\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _init_completion() {{\n                local cur prev words cword\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _init_completion() {{\n                local cur prev words cword\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _init_completion() {{\n                local cur prev words cword\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _init_completion() {{\n                local cur prev words cword\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _init_completion() {{\n                local cur prev words cword\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _init_completion() {{\n                local cur prev words cword\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _init_completion() {{\n                local cur prev words cword\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _init_completion() {{\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _borgmatic() {{\n                local cur prev words cword\n                COMPREPLY=()\n                cur=\"${{COMP_WORDS[COMP_CWORD]}}\"\n                prev=\"${{COMP_WORDS[COMP_CWORD-1]}}\"\n                words=(\"${{COMP_WORDS[@]}}\")\n                cword=${{COMP_CWORD}}\n\n                # Check if the current word is a subcommand\n                if [[ \"${{cur}}\" == -* ]]; then\n                    # Handle options\n                    local options\n                    options=$(compgen -A variable | grep -E \"^_borgmatic_\")\n                    COMPREPLY=($(compgen -W \"${{options}}\" -- \"${{cur}}\"))\n                else\n                    # Handle subcommands\n                    local subcommands\n                    subcommands=$(compgen -W \"${{all_action_parsers}}\" -- \"${{cur}}\")\n                    COMPREPLY=($(compgen -W \"${{subcommands}}\" -- \"${{cur}}\"))\n                fi\n            }}\n\n            _init_completion\n            _borgmatic\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''complete -F _borgmatic -o bashdefault -o default -o nospace borgmatic -n \"$action_parser_condition\" -n \"$exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _init_completion() {{\n                if [[ -z ${COMP_WORDS[1]} ]]; then\n                    return 0\n                fi\n                COMPREPLY=()\n                local cur prev words cword\n                cword=0\n                cur=\"${COMP_WORDS[COMP_CWORD]}\"\n                prev=\"${COMP_WORDS[COMP_CWORD-1]}\"\n                words=(\"${COMP_WORDS[@]}\")\n                cword=$((${#words[@]}-1))\n                COMPREPLY=($(compgen -W \"${all_action_parsers}\" -- \"${cur}\"))\n                if [[ ${#COMPREPLY[@]} -eq 0 ]]; then\n                    COMPREPLY=($(compgen -W \"${exact_option_args}\" -- \"${cur}\"))\n                fi\n            }}\n            _init_completion\n            complete -F _init_completion borgmatic\n            '''\n        )\n        + ('\\n# action_parser completions',)\n        + tuple(\n            f'''complete -c borgmatic -f -n \"$action_parser_condition\" -n \"$exact_option_condition\" -a '{action_name}' -d {shlex.quote(action_parser.description)}'''\n            for action_name, action_parser in action_parsers.choices.items()\n        )\n        + ('\\n# global flags',)\n        + tuple(\n            # -n is checked in order, so put faster / more likely to be true checks first\n            f'''complete -c borgmatic -f -n \"$exact_option_condition\" -a '{' '.join(action.option_strings)}' -d {", "def bash_completion():\n    '''\n    Return a bash completion script for the borgmatic command. Produce this by introspecting\n    borgmatic's command-line argument parsers.\n    '''\n    (\n        unused_global_parser,\n        action_parsers,\n        global_plus_action_parser,\n    ) = borgmatic.commands.arguments.make_parsers()\n\n    all_action_parsers = ' '.join(action for action in action_parsers.choices.keys())\n\n    exact_option_args = tuple(\n        ' '.join(action.option_strings)\n        for action_parser in action_parsers.choices.values()\n        for action in action_parser._actions\n        if has_exact_options(action)\n    ) + tuple(\n        ' '.join(action.option_strings)\n        for action in global_plus_action_parser._actions\n        if len(action.option_strings) > 0\n        if has_exact_options(action)\n    )\n\n    # Avert your eyes.\n    return '\\n'.join(\n        dedent_strip_as_tuple(\n            f'''\n            _init_completion() {{\n                if [[ $(basename \"$0\") == \"borgmatic\" ]]; then\n                    COMPREPLY=()\n                    local cur prev words cword\n                    _init_completion\n                fi\n            }}\n\n            _init_completion() {{\n                COMPREPLY=()\n                local cur prev words cword\n                _init_completion\n            }}\n\n            _borgmatic_check_version() {{\n                local this_filename=$(status current-filename)\n                local installed_script=$(borgmatic --bash-completion 2> /dev/null)\n                if [[ \"$this_filename\" != \"$installed_script\" ]] && [[ \"$installed_script\" != \"\" ]]; then\n                    echo \"{borgmatic.commands.completion.actions.upgrade_message(\n                        'bash',\n                        'borgmatic --bash-completion | sudo tee $this_filename',\n                        '$this_filename',\n                    )}\"\n                fi\n            }}\n\n            _borgmatic_current_arg() {{\n                local all_args=$(commandline -poc)\n                if [[ $(count $all_args) -lt 2 ]]; then\n                    return 1\n                fi\n                for arg in $argv; do\n                    if [[ \"$arg\" = \"$all_args[-1]\" ]]; then\n                        return 0\n                    fi\n                done\n                return 1\n            }}\n\n            _action_parser_condition=\"not __fish_seen_subcommand_from {all_action_parsers}\"\n            _exact_option_condition=\"not __borgmatic_current_arg {' '.join(exact_option_args)}\"\n            '''\n        )\n        + ('\\n# action"]}
{"_id": "62b438a266fea644fe22cc2e", "name": "parse_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 92784, "num_retrieved": 294, "inference_time": 21.850803151726723, "generate_results": ["def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level (0-4)')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, help='Format for log file')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap subparser\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', type=str, default='/etc/borgmatic', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, default=0, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate subparser", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--config-paths', nargs='+', help='Paths to configuration files')\n    global_parser.add_argument('--resolve-env', action='store_true', help='Resolve environment variables in configuration')\n    global_parser.add_argument('--overrides', nargs='+', help='Configuration file overrides')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a sample configuration')\n    generate_parser.add_argument('source', help='Source configuration file')\n    generate_parser.add_argument('destination', help='Destination configuration file')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file')\n    \n    # Add validate action\n    validate", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the argument parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--config-paths', nargs='+', help='Paths to configuration files')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action\n    generate_parser = subparsers.add_parser('generate', help='Generate a sample configuration')\n    generate_parser.add_argument('source', help='Source configuration file')\n    generate_parser.add_argument('destination', help='Destination configuration file')\n    generate_parser.add_argument('--overwrite', action='store_true', help='Overwrite destination file')\n    \n    # Add validate action\n    validate_parser = subparsers.add_parser('validate', help='Validate configuration files')\n    validate_parser.add_argument('config', help='Configuration file to validate')\n    validate_parser.add_argument('--show', action", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level (0=error, 1=answer, 2=some, 3=lots)')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, help='Format for log file')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', type=str, default='.', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, default=0, help='Number of path components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='", "def parse_arguments(*unparsed_arguments):\n    \"\"\"\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    \"\"\"\n    import argparse\n    import sys\n    \n    # Create the main parser\n    parser = argparse.ArgumentParser()\n    \n    # Add global arguments\n    global_parser = parser.add_argument_group('global')\n    global_parser.add_argument('--version', action='version', version='borgmatic 1.0')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Print bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Print fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--verbosity', type=int, default=0, help='Set verbosity level')\n    global_parser.add_argument('--syslog-verbosity', type=int, default=0, help='Set syslog verbosity level')\n    global_parser.add_argument('--log-file-verbosity', type=int, default=0, help='Set log file verbosity level')\n    global_parser.add_argument('--monitoring-verbosity', type=int, default=0, help='Set monitoring verbosity level')\n    global_parser.add_argument('--log-file', type=str, help='Path to log file')\n    global_parser.add_argument('--log-file-format', type=str, help='Format for log file')\n    \n    # Add subparsers for different actions\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add bootstrap action\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', type=str, default='/etc/borgmatic', help='Destination directory')\n    bootstrap_parser.add_argument('--strip-components', type=int, default=0, help='Number of components to strip')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress')\n    \n    # Add generate action\n    generate_parser = subparsers"]}
{"_id": "62b438a266fea644fe22cc2d", "name": "parse_subparser_arguments", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 92979, "num_retrieved": 295, "inference_time": 21.850803151726723, "generate_results": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, skip this subparser\n            continue\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, skip this subparser and continue with the next\n            continue\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = unparsed_arguments[:]\n    \n    for subparser_name, subparser in subparsers.items():\n        # Parse arguments for this subparser\n        try:\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the parsed arguments from remaining_arguments\n            remaining_arguments = remaining_arguments[parsed_args.__dict__.get('index', 0):]\n        except SystemExit:\n            # If parsing fails, continue to next subparser\n            pass\n    \n    return arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            # Create a parser for the subparser and parse the remaining arguments\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed from the remaining list\n            remaining_arguments = remaining_arguments[remaining_arguments.index(parsed_args) + 1:]\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            continue\n        except Exception:\n            # If parsing fails for any other reason, continue to the next subparser\n            continue\n    \n    return (arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n    \"\"\"\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n    \"\"\"\n    arguments = {}\n    remaining_arguments = list(unparsed_arguments)\n    \n    # Process each subparser in the order they appear in the subparsers dictionary\n    for subparser_name, subparser in subparsers.items():\n        # Try to parse arguments for this subparser\n        try:\n            # Create a parser for the subparser and parse the remaining arguments\n            parsed_args = subparser.parse_args(remaining_arguments)\n            arguments[subparser_name] = parsed_args\n            # Remove the arguments that were parsed\n            remaining_arguments = []\n            break\n        except SystemExit:\n            # If parsing fails, continue to the next subparser\n            pass\n    \n    # If no subparser was able to parse arguments, return empty arguments and the original arguments\n    if not arguments:\n        arguments = {}\n    \n    return (arguments, remaining_arguments)"]}
{"_id": "62b438a266fea644fe22cc2c", "name": "make_parsers", "project": "witten/borgmatic", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 92623, "num_retrieved": 291, "inference_time": 21.850803151726723, "generate_results": ["def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    \n    # Create the top-level parser\n    global_parser = argparse.ArgumentParser(add_help=False)\n    global_parser.add_argument('--version', action='store_true', help='Show version information')\n    global_parser.add_argument('--bash-completion', action='store_true', help='Generate bash completion script')\n    global_parser.add_argument('--fish-completion', action='store_true', help='Generate fish completion script')\n    global_parser.add_argument('--dry-run', action='store_true', help='Run in dry run mode')\n    global_parser.add_argument('--log-json', action='store_true', help='Output logs in JSON format')\n    global_parser.add_argument('--verbosity', type=int, choices=[0, 1, 2, 3, 4], default=0, help='Set verbosity level (0=error, 1=answer, 2=some, 3=lots)')\n    global_parser.add_argument('--no-color', action='store_true', help='Disable color output')\n    global_parser.add_argument('--config-paths', nargs='+', default=[], help='Paths to configuration files')\n    global_parser.add_argument('--config-overrides', nargs='+', default=[], help='Configuration file overrides in the form option.suboption=value')\n    \n    # Create subparsers for different actions\n    parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(dest='action', help='Available actions')\n    \n    # Add subparsers for each action\n    # bootstrap\n    bootstrap_parser = subparsers.add_parser('bootstrap', help='Bootstrap configuration from a repository')\n    bootstrap_parser.add_argument('repository', help='Repository path')\n    bootstrap_parser.add_argument('archive', help='Archive name')\n    bootstrap_parser.add_argument('--destination', help='Destination path for configuration files')\n    bootstrap_parser.add_argument('--strip-components', type=int, default=0, help='Number of path components to strip from the destination path')\n    bootstrap_parser.add_argument('--progress', action='store_true', help='Show progress during bootstrap')\n    \n    # generate\n    generate_parser = subparsers.add_parser('generate', help='Generate a sample configuration file')\n    generate_parser.add_argument('destination', help='Destination path for the generated configuration')\n    generate_parser.add_argument('--source', help='Source configuration file to merge into the generated configuration')\n    generate_parser.add_argument('--overwrite', action='", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    # Create the top-level parser\n    parser = argparse.ArgumentParser(\n        description='borgmatic - A Borg backup tool with configuration and automation',\n        prog='borgmatic',\n        add_help=False,\n    )\n    \n    # Add global arguments\n    global_parser = parser\n    \n    # Create subparsers for actions\n    subparsers = parser.add_subparsers(\n        dest='action',\n        help='Available actions',\n        required=True,\n    )\n    \n    # Create action parsers from the arguments module\n    action_parsers = {}\n    \n    # Add each action parser\n    for action_name, action_parser in borgmatic.commands.arguments.ACTIONS.items():\n        action_parser = argparse.ArgumentParser(\n            prog=f'borgmatic {action_name}',\n            description=action_parser.description,\n            add_help=False,\n        )\n        \n        # Add action-specific arguments\n        for action in action_parser._actions:\n            if action.option_strings:\n                action_parser.add_argument(*action.option_strings, **action.__dict__)\n        \n        action_parsers[action_name] = action_parser\n    \n    # Create a combined parser that includes both global and action-specific arguments\n    global_plus_action_parser = argparse.ArgumentParser(\n        prog='borgmatic',\n        add_help=False,\n    )\n    \n    # Add global arguments to the combined parser\n    for action in global_parser._actions:\n        if action.option_strings:\n            global_plus_action_parser.add_argument(*action.option_strings, **action.__dict__)\n    \n    # Add action-specific arguments to the combined parser\n    for action_name, action_parser in action_parsers.items():\n        for action in action_parser._actions:\n            if action.option_strings:\n                global_plus_action_parser.add_argument(*action.option_strings, **action.__dict__)\n    \n    return (global_parser, action_parsers, global_plus_action_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would require the full context of the borgmatic command-line argument parsing\n    # which is not provided in the retrieved context.\n    # The function is expected to return a tuple containing:\n    # - unused_global_parser: the top-level parser (not used)\n    # - action_parsers: a mapping of action names to their respective parsers\n    # - global_plus_action_parser: a combined parser for global and action-specific arguments\n    \n    # Since the full implementation is not available in the context,\n    # we return a dummy tuple with empty parsers.\n    unused_global_parser = argparse.ArgumentParser()\n    action_parsers = {}\n    global_plus_action_parser = argparse.ArgumentParser()\n    \n    return (unused_global_parser, action_parsers, global_plus_action_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would require the full context of the borgmatic command-line argument parsing\n    # which is not provided in the retrieved context.\n    # The function is expected to return a tuple containing:\n    # - unused_global_parser: the top-level parser (not used)\n    # - action_parsers: a mapping of action names to their respective parsers\n    # - global_plus_action_parser: a combined parser for global arguments and action-specific arguments\n    \n    # Since the actual implementation is not provided in the context,\n    # we return a dummy tuple with empty parsers.\n    unused_global_parser = argparse.ArgumentParser()\n    action_parsers = {}\n    global_plus_action_parser = argparse.ArgumentParser()\n    \n    return (unused_global_parser, action_parsers, global_plus_action_parser)", "def make_parsers():\n    \"\"\"\n    Build a top-level parser and its subparsers and return them as a tuple.\n    \"\"\"\n    import argparse\n    import borgmatic.commands.arguments\n    \n    # Create the top-level parser\n    global_parser = argparse.ArgumentParser(\n        prog='borgmatic',\n        description='A tool for managing Borg repositories with a focus on simplicity and ease of use.',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    \n    # Add global arguments\n    global_parser.add_argument(\n        '--version',\n        action='version',\n        version='borgmatic, version ' + importlib_metadata.version('borgmatic'),\n    )\n    \n    global_parser.add_argument(\n        '--bash-completion',\n        action='store_true',\n        help='Print bash completion script and exit.',\n    )\n    \n    global_parser.add_argument(\n        '--fish-completion',\n        action='store_true',\n        help='Print fish completion script and exit.',\n    )\n    \n    global_parser.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Run in dry-run mode (no actual operations performed).',\n    )\n    \n    global_parser.add_argument(\n        '--log-json',\n        action='store_true',\n        help='Output log messages in JSON format.',\n    )\n    \n    global_parser.add_argument(\n        '--verbosity',\n        choices=['disabled', 'error', 'answer', 'some', 'lots'],\n        default='some',\n        help='Set the verbosity level of the output.',\n    )\n    \n    global_parser.add_argument(\n        '--config-paths',\n        nargs='+',\n        default=[],\n        help='Paths to configuration files to load (can be directories or files).',\n    )\n    \n    # Create subparsers for actions\n    subparsers = global_parser.add_subparsers(\n        dest='action',\n        help='Available actions',\n        required=True,\n    )\n    \n    # Add subparsers for each action\n    action_parsers = {}\n    \n    # Add each action parser here\n    # This is a simplified version - in reality, each action would have its own parser\n    # with specific arguments\n    \n    # Example: create parser for 'create' action\n    create_parser = subparsers.add_parser('create', help='Create a backup archive')\n    create_parser.add_argument(\n        '--repository',\n        help='Repository path to use for the backup.',\n    )\n    create_parser.add_argument(\n        '--archive',\n        help='Name of the archive to create.',\n    )\n    create_parser"]}
{"_id": "62ece4982e6aefcf4aabbd5f", "name": "paging", "project": "infobloxopen/infoblox-client", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 198337, "num_retrieved": 377, "inference_time": 21.850803151726723, "generate_results": ["def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    \n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n    \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n    \n    # Initialize the result list\n    result = []\n    \n    # Process the response in pages\n    for item in response:\n        result.append(item)\n        if len(result) >= max_results:\n            yield result\n            result = []\n    \n    # Yield any remaining items\n    if result:\n        yield result", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Initialize the result list\n    result = []\n    \n    # Process the response in pages\n    for item in response:\n        result.append(item)\n        if len(result) >= max_results:\n            yield result\n            result = []\n    \n    # Yield any remaining items\n    if result:\n        yield result", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Initialize the result list\n    result = []\n    \n    # Process the response in pages\n    for item in response:\n        result.append(item)\n        # If we have reached the max_results, yield the current page and reset\n        if len(result) >= max_results:\n            yield result\n            result = []\n    \n    # Yield the last page if there are any remaining items\n    if result:\n        yield result", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Initialize the result list\n    result = []\n    \n    # Process the response in pages\n    for item in response:\n        result.append(item)\n        if len(result) >= max_results:\n            yield result\n            result = []\n    \n    # Yield the last page if there are remaining items\n    if result:\n        yield result", "def paging(response, max_results):\n    \"\"\"\n    Returns WAPI response page by page\n\n    Args:\n        response (list): WAPI response.\n        max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n    \"\"\"\n    if not response:\n        return\n    \n    # If max_results is negative, set it to 1000\n    if max_results < 0:\n        max_results = 1000\n        \n    # If max_results is 0, return empty generator\n    if max_results == 0:\n        return\n        \n    # Initialize the result list\n    result = []\n    \n    # Process the response in pages\n    for item in response:\n        result.append(item)\n        if len(result) >= max_results:\n            yield result\n            result = []\n    \n    # Yield the last page if it exists\n    if result:\n        yield result"]}
{"_id": "62ece4982e6aefcf4aabbd60", "name": "size_to_bytes", "project": "SoftwareHeritage/swh-lister", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 20441, "num_retrieved": 146, "inference_time": 21.850803151726723, "generate_results": ["def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size has a unit (K, M, G, etc.)\n    unit = size[-1].upper() if len(size) > 1 else None\n    \n    # Extract the numeric part\n    try:\n        value = int(size[:-1]) if unit else int(size)\n    except ValueError:\n        return 0\n    \n    # Convert to bytes based on unit\n    if unit == 'K':\n        return value * 1024\n    elif unit == 'M':\n        return value * 1024 * 1024\n    elif unit == 'G':\n        return value * 1024 * 1024 * 1024\n    else:\n        return value", "def size_to_bytes(size: str) -> int:\n    \"\"\"\n    Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000\n    \"\"\"\n    if not size:\n        return 0\n    \n    # Remove any whitespace\n    size = size.strip()\n    \n    # Check if size contains a unit\n    if 'K' in size:\n        # Extract the number part\n        num_part = size.replace('K', '').strip()\n        return int(num_part) * 1024\n    elif 'M' in size:\n        # Extract the number part\n        num_part = size.replace('M', '').strip()\n        return int(num_part) * 1024 * 1024\n    elif 'G' in size:\n        # Extract the number part\n        num_part = size.replace('G', '').strip()\n        return int(num_part) * 1024 * 1024 * 1024\n    else:\n        # No unit, assume it's in bytes\n        return int(size)"]}
{"_id": "62ece4982e6aefcf4aabbd61", "name": "_dictsum", "project": "ossobv/planb", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 76049, "num_retrieved": 251, "inference_time": 21.850803151726723, "generate_results": ["def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n    >>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n    {'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for key, value in d.items():\n            result[key] = result.get(key, 0) + value\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result", "def _dictsum(dicts):\n    \"\"\"\n    Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}\n    \"\"\"\n    result = {}\n    for d in dicts:\n        for k, v in d.items():\n            result[k] = result.get(k, 0) + v\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd62", "name": "_replace_url_args", "project": "SEED-platform/py-seed", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 23160, "num_retrieved": 60, "inference_time": 21.850803151726723, "generate_results": ["def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url", "def _replace_url_args(url, url_args):\n    \"\"\"\n    Replace any custom string URL items with values in args\n    \"\"\"\n    if url_args is None:\n        return url\n    \n    for key, value in url_args.items():\n        placeholder = f\"{{{key}}}\"\n        url = url.replace(placeholder, str(value))\n    \n    return url"]}
{"_id": "62ece4982e6aefcf4aabbd63", "name": "is_none_string", "project": "openstack/cinder", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 211139, "num_retrieved": 461, "inference_time": 21.850803151726723, "generate_results": ["def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    \n    # Check for common representations of None in strings\n    return val.lower() in ['none', 'null', 'nil', 'n/a', 'na', 'null', 'none', '']", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        return val.lower() in ('none', 'null', 'nil', 'n/a', 'na', 'null')\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        # Check for common representations of None in strings\n        return val.lower() in ['none', 'null', 'nil', 'n/a', 'na', 'null', 'none']\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if isinstance(val, str):\n        # Check for common representations of None\n        return val.lower() in ['none', 'null', 'nil', 'n/a', 'na', 'null', 'none']\n    return False", "def is_none_string(val: any) -> bool:\n    \"\"\"\n    Check if a string represents a None value.\n    \"\"\"\n    if not isinstance(val, str):\n        return False\n    \n    # Check for common representations of None in strings\n    return val.lower() in ['none', 'null', 'nil', 'n/a', 'na', 'null', 'none', '']"]}
{"_id": "62ece4982e6aefcf4aabbd65", "name": "parser_flags", "project": "witten/borgmatic", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 91951, "num_retrieved": 284, "inference_time": 21.850803151726723, "generate_results": ["def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            flags.append(f\"--{action.option_strings[0].lstrip('-')}\")\n    return \" \".join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            if action.option_strings:\n                flags.extend(action.option_strings)\n    return ' '.join(flags)", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if isinstance(action, argparse._StoreAction):\n            flags.append(f\"--{action.option_strings[0].lstrip('-')}\")\n    return \" \".join(sorted(set(flags)))", "def parser_flags(parser):\n    \"\"\"\n    Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n    \"\"\"\n    flags = []\n    for action in parser._actions:\n        if action.option_strings:\n            flags.extend(action.option_strings)\n    return ' '.join(flags)"]}
{"_id": "62ece4982e6aefcf4aabbd66", "name": "was_processed", "project": "burgerbecky/makeprojects", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 164776, "num_retrieved": 500, "inference_time": 21.850803151726723, "generate_results": ["def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Path '{path_name}' has already been processed.\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False", "def was_processed(processed, path_name, verbose):\n    \"\"\"\n    Check if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return True. Otherwise, add the entry to the set and return\n    False to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n    \"\"\"\n    # Convert to absolute path\n    abs_path = os.path.abspath(path_name)\n    \n    # Check if already processed\n    if abs_path in processed:\n        if verbose:\n            print(f\"Skipping already processed path: {abs_path}\")\n        return True\n    \n    # Add to processed set\n    processed.add(abs_path)\n    return False"]}
{"_id": "62ece4982e6aefcf4aabbd67", "name": "vertex3tuple", "project": "santoshphilip/eppy", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 84794, "num_retrieved": 407, "inference_time": 14.270571008324623, "generate_results": ["def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of (previous, current, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple: (previous, current, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple of (previous, current, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n    \n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple: (previous, current, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result", "def vertex3tuple(vertices):\n    \"\"\"\n    return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\n    polygon with vertices ABCD\n    Will return\n    DAB, ABC, BCD, CDA -> returns 3tuples\n    #A    B    C    D  -> of vertices\n    \"\"\"\n    if len(vertices) < 3:\n        return []\n    \n    result = []\n    n = len(vertices)\n    \n    for i in range(n):\n        # Get the current vertex and its two neighbors\n        prev_idx = (i - 1) % n\n        curr_idx = i\n        next_idx = (i + 1) % n\n        \n        # Create a 3-tuple: (previous, current, next)\n        triplet = (vertices[prev_idx], vertices[curr_idx], vertices[next_idx])\n        result.append(triplet)\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd68", "name": "int_to_string", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 1167, "num_retrieved": 5, "inference_time": 14.270571008324623, "generate_results": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    \n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n    \"\"\"\n    Convert a number to a string, using the given alphabet.\n\n    The output has the most significant digit first.\n    \"\"\"\n    if number < 0:\n        raise ValueError(\"Number must be non-negative\")\n    \n    if number == 0:\n        return alphabet[0] if alphabet else \"\"\n    \n    result = \"\"\n    base = len(alphabet)\n    \n    while number > 0:\n        remainder = number % base\n        result = alphabet[remainder] + result\n        number //= base\n    \n    if padding is not None and len(result) < padding:\n        result = alphabet[0] * (padding - len(result)) + result\n    \n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd69", "name": "_replace_register", "project": "openstack/neutron-lib", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 126167, "num_retrieved": 500, "inference_time": 14.270571008324623, "generate_results": ["def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number", "def _replace_register(flow_params, register_number, register_value):\n    \"\"\"\n    Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number\n    \"\"\"\n    for flow in flow_params:\n        if register_value in flow:\n            flow[register_value] = register_number"]}
{"_id": "62ece4982e6aefcf4aabbd6a", "name": "replace_dots", "project": "sipwise/repoapi", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 37299, "num_retrieved": 244, "inference_time": 14.270571008324623, "generate_results": ["def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)", "def replace_dots(value, arg):\n    \"\"\"\n    Replaces all values of '.' to arg from the given string\n    \"\"\"\n    return value.replace('.', arg)"]}
{"_id": "62ece4982e6aefcf4aabbd6b", "name": "subclasses", "project": "turicas/rows", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 121509, "num_retrieved": 467, "inference_time": 14.270571008324623, "generate_results": ["def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.append(subclass)\n        subclasses_list.extend(subclasses(subclass))\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list", "def subclasses(cls):\n    \"\"\"\n    Return all subclasses of a class, recursively\n    \"\"\"\n    subclasses_list = []\n    for subclass in cls.__subclasses__():\n        subclasses_list.extend(subclasses(subclass))\n        subclasses_list.append(subclass)\n    return subclasses_list"]}
{"_id": "62ece4982e6aefcf4aabbd6d", "name": "string_to_int", "project": "skorokithakis/shortuuid", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 1161, "num_retrieved": 5, "inference_time": 14.270571008324623, "generate_results": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result", "def string_to_int(string: str, alphabet: List[str]) -> int:\n    \"\"\"\n    Convert a string to a number, using the given alphabet.\n\n    The input is assumed to have the most significant digit first.\n    \"\"\"\n    if not string:\n        return 0\n    \n    alphabet_dict = {char: index for index, char in enumerate(alphabet)}\n    result = 0\n    for char in string:\n        result = result * len(alphabet) + alphabet_dict[char]\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd6f", "name": "get_repo_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 20613, "num_retrieved": 149, "inference_time": 14.270571008324623, "generate_results": ["import requests\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"xf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"-xzf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport subprocess\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, \"wb\") as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"xf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_output([\"tar\", \"xf\", str(archive_path), \"-C\", str(destination_path)])\n    \n    return destination_path", "import requests\nimport subprocess\nimport os\nfrom pathlib import Path\n\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    # Ensure destination path exists\n    destination_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download the archive\n    response = requests.get(url)\n    response.raise_for_status()\n    \n    # Save the archive to a temporary file\n    archive_path = destination_path / f\"archive.tar.gz\"\n    with open(archive_path, 'wb') as f:\n        f.write(response.content)\n    \n    # Extract the archive to the destination path\n    subprocess.check_call(['tar', '-xzf', str(archive_path), '-C', str(destination_path)])\n    \n    return destination_path"]}
{"_id": "62ece4982e6aefcf4aabbd70", "name": "os_is_mac", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 121777, "num_retrieved": 486, "inference_time": 14.270571008324623, "generate_results": ["import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\"", "import os\nimport platform\n\ndef os_is_mac():\n    \"\"\"\n    Checks if the os is macOS\n\n    :return: True if macOS\n    :rtype: bool\n    \"\"\"\n    return platform.system() == \"Darwin\""]}
{"_id": "62ece4982e6aefcf4aabbd71", "name": "regex_dict", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 165735, "num_retrieved": 500, "inference_time": 14.270571008324623, "generate_results": ["import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'.*', key)\n        regex_key = re.sub(r'\\?', r'.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'(.|)', regex_key)\n        # Ensure the regex starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'(.*)', key)\n        regex_key = re.sub(r'\\?', r'(.|)', regex_key)\n        # Ensure the pattern starts with ^ and ends with $\n        regex_key = f'^{regex_key}$'\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'.*', key)\n        regex_key = re.sub(r'\\?', r'.', regex_key)\n        result[regex_key] = value\n    return result", "import re\n\ndef regex_dict(item):\n    \"\"\"\n    Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n    \"\"\"\n    result = {}\n    for key, value in item.items():\n        # Convert wildcard pattern to regex\n        # Replace * with .*, and ? with .\n        regex_key = re.sub(r'\\*', r'.*', key)\n        regex_key = re.sub(r'\\?', r'.', regex_key)\n        result[regex_key] = value\n    return result"]}
{"_id": "62ece4982e6aefcf4aabbd72", "name": "unquote", "project": "ikus060/rdiffweb", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 77542, "num_retrieved": 259, "inference_time": 14.270571008324623, "generate_results": ["def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name", "def unquote(name):\n    \"\"\"\n    Remove quote from the given name.\n    \"\"\"\n    if not name:\n        return name\n    # Remove surrounding quotes if present\n    if name.startswith('\"') and name.endswith('\"'):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith('\"') and name.endswith(\"'\"):\n        return name[1:-1]\n    elif name.startswith(\"'\") and name.endswith('\"'):\n        return name[1:-1]\n    return name"]}
{"_id": "62ece4982e6aefcf4aabbd73", "name": "split", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 115448, "num_retrieved": 457, "inference_time": 14.270571008324623, "generate_results": ["import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n\n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules (no quotes, spaces separate)\n    if platform == 0:\n        # Split on spaces, but preserve quoted strings\n        # In Windows CMD, quoted strings are not parsed like in POSIX\n        # We'll use a simple split that handles basic cases\n        return [part.strip() for part in s.split() if part.strip()]\n    \n    # POSIX-like splitting (Unix/Linux/macOS)\n    # Use shlex.split as a fallback for complex cases\n    return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules (space separated, no quotes)\n    if platform == 0:\n        # Split on spaces, but preserve quoted strings\n        # Windows CMD treats quoted strings as single tokens\n        # We'll use a simple split that handles basic quoting\n        # This is a simplified version for Windows\n        result = []\n        current = \"\"\n        in_quotes = False\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                current += char\n            elif char == '\"' and in_quotes:\n                in_quotes = False\n                current += char\n                if current.strip():\n                    result.append(current.strip())\n                current = \"\"\n            else:\n                if char == ' ' and not in_quotes:\n                    if current.strip():\n                        result.append(current.strip())\n                    current = \"\"\n                else:\n                    current += char\n        \n        if current.strip():\n            result.append(current.strip())\n        \n        return result\n    \n    # POSIX (Linux, macOS) uses shlex.split behavior\n    else:\n        # Use shlex.split for POSIX systems\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules (no quotes, no escaping)\n    if platform == 0:\n        # Split on spaces, but preserve quoted strings\n        # Windows CMD doesn't handle complex quoting like POSIX\n        # Simple split on spaces, treating quoted strings as single tokens\n        # This is a simplified version for Windows\n        result = []\n        current = \"\"\n        in_quotes = False\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                current += char\n            elif char == '\"' and in_quotes:\n                in_quotes = False\n                result.append(current)\n                current = \"\"\n            elif char == ' ' and not in_quotes:\n                if current:\n                    result.append(current)\n                    current = \"\"\n            else:\n                current += char\n        \n        if current:\n            result.append(current)\n        \n        # Handle cases where there are no spaces or quotes\n        # This is a fallback to shlex.split for complex cases\n        if not result or len(result) == 1:\n            result = shlex.split(s)\n            \n        return result\n    \n    # POSIX (Linux, macOS) uses shlex.split with proper quoting\n    else:\n        # Use shlex.split for POSIX systems\n        # This handles quotes, escaping, and complex commands properly\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules (space separated, no quotes)\n    if platform == 0:\n        # Split on spaces, but preserve quoted strings\n        # Windows CMD treats quoted strings as single tokens\n        # Use a simple split that handles quoted strings\n        result = []\n        current = \"\"\n        in_quotes = False\n        \n        for char in s:\n            if char == '\"' and not in_quotes:\n                in_quotes = True\n                current += char\n            elif char == '\"' and in_quotes:\n                in_quotes = False\n                current += char\n                if current.strip():\n                    result.append(current.strip())\n                current = \"\"\n            else:\n                if char == ' ' and not in_quotes:\n                    if current.strip():\n                        result.append(current.strip())\n                    current = \"\"\n                else:\n                    current += char\n        \n        if current.strip():\n            result.append(current.strip())\n        \n        return result\n    \n    # POSIX (Linux, macOS) uses shlex.split behavior\n    else:\n        # Use shlex.split for POSIX platforms\n        return shlex.split(s)", "import re\nimport platform\nimport shlex\n\ndef split(s, platform='this'):\n    \"\"\"\n    Multi-platform variant of shlex.split() for command-line splitting.\n    For use with subprocess, for argv injection etc. Using fast REGEX.\n    \n    platform: 'this' = auto from current platform;\n              1 = POSIX;\n              0 = Windows/CMD\n              (other values reserved)\n    \"\"\"\n    if platform == 'this':\n        platform = 1 if platform.lower() in ['linux', 'darwin'] else 0\n    \n    # Windows CMD uses different splitting rules (no quotes, no escaping)\n    if platform == 0:\n        # Split on spaces, but preserve quoted strings\n        # Windows CMD doesn't handle complex quoting like POSIX\n        # Simple split on spaces, treating quoted strings as single tokens\n        # This is a simplified version for Windows\n        result = []\n        current = \"\"\n        in_quotes = False\n        \n        for char in s:\n            if char == ' ' and not in_quotes:\n                if current:\n                    result.append(current)\n                    current = \"\"\n            elif char == '\"' or char == \"'\":\n                if not in_quotes:\n                    in_quotes = True\n                    current += char\n                elif in_quotes and char == current[-1]:\n                    in_quotes = False\n                    current += char\n                else:\n                    current += char\n            else:\n                current += char\n        \n        if current:\n            result.append(current)\n        \n        return result\n    \n    # POSIX (Linux, macOS) uses shlex.split behavior\n    else:\n        # Use shlex.split for POSIX systems\n        return shlex.split(s)"]}
{"_id": "62ece4982e6aefcf4aabbd74", "name": "prepare_repository_from_archive", "project": "SoftwareHeritage/swh-lister", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 87070, "num_retrieved": 370, "inference_time": 14.270571008324623, "generate_results": ["import tarfile\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    # Ensure tmp_path is a Path object\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    archive_path = Path(archive_path)\n    if not archive_path.exists():\n        raise FileNotFoundError(f\"Archive not found: {archive_path}\")\n    \n    # Extract the archive\n    extract_dir = tmp_dir / archive_path.stem\n    extract_dir.mkdir(exist_ok=True)\n    \n    with tarfile.open(archive_path) as tar:\n        tar.extractall(path=extract_dir)\n    \n    # Construct the repository URL from the extracted directory\n    # The filename parameter can be used to specify a specific file\n    if filename:\n        file_path = extract_dir / filename\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File not found in extracted archive: {file_path}\")\n        # Return a URL that points to the file\n        return str(file_path)\n    else:\n        # Return the directory path as a URL\n        return str(extract_dir)", "import os\nimport tarfile\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    archive_path = Path(archive_path)\n    if not archive_path.exists():\n        raise FileNotFoundError(f\"Archive not found: {archive_path}\")\n    \n    # Extract the archive\n    extract_dir = tmp_dir / archive_path.stem\n    extract_dir.mkdir(exist_ok=True)\n    \n    with tarfile.open(archive_path) as tar:\n        tar.extractall(path=extract_dir)\n    \n    # Construct the repo URL from the extracted directory\n    # The filename parameter is optional and can be used to specify a specific file\n    # If no filename is provided, we use the directory name\n    if filename is None:\n        filename = extract_dir.name\n    \n    # Return the file repo URL (this is a placeholder - actual URL construction depends on context)\n    # In a real implementation, this would be constructed based on the actual repository structure\n    return f\"file://{extract_dir.absolute()}/{filename}\"", "import os\nimport tarfile\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    archive_path = Path(archive_path)\n    if not archive_path.exists():\n        raise FileNotFoundError(f\"Archive not found: {archive_path}\")\n    \n    # Extract the archive\n    extract_dir = tmp_dir / archive_path.name.split('.')[0]\n    extract_dir.mkdir(exist_ok=True)\n    \n    with tarfile.open(archive_path) as tar:\n        tar.extractall(path=extract_dir)\n    \n    # Construct the repo URL from the extracted directory\n    # Assuming the extracted directory contains a file that can be used as the origin URL\n    # This is a simplified implementation - the actual URL construction depends on the archive content\n    if filename:\n        # Use the provided filename as part of the URL\n        return f\"file://{extract_dir}/{filename}\"\n    else:\n        # Return a default URL based on the directory\n        return f\"file://{extract_dir}\"", "import subprocess\nimport tarfile\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    archive_path = Path(archive_path)\n    extract_dir = tmp_dir / archive_path.stem\n    \n    # Use tarfile to extract the archive\n    with tarfile.open(archive_path) as tar:\n        tar.extractall(path=extract_dir)\n    \n    # Construct the repository URL from the extracted directory\n    # The filename parameter is optional and can be used to specify a specific file\n    # If no filename is provided, we use the directory name\n    if filename:\n        file_path = extract_dir / filename\n        if not file_path.exists():\n            raise FileNotFoundError(f\"File {file_path} not found in extracted archive\")\n        return str(file_path)\n    else:\n        # Return the directory path as the repository URL\n        return str(extract_dir)", "import os\nimport tarfile\nfrom pathlib import Path\nfrom typing import Optional, Union\n\ndef prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\"\n) -> str:\n    \"\"\"\n    Given an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    \"\"\"\n    tmp_dir = Path(tmp_path)\n    tmp_dir.mkdir(exist_ok=True)\n    \n    # Extract the archive to a temporary directory\n    archive_path = Path(archive_path)\n    if not archive_path.exists():\n        raise FileNotFoundError(f\"Archive not found: {archive_path}\")\n    \n    # Extract the archive\n    extract_dir = tmp_dir / archive_path.stem\n    extract_dir.mkdir(exist_ok=True)\n    \n    with tarfile.open(archive_path) as tar:\n        tar.extractall(path=extract_dir)\n    \n    # Construct the repo URL based on the extracted files\n    # Assuming the repository is in a directory with a specific structure\n    # and the filename is provided or can be inferred from the archive\n    if filename is None:\n        # Use the first file in the directory as the filename\n        files = list(extract_dir.glob(\"*\"))\n        if not files:\n            raise ValueError(\"No files found in extracted archive\")\n        filename = files[0].name\n    \n    # Return a file repo URL (this is a placeholder - actual URL construction depends on context)\n    # In a real implementation, this would return a URL like https://example.com/repo/path/to/file\n    return f\"file://{extract_dir}/{filename}\""]}
{"_id": "62ece4982e6aefcf4aabbd75", "name": "addignored", "project": "commandline/flashbake", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 30634, "num_retrieved": 110, "inference_time": 14.270571008324623, "generate_results": ["import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=False)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter only the files that are in the ignored list\n        filtered_files = []\n        for file in ignored_files:\n            if file in ignored:\n                filtered_files.append(file)\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Return as a comma-separated string\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError:\n        return \"\"\n    except FileNotFoundError:\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter out files that are not in the ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Join with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=False)\n        if result.returncode != 0:\n            return \"\"\n        \n        # Split the output into lines and filter out empty lines\n        ignored_files = [line.strip() for line in result.stdout.splitlines() if line.strip()]\n        \n        # Sort the list of ignored files\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return \",\".join(ignored_files)\n    except FileNotFoundError:\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run(['git', 'check-ignore', '--files'], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().split('\\n')\n        \n        # Filter out empty lines and sort the list\n        ignored_files = [file.strip() for file in ignored_files if file.strip()]\n        ignored_files.sort()\n        \n        # Join the files with commas\n        return ','.join(ignored_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\"", "import subprocess\n\ndef addignored(ignored):\n    \"\"\"\n    Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.\n    \"\"\"\n    # Run git check-ignore to get the list of ignored files\n    try:\n        result = subprocess.run([\"git\", \"check-ignore\", \"--files\"], \n                               capture_output=True, text=True, check=True)\n        ignored_files = result.stdout.strip().splitlines()\n        \n        # Filter only the files that are in the ignored list\n        filtered_files = [file for file in ignored_files if file in ignored]\n        \n        # Sort the files\n        filtered_files.sort()\n        \n        # Join with commas\n        return ','.join(filtered_files)\n        \n    except subprocess.CalledProcessError as e:\n        # If git check-ignore fails, return empty string\n        return \"\"\n    except FileNotFoundError:\n        # If git is not found, return empty string\n        return \"\""]}
{"_id": "62ece4982e6aefcf4aabbd76", "name": "match", "project": "bazaar-projects/docopt-ng", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 3282, "num_retrieved": 30, "inference_time": 14.270571008324623, "generate_results": ["def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith('.docopt')", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")", "def match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n    return filename.endswith(\".docopt\")"]}
{"_id": "62ece4982e6aefcf4aabbd77", "name": "parse_frequency", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 88443, "num_retrieved": 281, "inference_time": 14.270571008324623, "generate_results": ["import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match a number followed by a unit of time\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f\"Unknown time unit '{unit}' in frequency '{frequency}'\")", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match a number followed by a unit of time\n    match = re.match(r'^(\\d+)\\s*([a-zA-Z]+)$', frequency.strip())\n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Map units to timedelta\n    unit_map = {\n        'second': timedelta(seconds=number),\n        'seconds': timedelta(seconds=number),\n        'minute': timedelta(minutes=number),\n        'minutes': timedelta(minutes=number),\n        'hour': timedelta(hours=number),\n        'hours': timedelta(hours=number),\n        'day': timedelta(days=number),\n        'days': timedelta(days=number),\n        'week': timedelta(weeks=number),\n        'weeks': timedelta(weeks=number),\n        'month': None,  # Months are not directly supported in timedelta\n        'months': None,  # Months are not directly supported in timedelta\n        'year': None,  # Years are not directly supported in timedelta\n        'years': None,  # Years are not directly supported in timedelta\n    }\n    \n    if unit in unit_map and unit_map[unit] is not None:\n        return unit_map[unit]\n    else:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match a number followed by a unit of time\n    match = re.match(r'^(\\d+)\\s+(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f\"Unknown time unit '{unit}' in frequency '{frequency}'\")", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match a number followed by a unit of time\n    match = re.match(r'^(\\d+)\\s*(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f\"Unknown time unit '{unit}' in frequency '{frequency}'\")", "import re\nfrom datetime import timedelta\n\ndef parse_frequency(frequency):\n    if frequency is None or frequency == \"always\":\n        return None\n    \n    # Match a number followed by a unit of time\n    match = re.match(r'^(\\d+)\\s*(weeks?|days?|hours?|minutes?|seconds?)$', frequency, re.IGNORECASE)\n    \n    if not match:\n        raise ValueError(f\"Cannot parse frequency '{frequency}'\")\n    \n    number = int(match.group(1))\n    unit = match.group(2).lower()\n    \n    # Convert unit to timedelta\n    if unit == 'weeks':\n        return timedelta(weeks=number)\n    elif unit == 'days':\n        return timedelta(days=number)\n    elif unit == 'hours':\n        return timedelta(hours=number)\n    elif unit == 'minutes':\n        return timedelta(minutes=number)\n    elif unit == 'seconds':\n        return timedelta(seconds=number)\n    else:\n        raise ValueError(f\"Unknown time unit '{unit}' in frequency '{frequency}'\")"]}
{"_id": "62ece4992e6aefcf4aabbd78", "name": "is_local", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 116384, "num_retrieved": 466, "inference_time": 14.270571008324623, "generate_results": ["import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, it's not a valid host\n        return False", "import socket\n\ndef is_local(host):\n    \"\"\"\n    Checks if the host is the localhost\n    \n    :param host: The hostname or ip\n    :return: True if the host is the localhost\n    \"\"\"\n    try:\n        # Try to resolve the hostname to an IP address\n        ip_address = socket.gethostbyname(host)\n        \n        # Check if the resolved IP is the loopback address (127.0.0.1)\n        return ip_address == '127.0.0.1'\n    except socket.gaierror:\n        # If hostname resolution fails, return False\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd79", "name": "make_find_paths", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 90792, "num_retrieved": 286, "inference_time": 23.110735222697258, "generate_results": ["import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('sh:', 'pp:', 'pp:', 'sh:')) and not path.startswith(('/', '\\\\')) else path\n        for path in find_paths\n    ]", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('sh:', 'pp:')) else path\n        for path in find_paths\n    ]", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('sh:', 'pp:', 'pp:', 'sh:')) and not path.startswith(('.', '..')) else path\n        for path in find_paths\n    ]", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('sh:', 'pp:', 'pp:', 'sh:')) else path\n        for path in find_paths\n    ]", "import re\ndef make_find_paths(find_paths):\n    \"\"\"\n    Given a sequence of path fragments or patterns as passed to `--find`, transform all path\n    fragments into glob patterns. Pass through existing patterns untouched.\n\n    For example, given find_paths of:\n\n      ['foo.txt', 'pp:root/somedir']\n\n    ... transform that into:\n\n      ['sh:**/*foo.txt*/**', 'pp:root/somedir']\n    \"\"\"\n    return [\n        f'sh:**/*{path}*/**' if path and not path.startswith(('sh:', 'pp:')) else path\n        for path in find_paths\n    ]"]}
{"_id": "62ece4992e6aefcf4aabbd7a", "name": "is_gitbash", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 116433, "num_retrieved": 466, "inference_time": 23.110735222697258, "generate_results": ["def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and os.path.exists(r\"C:\\Program Files\\Git\\bin\\sh.exe\")", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and os.path.exists(r\"C:\\Program Files\\Git\\git-bash.exe\")", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and os.path.exists(r\"C:\\Program Files\\Git\\bin\\sh.exe\")", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and os.path.exists(r\"C:\\Program Files\\Git\\bin\\sh.exe\")", "def is_gitbash():\n    \"\"\"\n    returns True if you run in a Windows gitbash\n\n    :return: True if gitbash\n    \"\"\"\n    return os_is_windows() and os.path.exists(r\"C:\\Program Files\\Git\\git-bash.exe\")"]}
{"_id": "62ece4992e6aefcf4aabbd7b", "name": "write_configuration", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 88259, "num_retrieved": 279, "inference_time": 23.110735222697258, "generate_results": ["import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create any containing directories as needed\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # If the file already exists and overwrite is False, abort\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create any containing directories as needed\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # If the file already exists and overwrite is False, abort\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create containing directories if they don't exist\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # Check if file already exists and overwrite is False\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create any containing directories as needed\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # If the file already exists and overwrite is False, abort\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)", "import os\nimport os.path\n\ndef write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n    \"\"\"\n    Given a target config filename and rendered config YAML, write it out to file. Create any\n    containing directories as needed. But if the file already exists and overwrite is False,\n    abort before writing anything.\n    \"\"\"\n    # Create any containing directories as needed\n    directory = os.path.dirname(config_filename)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n    \n    # If the file already exists and overwrite is False, abort\n    if os.path.exists(config_filename) and not overwrite:\n        return\n    \n    # Write the rendered config to the file\n    with open(config_filename, 'w') as file:\n        file.write(rendered_config)\n    \n    # Set the file mode\n    os.chmod(config_filename, mode)"]}
{"_id": "62ece4992e6aefcf4aabbd7c", "name": "oneline", "project": "cloudmesh/cloudmesh-common", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 101627, "num_retrieved": 388, "inference_time": 23.110735222697258, "generate_results": ["import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to a single line\n    :param seperator: the separator to join the commands (default is \" && \")\n    :return: a single line command string\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    command = seperator.join(lines)\n    \n    return command", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to a single line\n    :param seperator: the separator to join the commands (default is \" && \")\n    :return: a single line command with all commands joined by the separator\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    return seperator.join(lines)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to a single line\n    :param seperator: the separator to join the commands (default is \" && \")\n    :return: a single line command with all commands joined by the separator\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    return seperator.join(lines)", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to a single line\n    :param seperator: the separator to join the commands (default is \" && \")\n    :return: a single line command with all commands joined by the separator\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    oneline_command = seperator.join(lines)\n    \n    return oneline_command", "import textwrap\n\ndef oneline(script, seperator=\" && \"):\n    \"\"\"\n    converts a script to one line command.\n    This is useful to run a single ssh command and pass a one line script.\n\n    :param script: the script to be converted to one line\n    :param seperator: the separator to use between commands\n    :return: a single line command with all commands joined by the separator\n    \"\"\"\n    # Split the script into lines and strip whitespace\n    lines = [line.strip() for line in script.splitlines() if line.strip()]\n    \n    # Join the lines with the separator\n    one_line = seperator.join(lines)\n    \n    return one_line"]}
{"_id": "62ece4992e6aefcf4aabbd7d", "name": "subprocess_run_helper", "project": "rougier/matplotlib", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 207765, "num_retrieved": 391, "inference_time": 23.110735222697258, "generate_results": ["import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.__dict__[func.__name__] = func\n    \n    # Create a string representation of the module\n    module_code = f\"import sys\\n{func.__code__.co_code.decode('latin1')}\\n\"\n    \n    # Execute the module in a subprocess\n    env = os.environ.copy()\n    if extra_env:\n        env.update(extra_env)\n    \n    # Prepare the command line\n    command = [sys.executable, \"-c\", module_code]\n    \n    # Add additional arguments\n    command.extend(args)\n    \n    # Run the command with timeout\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        return result\n    except subprocess.TimeoutExpired:\n        raise TimeoutError(f\"Command timed out after {timeout} seconds\")", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.__dict__[func.__name__] = func\n    \n    # Create a string representation of the function call\n    function_call = f\"{func.__name__}({', '.join([repr(arg) for arg in args])})\"\n    \n    # Create the command to run\n    command = [sys.executable, \"-c\", f\"import {module_name}; {function_call}\"]\n    \n    # Add extra environment variables\n    if extra_env:\n        env = os.environ.copy()\n        env.update(extra_env)\n        command.extend([\"--env\", str(env)])\n    \n    # Run the command with timeout\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode, result.stdout, result.stderr\n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Command timed out\"", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_module_{id(func)}_{hash(func)}\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, None))\n    \n    # Add the function to the module\n    module.func = func\n    \n    # Create a string representation of the function call\n    # This is a simplified approach - in practice, you would need to\n    # create a proper module with the function defined\n    try:\n        # Import the function from the module\n        # This is a simplified approach - in reality, you would need\n        # to create a proper module with the function defined\n        module.__file__ = \"<generated>\"\n        \n        # Create a command to run the function\n        # This is a simplified approach - in reality, you would need\n        # to create a proper module with the function defined\n        command = [sys.executable, \"-c\", f\"import {module_name}; {module_name}.func(*{args})\"]\n        \n        # Set up the environment\n        env = os.environ.copy()\n        if extra_env:\n            env.update(extra_env)\n            \n        # Run the command with timeout\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        return result\n        \n    except Exception as e:\n        # If there's an error, return the error message\n        return {\"error\": str(e)}", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n    \n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.__dict__[func.__name__] = func\n    \n    # Create a string representation of the function call\n    call_str = f\"{func.__name__}({', '.join(repr(arg) for arg in args)})\"\n    \n    # Create the command to run\n    command = [sys.executable, \"-c\", f\"import {module_name}; {call_str}\"]\n    \n    # Add extra environment variables\n    if extra_env:\n        env = os.environ.copy()\n        env.update(extra_env)\n        command.extend([\"--env\", str(env)])\n    \n    # Run the command with timeout\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode, result.stdout, result.stderr\n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Command timed out\"", "import subprocess\nimport importlib\nimport sys\nfrom types import ModuleType\n\ndef subprocess_run_helper(func, *args, timeout, extra_env=None):\n    \"\"\"\n    Run a function in a sub-process.\n\n    Parameters\n    ----------\n    func : function\n        The function to be run.  It must be in a module that is importable.\n    *args : str\n        Any additional command line arguments to be passed in\n        the first argument to ``subprocess.run``.\n    extra_env : dict[str, str]\n        Any additional environment variables to be set for the subprocess.\n    \"\"\"\n    # Create a temporary module to hold the function\n    module_name = f\"temp_{hash(func):x}\"\n    module = importlib.util.module_from_spec(importlib.util.spec_from_file_location(module_name, \"<string>\"))\n    \n    # Define the function in the module\n    module.__dict__[func.__name__] = func\n    \n    # Create a string representation of the function call\n    function_call = f\"{func.__name__}({', '.join([repr(arg) for arg in args])})\"\n    \n    # Create the command to run\n    command = [sys.executable, \"-c\", f\"import {module_name}; {function_call}\"]\n    \n    # Add extra environment variables\n    if extra_env:\n        env = os.environ.copy()\n        env.update(extra_env)\n        command.extend([\"--env\", str(env)])\n    \n    # Run the command with timeout\n    try:\n        result = subprocess.run(\n            command,\n            timeout=timeout,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode, result.stdout, result.stderr\n    except subprocess.TimeoutExpired:\n        return -1, \"\", \"Command timed out\""]}
{"_id": "62ece4992e6aefcf4aabbd7e", "name": "_resolve_string", "project": "witten/borgmatic", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 91501, "num_retrieved": 288, "inference_time": 23.110735222697258, "generate_results": ["import os\nimport sys\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO-bar} or ${FOO:-bar}\n    match = matcher.group(0)\n    \n    # Extract the variable name (everything between the first and last curly brace)\n    var_name = match[1:-1]\n    \n    # Check if there's a default value (after a colon)\n    if ':' in var_name:\n        var_name, default_value = var_name.split(':', 1)\n    else:\n        default_value = None\n    \n    # Get the environment variable value\n    env_value = os.environ.get(var_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not set and no default value is provided\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nimport sys\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format is either ${VAR} or ${VAR:-DEFAULT}\n    match = matcher.group(0)\n    \n    # Extract the variable name (everything between the braces)\n    var_name = match[1:-1]\n    \n    # Check if there's a default value (after the colon)\n    if ':' in var_name:\n        var_name, default_value = var_name.split(':', 1)\n        default_value = default_value.strip()\n    else:\n        default_value = None\n    \n    # Get the environment variable value\n    env_value = os.environ.get(var_name)\n    \n    # If no value is found and no default is provided, raise an error\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nimport sys\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format is either ${VAR} or ${VAR:-default}\n    match = matcher.group(0)\n    if match.startswith('${') and match.endswith('}'):\n        # Remove the ${ and } to get the variable name and optional default\n        var_part = match[2:-1]\n        \n        # Check if there's a default value (after the colon)\n        if ':' in var_part:\n            var_name, default_value = var_part.split(':', 1)\n        else:\n            var_name = var_part\n            default_value = None\n            \n        # Get the environment variable value\n        env_value = os.environ.get(var_name)\n        \n        # If the variable is not defined and no default value is provided, raise an error\n        if env_value is None and default_value is None:\n            raise EnvironmentError(f'Environment variable {var_name} is not defined')\n            \n        # Return the value (either from environment or default)\n        return env_value if env_value is not None else default_value\n    else:\n        # Should not reach here given the pattern\n        raise ValueError(f'Invalid matcher format: {match}')", "import os\nimport sys\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format: ${FOO} or ${FOO-bar} or ${FOO:-bar}\n    match = matcher.group(0)\n    \n    # Extract the variable name (everything between the ${ and the first - or } or the end)\n    var_name = match[2:].split(':-')[0]\n    \n    # Check if there's a default value (after the :-)\n    default_value = None\n    if ':-' in match:\n        default_value = match.split(':-')[1]\n    \n    # Get the environment variable value\n    env_value = os.environ.get(var_name)\n    \n    # If the variable is not defined and no default value is provided, raise an error\n    if env_value is None and default_value is None:\n        raise EnvironmentError(f\"Environment variable '{var_name}' is not defined\")\n    \n    # Return the value (either from environment or default)\n    return env_value if env_value is not None else default_value", "import os\nimport sys\n\ndef _resolve_string(matcher):\n    \"\"\"\n    Get the value from environment given a matcher containing a name and an optional default value.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n    \"\"\"\n    # Extract the variable name and optional default value from the matcher\n    # Format is either ${VAR} or ${VAR:-DEFAULT}\n    match = matcher.group(0)\n    if match.startswith('${') and match.endswith('}'):\n        # Extract the variable name and optional default value\n        var_name = match[2:-1]\n        if ':' in var_name:\n            var_name, default_value = var_name.split(':', 1)\n        else:\n            default_value = None\n        \n        # Get the value from environment\n        value = os.environ.get(var_name)\n        \n        # If the variable is not defined and no default value is provided, raise an error\n        if value is None and default_value is None:\n            raise EnvironmentError(f'Environment variable {var_name} is not defined')\n            \n        # Return the value (either from environment or default value)\n        return value if value is not None else default_value"]}
{"_id": "62ece4992e6aefcf4aabbd7f", "name": "_parse_image_ref", "project": "openstack/cinder", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 197092, "num_retrieved": 500, "inference_time": 23.110735222697258, "generate_results": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed_url = urllib.parse.urlsplit(image_href)\n    \n    # Check if the URL is valid\n    if not parsed_url.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    # Extract the netloc (network location)\n    netloc = parsed_url.netloc\n    \n    # Check if the URL has a scheme (http or https)\n    scheme = parsed_url.scheme\n    \n    # Determine if SSL is used\n    use_ssl = scheme == 'https'\n    \n    # Extract the path and split it to get the image ID\n    path_parts = parsed_url.path.strip('/').split('/')\n    if len(path_parts) < 1:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    # The last part of the path should be the image ID\n    image_id = path_parts[-1]\n    \n    # Validate that the image ID is a valid UUID\n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image href: image ID is not a valid UUID\")\n    \n    return (image_id, netloc, use_ssl)", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed_url = urllib.parse.urlsplit(image_href)\n    \n    # Check if the URL is valid\n    if not parsed_url.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    # Extract the netloc (network location)\n    netloc = parsed_url.netloc\n    \n    # Check if the URL has a scheme (http or https)\n    scheme = parsed_url.scheme\n    \n    # Determine if SSL is used\n    use_ssl = scheme == 'https'\n    \n    # Extract the path and split it to get the image_id\n    path_parts = parsed_url.path.split('/')\n    if len(path_parts) < 2:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    # The image ID is the last part of the path\n    image_id = path_parts[-1]\n    \n    # Validate that the image_id is a valid UUID\n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image href: image ID is not a valid UUID\")\n    \n    return (image_id, netloc, use_ssl)", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed_url = urllib.parse.urlsplit(image_href)\n    \n    # Check if the URL is valid\n    if not parsed_url.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    # Extract the netloc (network location)\n    netloc = parsed_url.netloc\n    \n    # Determine if SSL is used based on the scheme\n    use_ssl = parsed_url.scheme == 'https'\n    \n    # Extract the path and get the image ID from the end of the path\n    path_parts = parsed_url.path.split('/')\n    if len(path_parts) < 2:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    image_id = path_parts[-1]\n    \n    # Validate that the image ID is a valid UUID\n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image href: image ID is not a valid UUID\")\n    \n    return (image_id, netloc, use_ssl)", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed_url = urllib.parse.urlsplit(image_href)\n    \n    # Extract the image_id from the path\n    path_parts = parsed_url.path.split('/')\n    if len(path_parts) < 2:\n        raise ValueError('Invalid image href: missing image ID')\n    \n    image_id = path_parts[-1]\n    \n    # Extract the netloc (network location)\n    netloc = parsed_url.netloc\n    \n    # Determine if SSL is used based on the scheme\n    use_ssl = parsed_url.scheme == 'https'\n    \n    return (image_id, netloc, use_ssl)", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n    \"\"\"\n    Parse an image href into composite parts.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n    \"\"\"\n    parsed_url = urllib.parse.urlsplit(image_href)\n    \n    # Check if the URL is valid\n    if not parsed_url.netloc:\n        raise ValueError(\"Invalid image href: missing network location\")\n    \n    # Extract the netloc (network location)\n    netloc = parsed_url.netloc\n    \n    # Check if the URL has a scheme\n    if not parsed_url.scheme:\n        # If no scheme is provided, assume it's HTTP\n        use_ssl = False\n    else:\n        # Determine if SSL is used based on the scheme\n        use_ssl = parsed_url.scheme.lower() == 'https'\n    \n    # Extract the path and split it to get the image ID\n    path_parts = parsed_url.path.strip('/').split('/')\n    if len(path_parts) < 1:\n        raise ValueError(\"Invalid image href: missing image ID in path\")\n    \n    # The last part of the path should be the image ID\n    image_id = path_parts[-1]\n    \n    # Validate that the image ID is a valid UUID\n    if not uuidutils.is_uuid_like(image_id):\n        raise ValueError(\"Invalid image href: image ID is not a valid UUID\")\n    \n    return (image_id, netloc, use_ssl)"]}
{"_id": "62ece4992e6aefcf4aabbd80", "name": "remove_ending_os_sep", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 166428, "num_retrieved": 500, "inference_time": 23.110735222697258, "generate_results": ["import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1:\n            if item.endswith(os.sep):\n                processed_list.append(item[:-1])\n            else:\n                processed_list.append(item)\n        else:\n            processed_list.append(item)\n    \n    return processed_list", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1:\n            # Check if the last character is the OS separator\n            if item.endswith(os.sep):\n                processed_list.append(item[:-1])\n            else:\n                processed_list.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return processed_list", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1:\n            if item.endswith(os.sep):\n                processed_list.append(item[:-1])\n            else:\n                processed_list.append(item)\n        else:\n            processed_list.append(item)\n    \n    return processed_list", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator. If so, the pathname seperator character\n    is removed.\n\n    Args:\n        input_list: list of strings\n\n    Returns:\n        Processed list of strings\n\n    Raises:\n        TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1:\n            # Check if the last character is the OS separator\n            if item.endswith(os.sep):\n                processed_list.append(item[:-1])\n            else:\n                processed_list.append(item)\n        else:\n            raise TypeError(\"All elements in input_list must be strings\")\n    \n    return processed_list", "import os\n\ndef remove_ending_os_sep(input_list):\n    \"\"\"\n    Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError\n    \"\"\"\n    if not isinstance(input_list, list):\n        raise TypeError(\"input_list must be a list\")\n    \n    processed_list = []\n    for item in input_list:\n        if isinstance(item, str) and len(item) > 1:\n            if item.endswith(os.sep):\n                processed_list.append(item[:-1])\n            else:\n                processed_list.append(item)\n        else:\n            processed_list.append(item)\n    \n    return processed_list"]}
{"_id": "62ece4992e6aefcf4aabbd82", "name": "get_pattern", "project": "kirankotari/shconfparser", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 2440, "num_retrieved": 10, "inference_time": 23.110735222697258, "generate_results": ["import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for safe matching\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words only\n    return r'\\b' + escaped_pattern + r'\\b'", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    return escaped_pattern", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for safe matching\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words only\n    return r'\\b' + escaped_pattern + r'\\b'", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words\n    return r'\\b' + escaped_pattern + r'\\b'", "import re\n\ndef get_pattern(pattern, strip=True):\n    \"\"\"\n    This method converts the given string to regex pattern\n    \"\"\"\n    if strip:\n        pattern = pattern.strip()\n    # Escape special regex characters and wrap in word boundaries for matching\n    escaped_pattern = re.escape(pattern)\n    # Add word boundaries to match whole words\n    return r'\\b' + escaped_pattern + r'\\b'"]}
{"_id": "62ece4992e6aefcf4aabbd83", "name": "run_command", "project": "burgerbecky/makeprojects", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 140933, "num_retrieved": 387, "inference_time": 23.110735222697258, "generate_results": ["import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \n    Args:\n        commands: List of command strings or a single command string.\n        args: Additional arguments to pass to the command.\n        cwd: Working directory for the command.\n        verbose: If True, print command to stdout.\n        hide_stderr: If True, do not print stderr.\n        env: Environment variables to use.\n        \n    Returns:\n        A tuple of (return_code, stdout, stderr) or (return_code, stdout) if stderr is None.\n    \"\"\"\n    # Convert to list if it's a string\n    if isinstance(commands, str):\n        commands = [commands]\n    \n    # Prepare the command\n    cmd = commands[:]\n    \n    # Add args if provided\n    if args:\n        cmd.extend(args)\n    \n    # Set working directory\n    if cwd is not None:\n        cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n    \n    # Print command if verbose\n    if verbose:\n        print(\" \".join(cmd))\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=True,\n            text=True,\n            env=env,\n            shell=False\n        )\n        \n        # Return the result\n        return result.returncode, result.stdout, result.stderr if not hide_stderr else None\n        \n    except FileNotFoundError:\n        # If the command is not found, return an error\n        return 127, None, \"Command not found: {}\".format(cmd[0])\n    except subprocess.SubprocessError as e:\n        # If there's a subprocess error, return an error\n        return e.returncode, None, str(e)\n    except Exception as e:\n        # For any other exception, return an error\n        return 1, None, str(e)", "import subprocess\nimport os\nfrom typing import List, Tuple, Optional\n\ndef run_command(commands: Tuple[str, ...], args: Optional[List[str]] = None, cwd: Optional[str] = None, verbose: bool = False, hide_stderr: bool = False, env: Optional[dict] = None) -> Tuple[int, str, str]:\n    \"\"\"\n    Call the given command(s).\n    \n    Args:\n        commands: Tuple of command strings to execute\n        args: Optional list of arguments to pass to the command\n        cwd: Optional working directory for the command\n        verbose: If True, print command output to stdout\n        hide_stderr: If True, do not print stderr output\n        env: Optional environment variables to set\n        \n    Returns:\n        Tuple of (return_code, stdout, stderr)\n    \"\"\"\n    # Prepare the command\n    cmd = list(commands)\n    \n    # Add arguments if provided\n    if args:\n        cmd.extend(args)\n    \n    # Set environment if provided\n    if env:\n        cmd_env = env.copy()\n    else:\n        cmd_env = os.environ.copy()\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=True,\n            text=True,\n            env=cmd_env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        \n        # Print output if verbose\n        if verbose:\n            print(\" \".join(cmd))\n            \n        # Print stderr if not hidden\n        if not hide_stderr and result.stderr:\n            print(result.stderr, file=sys.stderr)\n            \n        return result.returncode, result.stdout, result.stderr\n            \n    except FileNotFoundError as e:\n        return -1, \"\", str(e)\n    except subprocess.SubprocessError as e:\n        return -1, \"\", str(e)\n    except Exception as e:\n        return -1, \"\", str(e)", "import subprocess\nimport sys\nimport os\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \n    Args:\n        commands: List of command strings to execute.\n        args: Additional arguments to pass to the command.\n        cwd: Working directory for the command.\n        verbose: If True, print command to stdout before execution.\n        hide_stderr: If True, do not print stderr output.\n        env: Dictionary of environment variables to set.\n        \n    Returns:\n        A tuple of (return_code, stdout, stderr) where:\n        - return_code is the exit code of the command\n        - stdout is the standard output of the command\n        - stderr is the standard error output of the command\n    \"\"\"\n    # Prepare the command\n    cmd = commands if isinstance(commands, list) else [commands]\n    \n    # Add arguments if provided\n    if args:\n        cmd.extend(args)\n    \n    # Set working directory\n    if cwd is not None:\n        cmd = [cmd[0]] + cmd[1:]  # Keep the first element as the command name\n    \n    # Print command if verbose\n    if verbose:\n        print(\" \".join(cmd))\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=True,\n            text=True,\n            env=env,\n            shell=False\n        )\n        \n        # Return the result\n        return (result.returncode, result.stdout, result.stderr)\n        \n    except FileNotFoundError:\n        # Command not found\n        return (1, \"\", \"Command not found: \" + \" \".join(cmd))\n    except subprocess.SubprocessError as e:\n        # Subprocess error\n        return (1, \"\", str(e))\n    except Exception as e:\n        # Any other error\n        return (1, \"\", str(e))", "import subprocess\nimport os\nimport sys\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \n    Args:\n        commands: List of command strings or a single command string.\n        args: Additional arguments to pass to the command.\n        cwd: Working directory for the command.\n        verbose: If True, print command to stdout.\n        hide_stderr: If True, do not print stderr.\n        env: Dictionary of environment variables to set.\n    \n    Returns:\n        A tuple of (return_code, stdout, stderr) if successful, or (None, None, None) if an error occurred.\n    \"\"\"\n    # Convert to list if it's a string\n    if isinstance(commands, str):\n        commands = [commands]\n    \n    # Prepare the command\n    cmd = commands[:]\n    \n    # Add arguments if provided\n    if args:\n        cmd.extend(args)\n    \n    # Set working directory\n    if cwd is not None:\n        cmd.insert(0, 'cd')\n        cmd.insert(1, cwd)\n    \n    # Set environment variables\n    if env is not None:\n        os.environ.update(env)\n    \n    # Execute the command\n    try:\n        # Capture output\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            shell=False,\n            check=False,\n            env=env\n        )\n        \n        # Return the result\n        return (result.returncode, result.stdout, result.stderr)\n        \n    except subprocess.SubprocessError as e:\n        # Return error information\n        return (None, None, str(e))", "import subprocess\nimport os\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"\n    Call the given command(s).\n    \n    Args:\n        commands: List of command strings or a single command string.\n        args: Additional arguments to pass to the command.\n        cwd: Working directory for the command.\n        verbose: If True, print command output.\n        hide_stderr: If True, hide stderr output.\n        env: Environment variables to set.\n        \n    Returns:\n        A tuple of (return_code, stdout, stderr) or (return_code, stdout) if stderr is hidden.\n    \"\"\"\n    # Handle single command vs list of commands\n    if isinstance(commands, str):\n        commands = [commands]\n    \n    # Prepare the command\n    cmd = commands[:]\n    \n    # Add args if provided\n    if args:\n        cmd.extend(args)\n    \n    # Set working directory\n    if cwd is not None:\n        cmd = [cmd[0]] + cmd[1:]  # Keep first element as command name\n    \n    # Execute the command\n    try:\n        result = subprocess.run(\n            cmd,\n            cwd=cwd,\n            capture_output=True,\n            text=True,\n            env=env,\n            shell=False\n        )\n        \n        # Return the result\n        if hide_stderr:\n            return result.returncode, result.stdout\n        else:\n            return result.returncode, result.stdout, result.stderr\n            \n    except subprocess.SubprocessError as e:\n        return e.returncode, e.output, str(e)"]}
{"_id": "62ece4992e6aefcf4aabbd84", "name": "is_ipv4", "project": "gopad/gopad-python", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 59, "num_retrieved": 0, "inference_time": 23.110735222697258, "generate_results": ["import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ValueError:\n        return False", "import ipaddress\n\ndef is_ipv4(target):\n    \"\"\"\n    Test if IPv4 address or not\n    \n    \"\"\"\n    try:\n        ipaddress.IPv4Address(target)\n        return True\n    except ipaddress.AddressValueError:\n        return False"]}
{"_id": "62ece4992e6aefcf4aabbd85", "name": "find_roots", "project": "mwatts15/rdflib", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 171606, "num_retrieved": 500, "inference_time": 23.110735222697258, "generate_results": ["def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no triple with (subject, prop, ?))\n    for subject in graph.subjects(prop):\n        # Check if this subject has a parent (i.e., if there's a triple (subject, prop, parent))\n        parent = None\n        for parent_obj in graph.objects(subject, prop):\n            parent = parent_obj\n            break\n        \n        # If no parent found, this is a root\n        if parent is None:\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no triple with (subject, prop, ?))\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the subject has no parent, it's a root\n        if not any((subject, prop, parent) in graph for parent in graph.subjects(predicate=predicate)):\n            roots.add(subject)\n    \n    # For each subject that has a parent, recursively check if it's a root\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the subject has a parent, check if that parent is a root\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(predicate=predicate)):\n            roots.add(subject)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no triple where they are the object)\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If this subject is not already in the roots set, add it\n        if subject not in roots:\n            roots.add(subject)\n    \n    # Recursively check if any of the roots have children that might be part of a hierarchy\n    # We need to find all nodes that are not pointed to by any other node\n    # This is equivalent to finding all nodes that are not the object of any triple with predicate prop\n    \n    # Get all objects of triples with predicate prop\n    objects = set()\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        objects.add(object_)\n    \n    # Remove any nodes that are in the objects set from the roots set\n    # This ensures that only nodes that are not children of any other node remain\n    roots = {root for root in roots if root not in objects}\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no triple of the form (subject, prop, ?))\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If this subject has no parent, it's a root\n        if not any((subject, prop, parent) in graph for parent in graph.subjects(predicate=predicate)):\n            roots.add(subject)\n    \n    # For each subject that has a parent, recursively check if it's a root\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the object has no parent, it's a root\n        if not any((object_, prop, parent) in graph for parent in graph.subjects(predicate=predicate)):\n            roots.add(object_)\n    \n    return roots", "def find_roots(\n    graph: \"Graph\", \n    prop: \"URIRef\", \n    roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n    \"\"\"\n    if roots is None:\n        roots = set()\n    \n    # Get all subjects that have no parent (i.e., no triple with (subject, prop, ?))\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If this subject has no parent, it is a root\n        if not any((subject, prop, parent) in graph for parent in graph.subjects(predicate=predicate)):\n            roots.add(subject)\n    \n    # For each subject that has a parent, recursively check if it's a root\n    for subject, predicate, object_ in graph.triples((None, prop, None)):\n        # If the subject has a parent, check if that parent is a root\n        if object_ in roots:\n            continue\n        # Otherwise, recursively check if the parent is a root\n        if object_ not in roots:\n            roots.add(object_)\n    \n    return roots"]}
{"_id": "62ece4992e6aefcf4aabbd86", "name": "_dump_string", "project": "ansible-security/ansible_collections.ibm.qradar", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 16464, "num_retrieved": 45, "inference_time": 23.110735222697258, "generate_results": ["import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()", "import yaml\ndef _dump_string(obj, dumper=None):\n    \"\"\"\n    Dump to a py2-unicode or py3-string\n    \"\"\"\n    if dumper is None:\n        dumper = yaml.Dumper\n    return yaml.dump(obj, Dumper=dumper).strip()"]}
{"_id": "62ece4992e6aefcf4aabbd87", "name": "build_app_logger", "project": "scrolltech/apphelpers", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 7074, "num_retrieved": 39, "inference_time": 23.110735222697258, "generate_results": ["import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create file handler\n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n    \n    return logger", "import os\nimport logging\n\ndef build_app_logger(name='app', logfile='app.log', debug=True):\n    \"\"\"\n    General purpose application logger. Useful mainly for debugging\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Prevent adding multiple handlers\n    if logger.handlers:\n        return logger\n    \n    # Create console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create file handler\n    log_dir = os.path.dirname(logfile) if os.path.dirname(logfile) else '.'\n    os.makedirs(log_dir, exist_ok=True)\n    \n    file_handler = logging.FileHandler(logfile)\n    file_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n    \n    # Create formatter\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    console_handler.setFormatter(formatter)\n    file_handler.setFormatter(formatter)\n    \n    # Add handlers to logger\n    logger.addHandler(console_handler)\n    logger.addHandler(file_handler)\n    \n    return logger"]}
{"_id": "62ece4992e6aefcf4aabbd88", "name": "make_array", "project": "sunpy/radiospectra", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 20418, "num_retrieved": 67, "inference_time": 23.110735222697258, "generate_results": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)", "def make_array(shape, dtype=np.dtype(\"float32\")):\n    \"\"\"\n    Function to create an array with shape and dtype.\n\n    Parameters\n    ----------\n    shape : tuple\n        shape of the array to create\n    dtype : `numpy.dtype`\n        data-type of the array to create\n    \"\"\"\n    return np.empty(shape, dtype=dtype)"]}
{"_id": "62ece4992e6aefcf4aabbd89", "name": "gaussian", "project": "ufo-kit/concert", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 128706, "num_retrieved": 399, "inference_time": 23.110735222697258, "generate_results": ["def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    return np.exp(-((x - 0.2) ** 2) / (2 * 0.1 ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))", "def gaussian(x):\n    \"\"\"\n    Gaussian centered around 0.2 with a sigma of 0.1.\n    \"\"\"\n    mu = 0.2\n    sigma = 0.1\n    return np.exp(-((x - mu) ** 2) / (2 * sigma ** 2))"]}
{"_id": "62ece4992e6aefcf4aabbd8a", "name": "load_configurations", "project": "witten/borgmatic", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 87523, "num_retrieved": 290, "inference_time": 25.27196805179119, "generate_results": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            error_msg = f'An error occurred while parsing a configuration file at {config_filename}: {str(error)}'\n            logs.append(logging.makeLogRecord(\n                dict(\n                    levelno=logging.ERROR,\n                    levelname='ERROR',\n                    msg=error_msg,\n                )\n            ))\n    \n    return configs, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            error_msg = f'An error occurred while parsing a configuration file at {config_filename}: {str(error)}'\n            logs.append(logging.makeLogRecord(\n                dict(\n                    levelno=logging.ERROR,\n                    levelname='ERROR',\n                    msg=error_msg,\n                )\n            ))\n    \n    return configs, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            error_msg = f'An error occurred while parsing a configuration file at {config_filename}: {str(error)}'\n            logs.append(logging.makeLogRecord(\n                dict(\n                    levelno=logging.ERROR,\n                    levelname='ERROR',\n                    msg=error_msg,\n                )\n            ))\n    \n    return configs, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            logs.append(\n                logging.makeLogRecord(\n                    dict(\n                        levelno=logging.ERROR,\n                        levelname='ERROR',\n                        msg=f'Error parsing configuration file at {config_filename}: {str(error)}',\n                    )\n                )\n            )\n    \n    return configs, logs", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n    \"\"\"\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    \"\"\"\n    configs = {}\n    logs = []\n    \n    for config_filename in config_filenames:\n        try:\n            config, config_logs = parse_configuration(\n                config_filename, \n                schema_filename(), \n                overrides=overrides, \n                resolve_env=resolve_env\n            )\n            configs[config_filename] = config\n            logs.extend(config_logs)\n        except Exception as error:\n            error_msg = f'An error occurred while parsing a configuration file at {config_filename}: {str(error)}'\n            logs.append(logging.makeLogRecord(\n                dict(\n                    levelno=logging.ERROR,\n                    levelname='ERROR',\n                    msg=error_msg,\n                )\n            ))\n    \n    return configs, logs"]}
{"_id": "62ece4992e6aefcf4aabbd8b", "name": "force_string", "project": "awsteiner/o2sclpy", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 107766, "num_retrieved": 500, "inference_time": 25.27196805179119, "generate_results": ["def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj", "def force_string(obj):\n    \"\"\"\n    This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. \n    \"\"\"\n    if isinstance(obj, str):\n        return obj.encode('utf-8')\n    return obj"]}
{"_id": "62e60723d76274f8a4026b76", "name": "from_ticks", "project": "neo4j/neo4j-python-driver", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 184671, "num_retrieved": 500, "inference_time": 25.27196805179119, "generate_results": ["@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not 0 <= ticks < 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return cls(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\n    def from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not 0 <= ticks < 86400000000000:\n        raise ValueError(\"Ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not 0 <= ticks < 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return cls(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\ndef from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n    :param ticks: nanoseconds since midnight\n    :type ticks: int\n    :param tz: optional timezone\n    :type tz: datetime.tzinfo\n    :rtype: Time\n    :raises ValueError: if ticks is out of bounds (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not 0 <= ticks < 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    # Convert nanoseconds to seconds and nanoseconds\n    seconds = ticks // 1000000000\n    nanoseconds = ticks % 1000000000\n    \n    # Convert seconds to hours, minutes, and remaining seconds\n    hours, remainder = divmod(seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    \n    return cls(hours, minutes, seconds, nanoseconds, tzinfo=tz)", "@classmethod\n    def from_ticks(cls, ticks, tz=None):\n    \"\"\"\n    Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n    \"\"\"\n    if not 0 <= ticks < 86400000000000:\n        raise ValueError(\"ticks must be in range 0 <= ticks < 86400000000000\")\n    \n    seconds, nanoseconds = divmod(ticks, 1000000000)\n    minutes, seconds = divmod(seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    \n    return Time(hours, minutes, seconds, nanoseconds, tzinfo=tz)"]}
{"_id": "62e60873d76274f8a4026bd8", "name": "protocol_handlers", "project": "neo4j/neo4j-python-driver", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 200514, "num_retrieved": 500, "inference_time": 25.27196805179119, "generate_results": ["@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for version, handler in cls._HANDLERS.items():\n            if version == protocol_version:\n                handlers[version] = handler\n        return handlers\n    \n    # If no specific version is provided, return all supported versions\n    handlers = {}\n    for version, handler in cls._HANDLERS.items():\n        handlers[version] = handler\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for version, handler in cls._HANDLERS.items():\n            if version == protocol_version:\n                handlers[version] = handler\n        return handlers\n    \n    # Return all available handlers if no specific version is provided\n    handlers = {}\n    for version, handler in cls._HANDLERS.items():\n        handlers[version] = handler\n    return handlers", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for handler_class in cls._get_all_handlers():\n            if handler_class.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler_class\n                break\n        return handlers\n    \n    # Return all available handlers if no specific version is provided\n    handlers = {}\n    for handler_class in cls._get_all_handlers():\n        version = handler_class.PROTOCOL_VERSION\n        handlers[version] = handler_class\n    return handlers\n\ndef _get_all_handlers(self):\n    \"\"\"Helper method to get all protocol handler classes.\"\"\"\n    # This is a placeholder implementation - in a real scenario,\n    # this would return all the protocol handler classes defined\n    # in the class hierarchy.\n    return [\n        AsyncBolt4x2, AsyncBolt4x1, AsyncBolt4x4, AsyncBolt4x3,\n        AsyncBolt4x0, AsyncBolt3, AsyncBolt5x3, AsyncBolt5x2,\n        AsyncBolt5x1, AsyncBolt5x0, AsyncBolt4x2, AsyncBolt4x1,\n        AsyncBolt4x4, AsyncBolt4x3, AsyncBolt4x0, AsyncBolt3,\n        AsyncBolt5x3, AsyncBolt5x2, AsyncBolt5x1, AsyncBolt5x0,\n        AsyncBolt4x2, AsyncBolt4x1, AsyncBolt4x4, AsyncBolt4x3,\n        AsyncBolt4x0, Async", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the version is supported by any of the defined handlers\n        handlers = {}\n        for handler_class in cls._get_all_handlers():\n            if handler_class.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler_class\n                break\n        return handlers\n    \n    # Return all available handlers if no specific version is provided\n    handlers = {}\n    for handler_class in cls._get_all_handlers():\n        version = handler_class.PROTOCOL_VERSION\n        handlers[version] = handler_class\n    return handlers\n\ndef _get_all_handlers(self):\n    \"\"\"Helper method to get all protocol handler classes.\"\"\"\n    # This is a placeholder - in a real implementation, this would\n    # return all the protocol handler classes defined in the class\n    # (e.g., AsyncBolt4x1, AsyncBolt4x2, etc.)\n    return [\n        AsyncBolt4x1,\n        AsyncBolt4x2,\n        AsyncBolt4x3,\n        AsyncBolt4x4,\n        AsyncBolt5x1,\n        AsyncBolt5x2,\n        AsyncBolt5x3,\n        AsyncBolt3,\n        AsyncBolt4x0,\n        AsyncBolt5x0,\n        AsyncBolt5x4,\n        AsyncBolt4x0,\n        AsyncBolt4x1,\n        AsyncBolt4x2,\n        AsyncBolt4x3,\n        AsyncBolt4x4,\n        AsyncBolt5x1,\n        AsyncBolt5x2,\n        AsyncBolt5x3,\n        AsyncBolt5x4,\n        AsyncB", "@classmethod\ndef protocol_handlers(cls, protocol_version=None):\n    \"\"\"\n    Return a dictionary of available Bolt protocol handlers,\n    keyed by version tuple. If an explicit protocol version is\n    provided, the dictionary will contain either zero or one items,\n    depending on whether that version is supported. If no protocol\n    version is provided, all available versions will be returned.\n\n    :param protocol_version: tuple identifying a specific protocol\n        version (e.g. (3, 5)) or None\n    :return: dictionary of version tuple to handler class for all\n        relevant and supported protocol versions\n    :raise TypeError: if protocol version is not passed in a tuple\n    \"\"\"\n    if protocol_version is not None:\n        if not isinstance(protocol_version, tuple):\n            raise TypeError(\"protocol_version must be a tuple\")\n        \n        # Check if the version is supported by any of the known handlers\n        handlers = {}\n        for handler_class in cls._get_all_handlers():\n            if handler_class.PROTOCOL_VERSION == protocol_version:\n                handlers[protocol_version] = handler_class\n                break\n        return handlers\n    \n    # Return all supported protocol versions\n    handlers = {}\n    for handler_class in cls._get_all_handlers():\n        version = handler_class.PROTOCOL_VERSION\n        handlers[version] = handler_class\n    return handlers\n\ndef _get_all_handlers(self):\n    \"\"\"Helper method to get all protocol handlers from the class.\"\"\"\n    # This is a placeholder - in a real implementation, this would\n    # return all the protocol handler classes defined in the class\n    # This would typically be defined in the class itself or in a\n    # separate registry\n    return [\n        AsyncBolt4x2,\n        AsyncBolt4x1,\n        AsyncBolt4x0,\n        AsyncBolt3,\n        AsyncBolt5x3,\n        AsyncBolt5x2,\n        AsyncBolt5x1,\n        AsyncBolt5x0,\n        AsyncBolt4x3,\n        AsyncBolt4x4,\n        AsyncBolt4x4,\n        AsyncBolt4x3,\n        AsyncBolt4x2,\n        AsyncBolt4x1,\n        AsyncBolt4x0,\n        AsyncBolt3,\n        AsyncBolt5x3,\n        AsyncBolt5x2,\n        AsyncBolt5x1,\n        AsyncBolt5x0,\n        AsyncBolt4x3,\n        Async"]}
{"_id": "62e60e3bd76274f8a4026d1a", "name": "from_raw_values", "project": "neo4j/neo4j-python-driver", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 191108, "num_retrieved": 500, "inference_time": 25.27196805179119, "generate_results": ["@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if not values:\n        return cls()\n    \n    # Convert the iterable to a set to ensure uniqueness\n    raw_values = set(values)\n    \n    # Create and return a Bookmarks instance with the raw values\n    return cls(raw_values)", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if isinstance(values, Bookmarks):\n        return values\n    return cls(_bookmarks_to_set(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if isinstance(values, Bookmarks):\n        return values\n    return cls(_bookmarks_to_set(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if isinstance(values, Bookmarks):\n        return values\n    return cls(_bookmarks_to_set(values))", "@classmethod\ndef from_raw_values(cls, values):\n    \"\"\"\n    Create a Bookmarks object from a list of raw bookmark string values.\n\n    You should not need to use this method unless you want to deserialize\n    bookmarks.\n\n    :param values: ASCII string values (raw bookmarks)\n    :type values: Iterable[str]\n    \"\"\"\n    if isinstance(values, Bookmarks):\n        return values\n    return cls(_bookmarks_to_set(values))"]}
{"_id": "62b87b199a0c4fa8b80b354c", "name": "_get_seq_with_type", "project": "ynikitenko/lena", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 118829, "num_retrieved": 376, "inference_time": 25.27196805179119, "generate_results": ["def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.LenaSequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )", "def _get_seq_with_type(seq, bufsize=None):\n    \"\"\"\n    Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n    \"\"\"\n    if isinstance(seq, lena.core.Sequence):\n        return seq, \"sequence\"\n    elif isinstance(seq, lena.core.FillComputeSeq):\n        return seq, \"fill_compute\"\n    elif isinstance(seq, lena.core.FillRequestSeq):\n        return seq, \"fill_request\"\n    else:\n        raise exceptions.LenaTypeError(\n            \"unknown argument type. Must be one of \"\n            \"FillComputeSeq, FillRequestSeq or Source, \"\n            \"{} provided\".format(seq)\n        )"]}
{"_id": "62b87b4f9a0c4fa8b80b3581", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 116964, "num_retrieved": 378, "inference_time": 25.27196805179119, "generate_results": ["def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = integral(self.bins, self.edges)\n            else:\n                # If scale was not computed before, compute it\n                if self._scale is None:\n                    self._scale = integral(self.bins, self.edges)\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute the scaling factor\n        scale_factor = other / self._scale\n        # Apply scaling to bins\n        for i, bin_content in enumerate(self.bins):\n            self.bins[i] = bin_content * scale_factor\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale if not computed or recompute\n        if self._scale is None and not recompute:\n            # Scale not computed, compute it\n            self._scale = self.integral()\n        return self._scale\n    \n    # Rescale to other value\n    if self._scale == 0:\n        raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n    \n    # Compute current integral\n    current_integral = self.integral()\n    if current_integral == 0:\n        raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n    \n    # Scale factor\n    scale_factor = other / current_integral\n    \n    # Apply scale factor to bins\n    for i in range(len(self.bins)):\n        for j in range(len(self.bins[i])):\n            self.bins[i][j] *= scale_factor\n    \n    # Update scale\n    self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral()\n            else:\n                # If scale was not computed before, compute it\n                if self._scale is None:\n                    self._scale = self.integral()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute the scaling factor\n        scale_factor = other / self._scale\n        # Apply scaling to bins\n        for i in range(len(self.bins)):\n            for j in range(len(self.bins[i])):\n                self.bins[i][j] *= scale_factor\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral()\n            else:\n                # If scale was not computed before, compute it\n                if self._scale is None:\n                    self._scale = self.integral()\n        return self._scale\n    else:\n        # Rescale to the given value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute the scaling factor\n        scale_factor = other / self._scale\n        # Apply the scaling factor to the histogram bins\n        for i in range(len(self.bins)):\n            for j in range(len(self.bins[i])):\n                self.bins[i][j] *= scale_factor\n        self._scale = other", "def scale(self, other=None, recompute=False):\n    \"\"\"\n    Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n    \"\"\"\n    if other is None:\n        # Return scale of this histogram\n        if self._scale is None:\n            # Compute scale if not already computed\n            if recompute:\n                self._scale = self.integral()\n            else:\n                # If scale was not computed before, compute it\n                if self._scale is None:\n                    self._scale = self.integral()\n        return self._scale\n    else:\n        # Rescale to other value\n        if self._scale == 0:\n            raise lena.core.LenaValueError(\"Histogram with zero scale cannot be rescaled\")\n        # Compute the scaling factor\n        scale_factor = other / self._scale\n        # Apply scaling to bins\n        for i in range(len(self.bins)):\n            for j in range(len(self.bins[i])):\n                self.bins[i][j] *= scale_factor\n        self._scale = other"]}
{"_id": "62b87b519a0c4fa8b80b3583", "name": "scale", "project": "ynikitenko/lena", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 114494, "num_retrieved": 363, "inference_time": 25.27196805179119, "generate_results": ["def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        # Assuming the graph has a scale attribute or method\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    # Only rescale the last coordinate (e.g., y for 2D, z for 3D)\n    if self.dim == 2:\n        # For 2D graph, rescale the y coordinate\n        y_coords = self.coords[1]\n        y_errors = self.coords[2] if self.dim == 2 and hasattr(self, 'coords') and len(self.coords) > 2 else None\n        \n        # Scale the y coordinates\n        scale_factor = other\n        if hasattr(self, '_scale') and self._scale == 0:\n            raise lena.core.LenaValueError(\"Cannot rescale graph with zero scale\")\n            \n        # Apply scale to y coordinates\n        self.coords[1] = [val * scale_factor for val in y_coords]\n        \n        # Apply scale to y errors if they exist\n        if y_errors is not None:\n            self.coords[2] = [val * scale_factor for val in y_errors]\n            \n    elif self.dim == 3:\n        # For 3D graph, rescale the z coordinate\n        z_coords = self.coords[2]\n        z_errors = self.coords[3] if hasattr(self, 'coords') and len(self.coords) > 3 else None\n        \n        # Scale the z coordinates\n        scale_factor = other\n        if hasattr(self, '_scale') and self._scale == 0:\n            raise lena.core.LenaValueError(\"Cannot rescale graph with zero scale\")\n            \n        # Apply scale to z coordinates\n        self.coords[2]", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the current scale of the graph\n        # Assuming the graph has a scale attribute or method to get scale\n        return self._scale\n    \n    # Rescale the graph to the provided value\n    # Get the field names of the graph\n    field_names = self.field_names\n    \n    # Only rescale the last coordinate (e.g., y for 2D, z for 3D)\n    if len(field_names) > 0:\n        last_field = field_names[-1]\n        \n        # Get the coordinates and errors for the last field\n        # Assuming the graph has a method to get coordinates\n        coords = self.coords\n        if len(coords) > 0:\n            # Scale the last coordinate (and its errors if present)\n            # This is a simplified implementation - actual implementation\n            # would depend on the specific graph structure\n            for i in range(len(coords)):\n                # Scale the value of the last coordinate\n                # This would typically be done by modifying the coordinates\n                # and errors in the graph's internal representation\n                pass\n                \n            # Update the scale of the graph\n            self._scale = other\n    \n    # If the graph has unknown or zero scale, raise an error\n    if self._scale is None or self._scale == 0:\n        raise lena.core.LenaValueError(\n            \"could not rescale graph with unknown or zero scale\"\n        )\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        # Assuming scale is stored in a field or attribute\n        # In the context of the provided code, scale might be stored in the graph's context\n        # or in a specific attribute\n        return self._scale\n    \n    # If other is provided, rescale to that value\n    # Only the last coordinate is rescaled\n    # All errors are rescaled together with their coordinate\n    if self._scale is None or self._scale == 0:\n        raise lena.core.LenaValueError(\n            \"could not determine the scale of the graph\"\n        )\n    \n    # Scale the last coordinate and its errors\n    # Assuming the graph has a method to scale its coordinates\n    # This is a simplified implementation based on the docstring\n    # In a real implementation, this would interact with the graph's internal structure\n    self._scale = other\n    \n    # Scale the last coordinate and its errors\n    # This would typically be handled by the graph's internal scaling logic\n    # For now, we just update the scale attribute\n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        # Assuming the scale is stored in a field or property\n        # In the context of the provided code, scale might be stored in the graph's context\n        # or in a property that is not explicitly defined here\n        # For now, we return a placeholder value\n        return self._scale\n    \n    # If other is provided, rescale to that value\n    # Check if the graph has unknown or zero scale\n    if self._scale is None or self._scale == 0:\n        raise lena.core.LenaValueError(\n            \"graph has unknown or zero scale, cannot rescale\"\n        )\n    \n    # Rescale the graph to the provided value\n    # Only the last coordinate is rescaled\n    # All errors are rescaled together with their coordinate\n    # This is a simplified implementation - actual implementation would depend on the graph's internal structure\n    # and how scaling is handled in the underlying system\n    \n    # Assuming the graph has a method to scale its coordinates\n    # and that the last coordinate is the one to be scaled\n    # We would typically access the coordinates and scale them\n    # For example, if the graph has coordinates x, y, z, then y and z would be scaled\n    # But the exact implementation would depend on the graph's internal structure\n    \n    # For now, we just update the scale property\n    self._scale = other\n    \n    return self._scale", "def scale(self, other=None):\n    \"\"\"\n    Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n    \"\"\"\n    if other is None:\n        # Return the scale of this graph\n        # Assuming the graph has a scale attribute or method to get scale\n        return self._scale\n    \n    # Rescale to the provided value\n    # Get the last coordinate field (e.g., y for 2D, z for 3D)\n    field_names = self.field_names\n    if len(field_names) == 0:\n        raise lena.core.LenaValueError(\"graph has no field names\")\n    \n    # Get the last coordinate field\n    last_field = field_names[-1]\n    \n    # Scale the last coordinate and its error (if any)\n    # Assuming the graph has a method to scale a field\n    if hasattr(self, 'scale_field'):\n        # Scale the last field\n        self.scale_field(last_field, other)\n    else:\n        # Fallback: if no scale_field method, try to access scale directly\n        # and rescale the last coordinate\n        if self._scale is None:\n            raise lena.core.LenaValueError(\"graph has unknown scale\")\n        \n        # Rescale the last coordinate by multiplying by other\n        # This is a simplified implementation - actual implementation\n        # would depend on the graph's internal structure\n        pass\n    \n    # Update the scale attribute\n    self._scale = other\n    \n    return self._scale"]}
{"_id": "62b87b869a0c4fa8b80b35e1", "name": "hist_to_graph", "project": "ynikitenko/lena", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 117516, "num_retrieved": 395, "inference_time": 25.27196805179119, "generate_results": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n    \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    dim = len(edges)\n    \n    # Get bin coordinates based on get_coordinate\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(dim)]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(dim)]\n    elif get_coordinate == \"middle\":\n        bin_coords = [edges[i][len(edges[i]) // 2] for i in range(dim)]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right' or 'middle', got {get_coordinate}\")\n    \n    # Create points list\n    points = []\n    \n    # Iterate through bins\n    for i in range(len(edges[0]) - 1 if dim == 1 else len(edges[0]) - 1):\n        for j in range(len(edges[", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import copy\n    \n    # Default make_value is the bin content\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n    \n    # Validate get_coordinate\n    valid_coordinates = [\"left\", \"right\", \"middle\"]\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Get edges and bins from histogram\n    edges = hist.edges\n    bins = hist.bins\n    \n    # Determine the dimension of the histogram\n    dim = len(edges)\n    \n    # Initialize points list\n    points = []\n    \n    # Process each bin\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # Get bin content\n            bin_content = bins[i][j]\n            \n            # Get coordinate based on get_coordinate parameter\n            if dim == 1:\n                # For 1D histogram\n                if get_coordinate == \"left", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n        \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    dim = len(edges)\n    \n    # Get bin coordinates based on get_coordinate\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(dim)]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(dim)]\n    elif get_coordinate == \"middle\":\n        bin_coords = [edges[i][len(edges[i]) // 2] for i in range(dim)]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Create points for the graph\n    points = []\n    for i in range(len(edges[0]) - 1 if dim == 1 else len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    import copy\n    \n    # Default make_value is the bin content\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n    \n    # Validate get_coordinate\n    valid_coordinates = [\"left\", \"right\", \"middle\"]\n    if get_coordinate not in valid_coordinates:\n        raise ValueError(f\"get_coordinate must be one of {valid_coordinates}, got {get_coordinate}\")\n    \n    # Get bin edges and create coordinate points\n    edges = hist.edges\n    dim = len(edges)\n    \n    # Create points list\n    points = []\n    \n    # Process each bin\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # Get bin content\n            bin_content = hist.bins[i][j]\n            \n            # Get coordinate based on get_coordinate parameter\n            if dim == 1:\n                # For 1D histogram\n                if get_coordinate == \"left\":\n                    x = edges[0][i]\n                elif get", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"\n    Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n    \"\"\"\n    if make_value is None:\n        make_value = lambda bin_val: bin_val\n        \n    # Determine the coordinate based on get_coordinate\n    edges = hist.edges\n    dim = len(edges)\n    \n    # Get bin coordinates based on get_coordinate\n    if get_coordinate == \"left\":\n        bin_coords = [edges[i][0] for i in range(dim)]\n    elif get_coordinate == \"right\":\n        bin_coords = [edges[i][-1] for i in range(dim)]\n    elif get_coordinate == \"middle\":\n        bin_coords = [edges[i][len(edges[i]) // 2] for i in range(dim)]\n    else:\n        raise ValueError(f\"get_coordinate must be one of 'left', 'right', or 'middle', got {get_coordinate}\")\n    \n    # Create points list\n    points = []\n    \n    # Iterate through bins\n    for i in range(len(edges[0]) - 1):\n        for j in range(len(edges[1]) - 1):\n            # Get bin content\n            bin"]}
{"_id": "62b8b4baeb7e40a82d2d1136", "name": "_verify", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 99881, "num_retrieved": 469, "inference_time": 25.27196805179119, "generate_results": ["def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names():\n            if not hasattr(candidate, name):\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names():\n            method = getattr(candidate, name, None)\n            if method is not None and hasattr(method, 'getSignatureInfo'):\n                # Compare method signatures\n                method_sig = method.getSignatureInfo()\n                iface_sig = iface.getSignatureInfo()\n                # This is a simplified check - actual signature comparison would be more complex\n                if method_sig != iface_sig:\n                    errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        pass\n\n    # Step 4: Check attributes\n    try:\n        for name in iface.names():\n            attr = getattr(candidate, name, None)\n            if attr is", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, 'method', candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenImplementation(iface, name, candidate))\n                # Check signature (simplified - actual signature checking would be more complex)\n                if hasattr(desc, 'getSignatureInfo'):\n                    signature_info = desc.getSignatureInfo()\n                    # This is a simplified check - actual signature validation would be more comprehensive\n                    pass\n    except AttributeError:\n        pass\n\n    # Step 4: Check attributes\n    try:\n        for", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, 'method', candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenImplementation(iface, name, candidate))\n                # Check signature (simplified - actual signature checking would be more complex)\n                if hasattr(desc, 'getSignatureInfo'):\n                    method_sig = method.getSignatureInfo()\n                    desc_sig = desc.getSignatureInfo()\n                    if method_sig != desc_sig:\n                        errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        pass\n\n    # Step", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if not hasattr(candidate, name):\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, 'method', candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name, desc in iface.namesAndDescriptions(all=True):\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__call__'):\n                    errors.append(BrokenImplementation(iface, name, candidate))\n                # Check signature (simplified - actual signature checking would be more complex)\n                if hasattr(desc, 'getSignatureInfo'):\n                    signature_info = desc.getSignatureInfo()\n                    # This is a simplified check - actual signature validation would be more thorough\n                    if signature_info.get('positional') and not all(\n                        hasattr(method, '__call__') for _ in", "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check if candidate defines all necessary methods\n    try:\n        for name in iface.names():\n            if not hasattr(candidate, name):\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names():\n            if hasattr(candidate, name):\n                method = getattr(candidate, name)\n                if not hasattr(method, '__signature__'):\n                    # Check if method has correct signature using introspection\n                    try:\n                        sig = method.__code__.co_varnames\n                        # This is a simplified check - in practice, we'd need\n                        # more detailed signature comparison\n                        pass\n                    except AttributeError:\n                        errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 4: Check necessary attributes\n    try:\n        for name"]}
{"_id": "62b8b4baeb7e40a82d2d1137", "name": "verifyObject", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 99879, "num_retrieved": 469, "inference_time": 25.27196805179119, "generate_results": ["def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n            else:\n                # Check method signature\n                try:\n                    method_info = method.getSignatureInfo()\n                    if method_info['positional'] or method_info['required'] or method_info['optional']:\n                        # Verify method signature matches interface\n                        pass\n                except AttributeError:\n                    errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError as e:\n        errors.append(Invalid(\"Missing method in candidate\", candidate))\n\n    # Step 3: Check all necessary attributes\n    try:\n        for name in iface.names(all=True):\n            attr = getattr(candidate, name, None)\n            if attr is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError as e:\n        errors.append(Invalid(\"Missing attribute in candidate\", candidate))", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n            elif not hasattr(method, '__call__'):\n                # Check if method is a valid callable\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None:\n                # Check if method has correct signature\n                try:\n                    method_signature = method.getSignatureInfo()\n                    iface_method = iface.get(name)\n                    if iface_method is not None:\n                        # Compare method signatures\n                        if method_signature['positional'] != iface_method.positional:\n                            errors.append(BrokenImplementation(iface, name, candidate))", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n            elif not hasattr(method, '__call__'):\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None:\n                # Check if method has correct signature\n                try:\n                    method_info = method.getSignatureInfo()\n                    iface_method = iface.get(name)\n                    if iface_method is not None:\n                        # Compare method signatures\n                        if method_info['positional'] != iface_method.positional:\n                            errors.append(BrokenImplementation(iface, name, candidate))\n                except AttributeError:\n                    errors.append(BrokenImplementation(if", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n            elif not hasattr(method, '__call__'):\n                # Check if method is a valid callable\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError as e:\n        errors.append(Invalid(\"Attribute error during method verification: %s\" % str(e)))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None:\n                # Check if method has correct signature\n                try:\n                    method_signature = method.getSignatureInfo()\n                    iface_method = iface.get(name)\n                    if iface_method is not None:\n                        # Compare method signatures\n                        if method_signature['positional'] != iface_method.positional:\n                            errors.append(Invalid(\"", "def verifyObject(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n    \"\"\"\n    errors = []\n\n    # Step 1: Check if candidate claims to provide the interface\n    if not tentative:\n        if not iface.providedBy(candidate):\n            errors.append(DoesNotImplement(iface, candidate))\n\n    # Step 2: Check all necessary methods\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is None:\n                errors.append(BrokenImplementation(iface, name, candidate))\n            elif not isinstance(method, (FunctionType, MethodType)):\n                errors.append(BrokenImplementation(iface, name, candidate))\n    except AttributeError:\n        errors.append(BrokenImplementation(iface, name, candidate))\n\n    # Step 3: Check method signatures\n    try:\n        for name in iface.names(all=True):\n            method = getattr(candidate, name, None)\n            if method is not None:\n                try:\n                    method_sig = method.getSignatureInfo()\n                    iface_sig = iface.getSignatureInfo()\n                    if method_sig != iface_sig:\n                        errors.append(BrokenMethodImplementation(method, \"method signature mismatch\", method, iface))\n                except AttributeError:\n                    errors.append(BrokenMethodImplementation(method, \"method signature could not be determined\", method, iface))\n    except AttributeError:\n        pass"]}
{"_id": "62b8b4c1eb7e40a82d2d1139", "name": "verifyClass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 83123, "num_retrieved": 340, "inference_time": 25.27196805179119, "generate_results": ["def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate is a class\n    if not isinstance(candidate, type):\n        return False\n    \n    # Check if the candidate implements the interface\n    try:\n        # Use the implementedBy function to check if the class implements the interface\n        from zope.interface.declarations import implementedBy\n        spec = implementedBy(candidate)\n        \n        # Check if the interface is in the declared interfaces\n        if not spec:\n            return False\n            \n        # Check if the interface is in the specification\n        for interface in spec.interfaces():\n            if interface is iface:\n                return True\n                \n        return False\n        \n    except Exception:\n        return False", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate is a class\n    if not isinstance(candidate, type):\n        return False\n    \n    # Check if the candidate implements the interface\n    try:\n        from zope.interface import implementedBy\n        if not implementedBy(candidate).contains(iface):\n            return False\n    except ImportError:\n        # Fallback for environments without zope.interface\n        pass\n    \n    # If tentative is True, we allow for more lenient verification\n    if tentative:\n        # In tentative mode, we only check if the interface is in the class's bases\n        try:\n            from zope.interface import implementedBy\n            return iface in implementedBy(candidate)\n        except ImportError:\n            # Fallback for environments without zope.interface\n            return hasattr(candidate, '__bases__') and iface in candidate.__bases__\n    \n    # Otherwise, we require a strict verification\n    try:\n        from zope.interface import implementedBy\n        return iface in implementedBy(candidate)\n    except ImportError:\n        # Fallback for environments without zope.interface\n        return hasattr(candidate, '__bases__') and iface in candidate.__bases__", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate is a class\n    if not isinstance(candidate, type):\n        return False\n    \n    # Check if the candidate implements the interface\n    try:\n        from zope.interface import implementedBy\n        if not implementedBy(candidate).contains(iface):\n            return False\n    except ImportError:\n        # Fallback for environments without zope.interface\n        pass\n    \n    # If tentative is True, we allow for some flexibility in the verification\n    if tentative:\n        # In tentative mode, we only check if the candidate has a direct implementation\n        # of the interface, without strict inheritance checks\n        try:\n            from zope.interface import providedBy\n            return providedBy(candidate).contains(iface)\n        except ImportError:\n            pass\n    \n    return True", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate class implements the interface\n    try:\n        # Use the implementedBy function to check if the class implements the interface\n        if not hasattr(candidate, '__implemented__'):\n            # If no __implemented__ attribute, check if the class has a __provides__ attribute\n            if hasattr(candidate, '__provides__'):\n                # If __provides__ exists, it should contain the interface\n                if not hasattr(candidate.__provides__, '__bases__'):\n                    return False\n                # Check if the interface is in the bases of the provides specification\n                return any(iface.extends(b) for b in candidate.__provides__.__bases__)\n            else:\n                # No __provides__ attribute, so no interface is declared\n                return False\n        else:\n            # Check if the interface is in the implementedBy specification\n            implemented_spec = implementedBy(candidate)\n            if not hasattr(implemented_spec, '__bases__'):\n                return False\n            # Check if the interface is in the bases of the implemented specification\n            return any(iface.extends(b) for b in implemented_spec.__bases__)\n    except AttributeError:\n        # If any attribute is missing, the class cannot provide the interface\n        return False", "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    # Check if the candidate is a class\n    if not isinstance(candidate, type):\n        return False\n    \n    # Check if the candidate implements the interface\n    try:\n        # Use the implementedBy function to check if the class implements the interface\n        from zope.interface.declarations import implementedBy\n        spec = implementedBy(candidate)\n        # Check if the interface is in the declared interfaces\n        if not spec.extends(iface):\n            return False\n    except Exception:\n        return False\n    \n    # If tentative is True, we allow for more lenient checking\n    if tentative:\n        # In tentative mode, we only check if the interface is in the base classes\n        try:\n            for base in candidate.__bases__:\n                if iface.extends(base):\n                    return True\n        except Exception:\n            pass\n    \n    return True"]}
{"_id": "62b8b559eb7e40a82d2d11f6", "name": "determineMetaclass", "project": "pexip/os-zope", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 80344, "num_retrieved": 319, "inference_time": 25.27196805179119, "generate_results": ["def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base class has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass defined\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass is found, use the default metaclass\n    return type", "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"\n    Determine metaclass from 1+ bases and optional explicit __metaclass__\n    \"\"\"\n    if explicit_mc is not None:\n        return explicit_mc\n    \n    # If there are no bases, use the default metaclass\n    if not bases:\n        return type\n    \n    # Check if any base has a metaclass\n    for base in bases:\n        if hasattr(base, '__metaclass__'):\n            return base.__metaclass__\n    \n    # If no explicit metaclass found, use the default metaclass\n    return type"]}
{"_id": "62b8d22a48ba5a41d1c3f47d", "name": "pop", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 11633, "num_retrieved": 62, "inference_time": 25.27196805179119, "generate_results": ["def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default", "def pop(self, key, default=__marker):\n    \"\"\"\n    D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n    \"\"\"\n    try:\n        value = self[key]\n        del self[key]\n        return value\n    except KeyError:\n        if default is __marker:\n            raise KeyError(key)\n        return default"]}
{"_id": "62b8d23748ba5a41d1c3f497", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 9729, "num_retrieved": 47, "inference_time": 25.27196805179119, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n    try:\n        key = next(iter(self.__order))\n    except StopIteration:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23a48ba5a41d1c3f499", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 25.27196805179119, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23c48ba5a41d1c3f49b", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 8938, "num_retrieved": 45, "inference_time": 12.720145389437675, "generate_results": ["def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))", "def popitem(self):\n    \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n    try:\n        key = self.__choice(list(self))\n    except IndexError:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    else:\n        return (key, self.pop(key))"]}
{"_id": "62b8d23e48ba5a41d1c3f49e", "name": "popitem", "project": "pexip/os-python-cachetools", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 10009, "num_retrieved": 48, "inference_time": 12.720145389437675, "generate_results": ["def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    # Remove the key-value pair\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    # Remove the key-value pair\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    import random\n    keys = list(self.__data.keys())\n    key = random.choice(keys)\n    value = self.__data.pop(key)\n    del self.__size[key]\n    self.__currsize -= self.__size[key]\n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__)\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    # Remove the key-value pair\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)", "def popitem(self):\n    \"\"\"\n    Remove and return a random `(key, value)` pair.\n    \"\"\"\n    if not self:\n        raise KeyError('%s is empty' % type(self).__name__) from None\n    \n    # Get a random key from the cache\n    keys_list = list(self.__data.keys())\n    key = random.choice(keys_list)\n    \n    # Remove the key and return the (key, value) pair\n    value = self.__data.pop(key)\n    size = self.__size.pop(key)\n    self.__currsize -= size\n    \n    return (key, value)"]}
{"_id": "62b43425903eeb48555d3ea1", "name": "_create_in_regex", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 28011, "num_retrieved": 72, "inference_time": 12.720145389437675, "generate_results": ["def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a regular expression pattern to match in-style parameters\n    # This pattern should match the parameter style defined by the in_style\n    # attribute of the converter instance\n    \n    # The pattern will match:\n    # 1. Named parameters (e.g., :param1:, :param2:)\n    # 2. Numeric parameters (e.g., %1, %2, %3)\n    # 3. Escape sequences (e.g., \\escape)\n    \n    # Using the param_regex from the in_style to define the pattern\n    # The pattern is used to extract parameters from the SQL string\n    \n    import re\n    \n    # Create the pattern based on the in_style's param_regex\n    # This pattern will match the in-style parameter format\n    pattern = re.compile(self._in_style.param_regex)\n    \n    return pattern", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a pattern that matches in-style parameters\n    # This pattern should match the parameter style defined by self._in_style\n    # It will match either named parameters (e.g., :param1:) or ordinal parameters (e.g., %s)\n    pattern = self._in_style.param_regex\n    \n    # If the pattern is not provided, create a default one\n    if not pattern:\n        # Default pattern for named parameters\n        pattern = r':(?P<param>[a-zA-Z_][a-zA-Z0-9_]*)'\n    \n    # Compile the pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a pattern that matches in-style parameters\n    # This pattern should match the parameter style defined by self._in_style\n    # It will match either named parameters or numeric parameters based on the style\n    \n    # Base pattern for parameter matching\n    pattern = self._in_style.param_regex\n    \n    # Add escape sequence handling\n    escape_pattern = self._in_style.escape_regex\n    \n    # Create the full pattern that matches parameters\n    # This pattern will match either named or numeric parameters\n    # and handle escape sequences\n    full_pattern = f\"({escape_pattern}|{pattern})\"\n    \n    # Compile the pattern\n    return re.compile(full_pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a regular expression that matches in-style parameters\n    # This pattern should match escape sequences, named parameters, and numeric parameters\n    # based on the in-style defined in the converter\n    pattern = self._in_style.param_regex\n    \n    # If escape_char is provided, we need to ensure that escape sequences are properly handled\n    # The pattern should match the escape sequence followed by the parameter\n    if self._in_style.escape_char:\n        # Escape the escape character in the regex to prevent it from being matched as a literal\n        escape_char = self._in_style.escape_char\n        # Create a pattern that matches escape sequences\n        escape_pattern = f\"(?P<escape>{escape_char}(?P<param>[^{escape_char}]+))\"\n        # Combine with the main parameter pattern\n        pattern = f\"{escape_pattern}|{pattern}\"\n    \n    # Compile the pattern\n    return re.compile(pattern)", "def _create_in_regex(self) -> Pattern:\n    \"\"\"\n    Create the in-style parameter regular expression.\n\n    Returns the in-style parameter regular expression (:class:`re.Pattern`).\n    \"\"\"\n    # Create a regex pattern that matches in-style parameters\n    # This pattern should match the parameter style defined by self._in_style\n    # It will match named parameters, numeric parameters, and escape sequences\n    \n    # Base pattern for in-style parameters\n    pattern = self._in_style.param_regex\n    \n    # Add escape sequence handling\n    escape_regex = self._in_style.escape_regex\n    if escape_regex:\n        # Escape sequences are matched with the escape character\n        pattern = f\"({escape_regex})|{pattern}\"\n    \n    # Create the final pattern with proper grouping\n    return re.compile(pattern)"]}
{"_id": "62b43426903eeb48555d3ea2", "name": "_create_converter", "project": "cpburnz/python-sql-parameters", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 27900, "num_retrieved": 71, "inference_time": 12.720145389437675, "generate_results": ["def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the converter based on in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create appropriate converter based on style types\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NumericToNamedConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NumericToOrdinalConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter()\n    \n    # Default fallback\n    return _converting.Converter()", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the converter based on in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create appropriate converter based on style combinations\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NumericToNamedConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NumericToOrdinalConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter()\n    \n    # Default fallback\n    return _converting.Converter()", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the converter based on in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create appropriate converter based on style types\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NumericToNamedConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NumericToOrdinalConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter()\n    \n    # Default fallback\n    return _converting.Converter()", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Determine the converter based on in_style and out_style\n    in_style = self._in_style\n    out_style = self._out_style\n    \n    # Create appropriate converter based on style combinations\n    if isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NamedToNamedConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NamedToNumericConverter()\n    elif isinstance(in_style, _styles.NamedStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NamedToOrdinalConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NamedStyle):\n        return NumericToNamedConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.NumericStyle):\n        return NumericToNumericConverter()\n    elif isinstance(in_style, _styles.NumericStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return NumericToOrdinalConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NamedStyle):\n        return OrdinalToNamedConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.NumericStyle):\n        return OrdinalToNumericConverter()\n    elif isinstance(in_style, _styles.OrdinalStyle) and isinstance(out_style, _styles.OrdinalStyle):\n        return OrdinalToOrdinalConverter()\n    \n    # Default fallback\n    return _converting.Converter()", "def _create_converter(self) -> _converting._Converter:\n    \"\"\"\n    Create the parameter style converter.\n\n    Returns the parameter style converter (:class:`._converting._Converter`).\n    \"\"\"\n    # Based on the context, this function should create a converter instance\n    # using the style parameters defined in the class\n    return self._converter_class(\n        escape_char=self._escape_char,\n        expand_tuples=self._expand_tuples,\n        in_regex=self._in_regex,\n        in_style=self._in_style,\n        out_style=self._out_style\n    )"]}
{"_id": "62b8966c755ee91dce50a154", "name": "isoparse", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 56503, "num_retrieved": 217, "inference_time": 12.720145389437675, "generate_results": ["@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`", "@_takes_ascii\ndef isoparse(self, dt_str):\n    \"\"\"\n    Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n    An ISO-8601 datetime string consists of a date portion, followed\n    optionally by a time portion - the date and time portions are separated\n    by a single character separator, which is ``T`` in the official\n    standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n    combined with a time portion.\n\n    Supported date formats are:\n\n    Common:\n\n    - ``YYYY``\n    - ``YYYY-MM`` or ``YYYYMM``\n    - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n    Uncommon:\n\n    - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n    - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n    The ISO week and day numbering follows the same logic as\n    :func:`datetime.date.isocalendar`.\n\n    Supported time formats are:\n\n    - ``hh``\n    - ``hh:mm`` or ``hhmm``\n    - ``hh:mm:ss`` or ``hhmmss``\n    - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n    Midnight is a special case for `hh`, as the standard supports both\n    00:00 and 24:00 as a representation. The decimal separator can be\n    either a dot or a comma.\n\n\n    .. caution::\n\n        Support for fractional components other than seconds is part of the\n        ISO-8601 standard, but is not currently implemented in this parser.\n\n    Supported time zone offset formats are:\n\n    - `Z` (UTC)\n    - `HH:MM`\n    - `HHMM`\n    - `HH`\n\n    Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n    with the exception of UTC, which will be represented as\n    :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n    as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n    :param dt_str:\n        A string or stream containing only an ISO-8601 datetime string\n\n    :return:\n        Returns a :class:`"]}
{"_id": "62b896de755ee91dce50a183", "name": "parse", "project": "pexip/os-python-dateutil", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 50336, "num_retrieved": 190, "inference_time": 12.720145389437675, "generate_results": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is", "def parse(self, timestr, default=None,\n          ignoretz=False, tzinfos=None, **kwargs):\n    \"\"\"\n    Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is"]}
{"_id": "62b8a4a4755ee91dce50a3d3", "name": "fromutc", "project": "pexip/os-python-dateutil", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 53147, "num_retrieved": 194, "inference_time": 12.720145389437675, "generate_results": ["@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # The fold attribute indicates whether we're in the first (fold=0) \n        # or second (fold=1) occurrence of the ambiguous time\n        # We use the fold attribute to determine which one to return\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 0, we're in the first occurrence (earlier in time)\n        # If fold is 1, we're in the second occurrence (later in time)\n        # We return the appropriate fold\n        return enfold(dt, fold=fold)\n    \n    # If not ambiguous, simply return the datetime with the correct offset\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is None:\n        return dt\n    \n    # Determine if the datetime is ambiguous\n    is_ambiguous = self.is_ambiguous(dt)\n    \n    # If the datetime is ambiguous, we need to determine which fold to use\n    # The fold attribute indicates whether we are in the first (fold=0) or \n    # second (fold=1) occurrence of the ambiguous time\n    fold = getattr(dt, 'fold', 0)\n    \n    # If the datetime is not ambiguous, return the original datetime\n    if not is_ambiguous:\n        return dt.replace(tzinfo=self)\n    \n    # If the datetime is ambiguous, we need to determine which fold to use\n    # The fold attribute indicates whether we are in the first (fold=0) or \n    # second (fold=1) occurrence of the ambiguous time\n    # We use fold=0 for the earlier occurrence and fold=1 for the later occurrence\n    # Since we are converting from UTC, we need to determine which fold to use\n    # based on the time zone rules\n    \n    # Create a new datetime with the same time but in the target timezone\n    # and with the appropriate fold\n    if fold == 0:\n        # Use the earlier occurrence\n        new_dt = enfold(dt, fold=0)\n    else:\n        # Use the later occurrence\n        new_dt = enfold(dt, fold=1)\n    \n    # Return the new datetime with the target timezone\n    return new_dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # The fold attribute indicates whether we're in the first (fold=0) \n        # or second (fold=1) occurrence of the ambiguous time\n        # We use the fold value to determine which time to return\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence (after DST change)\n        # If fold is 0, we're in the first occurrence (before DST change)\n        # We return the appropriate fold\n        return enfold(dt, fold=fold)\n    \n    # If not ambiguous, simply convert to local time\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # The fold attribute indicates whether we're in the first or second occurrence\n        # of the ambiguous time\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 1, we're in the second occurrence (the one that comes after DST changes)\n        # If fold is 0, we're in the first occurrence (the one that comes before DST changes)\n        # We need to return the appropriate fold\n        return enfold(dt, fold=fold)\n    \n    # If not ambiguous, just return the datetime with the correct timezone\n    return dt.replace(tzinfo=self)", "@_validate_fromutc_inputs\ndef fromutc(self, dt):\n    \"\"\"\n    Given a timezone-aware datetime in a given timezone, calculates a\n    timezone-aware datetime in a new timezone.\n\n    Since this is the one time that we *know* we have an unambiguous\n    datetime object, we take this opportunity to determine whether the\n    datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n    occurrence, chronologically, of the ambiguous datetime).\n\n    :param dt:\n        A timezone-aware :class:`datetime.datetime` object.\n    \"\"\"\n    # If the datetime is already in UTC, return it as is\n    if dt.tzinfo is UTC:\n        return dt\n    \n    # Check if the datetime is ambiguous\n    if self.is_ambiguous(dt):\n        # For ambiguous times, we need to determine which fold to use\n        # The fold attribute indicates whether we're in the first (fold=0) \n        # or second (fold=1) occurrence of the ambiguous time\n        # We use the fold value to determine which time to return\n        fold = getattr(dt, 'fold', 0)\n        # If fold is 0, we return the first occurrence (standard time)\n        # If fold is 1, we return the second occurrence (daylight saving time)\n        # This is consistent with the behavior of the timezone-aware datetime\n        # when converting from UTC\n        return enfold(dt, fold=fold)\n    \n    # If not ambiguous, simply convert to the local timezone\n    return dt.replace(tzinfo=self)"]}
{"_id": "62b8a7b2755ee91dce50a4a7", "name": "default_tzinfo", "project": "pexip/os-python-dateutil", "level": "self_contained", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 60209, "num_retrieved": 230, "inference_time": 12.720145389437675, "generate_results": ["def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt", "def default_tzinfo(dt, tzinfo):\n    \"\"\"\n    Sets the tzinfo parameter on naive datetimes only\n\n    This is useful for example when you are provided a datetime that may have\n    either an implicit or explicit time zone, such as when parsing a time zone\n    string.\n\n    .. doctest::\n\n        >>> from dateutil.tz import tzoffset\n        >>> from dateutil.parser import parse\n        >>> from dateutil.utils import default_tzinfo\n        >>> dflt_tz = tzoffset(\"EST\", -18000)\n        >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n        2014-01-01 12:30:00+00:00\n        >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n        2014-01-01 12:30:00-05:00\n\n    :param dt:\n        The datetime on which to replace the time zone\n\n    :param tzinfo:\n        The :py:class:`datetime.tzinfo` subclass instance to assign to\n        ``dt`` if (and only if) it is naive.\n\n    :return:\n        Returns an aware :py:class:`datetime.datetime`.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tzinfo)\n    return dt"]}
{"_id": "6305f9991d275c6667163c50", "name": "set_cut_chars", "project": "MozillaSecurity/lithium", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 19049, "num_retrieved": 67, "inference_time": 12.720145389437675, "generate_results": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n    \"\"\"Set the bytes used to delimit slice points.\"\"\"\n    self._cut_before = before\n    self._cut_after = after"]}
{"_id": "6306292052e177c0ba469f09", "name": "identify_request", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 107425, "num_retrieved": 382, "inference_time": 12.720145389437675, "generate_results": ["def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check for legacy payload (public message)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check for private message\n    if request.headers.get('X-Diaspora-Private'):\n        return \"diaspora\"\n    \n    # Check for public message in body\n    body = request.body\n    if body and b\"public\" in body:\n        return \"diaspora\"\n    \n    # Check for private message in body\n    if body and b\"private\" in body:\n        return \"diaspora\"\n    \n    # Check for legacy payload in URL\n    url = request.url\n    if url and \"diaspora\" in url:\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check for legacy payload (public message)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check for private message\n    if request.headers.get('X-Diaspora-Private-Message'):\n        return \"diaspora\"\n    \n    # Check for public message in body\n    body = request.body\n    if body and b\"public\" in body:\n        return \"diaspora\"\n    \n    # Check for private message in body\n    if body and b\"private\" in body:\n        return \"diaspora\"\n    \n    # Check for Diaspora-specific headers\n    if request.headers.get('X-Diaspora-Handle'):\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message (private payload)\n    if request.headers.get('X-Diaspora-Private-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a legacy payload (public message with specific headers)\n    if request.headers.get('X-Diaspora-Message-Type') == 'public':\n        return \"diaspora\"\n    \n    # Check if the request has a specific content type for Diaspora\n    content_type = request.headers.get('Content-Type', '')\n    if 'application/diaspora+xml' in content_type or 'application/diaspora' in content_type:\n        return \"diaspora\"\n    \n    return None", "def identify_request(request: RequestType):\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (Diaspora public message)\n    if request.headers.get('Diaspora-Message'):\n        return \"diaspora\"\n    \n    # Check if the request has a private message (Diaspora private message)\n    if request.headers.get('Diaspora-Private-Message'):\n        return \"diaspora\"\n    \n    # Check if the request is a legacy payload (Diaspora legacy format)\n    if request.body.startswith(b\"<XML>\"):\n        return \"diaspora\"\n    \n    # If none of the above, return None\n    return None", "def identify_request(request: RequestType) -> Optional[str]:\n    \"\"\"\n    Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Check if the request has a public message (legacy payload)\n    if request.headers.get('X-Diaspora-Message') or request.headers.get('X-Diaspora-Private-Message'):\n        # Check for legacy Diaspora message headers\n        if request.headers.get('X-Diaspora-Message'):\n            return \"diaspora\"\n    \n    # Check if the request body contains a Diaspora-specific XML structure\n    body = request.body\n    if body and isinstance(body, str):\n        # Look for XML structure that indicates a Diaspora message\n        if '<XML' in body or '<status_message' in body or '<comment' in body or '<profile' in body:\n            return \"diaspora\"\n    \n    # Check for private message headers\n    if request.headers.get('X-Diaspora-Private-Message'):\n        return \"diaspora\"\n    \n    return None"]}
{"_id": "6306292152e177c0ba469f0d", "name": "identify_request", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 94313, "num_retrieved": 297, "inference_time": 12.720145389437675, "generate_results": ["def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    if request.url.startswith('https://matrix.') or request.url.startswith('http://matrix.'):\n        return True\n    \n    # Check for Matrix-specific headers\n    if request.headers.get('X-Matrix-Server') or request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check for Matrix-specific methods\n    if request.method in ['POST', 'PUT', 'DELETE']:\n        # Matrix requests often use these methods for operations\n        # Check if the request body contains matrix-specific content\n        body = request.body\n        if body and ('matrix' in body.lower() or 'm.' in body.lower()):\n            return True\n    \n    # Check for Matrix-specific query parameters\n    query_params = request.url.split('?')[1] if '?' in request.url else ''\n    if query_params:\n        if 'matrix' in query_params.lower() or 'm.' in query_params.lower():\n            return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    if request.url.startswith('https://matrix.') or request.url.startswith('http://matrix.'):\n        return True\n    \n    # Check for Matrix-specific headers\n    if request.headers.get('X-Matrix-Server') or request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check for Matrix-specific methods\n    if request.method in ['POST', 'PUT', 'DELETE']:\n        # Matrix requests often use these methods for operations\n        # Check if the request body contains Matrix-specific content\n        if request.body and b'matrix' in request.body:\n            return True\n    \n    # Check for Matrix-specific path patterns\n    path = request.url.split('/')[-1]\n    if path in ['_matrix', 'matrix', 'client', 'server']:\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    if request.url.startswith('https://matrix.') or request.url.startswith('http://matrix.'):\n        return True\n    \n    # Check for Matrix-specific headers\n    if request.headers.get('X-Matrix-Server') or request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check for Matrix-specific methods\n    if request.method in ['POST', 'PUT', 'DELETE']:\n        # Matrix requests often use these methods for operations\n        return True\n    \n    # Check for Matrix-specific content types\n    content_type = request.headers.get('Content-Type', '')\n    if 'application/matrix' in content_type or 'application/x-matrix' in content_type:\n        return True\n    \n    # Check for Matrix-specific query parameters\n    query_params = request.url.split('?')[1] if '?' in request.url else ''\n    if 'matrix' in query_params or 'm.' in query_params:\n        return True\n    \n    # Check for Matrix-specific paths\n    path = request.url.split('/')[-1]\n    if path.startswith('matrix') or path.startswith('m.'):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    url = request.url\n    if not url:\n        return False\n    \n    # Matrix requests typically have specific paths like /_matrix/client/r0\n    matrix_paths = [\n        '/_matrix/client/r0',\n        '/_matrix/client/api/v1',\n        '/_matrix/federation/v1',\n        '/_matrix/media/r0'\n    ]\n    \n    # Check if the URL contains any matrix paths\n    for path in matrix_paths:\n        if path in url:\n            return True\n    \n    # Check if the request method is POST and the URL contains a matrix endpoint\n    if request.method == 'POST' and any(path in url for path in matrix_paths):\n        return True\n    \n    # Check if the request headers contain matrix-related information\n    headers = request.headers\n    if headers.get('X-Matrix-Server') or headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check if the request body contains matrix-specific content\n    body = request.body\n    if body and ('matrix' in body or 'homeserver' in body or 'appservice' in body):\n        return True\n    \n    return False", "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # Check if the request URL contains matrix-related paths or domains\n    if request.url.startswith('https://matrix.') or request.url.startswith('http://matrix.'):\n        return True\n    \n    # Check for Matrix-specific headers\n    if request.headers.get('X-Matrix-Server') or request.headers.get('X-Matrix-Client'):\n        return True\n    \n    # Check for Matrix-specific methods\n    if request.method in ['POST', 'PUT', 'DELETE']:\n        # Matrix requests often use these methods for operations\n        # Check if the request body contains Matrix-specific content\n        body = request.body\n        if body and ('matrix' in body.lower() or 'm.' in body.lower()):\n            return True\n    \n    # Check for Matrix-specific query parameters\n    query_params = request.url.split('?')[1] if '?' in request.url else ''\n    if query_params:\n        if 'matrix' in query_params.lower() or 'm.' in query_params.lower():\n            return True\n    \n    return False"]}
{"_id": "6306292252e177c0ba469f11", "name": "format_dt", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 93957, "num_retrieved": 298, "inference_time": 12.720145389437675, "generate_results": ["def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if isinstance(dt, datetime.datetime):\n        return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(dt, datetime.date):\n        return dt.strftime(\"%Y-%m-%d\")\n    else:\n        # Fallback for other types\n        return str(dt)", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if dt is None:\n        return None\n    # Format as ISO 8601 with timezone offset\n    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")", "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    if isinstance(dt, datetime.datetime):\n        return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(dt, datetime.date):\n        return dt.strftime(\"%Y-%m-%d\")\n    else:\n        # Fallback for other types\n        return str(dt)"]}
{"_id": "6306292352e177c0ba469f1d", "name": "find_tags", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 113699, "num_retrieved": 426, "inference_time": 12.720145389437675, "generate_results": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the code block and any following lines until the closing ``` \n            # We'll skip the entire code block\n            continue\n            \n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        # We'll skip the entire code block\n        if line.strip().startswith('```'):\n            # Skip the code block and any following lines until the closing ``` \n            # We'll skip the entire code block\n            continue\n            \n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        # We'll skip the entire code block\n        if line.strip().startswith('```'):\n            # Skip the code block and any following lines until the closing ``` \n            # We'll skip the entire code block\n            continue\n            \n        # Find tags in the line\n        # Use regex to find tags (words starting with #)\n        # We need to skip tags inside code blocks\n        # We'll use a simple approach: find all # followed by word characters\n        import re\n        tag_matches = re.finditer(r'#([a-zA-Z0-9_]+)', line)\n        \n        for match in tag_matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag with the result of the replacer function\n            if replacer:\n                replacement = replacer(tag)\n                result = result.replace(f'#{tag}', replacement)\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    # Split text into lines\n    lines = text.splitlines()\n    in_code_block = False\n    code_block_type = None\n    \n    for line in lines:\n        # Check if line starts with code block markers\n        if line.strip().startswith('```'):\n            in_code_block = not in_code_block\n            if in_code_block:\n                code_block_type = line.strip()[3:].strip()\n            else:\n                code_block_type = None\n            continue\n            \n        # If in code block, skip processing\n        if in_code_block:\n            continue\n            \n        # Process line for tags\n        # Remove any existing tags (e.g., #tag) and extract them\n        # Use regex to find tags that are not inside code blocks\n        tag_pattern = r'(?<!\\w)#([a-zA-Z0-9_]+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # Replace the tag with the result of the replacer function if provided\n            if replacer:\n                replacement = replacer(tag)\n                result = result.replace(f'#{tag}', replacement)\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Code block starts, skip until we find the closing ``` line\n            code_block_start = i\n            code_block_end = -1\n            for j in range(i + 1, len(lines)):\n                if lines[j].strip().startswith('```'):\n                    code_block_end = j\n                    break\n            if code_block_end != -1:\n                # Skip the entire code block\n                continue\n                \n        # Check for tags (words starting with #)\n        # We need to handle the case where tags might be inside code blocks\n        # by not processing them if they're inside a code block\n        # We'll use a simple approach: look for # followed by non-whitespace\n        # and ensure we're not in a code block\n        \n        # Find all tag matches\n        tag_matches = []\n        start = 0\n        while True:\n            pos = line.find('#', start)\n            if pos == -1:\n                break\n            # Check if this is inside a code block\n            if i == code_block_start:\n                # Skip if we're in a code block\n                start = pos + 1\n                continue\n                \n            # Check if the tag is followed by non-whitespace\n            end = pos + 1\n            while end < len(line) and line[end] not in ' \\t\\n':\n                end += 1\n            tag = line[pos+1:end]\n            \n            # Add to tags set\n            tags.add(tag)\n            \n            # Replace the tag in the result text\n            if replacer:\n                replacement = replacer(tag)\n                result = result.replace(f\"#{tag}\", replacement)\n            \n            start = end\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Skip the code block and all subsequent lines until the closing ``` \n            # We'll handle the code block by skipping it\n            # Find the closing ``` for this block\n            code_block_end = -1\n            for j in range(i + 1, len(lines)):\n                if lines[j].strip().startswith('```'):\n                    code_block_end = j\n                    break\n            if code_block_end != -1:\n                # Skip the code block\n                lines = lines[:i] + lines[code_block_end + 1:]\n                continue\n        \n        # Remove code block markers\n        line = line.strip()\n        \n        # Find tags in the line\n        # Tags are in the format #tag\n        # We need to handle the case where tags might be inside code blocks\n        # We'll use a regex to find tags\n        import re\n        tag_pattern = r'#([a-zA-Z0-9_]+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If a replacer is provided, replace the tag\n            if replacer:\n                replacement = replacer(tag)\n                result = result.replace(f'#{tag}', replacement)\n    \n    return tags, result", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"\n    Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    tags = set()\n    result = text\n    \n    # Split text into lines\n    lines = text.splitlines()\n    \n    # Process each line\n    for i, line in enumerate(lines):\n        # Check if line is inside a code block (starts with ``` or ``` followed by language)\n        if line.strip().startswith('```'):\n            # Toggle code block state\n            in_code_block = not in_code_block\n            continue\n            \n        # If inside a code block, skip processing\n        if in_code_block:\n            continue\n            \n        # Find tags in the line\n        # Use regex to find tags (e.g., #tag)\n        # Tags are ignored if they are inside code blocks\n        tag_pattern = r'#([a-zA-Z0-9_]+)'\n        matches = re.finditer(tag_pattern, line)\n        \n        for match in matches:\n            tag = match.group(1)\n            tags.add(tag)\n            \n            # If replacer is provided, replace the tag with the result of the replacer function\n            if replacer:\n                result = result.replace(f'#{tag}', replacer(tag))\n    \n    return tags, result"]}
{"_id": "6306292352e177c0ba469f1e", "name": "process_text_links", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 88754, "num_retrieved": 280, "inference_time": 12.720145389437675, "generate_results": ["def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    import re\n    \n    # First, replace any existing links with a placeholder to avoid double processing\n    # This handles links like http://example.com or https://example.com\n    # We'll use a regex to find URLs and replace them with a placeholder\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    placeholder_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)'\n    \n    # Replace URLs with a placeholder\n    text_with_placeholders = re.sub(placeholder_pattern, '[[URL_PLACEHOLDER]]', text)\n    \n    # Process the text to linkify URLs\n    # This will convert URLs like http://example.com to <a href=\"http://example.com\">http://example.com</a>\n    # We need to handle both http and https\n    processed_text = re.sub(\n        r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\s]+)',\n        lambda match: f'<a href=\"{match.group(1)}\" target=\"_blank\" rel=\"noopener noreferrer\">{match.group(1)}</a>',\n        text\n    )\n    \n    # Replace the placeholder back with the processed links\n    # This is a simplified version - in reality, we'd need to ensure we don't double-process\n    # For now, we just return the processed text\n    return processed_text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs with HTML links\n    import re\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)\\-]+)'\n    # Replace with HTML link\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with HTML links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace mentions (e.g. @username) with HTML links\n    mention_pattern = r'@([a-zA-Z0-9_]+)'\n    text = re.sub(mention_pattern, r'<a class=\"h-card\" href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs with HTML links\n    import re\n    # Match URLs (including http/https, and with optional www)\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)]+|[a-zA-Z0-9][a-zA-Z0-9-]*\\.[a-zA-Z0-9]+(?:\\.[a-zA-Z0-9]+)*)'\n    \n    def linkify(match):\n        url = match.group(0)\n        # Check if it's a valid URL\n        if url.startswith('http'):\n            # Add attributes for link\n            return f'<a href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">{url}</a>'\n        else:\n            # For non-HTTP links, just return as is\n            return url\n    \n    # Replace URLs with linkified versions\n    result = re.sub(url_pattern, linkify, text)\n    \n    # Also handle email addresses\n    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n    result = re.sub(email_pattern, \n                    lambda m: f'<a href=\"mailto:{m.group(0)}\" target=\"_blank\" rel=\"noopener noreferrer\">{m.group(0)}</a>', \n                    result)\n    \n    return result", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs with HTML links\n    import re\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)]+)' \n    # Replace with HTML link\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with HTML links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    return text", "def process_text_links(text):\n    \"\"\"\n    Process links in text, adding some attributes and linkifying textual links.\n    \"\"\"\n    # Replace URLs with HTML links\n    import re\n    # Match URLs that start with http:// or https://\n    url_pattern = r'(https?://[^\\s<>\"{}|\\\\^`{}\\[\\]\\(\\)]+)'\n    # Replace with HTML link\n    text = re.sub(url_pattern, r'<a href=\"\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    # Replace email addresses with HTML links\n    email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'\n    text = re.sub(email_pattern, r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>', text)\n    \n    return text"]}
{"_id": "6306292652e177c0ba469f34", "name": "fetch_content_type", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 83747, "num_retrieved": 322, "inference_time": 12.720145389437675, "generate_results": ["def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None", "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, timeout=60)\n        if response.status_code == 200:\n            return response.headers.get('Content-Type')\n    except Exception as ex:\n        logger.warning(\"fetch_content_type - failed to fetch content type for %s: %s\", url, ex)\n    return None"]}
{"_id": "6306292a52e177c0ba469f41", "name": "test_tag", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 82020, "num_retrieved": 257, "inference_time": 12.720145389437675, "generate_results": ["def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Remove any non-alphanumeric characters except for underscores and hyphens\n    cleaned_tag = tag.strip().replace('#', '').replace('@', '')\n    \n    # Tags should only contain alphanumeric characters, underscores, and hyphens\n    if not cleaned_tag.isalnum() and not all(c.isalnum() or c in ['_', '-'] for c in cleaned_tag):\n        return False\n    \n    # Tags should not start or end with a hyphen or underscore\n    if cleaned_tag.startswith('-') or cleaned_tag.startswith('_') or \\\n       cleaned_tag.endswith('-') or cleaned_tag.endswith('_'):\n        return False\n    \n    # Tags should be at least 1 character long\n    if len(cleaned_tag) < 1:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should:\n    # 1. Start with a letter or underscore\n    # 2. Contain only letters, numbers, underscores, and hyphens\n    # 3. Not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Check if tag starts with a letter or underscore\n    if not (tag[0].isalpha() or tag[0] == '_'):\n        return False\n    \n    # Check if tag contains only valid characters\n    for char in tag:\n        if not (char.isalnum() or char in ['_', '-']):\n            return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should:\n    # 1. Start with a letter or underscore\n    # 2. Contain only letters, numbers, underscores, and hyphens\n    # 3. Not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Check if tag starts with a letter or underscore\n    if not (tag[0].isalpha() or tag[0] == '_'):\n        return False\n    \n    # Check if tag contains only valid characters\n    for char in tag:\n        if not (char.isalnum() or char in ['_', '-']):\n            return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # Tags should not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Tags should not contain spaces or special characters (except for hashtags)\n    # Remove any non-alphanumeric characters except for underscores and hyphens\n    cleaned_tag = tag.strip().replace('#', '').replace('@', '')\n    \n    # Tags should only contain alphanumeric characters, underscores, and hyphens\n    if not cleaned_tag.isalnum() and not all(c.isalnum() or c in ['_', '-'] for c in cleaned_tag):\n        return False\n    \n    # Tags should not start or end with a hyphen or underscore\n    if cleaned_tag.startswith('-') or cleaned_tag.startswith('_') or \\\n       cleaned_tag.endswith('-') or cleaned_tag.endswith('_'):\n        return False\n    \n    # Tags should be at least 1 character long after cleaning\n    if len(cleaned_tag) == 0:\n        return False\n    \n    # Tags should not be too long (e.g., more than 20 characters)\n    if len(tag) > 20:\n        return False\n    \n    return True", "def test_tag(tag: str) -> bool:\n    \"\"\"\n    Test a word whether it could be accepted as a tag.\n    \"\"\"\n    # A valid tag should:\n    # 1. Start with a letter or underscore\n    # 2. Contain only letters, numbers, underscores, and hyphens\n    # 3. Not be empty\n    if not tag or not tag.strip():\n        return False\n    \n    # Check if it starts with a letter or underscore\n    if not (tag[0].isalpha() or tag[0] == '_'):\n        return False\n    \n    # Check if it contains only valid characters\n    for char in tag:\n        if not (char.isalnum() or char in ['_', '-']):\n            return False\n    \n    return True"]}
{"_id": "6306298b52e177c0ba469fdc", "name": "xml_children_as_dict", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 107219, "num_retrieved": 406, "inference_time": 18.676797091960907, "generate_results": ["def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    children_dict = {}\n    for child in node:\n        children_dict[child.tag] = child.text\n    return children_dict", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    children_dict = {}\n    for child in node:\n        children_dict[child.tag] = child.text\n    return children_dict", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    children_dict = {}\n    for child in node:\n        children_dict[child.tag] = child.text\n    return children_dict", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    result = {}\n    for child in node:\n        result[child.tag] = child.text\n    return result", "def xml_children_as_dict(node):\n    \"\"\"\n    Turn the children of node <xml> into a dict, keyed by tag name.\n\n    This is only a shallow conversation - child nodes are not recursively processed.\n    \"\"\"\n    children_dict = {}\n    for child in node:\n        children_dict[child.tag] = child.text\n    return children_dict"]}
{"_id": "6306299052e177c0ba469fe8", "name": "check_sender_and_entity_handle_match", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 102132, "num_retrieved": 378, "inference_time": 18.676797091960907, "generate_results": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles to ensure consistent comparison\n    sender_normalized = sender_handle.lower().strip()\n    entity_normalized = entity_handle.lower().strip()\n    \n    # Check if the handles match\n    return sender_normalized == entity_normalized", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if sender_handle is None or entity_handle is None:\n        return False\n    \n    # Normalize handles to ensure consistent comparison\n    sender_normalized = sender_handle.lower().strip()\n    entity_normalized = entity_handle.lower().strip()\n    \n    # Check if the handles match\n    return sender_normalized == entity_normalized", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if not sender_handle or not entity_handle:\n        return False\n    \n    # Normalize handles to ensure consistent comparison\n    sender_normalized = sender_handle.lower().strip()\n    entity_normalized = entity_handle.lower().strip()\n    \n    # Check if the handles match exactly\n    return sender_normalized == entity_normalized", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if not sender_handle or not entity_handle:\n        return False\n    \n    # Normalize handles to ensure consistent comparison\n    sender_normalized = sender_handle.lower().strip()\n    entity_normalized = entity_handle.lower().strip()\n    \n    # Check if the handles match exactly\n    return sender_normalized == entity_normalized", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n    \"\"\"\n    Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.\n    \"\"\"\n    if not sender_handle or not entity_handle:\n        return False\n    \n    # Normalize handles by removing any trailing or leading whitespace\n    sender_handle = sender_handle.strip()\n    entity_handle = entity_handle.strip()\n    \n    # Compare the handles directly\n    return sender_handle == entity_handle"]}
{"_id": "630629b952e177c0ba46a043", "name": "get_nodeinfo_well_known_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 111389, "num_retrieved": 442, "inference_time": 18.676797091960907, "generate_results": ["def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    doc = {\n        \"version\": \"1.0\",\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/links/1.0\",\n                \"href\": url + path\n            }\n        ]\n    }\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    doc = {\n        \"version\": \"1.0\",\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/links\",\n                \"href\": url + path\n            }\n        ],\n        \"software\": {\n            \"name\": \"Diaspora\",\n            \"version\": \"unknown\"\n        },\n        \"metadata\": {\n            \"nodeName\": url.split(\"//\")[1].split(\"/\")[0],\n            \"adminAccount\": None\n        }\n    }\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    doc = {\n        \"version\": \"1.0\",\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/links/1.0\",\n                \"href\": url + path\n            }\n        ]\n    }\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    doc = {\n        \"version\": \"1.0\",\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/links/1.0\",\n                \"href\": url + path\n            }\n        ]\n    }\n    \n    # Add optional metadata if needed\n    if url:\n        doc[\"metadata\"] = {\n            \"nodeName\": url.split(\"//\")[1].split(\"/\")[0],\n            \"nodeDescription\": \"A Diaspora-compatible node\"\n        }\n    \n    return doc", "def get_nodeinfo_well_known_document(url, document_path=None):\n    \"\"\"\n    Generate a NodeInfo .well-known document.\n\n    See spec: http://nodeinfo.diaspora.software\n\n    :arg url: The full base url with protocol, ie https://example.com\n    :arg document_path: Custom NodeInfo document path if supplied (optional)\n    :returns: dict\n    \"\"\"\n    # Default path if not provided\n    path = document_path or '/.well-known/nodeinfo'\n    \n    # Create the base document structure\n    doc = {\n        \"version\": \"1.0\",\n        \"links\": [\n            {\n                \"rel\": \"http://nodeinfo.diaspora.software/links\",\n                \"href\": url + path\n            }\n        ]\n    }\n    \n    # Add optional metadata if needed\n    if url:\n        doc[\"metadata\"] = {\n            \"nodeName\": url.split(\"//\")[1].split(\"/\")[0],\n            \"nodeDescription\": \"A Diaspora-compatible node\"\n        }\n    \n    return doc"]}
{"_id": "630629d052e177c0ba46a0a1", "name": "verify_relayable_signature", "project": "jaywink/federation", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 74034, "num_retrieved": 236, "inference_time": 18.676797091960907, "generate_results": ["def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise SignatureVerificationError(\"No data element found in the document\")\n            \n        data_text = data.text\n        \n        # Create the signature content (data + metadata)\n        sig_contents = f\"{data_text}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Import the public key\n        key = RSA.importKey(public_key)\n        \n        # Create a PKCS1 v1.5 verifier\n        verifier = pkcs1_15.new(key)\n        \n        # Decode the signature\n        sig_bytes = urlsafe_b64decode(signature)\n        \n        # Verify the signature\n        verifier.verify(sig_hash, sig_bytes)\n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise SignatureVerificationError(\"No data element found in the document\")\n            \n        data_text = data.text\n        \n        # Create the signature content\n        sig_contents = f\"{data_text}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Verify the signature using the public key\n        cipher = PKCS1_v1_5.new(RSA.importKey(public_key))\n        if not cipher.verify(sig_hash, urlsafe_b64decode(signature)):\n            raise SignatureVerificationError(\"Signature verification failed\")\n            \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = \"\"\n        # The data to sign is the XML content with specific elements\n        # In Diaspora protocol, the data is typically the XML content with specific tags\n        # We need to extract the relevant data for signing\n        for elem in root.iter():\n            if elem.tag == \"{http://salmon-protocol.org/ns/magic-env}data\":\n                data = elem.text or \"\"\n                break\n        \n        if not data:\n            raise SignatureVerificationError(\"No data found in the document to verify signature\")\n        \n        # Create the signature content (data + metadata)\n        sig_contents = f\"{data}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Hash the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Verify the signature using the public key\n        cipher = PKCS1_v1_5.new(RSA.importKey(public_key))\n        if not cipher.verify(sig_hash, urlsafe_b64decode(signature)):\n            raise SignatureVerificationError(\"Signature verification failed\")\n            \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise SignatureVerificationError(\"No data element found in the document\")\n            \n        data_text = data.text\n        if not data_text:\n            raise SignatureVerificationError(\"Data element is empty\")\n            \n        # Create the signature content\n        sig_contents = '.'.join([\n            data_text,\n            b64encode(b\"application/xml\").decode(\"ascii\"),\n            b64encode(b\"base64url\").decode(\"ascii\"),\n            b64encode(b\"RSA-SHA256\").decode(\"ascii\")\n        ])\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode(\"ascii\"))\n        \n        # Import the public key\n        public_key_obj = RSA.importKey(public_key)\n        \n        # Create the verifier\n        verifier = pkcs1_15.new(public_key_obj)\n        \n        # Verify the signature\n        sig = urlsafe_b64decode(signature)\n        verifier.verify(sig_hash, sig)\n        \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")", "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    try:\n        # Parse the XML document\n        root = etree.fromstring(doc)\n        \n        # Extract the data to be signed\n        data = root.find(\".//{http://salmon-protocol.org/ns/magic-env}data\")\n        if data is None:\n            raise SignatureVerificationError(\"No data element found in the document\")\n            \n        data_text = data.text\n        \n        # Create the signature content (data + metadata)\n        sig_contents = f\"{data_text}.{b64encode(b'application/xml').decode('ascii')}.{b64encode(b'base64url').decode('ascii')}.{b64encode(b'RSA-SHA256').decode('ascii')}\"\n        \n        # Create the hash of the signature content\n        sig_hash = SHA256.new(sig_contents.encode('ascii'))\n        \n        # Import the public key\n        pk = RSA.importKey(public_key)\n        \n        # Verify the signature\n        cipher = PKCS1_v1_5.new(pk)\n        sig_bytes = urlsafe_b64decode(signature)\n        \n        if not cipher.verify(sig_hash, sig_bytes):\n            raise SignatureVerificationError(\"Signature verification failed\")\n            \n        return True\n        \n    except Exception as e:\n        raise SignatureVerificationError(f\"Signature verification failed: {str(e)}\")"]}
{"_id": "630629e052e177c0ba46a0c4", "name": "parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 100693, "num_retrieved": 351, "inference_time": 18.676797091960907, "generate_results": ["def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    import json\n    from lxml import etree\n    \n    # Try to parse as JSON first (new format)\n    try:\n        data = json.loads(document)\n        # Extract subject and links\n        result = {\n            \"subject\": data.get(\"subject\"),\n            \"aliases\": data.get(\"aliases\", []),\n            \"links\": []\n        }\n        \n        # Extract links from JSON\n        for link in data.get(\"links\", []):\n            result[\"links\"].append({\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            })\n        \n        return result\n    except json.JSONDecodeError:\n        pass\n    \n    # If JSON parsing fails, try to parse as XRD (old format)\n    try:\n        # Parse the XRD document\n        root = etree.fromstring(document)\n        \n        # Extract subject\n        subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n        subject = subject_element.text if subject_element is not None else None\n        \n        # Extract aliases\n        aliases = []\n        alias_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")\n        for alias in alias_elements:\n            aliases.append(alias.text)\n        \n        # Extract links\n        links = []\n        link_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\")\n        for link in link_elements:\n            link_dict = {\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            }\n            links.append(link_dict)\n        \n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n    except Exception:\n        # If parsing fails, return empty dict\n        return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        import json\n        json_data = json.loads(document)\n        \n        # Extract the subject and aliases from the JSON\n        subject = json_data.get(\"subject\")\n        aliases = json_data.get(\"aliases\", [])\n        \n        # Extract links from the JSON\n        links = []\n        for link in json_data.get(\"links\", []):\n            links.append({\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            })\n        \n        # Return the parsed data\n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n        \n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        # XRD format has a structure with elements and links\n        from lxml import etree\n        try:\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n            subject = subject_element.get(\"value\") if subject_element is not None else None\n            \n            # Extract aliases from XRD\n            aliases = []\n            alias_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")\n            for alias in alias_elements:\n                aliases.append(alias.get(\"value\"))\n            \n            # Extract links from XRD\n            links = []\n            link_elements = root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\")\n            for link in link_elements:\n                link_dict = {\n                    \"rel\": link.get(\"rel\"),\n                    \"type\": link.get(\"type\"),\n                    \"href\": link.get(\"href\")\n                }\n                links.append(link_dict)\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n            \n        except Exception as e:\n            # If both parsing methods fail, return an empty dictionary\n            return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        import json\n        json_data = json.loads(document)\n        \n        # Extract subject and aliases from JSON\n        subject = json_data.get(\"subject\")\n        aliases = json_data.get(\"aliases\", [])\n        \n        # Extract links from JSON\n        links = []\n        for link in json_data.get(\"links\", []):\n            links.append({\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            })\n        \n        # Return parsed data\n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n        \n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        from lxml import etree\n        try:\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n            subject = subject_element.text if subject_element is not None else None\n            \n            # Extract aliases from XRD\n            aliases = []\n            for alias in root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\"):\n                aliases.append(alias.text)\n            \n            # Extract links from XRD\n            links = []\n            for link in root.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\"):\n                link_dict = {\n                    \"rel\": link.get(\"rel\"),\n                    \"type\": link.get(\"type\"),\n                    \"href\": link.get(\"href\")\n                }\n                links.append(link_dict)\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n            \n        except Exception as e:\n            # If both parsing methods fail, return empty dict\n            return {}", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        import json\n        data = json.loads(document)\n        \n        # Extract the subject and aliases from the JSON\n        subject = data.get(\"subject\")\n        aliases = data.get(\"aliases\", [])\n        \n        # Extract links from the JSON\n        links = data.get(\"links\", [])\n        \n        # Return the parsed data\n        return {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n        \n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        # XRD format has a structure with elements and links\n        from lxml import etree\n        try:\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n            subject = subject_element.get(\"value\") if subject_element is not None else None\n            \n            # Extract aliases from XRD\n            aliases_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")\n            aliases = []\n            if aliases_element is not None:\n                aliases = [alias.get(\"value\") for alias in aliases_element.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")]\n            \n            # Extract links from XRD\n            links_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\")\n            links = []\n            if links_element is not None:\n                for link in links_element.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\"):\n                    link_data = {\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    }\n                    links.append(link_data)\n            \n            return {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }\n            \n        except Exception:\n            # If both", "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n    \n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    try:\n        # Try to parse as JSON first (new format)\n        import json\n        data = json.loads(document)\n        \n        # Extract subject and aliases from JSON\n        subject = data.get(\"subject\")\n        aliases = data.get(\"aliases\", [])\n        \n        # Extract links from JSON\n        links = data.get(\"links\", [])\n        \n        # Build result dictionary\n        result = {\n            \"subject\": subject,\n            \"aliases\": aliases,\n            \"links\": links\n        }\n        \n        return result\n        \n    except json.JSONDecodeError:\n        # If JSON parsing fails, try to parse as XRD (old format)\n        # XRD format has a structure with elements and links\n        try:\n            from lxml import etree\n            root = etree.fromstring(document)\n            \n            # Extract subject from XRD\n            subject_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\")\n            subject = subject_element.get(\"href\") if subject_element is not None else None\n            \n            # Extract aliases from XRD\n            aliases_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")\n            aliases = []\n            if aliases_element is not None:\n                aliases = [alias.get(\"href\") for alias in aliases_element.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Alias\")]\n            \n            # Extract links from XRD\n            links_element = root.find(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\")\n            links = []\n            if links_element is not None:\n                for link in links_element.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\"):\n                    link_dict = {\n                        \"rel\": link.get(\"rel\"),\n                        \"type\": link.get(\"type\"),\n                        \"href\": link.get(\"href\")\n                    }\n                    links.append(link_dict)\n            \n            # Build result dictionary\n            result = {\n                \"subject\": subject,\n                \"aliases\": aliases,\n                \"links\": links\n            }"]}
{"_id": "630629e152e177c0ba46a0d1", "name": "try_retrieve_webfinger_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 105967, "num_retrieved": 377, "inference_time": 18.676797091960907, "generate_results": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the handle\n        # The handle is expected to be in the format username@domain.tld\n        # We'll use the fetch_document function from the utils.network module\n        # to retrieve the document from the host\n        document, status_code, exception = fetch_document(\n            host=handle.split(\"@\")[1],\n            path=\"/.well-known/webfinger\",\n            cache=False\n        )\n        # If we successfully retrieved the document, return it\n        if document:\n            return document\n        # If we couldn't retrieve the document, return None\n        return None\n    except Exception:\n        # If there's any error during the retrieval, return None\n        # The function should not raise an exception\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the handle\n        # The handle is expected to be in the format username@domain.tld\n        # We'll use a standard webfinger endpoint: https://host/.well-known/webfinger\n        domain = handle.split(\"@\")[1]\n        url = f\"https://{domain}/.well-known/webfinger\"\n        \n        # Fetch the document\n        document, status_code, exception = fetch_document(url)\n        \n        # If we successfully retrieved a document, return it\n        if document:\n            return document\n            \n        # If no document was found, return None\n        return None\n        \n    except Exception:\n        # Return None if any error occurs (does not raise)\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the handle\n        # The handle is expected to be in the format username@domain.tld\n        # We'll use the fetch_document function from the utils.network module\n        # to retrieve the webfinger document from the specified host\n        document, status_code, exception = fetch_document(\n            host=handle.split(\"@\")[1],\n            path=\"/.well-known/webfinger\",\n            cache=False\n        )\n        # If we successfully retrieved the document, return it\n        if document:\n            return document\n        # If we couldn't retrieve the document, return None\n        return None\n    except Exception:\n        # If there's any error during the retrieval, return None without raising\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Attempt to fetch the webfinger document using the provided handle\n        # The handle is expected to be in the format username@domain.tld\n        # We'll use the fetch_document function from the utils.network module\n        # to retrieve the document, with a default timeout and cache settings\n        document, status_code, exception = fetch_document(\n            handle, \n            cache=False, \n            extra_headers={'accept': 'application/jrd+json'}\n        )\n        \n        # If we successfully retrieved a document, return it\n        if document:\n            return document\n            \n        # If no document was found or the request failed, return None\n        return None\n        \n    except Exception as e:\n        # Log the error but do not raise it (as per docstring)\n        logger.warning(\"try_retrieve_webfinger_document: failed to retrieve webfinger document for %s: %s\", handle, e)\n        return None", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        # Use fetch_document to retrieve the webfinger document\n        # The function should handle any network errors gracefully\n        document, status_code, exception = fetch_document(\n            host=handle.split(\"@\")[1],\n            path=\"/.well-known/webfinger\",\n            cache=False\n        )\n        # Return the document if successfully retrieved, otherwise return None\n        return document if document else None\n    except Exception:\n        # Do not raise if there's an error during retrieval\n        return None"]}
{"_id": "630629e152e177c0ba46a0d2", "name": "retrieve_and_parse_diaspora_webfinger", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 99289, "num_retrieved": 350, "inference_time": 18.676797091960907, "generate_results": ["def retrieve_and_parse_diaspora_webfinger(handle: str) -> Dict:\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return {}\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        return {}\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=\"/.well-known/webfinger?resource=acct:%s\" % quote(handle)\n    )\n    \n    if not document or exception:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Extract links from the webfinger document\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [],\n        \"links\": []\n    }\n    \n    # Add links from the document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/diaspora+json\"\n            })\n        elif link.get(\"rel\") == \"profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"profile-page\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"seed_location\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"hcard\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n    \n    # Add aliases if available\n    if doc.get(\"aliases\"):\n        webfinger[\"aliases\"] = [alias for alias in doc.get(\"aliases\", [])]\n    \n    return webfinger", "def retrieve_and_parse_diaspora_webfinger(handle: str) -> Optional[Dict]:\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return None\n    \n    # Create a webfinger object with the provided handle\n    webfinger = RFC7033Webfinger(\n        id=None,\n        handle=handle,\n        guid=None,\n        base_url=None,\n        profile_path=None,\n        hcard_path=None,\n        atom_path=None,\n        search_path=None\n    )\n    \n    # Fetch the webfinger document from the remote server\n    document, code, exception = fetch_document(\n        host=handle.split(\"@\")[1],\n        path=\"/.well-known/webfinger?resource=acct:%s\" % quote(handle),\n        cache=False\n    )\n    \n    if not document or exception:\n        return None\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return None\n    \n    # Parse the webfinger document and return the relevant links\n    links = []\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/activity+json\":\n            links.append({\n                \"rel\": link.get(\"rel\"),\n                \"type\": link.get(\"type\"),\n                \"href\": link.get(\"href\")\n            })\n    \n    # Return the parsed webfinger document\n    return {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [],\n        \"links\": links\n    }", "def retrieve_and_parse_diaspora_webfinger(handle: str) -> Dict:\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return {}\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        return {}\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=\"/.well-known/webfinger?resource=acct:%s\" % quote(handle)\n    )\n    \n    if not document or exception:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Parse the webfinger document to extract links\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/diaspora+json\"\n            })\n        elif link.get(\"rel\") == \"profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"profile-page\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"seed_location\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"hcard\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n    \n    # Add the webfinger document to the result\n    return webfinger", "def retrieve_and_parse_diaspora_webfinger(handle: str) -> Dict:\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return {}\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        return {}\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=\"/.well-known/webfinger?resource=acct:%s\" % quote(handle)\n    )\n    \n    if not document or exception:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Parse the webfinger document to extract links\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/diaspora+json\"\n            })\n        elif link.get(\"rel\") == \"profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"profile-page\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"seed_location\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"hcard\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n    \n    # Add aliases if available\n    if doc.get(\"aliases\"):\n        webfinger[\"aliases\"] = [alias for alias in doc.get(\"aliases\", [])]\n    \n    return webfinger", "def retrieve_and_parse_diaspora_webfinger(handle: str) -> Dict:\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    if not validate_handle(handle):\n        return {}\n    \n    # Extract host from handle\n    try:\n        host = handle.split(\"@\")[1]\n    except (AttributeError, IndexError):\n        return {}\n    \n    # Fetch the webfinger document\n    document, code, exception = fetch_document(\n        host=host, \n        path=\"/.well-known/webfinger?resource=acct:%s\" % quote(handle)\n    )\n    \n    if not document or exception:\n        return {}\n    \n    try:\n        doc = json.loads(document)\n    except json.JSONDecodeError:\n        return {}\n    \n    # Parse the webfinger document to extract links\n    webfinger = {\n        \"subject\": \"acct:%s\" % handle,\n        \"aliases\": [],\n        \"links\": []\n    }\n    \n    # Add links from the webfinger document\n    for link in doc.get(\"links\", []):\n        if link.get(\"rel\") == \"self\" and link.get(\"type\") == \"application/diaspora+json\":\n            webfinger[\"links\"].append({\n                \"rel\": \"self\",\n                \"href\": link[\"href\"],\n                \"type\": \"application/diaspora+json\"\n            })\n        elif link.get(\"rel\") == \"profile-page\":\n            webfinger[\"links\"].append({\n                \"rel\": \"profile-page\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n            webfinger[\"links\"].append({\n                \"rel\": \"seed_location\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n        elif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n            webfinger[\"links\"].append({\n                \"rel\": \"hcard\",\n                \"href\": link[\"href\"],\n                \"type\": \"text/html\"\n            })\n    \n    # Add aliases if available\n    if \"aliases\" in doc:\n        webfinger[\"aliases\"] = [alias for alias in doc[\"aliases\"]]\n    \n    return webfinger"]}
{"_id": "630629e252e177c0ba46a0d6", "name": "retrieve_diaspora_host_meta", "project": "jaywink/federation", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 77903, "num_retrieved": 312, "inference_time": 18.676797091960907, "generate_results": ["def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Use the existing fetch_document function to retrieve the /.well-known/host-meta document\n    # The path is typically /.well-known/host-meta for Diaspora hosts\n    doc, code, exception = fetch_document(host=host, path=\"/.well-known/host-meta\")\n    \n    if exception:\n        logger.debug(\"retrieve_diaspora_host_meta: failed to fetch host-meta document: %s, %s\", code, exception)\n        return None\n    \n    if not doc:\n        return None\n    \n    # Parse the document and return as XRD instance\n    # The context suggests that the host-meta document is an XRD document\n    # which is already handled by the BaseHostMeta class\n    try:\n        # Create a DiasporaHostMeta instance with the host information\n        hostmeta = DiasporaHostMeta(webfinger_host=host)\n        # Render the document\n        return hostmeta.render()\n    except Exception as e:\n        logger.debug(\"retrieve_diaspora_host_meta: failed to parse host-meta document: %s\", e)\n        return None", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Create a template for Diaspora host-meta\n    template = \"diaspora\"\n    \n    # Use the generate_host_meta function to create the host-meta document\n    # We need to pass the host as a parameter to the template\n    # The template requires webfinger_host, which we can derive from the host\n    webfinger_host = host\n    \n    # Create the host-meta instance with the required parameters\n    hostmeta = generate_host_meta(\n        template=template,\n        webfinger_host=webfinger_host\n    )\n    \n    return hostmeta", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Use the fetch_document function to retrieve the /.well-known/host-meta document\n    doc, code, exception = fetch_document(\n        host=host, \n        path=\"/.well-known/host-meta\"\n    )\n    \n    if not doc:\n        return None\n        \n    try:\n        # Parse the document as JSON\n        doc = json.loads(doc)\n    except json.JSONDecodeError:\n        return None\n        \n    # Create a DiasporaHostMeta instance with the webfinger_host set to the host\n    webfinger_host = host\n    hostmeta = DiasporaHostMeta(webfinger_host=webfinger_host)\n    \n    # Return the rendered XRD document\n    return hostmeta.render()", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Create a template for Diaspora host-meta\n    template = \"diaspora\"\n    \n    # Use the generate_host_meta function to create the host-meta document\n    # We need to pass the host as a parameter to the template\n    # The template requires webfinger_host, which we can derive from the host\n    webfinger_host = host\n    \n    # Pass the required parameters to generate_host_meta\n    hostmeta = generate_host_meta(template=template, webfinger_host=webfinger_host)\n    \n    return hostmeta", "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: XRD instance\n    \"\"\"\n    # Use the fetch_document function to retrieve the /.well-known/host-meta document\n    # The path is typically /.well-known/host-meta for Diaspora\n    doc, code, exception = fetch_document(host=host, path=\"/.well-known/host-meta\")\n    \n    if not doc:\n        return None\n        \n    try:\n        # Parse the document as XRD\n        xrd = XRD()\n        # Assuming the document is already in XRD format, we just need to parse it\n        # The actual parsing would depend on the structure of the document\n        # For now, we'll just return a new XRD instance with the document content\n        xrd.from_xml(doc)\n        return xrd\n    except Exception as e:\n        logger.error(\"Failed to parse host-meta document: %s\", e)\n        return None"]}
{"_id": "630629e752e177c0ba46a0fb", "name": "send_document", "project": "jaywink/federation", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 117781, "num_retrieved": 500, "inference_time": 18.676797091960907, "generate_results": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url=url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"\n    Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    try:\n        response = requests.post(\n            url,\n            data=data,\n            timeout=timeout,\n            method=method,\n            *args,\n            **kwargs\n        )\n        return response.status_code, None\n    except requests.exceptions.RequestException as ex:\n        return None, ex"]}
{"_id": "6306091073426c38ae68acac", "name": "dict_insert", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 104300, "num_retrieved": 352, "inference_time": 18.676797091960907, "generate_results": ["def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    keys_list = [key] + list(keys)\n    \n    for i, k in enumerate(keys_list):\n        if i < len(keys_list) - 1:\n            # If not the last key, ensure the key exists as a dictionary\n            if k not in current:\n                current[k] = {}\n            current = current[k]\n        else:\n            # If the last key, set the value\n            current[k] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    keys_list = [key] + list(keys)\n    \n    for i, k in enumerate(keys_list):\n        if i < len(keys_list) - 1:\n            # If not the last key, ensure the key exists and is a dict\n            if k not in current:\n                current[k] = {}\n            current = current[k]\n        else:\n            # If the last key, set the value\n            current[k] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    keys_list = [key] + list(keys)\n    \n    for i, k in enumerate(keys_list):\n        if i == len(keys_list) - 1:\n            current[k] = val\n        else:\n            if k not in current:\n                current[k] = {}\n            current = current[k]", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    keys_list = [key] + list(keys)\n    \n    for i, k in enumerate(keys_list):\n        if i < len(keys_list) - 1:\n            # If not the last key, ensure the key exists and is a dict\n            if k not in current:\n                current[k] = {}\n            current = current[k]\n        else:\n            # If the last key, set the value\n            current[k] = val", "def dict_insert(dic, val, key, *keys):\n    \"\"\"\n    insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    current = dic\n    keys_list = [key] + list(keys)\n    \n    for i, k in enumerate(keys_list):\n        if i == len(keys_list) - 1:\n            current[k] = val\n        else:\n            if k not in current:\n                current[k] = {}\n            current = current[k]"]}
{"_id": "6306091a73426c38ae68acc8", "name": "list_of_file_names", "project": "redhat-openstack/infrared", "level": "plib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 90550, "num_retrieved": 250, "inference_time": 18.676797091960907, "generate_results": ["def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        \"\"\"Represents List of file names for specific path\n\n        It support value auto propagation, based on plugin_path and\n        spec_option['lookup_dir'])\n        \"\"\"\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        \"\"\"Represents List of file names for specific path\n\n        It support value auto propagation, based on plugin_path and\n        spec_option['lookup_dir'])\n        \"\"\"\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        \"\"\"Represents List of file names for specific path\n\n        It support value auto propagation, based on plugin_path and\n        spec_option['lookup_dir'])\n        \"\"\"\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        \"\"\"Represents List of file names for specific path\n\n        It support value auto propagation, based on plugin_path and\n        spec_option['lookup_dir'])\n        \"\"\"\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n    \"\"\"\n    Create a new IniType complex type\n    \"\"\"\n    class ListOfFileNames(ComplexType):\n        \"\"\"Represents List of file names for specific path\n\n        It support value auto propagation, based on plugin_path and\n        spec_option['lookup_dir'])\n        \"\"\"\n        ARG_SEPARATOR = ','\n        VALUES_AUTO_PROPAGATION = True\n\n        @property\n        def plugin_path(self):\n            return self.settings_dirs[2]\n\n        @property\n        def lookup_dir(self):\n            return self.spec_option['lookup_dir']\n\n        @property\n        def files_path(self):\n            return os.path.join(self.plugin_path, self.lookup_dir)\n\n        def get_allowed_values(self):\n            \"\"\"Generate list of file names in specific path \"\"\"\n            result = list(map((lambda name: os.path.splitext(name)[0]),\n                              os.listdir(self.files_path)))\n            result.sort()\n            return result\n\n        def validate(self, value):\n            \"\"\"Validate if value is allowed\"\"\"\n            allowed_values = self.get_allowed_values()\n            if value not in allowed_values:\n                raise exceptions.IRFileNotFoundException(self.files_path)\n\n        def resolve(self, value):\n            result = value.split(self.ARG_SEPARATOR)\n            for arg in result:\n                self.validate(arg)\n            return result\n\n    return ListOfFileNames(settings_dirs, spec_option)"]}
{"_id": "6306091b73426c38ae68acd7", "name": "ansible_config_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 85281, "num_retrieved": 275, "inference_time": 18.676797091960907, "generate_results": ["@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Initialize the AnsibleConfigManager if not already done\n    if not hasattr(cls, '_ansible_config_manager'):\n        cls._ansible_config_manager = AnsibleConfigManager(infrared_home=cls.infrared_home)\n    return cls._ansible_config_manager", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Initialize the AnsibleConfigManager with the infrared home directory\n    # The infrared home directory is typically set in the environment or inferred from the context\n    # For now, we'll assume it's available through CoreServices or environment\n    infrared_home = CoreServices.infrared_home()\n    return AnsibleConfigManager(infrared_home)", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Initialize the AnsibleConfigManager with the infrared home directory\n    # The infrared home directory is typically set in the environment or inferred from configuration\n    # For now, we'll assume it's available through CoreServices or environment\n    infrared_home = CoreServices.infrared_home()\n    \n    # Create and return the AnsibleConfigManager instance\n    return AnsibleConfigManager(infrared_home)", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Initialize the AnsibleConfigManager with the infrared home directory\n    # The infrared home directory is typically set in the environment or \n    # derived from the current working directory\n    infrared_home = os.environ.get('INFRARED_HOME', os.getcwd())\n    return AnsibleConfigManager(infrared_home)", "@classmethod\ndef ansible_config_manager(cls):\n    \"\"\"\n    Gets the ansible config manager.\n    \"\"\"\n    # Initialize the AnsibleConfigManager with the infrared home directory\n    # The infrared home directory is typically set in the environment or \n    # derived from the current working directory\n    infrared_home = os.environ.get('INFRARED_HOME', os.getcwd())\n    return AnsibleConfigManager(infrared_home)"]}
{"_id": "6306091b73426c38ae68acd9", "name": "workspace_manager", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 77354, "num_retrieved": 249, "inference_time": 18.676797091960907, "generate_results": ["@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton or cached instance\n    # based on the context where it's used in the codebase\n    if not hasattr(cls, '_workspace_manager'):\n        # Initialize the workspace manager with a base directory\n        # This is inferred from the context where WorkspaceManager is used\n        # and the default workspace directory is typically defined in configuration\n        from infrared.core.services import CoreServices\n        workspace_dir = CoreServices.workspace_dir()  # This would be defined in CoreServices\n        cls._workspace_manager = WorkspaceManager(workspace_dir)\n    return cls._workspace_manager", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and state management\n    # In the context provided, the workspace manager is likely a singleton\n    # that is initialized and managed through CoreServices\n    return CoreServices.workspace_manager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # Assuming the workspace manager is a singleton and stored in a class variable\n    if not hasattr(cls, '_workspace_manager'):\n        # Initialize the workspace manager with a base directory (e.g., infrared workspaces directory)\n        # This is a simplified implementation; in reality, the actual path would be configured\n        # through configuration files or environment variables\n        from infrared.core.services import CoreServices\n        workspace_dir = CoreServices.get_config('workspaces_dir') or os.path.expanduser('~/.infrared/workspaces')\n        cls._workspace_manager = WorkspaceManager(workspace_dir)\n    \n    return cls._workspace_manager", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and state management\n    # In a real implementation, this would return an instance of WorkspaceManager\n    # that manages workspaces based on the application's configuration\n    return WorkspaceManager()", "@classmethod\ndef workspace_manager(cls):\n    \"\"\"\n    Gets the workspace manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the specific context and class structure\n    # In the context provided, the workspace manager is likely a singleton\n    # or a service that is initialized and cached\n    # Since the exact implementation is not provided in the context,\n    # we return a dummy instance for demonstration purposes\n    # In a real implementation, this would return the actual workspace manager instance\n    return WorkspaceManager()"]}
{"_id": "6306091b73426c38ae68acda", "name": "plugins_manager", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 81235, "num_retrieved": 268, "inference_time": 18.676797091960907, "generate_results": ["@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the internal state of the CoreServices class\n    # which is not fully provided in the context.\n    # In a real implementation, this would return an instance of \n    # InfraredPluginManager from CoreServices.\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the internal state and class structure of the CoreServices\n    # In a real implementation, this would return an instance of the plugin manager\n    # that is managed by CoreServices\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the internal state and class structure of the CoreServices\n    # In a real implementation, this would return an instance of the plugin manager\n    # that is registered and managed by CoreServices\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the internal state of the CoreServices class\n    # which is not fully provided in the context.\n    # In a real implementation, this would return an instance of \n    # InfraredPluginManager from CoreServices.\n    return CoreServices.plugins_manager()", "@classmethod\ndef plugins_manager(cls):\n    \"\"\"\n    Gets the plugin manager.\n    \"\"\"\n    # This is a placeholder implementation as the actual implementation\n    # would depend on the internal state of the CoreServices class\n    # which is not fully provided in the context.\n    # In a real implementation, this would return an instance of \n    # InfraredPluginManager from CoreServices.\n    return CoreServices.plugins_manager()"]}
{"_id": "6306091c73426c38ae68acdc", "name": "validate_from_content", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 103247, "num_retrieved": 362, "inference_time": 18.676797091960907, "generate_results": ["@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_dict = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Error parsing YAML content: {exc}\")\n\n    # Define required fields and validate their presence\n    required_fields = [\n        'subparsers',\n        'config',\n        'plugin_type',\n        'description'\n    ]\n\n    for field in required_fields:\n        if field not in spec_dict:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate subparsers has at least one parser\n    if not spec_dict['subparsers']:\n        raise IRValidatorException(\"subparsers must contain at least one parser\")\n\n    # Validate config has required fields\n    config = spec_dict['config']\n    required_config_fields = ['plugin_type', 'description']\n    for field in required_config_fields:\n        if field not in config:\n            raise IRValidatorException(f\"Missing required field in config: {field}\")\n\n    # Validate description is present\n    if not spec_dict['description']:\n        raise IRValidatorException(\"Description field is required\")\n\n    # Validate plugin_type is present\n    if not spec_dict['plugin_type']:\n        raise IRValidatorException(\"Plugin type field is required\")\n\n    return spec_dict", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Error parsing YAML content: {exc}\")\n\n    # Define required fields and validate their presence\n    required_fields = [\n        'subparsers',\n        'config',\n        'plugin_type',\n        'description'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate subparsers has at least one parser\n    if not spec_data['subparsers']:\n        raise IRValidatorException(\"subparsers must contain at least one parser\")\n\n    # Validate config has required fields\n    config = spec_data['config']\n    required_config_fields = ['plugin_type', 'description']\n    for field in required_config_fields:\n        if field not in config:\n            raise IRValidatorException(f\"Missing required config field: {field}\")\n\n    # Validate description is present\n    if not spec_data['description']:\n        raise IRValidatorException(\"Description field is required\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Error parsing YAML content: {exc}\")\n\n    # Define required fields and validate their presence\n    required_fields = [\n        'subparsers',\n        'config',\n        'plugin_type',\n        'description'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate subparsers has at least one parser\n    if not spec_data['subparsers']:\n        raise IRValidatorException(\"subparsers must contain at least one parser\")\n\n    # Validate config has required fields\n    config = spec_data['config']\n    required_config_fields = ['plugin_type', 'description']\n    for field in required_config_fields:\n        if field not in config:\n            raise IRValidatorException(f\"Missing required field in config: {field}\")\n\n    # Validate description is present\n    if not spec_data['description']:\n        raise IRValidatorException(\"Description field is required\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Error parsing YAML content: {exc}\")\n\n    # Define required fields and validate their presence\n    required_fields = [\n        'subparsers',\n        'config',\n        'plugin_type',\n        'description'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate subparsers has at least one parser\n    if not spec_data['subparsers']:\n        raise IRValidatorException(\"subparsers must contain at least one parser\")\n\n    # Validate config has required fields\n    config = spec_data['config']\n    required_config_fields = ['plugin_type', 'description']\n    for field in required_config_fields:\n        if field not in config:\n            raise IRValidatorException(f\"Missing required field in config: {field}\")\n\n    # Validate description is present\n    if not spec_data['description']:\n        raise IRValidatorException(\"Description field is required\")\n\n    # Validate plugin_type is present\n    if not spec_data['plugin_type']:\n        raise IRValidatorException(\"Plugin type field is required\")\n\n    return spec_data", "@classmethod\ndef validate_from_content(cls, spec_content=None):\n    \"\"\"\n    validates that spec (YAML) content has all required fields\n\n    :param spec_content: content of spec file\n    :raise IRValidatorException: when mandatory data is missing in spec file\n    :return: Dictionary with data loaded from a spec (YAML) file\n    \"\"\"\n    if spec_content is None:\n        raise IRValidatorException(\"Spec content is required\")\n\n    try:\n        spec_data = yaml.safe_load(spec_content)\n    except yaml.YAMLError as exc:\n        raise IRValidatorException(f\"Error parsing YAML content: {exc}\")\n\n    # Define required fields and validate their presence\n    required_fields = [\n        'subparsers',\n        'config',\n        'plugin_type',\n        'description'\n    ]\n\n    for field in required_fields:\n        if field not in spec_data:\n            raise IRValidatorException(f\"Missing required field: {field}\")\n\n    # Validate subparsers has at least one parser\n    if not spec_data['subparsers']:\n        raise IRValidatorException(\"subparsers must contain at least one parser\")\n\n    # Validate config has required fields\n    config = spec_data['config']\n    required_config_fields = ['plugin_type', 'description']\n    for field in required_config_fields:\n        if field not in config:\n            raise IRValidatorException(f\"Missing required field in config: {field}\")\n\n    # Validate plugin_type is present and not empty\n    if not spec_data['plugin_type']:\n        raise IRValidatorException(\"plugin_type must be specified\")\n\n    # Validate description is present and not empty\n    if not spec_data['description']:\n        raise IRValidatorException(\"description must be specified\")\n\n    return spec_data"]}
{"_id": "6306091c73426c38ae68acdd", "name": "validate_from_file", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 103909, "num_retrieved": 369, "inference_time": 18.676797091960907, "generate_results": ["@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist or is not a file\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or contains invalid content\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist or is not a file\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or contains invalid content\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist or is not a file\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or contains invalid content\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if yaml_file is None:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist or is not a file\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or contains invalid content\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")", "@classmethod\ndef validate_from_file(cls, yaml_file=None):\n    \"\"\"\n    Loads & validates that a YAML file has all required fields\n\n    :param yaml_file: Path to YAML file\n    :raise IRValidatorException: when mandatory data is missing in file\n    :return: Dictionary with data loaded from a YAML file\n    \"\"\"\n    if not yaml_file:\n        raise IRValidatorException(\"YAML file path is required\")\n\n    if not os.path.isfile(yaml_file):\n        raise IRValidatorException(f\"YAML file '{yaml_file}' does not exist or is not a file\")\n\n    try:\n        with open(yaml_file, 'r') as file:\n            data = yaml.safe_load(file)\n            if data is None:\n                raise IRValidatorException(f\"YAML file '{yaml_file}' is empty or contains invalid content\")\n            return data\n    except yaml.YAMLError as e:\n        raise IRValidatorException(f\"Invalid YAML format in file '{yaml_file}': {str(e)}\")\n    except Exception as e:\n        raise IRValidatorException(f\"Error reading YAML file '{yaml_file}': {str(e)}\")"]}
{"_id": "6306091d73426c38ae68ace5", "name": "_include_groups", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 96227, "num_retrieved": 292, "inference_time": 13.900726526975632, "generate_results": ["def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical behavior, it likely handles\n    # including groups from shared groups or other sources\n    # in the parser dictionary\n    pass", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical pattern, it likely handles\n    # including groups from shared groups or other sources\n    # in the parser dictionary\n    pass", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical pattern, it likely handles\n    # including groups from shared groups or other sources\n    # in the parser dictionary\n    pass", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not directly implemented in the provided context\n    # Based on the context, it appears to be related to handling group inclusion\n    # in spec files, potentially expanding shared groups or merging group definitions\n    # However, no explicit implementation for _include_groups is present\n    \n    # Since the context does not contain the implementation of this specific function,\n    # and based on the pattern of similar functions in the context (like add_shared_groups),\n    # we can infer that this function might be responsible for processing include directives\n    # in the spec files to merge or expand group definitions.\n    \n    # Placeholder implementation - actual implementation would depend on the spec file format\n    # and how groups are defined in the spec files.\n    \n    # If there were a mechanism to resolve includes, it would likely involve:\n    # 1. Parsing the include directive from the spec\n    # 2. Resolving the referenced groups\n    # 3. Merging them into the parser_dict\n    \n    # As no specific implementation is provided in the context, we return an empty dict\n    # This is a minimal implementation that does nothing - the actual behavior would\n    # need to be defined based on the spec file format and requirements.\n    \n    return parser_dict", "def _include_groups(self, parser_dict):\n    \"\"\"\n    Resolves the include dict directive in the spec files.\n    \"\"\"\n    # This function is not implemented in the provided context\n    # Based on the name and typical pattern, it likely handles\n    # including groups from shared groups or other sources\n    # in the parser dictionary\n    pass"]}
{"_id": "6306092373426c38ae68acfa", "name": "get_spec_defaults", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 79560, "num_retrieved": 241, "inference_time": 13.900726526975632, "generate_results": ["def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary\n    spec_dict = self.specification.spec_dict\n    \n    # Process subparsers and their options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        # Process options for this subparser\n        options = subparser_dict.get('options', {})\n        for option_name, option_dict in options.items():\n            # Resolve the value based on the option type\n            if 'type' in option_dict:\n                # Handle different types of options\n                if option_dict['type'] == 'str':\n                    result[option_name] = option_dict.get('default', '')\n                elif option_dict['type'] == 'int':\n                    result[option_name] = option_dict.get('default', 0)\n                elif option_dict['type'] == 'bool':\n                    result[option_name] = option_dict.get('default', False)\n                elif option_dict['type'] == 'list':\n                    result[option_name] = option_dict.get('default', [])\n                elif option_dict['type'] == 'dict':\n                    result[option_name] = option_dict.get('default', {})\n            else:\n                # Default value if no type specified\n                result[option_name] = option_dict.get('default', '')\n    \n    # Add any default values from the main spec\n    main_options = spec_dict.get('options', {})\n    for option_name, option_dict in main_options.items():\n        if 'type' in option_dict:\n            result[option_name] = option_dict.get('default', '')\n    \n    return result", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary from the plugin\n    spec_dict = self.specification.spec_dict\n    \n    # Process subparsers and their options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        # Get the options from the subparser\n        options = subparser_dict.get('options', {})\n        \n        # Process each option\n        for option_name, option_dict in options.items():\n            # Resolve the value based on the option type\n            if 'type' in option_dict:\n                # Handle different types of options\n                if option_dict['type'] == 'str':\n                    # For string options, get the default value from the spec\n                    default_value = option_dict.get('default', '')\n                    result[option_name] = default_value\n                elif option_dict['type'] == 'bool':\n                    # For boolean options, use the default value\n                    default_value = option_dict.get('default', False)\n                    result[option_name] = default_value\n                elif option_dict['type'] == 'int':\n                    # For integer options, use the default value\n                    default_value = option_dict.get('default', 0)\n                    result[option_name] = default_value\n                elif option_dict['type'] == 'list':\n                    # For list options, use the default value\n                    default_value = option_dict.get('default', [])\n                    result[option_name] = default_value\n                elif option_dict['type'] == 'dict':\n                    # For dictionary options, use the default value\n                    default_value = option_dict.get('default', {})\n                    result[option_name] = default_value\n            else:\n                # Default to empty value if no type specified\n                result[option_name] = option_dict.get('default', '')\n    \n    # Add any additional defaults from the plugin configuration\n    plugin_config = self.plugin.config\n    if plugin_config:\n        for key, value in plugin_config.items():\n            if key not in result:\n                result[key] = value\n    \n    return result", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary\n    spec_dict = self.specification.spec_dict\n    \n    # Process subparsers and their options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        # Get the options from the subparser\n        options = subparser_dict.get('options', {})\n        \n        # Process each option\n        for option_name, option_dict in options.items():\n            # Get the default value from the option dictionary\n            default_value = option_dict.get('default')\n            \n            # If default value exists, add it to the result\n            if default_value is not None:\n                result[option_name] = default_value\n    \n    # Process groups and their options\n    for group_dict in spec_dict.get('groups', []):\n        for option_name, option_dict in group_dict.get('options', {}).items():\n            # Get the default value from the option dictionary\n            default_value = option_dict.get('default')\n            \n            # If default value exists, add it to the result\n            if default_value is not None:\n                result[option_name] = default_value\n    \n    return result", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary\n    spec_dict = self.specification.spec_dict\n    \n    # Process subparsers and their options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        # Process options for this subparser\n        options = subparser_dict.get('options', {})\n        for option_name, option_spec in options.items():\n            # Resolve option value based on spec\n            if 'default' in option_spec:\n                result[option_name] = option_spec['default']\n    \n    # Process groups and their options\n    for group_dict in spec_dict.get('groups', []):\n        for option_name, option_spec in group_dict.get('options', {}).items():\n            if 'default' in option_spec:\n                result[option_name] = option_spec['default']\n    \n    # Add any defaults from the main parser\n    main_options = spec_dict.get('options', {})\n    for option_name, option_spec in main_options.items():\n        if 'default' in option_spec:\n            result[option_name] = option_spec['default']\n    \n    return result", "def get_spec_defaults(self):\n    \"\"\"\n    Resolve arguments' values from spec and other sources.\n    \"\"\"\n    # Initialize result dictionary\n    result = {}\n    \n    # Get the specification dictionary\n    spec_dict = self.specification.spec_dict\n    \n    # Process subparsers and their options\n    for subparser_name, subparser_dict in spec_dict.get('subparsers', {}).items():\n        # Process options for this subparser\n        options = subparser_dict.get('options', {})\n        for option_name, option_dict in options.items():\n            # Resolve the value based on the option configuration\n            if 'default' in option_dict:\n                result[option_name] = option_dict['default']\n    \n    # Process groups and their options\n    for group_dict in spec_dict.get('groups', []):\n        for option_name, option_dict in group_dict.get('options', {}).items():\n            if 'default' in option_dict:\n                result[option_name] = option_dict['default']\n    \n    # Add any defaults from the main parser\n    main_options = spec_dict.get('options', {})\n    for option_name, option_dict in main_options.items():\n        if 'default' in option_dict:\n            result[option_name] = option_dict['default']\n    \n    return result"]}
{"_id": "6306092973426c38ae68ad01", "name": "get_deprecated_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 78342, "num_retrieved": 230, "inference_time": 13.900726526975632, "generate_results": ["def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}", "def get_deprecated_args(self):\n    \"\"\"\n    Returning dict with options which deprecate others.\n    \"\"\"\n    return {}"]}
{"_id": "6306092c73426c38ae68ad02", "name": "validate_arg_deprecation", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 90135, "num_retrieved": 305, "inference_time": 13.900726526975632, "generate_results": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # TODO: Implement this function\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - no specific deprecation rules found in context\n    # In a real implementation, this would check for known deprecated arguments\n    # and print warnings or errors accordingly\n    pass", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual implementation would depend on\n    # specific deprecation rules defined in the codebase\n    deprecated_args = []\n    \n    # Example logic - this would be replaced with actual deprecation checks\n    # based on the specific arguments that are deprecated\n    \n    # Check for deprecated CLI arguments\n    for arg in cli_args:\n        if arg in ['--old-option', '--deprecated-flag']:\n            deprecated_args.append(arg)\n    \n    # Check for deprecated answer file arguments\n    for arg in answer_file_args:\n        if arg in ['old_option', 'deprecated_flag']:\n            deprecated_args.append(arg)\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"Deprecated arguments found:\")\n        for arg in deprecated_args:\n            print(f\"  {arg}\")\n        print(\"Please update your command to use the new argument names.\")\n    else:\n        print(\"No deprecated arguments found.\")", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual implementation would depend on\n    # specific deprecation rules defined in the codebase\n    deprecated_args = []\n    \n    # Example logic - this would be replaced with actual deprecation checks\n    # based on the specific arguments that are deprecated\n    \n    # Check for deprecated CLI arguments\n    for arg in cli_args:\n        if arg in ['--old-option', '--deprecated-flag']:\n            deprecated_args.append(arg)\n    \n    # Check for deprecated answer file arguments\n    for arg in answer_file_args:\n        if arg in ['old_option', 'deprecated_flag']:\n            deprecated_args.append(arg)\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"The following arguments are deprecated and will be removed in a future version:\")\n        for arg in deprecated_args:\n            print(f\"  --{arg}\")\n    \n    return deprecated_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n    \"\"\"\n    Validates and prints the deprecated arguments.\n\n    :param cli_args: the dict of arguments from cli\n    :param answer_file_args: the dict of arguments from files\n    \"\"\"\n    # Placeholder implementation - actual implementation would depend on\n    # specific deprecation rules defined in the codebase\n    deprecated_args = []\n    \n    # Example logic - this would be replaced with actual deprecation checks\n    # based on the specific arguments that are deprecated\n    \n    # Check for deprecated arguments in cli_args\n    for arg in cli_args:\n        if arg in ['--old-option', '--deprecated-flag']:\n            deprecated_args.append(arg)\n    \n    # Check for deprecated arguments in answer_file_args\n    for arg in answer_file_args:\n        if arg in ['--old-option', '--deprecated-flag']:\n            deprecated_args.append(arg)\n    \n    # Print deprecated arguments if found\n    if deprecated_args:\n        print(\"The following arguments are deprecated and will be removed in a future version:\")\n        for arg in deprecated_args:\n            print(f\"  --{arg}\")\n    \n    return deprecated_args"]}
{"_id": "6306092d73426c38ae68ad04", "name": "get_parser_option_specs", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 103173, "num_retrieved": 341, "inference_time": 13.900726526975632, "generate_results": ["def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)", "def get_parser_option_specs(self, command_name):\n    \"\"\"\n    Gets all the options for the specified command\n\n    :param command_name: the command name (main, virsh, ospd, etc...)\n    :return: the list of all command options\n    \"\"\"\n    return self.spec_helper.get_parser_option_specs(command_name)"]}
{"_id": "6306092d73426c38ae68ad05", "name": "get_option_spec", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 98501, "num_retrieved": 301, "inference_time": 13.900726526975632, "generate_results": ["def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)", "def get_option_spec(self, command_name, argument_name):\n    \"\"\"\n    Gets the specification for the specified option name.\n    \"\"\"\n    return self.spec_helper.get_option_spec(command_name, argument_name)"]}
{"_id": "6306092d73426c38ae68ad06", "name": "get_silent_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 90292, "num_retrieved": 304, "inference_time": 13.900726526975632, "generate_results": ["def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []", "def get_silent_args(self, args):\n    \"\"\"\n    list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n    \"\"\"\n    # TODO: Implement this function\n    return []"]}
{"_id": "6306092d73426c38ae68ad07", "name": "validate_requires_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 88563, "num_retrieved": 275, "inference_time": 13.900726526975632, "generate_results": ["def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    missing_args = []\n    \n    for arg in required_args:\n        if arg not in args:\n            missing_args.append(arg)\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.get_required_args()\n    for arg in required_args:\n        if arg not in args:\n            raise IRRequiredArgsMissingException({arg: [arg]})", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    missing_args = []\n    \n    for arg in required_args:\n        if arg not in args:\n            missing_args.append(arg)\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    missing_args = []\n    \n    for arg in required_args:\n        if arg not in args:\n            missing_args.append(arg)\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)", "def validate_requires_args(self, args):\n    \"\"\"\n    Check if all the required arguments have been provided.\n    \"\"\"\n    required_args = self.required_args\n    missing_args = []\n    \n    for arg in required_args:\n        if arg not in args:\n            missing_args.append(arg)\n    \n    if missing_args:\n        raise IRRequiredArgsMissingException(missing_args)"]}
{"_id": "6306092d73426c38ae68ad08", "name": "_get_conditionally_required_args", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 93401, "num_retrieved": 319, "inference_time": 13.900726526975632, "generate_results": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        option_name = option_spec['name']\n        required_when = option_spec.get('required_when')\n        \n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        condition_met = False\n        for condition in required_when:\n            # Extract the condition value from args\n            condition_value = args.get(command_name, {}).get(condition, None)\n            \n            # Check if the condition matches the expected value\n            if condition_value is not None:\n                condition_met = True\n                break\n                \n        # If condition is met, add the argument to matched list\n        if condition_met:\n            matched_args.append(option_name)\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        option_name = option_spec['name']\n        required_when = option_spec.get('required_when')\n        \n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        condition_met = False\n        for condition in required_when:\n            # Extract the condition value from args\n            condition_value = args.get(command_name, {}).get(condition, None)\n            \n            # Check if the condition matches the expected value\n            if condition_value is not None:\n                condition_met = True\n                break\n                \n        # If condition is met, add the argument to matched list\n        if condition_met:\n            matched_args.append(option_name)\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        option_name = option_spec['name']\n        required_when = option_spec.get('required_when')\n        \n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        condition_met = False\n        for condition in required_when:\n            # Extract the condition value from args\n            condition_value = args.get(command_name, {}).get(condition, None)\n            \n            # Check if the condition matches the expected value\n            if condition_value is not None:\n                # For simple string comparison\n                if condition_value == condition:\n                    condition_met = True\n                    break\n                    \n        # If condition is met, add the argument to matched list\n        if condition_met:\n            matched_args.append(option_name)\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        option_name = option_spec['name']\n        required_when = option_spec.get('required_when')\n        \n        if not required_when:\n            continue\n            \n        # Check if the required_when condition is met\n        condition_met = False\n        condition_args = required_when.split(',')\n        \n        # Evaluate the condition based on the provided args\n        for condition_arg in condition_args:\n            condition_arg = condition_arg.strip()\n            if condition_arg.startswith('!'):\n                # Negated condition\n                negated_condition = condition_arg[1:]\n                if negated_condition in args.get(command_name, {}):\n                    condition_met = False\n                    break\n                else:\n                    condition_met = True\n            else:\n                # Positive condition\n                if condition_arg in args.get(command_name, {}):\n                    condition_met = True\n                    break\n        \n        # If condition is met, add the argument to the matched list\n        if condition_met:\n            matched_args.append(option_name)\n            \n    return matched_args", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                     args):\n    \"\"\"\n    List arguments with ``required_when`` condition matched.\n\n    :param command_name: the command name.\n    :param options_spec: the list of command spec options.\n    :param args: the received input arguments\n    :return: list, list of argument names with matched ``required_when``\n        condition\n    \"\"\"\n    matched_args = []\n    for option_spec in options_spec:\n        option_name = option_spec['name']\n        required_when = option_spec.get('required_when')\n        \n        if not required_when:\n            continue\n            \n        # Check if required_when condition is met\n        condition_met = False\n        for condition in required_when:\n            # Extract the condition value from args\n            condition_value = args.get(command_name, {}).get(condition, None)\n            \n            # Evaluate the condition\n            if condition_value is not None:\n                condition_met = True\n                break\n                \n        # If condition is met, add the argument to matched list\n        if condition_met:\n            matched_args.append(option_name)\n            \n    return matched_args"]}
{"_id": "6306092e73426c38ae68ad09", "name": "validate_length_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 91657, "num_retrieved": 298, "inference_time": 13.900726526975632, "generate_results": ["def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str) and len(arg_value) > 1000:  # Example length check\n            raise IRInvalidLengthException([(arg_name, arg_value, 1000)])\n    return True", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str) and len(arg_value) > 1000:  # Assuming max length is 1000 for demonstration\n            raise IRInvalidLengthException([(arg_name, arg_value, 1000)])", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str) and len(arg_value) > 1000:  # Assuming 1000 is a reasonable max length\n            raise IRInvalidLengthException([\n                (arg_name, arg_value, 1000)\n            ])", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str) and len(arg_value) > 1000:  # Assuming max length is 1000 for demonstration\n            raise IRInvalidLengthException([(arg_name, arg_value, 1000)])", "def validate_length_args(self, args):\n    \"\"\"\n    Check if value of arguments is not longer than length specified.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, str) and len(arg_value) > 1000:  # Assuming max length is 1000 for safety\n            raise IRInvalidLengthException([\n                (arg_name, arg_value, 1000)\n            ])"]}
{"_id": "6306092e73426c38ae68ad0a", "name": "validate_choices_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 91212, "num_retrieved": 294, "inference_time": 13.900726526975632, "generate_results": ["def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            # Get the option specification for this argument\n            option_spec = self.get_option_spec(command_name, arg_name)\n            if not option_spec:\n                continue\n                \n            # Check if the argument has a 'choices' field\n            if 'choices' not in option_spec:\n                continue\n                \n            available_choices = option_spec['choices']\n            if arg_value not in available_choices:\n                invalid_options.append((arg_name, arg_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            # Get the option specification for this argument\n            option_spec = self.get_option_spec(command_name, arg_name)\n            if not option_spec:\n                continue\n                \n            # Check if the argument has a 'choices' field\n            if 'choices' not in option_spec:\n                continue\n                \n            available_choices = option_spec['choices']\n            if arg_value not in available_choices:\n                invalid_options.append((arg_name, arg_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            # Get the option specification for this argument\n            option_spec = self.get_option_spec(command_name, arg_name)\n            if option_spec and 'choices' in option_spec:\n                available_choices = option_spec['choices']\n                if arg_value not in available_choices:\n                    invalid_options.append((command_name, arg_name, arg_value, available_choices))\n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            # Get the option specification for this argument\n            option_spec = self.get_option_spec(command_name, arg_name)\n            if not option_spec:\n                continue\n                \n            # Check if the argument has a 'choices' field\n            if 'choices' not in option_spec:\n                continue\n                \n            available_choices = option_spec['choices']\n            if arg_value not in available_choices:\n                invalid_options.append((arg_name, arg_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)", "def validate_choices_args(self, args):\n    \"\"\"\n    Check if value of choice arguments is one of the available choices.\n\n    :param args: The received arguments.\n    \"\"\"\n    invalid_options = []\n    for command_name, command_args in args.items():\n        for arg_name, arg_value in command_args.items():\n            # Get the option specification for this argument\n            option_spec = self.get_option_spec(command_name, arg_name)\n            if not option_spec:\n                continue\n                \n            # Check if the argument has a 'choices' field\n            if 'choices' not in option_spec:\n                continue\n                \n            available_choices = option_spec['choices']\n            if arg_value not in available_choices:\n                invalid_options.append((arg_name, arg_value, available_choices))\n    \n    if invalid_options:\n        raise IRInvalidChoiceException(invalid_options)"]}
{"_id": "6306092e73426c38ae68ad0b", "name": "validate_min_max_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 91281, "num_retrieved": 295, "inference_time": 13.900726526975632, "generate_results": ["def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, (int, float)):\n            min_val = self.get_option_spec(arg_name, 'min')\n            max_val = self.get_option_spec(arg_name, 'max')\n            \n            if min_val is not None and max_val is not None:\n                if not (min_val <= arg_value <= max_val):\n                    raise UtilError(f\"Argument '{arg_name}' value {arg_value} is not within the range [{min_val}, {max_val}]\")\n            elif min_val is not None:\n                if arg_value < min_val:\n                    raise UtilError(f\"Argument '{arg_name}' value {arg_value} is less than minimum value {min_val}\")\n            elif max_val is not None:\n                if arg_value > max_val:\n                    raise UtilError(f\"Argument '{arg_name}' value {arg_value} is greater than maximum value {max_val}\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if not isinstance(arg_value, (int, float)):\n            continue\n            \n        min_val = self._get_min_value(arg_name)\n        max_val = self._get_max_value(arg_name)\n        \n        if min_val is not None and max_val is not None:\n            if not (min_val <= arg_value <= max_val):\n                raise UtilError(f\"Argument '{arg_name}' has value {arg_value} which is not between {min_val} and {max_val}\")\n        elif min_val is not None:\n            if arg_value < min_val:\n                raise UtilError(f\"Argument '{arg_name}' has value {arg_value} which is less than minimum value {min_val}\")\n        elif max_val is not None:\n            if arg_value > max_val:\n                raise UtilError(f\"Argument '{arg_name}' has value {arg_value} which is greater than maximum value {max_val}\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, (int, float)):\n            min_val = self._get_min_value(arg_name)\n            max_val = self._get_max_value(arg_name)\n            if min_val is not None and max_val is not None:\n                if not (min_val <= arg_value <= max_val):\n                    raise UtilError(f\"Argument '{arg_name}' has value {arg_value} \"\n                                   f\"which is not between minimum ({min_val}) and \"\n                                   f\"maximum ({max_val}) values\")\n    return True", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if isinstance(arg_value, (int, float)):\n            min_val = self.get_min_value(arg_name)\n            max_val = self.get_max_value(arg_name)\n            if min_val is not None and max_val is not None:\n                if not (min_val <= arg_value <= max_val):\n                    raise UtilError(f\"Argument '{arg_name}' has value {arg_value} \"\n                                   f\"which is not between minimum {min_val} and \"\n                                   f\"maximum {max_val}\")", "def validate_min_max_args(self, args):\n    \"\"\"\n    Check if value of arguments is between minimum and maximum values.\n\n    :param args: The received arguments.\n    \"\"\"\n    for arg_name, arg_value in args.items():\n        if not isinstance(arg_value, (int, float)):\n            continue\n            \n        min_val = self._get_min_value(arg_name)\n        max_val = self._get_max_value(arg_name)\n        \n        if min_val is not None and max_val is not None:\n            if not (min_val <= arg_value <= max_val):\n                raise UtilError(f\"Argument '{arg_name}' has value {arg_value} which is not between min {min_val} and max {max_val}\")\n        elif min_val is not None:\n            if arg_value < min_val:\n                raise UtilError(f\"Argument '{arg_name}' has value {arg_value} which is less than minimum {min_val}\")\n        elif max_val is not None:\n            if arg_value > max_val:\n                raise UtilError(f\"Argument '{arg_name}' has value {arg_value} which is greater than maximum {max_val}\")"]}
{"_id": "6306092e73426c38ae68ad0d", "name": "create_complex_argumet_type", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 92770, "num_retrieved": 314, "inference_time": 13.900726526975632, "generate_results": ["def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    settings_dirs = self.get_settings_dirs(subcommand)\n    return globals()[type_name](option_name, settings_dirs, subcommand, spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    settings_dirs = self.get_settings_dirs(subcommand)\n    return globals()[type_name](option_name, settings_dirs, subcommand, spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    settings_dirs = self.get_settings_dirs(subcommand)\n    return globals()[type_name](option_name, settings_dirs, subcommand, spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    settings_dirs = self.get_settings_dirs(subcommand)\n    return globals()[type_name](option_name, settings_dirs, subcommand, spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name, spec_option):\n    \"\"\"\n    Build the complex argument type\n\n    :param subcommand: the command name\n    :param type_name: the complex type name\n    :param option_name: the option name\n    :param spec_option: option's specifications\n    :return: the complex type instance\n    \"\"\"\n    settings_dirs = self.settings_dirs\n    return globals()[type_name](arg_name=option_name,\n                               settings_dirs=settings_dirs,\n                               sub_command_name=subcommand,\n                               spec_option=spec_option)"]}
{"_id": "6306092e73426c38ae68ad0f", "name": "get_nested_custom_and_control_args", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 96389, "num_retrieved": 347, "inference_time": 13.900726526975632, "generate_results": ["def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process all arguments to separate control and nested\n    for arg_key, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These typically include: dry-run, generate-answers-file, verbose, etc.\n        if arg_key in ['dry-run', 'generate-answers-file', 'verbose', 'debug', 'output']:\n            control_args[arg_key] = arg_value\n        else:\n            # All other arguments are considered nested or custom\n            # For now, we'll treat them as nested arguments\n            # Custom arguments would be handled by specific type handlers\n            nested_args[arg_key] = arg_value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process all arguments to separate control and nested\n    for arg_name, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These are typically flags or options that control IR behavior\n        # such as debug, dry-run, verbose, etc.\n        if arg_name in ['debug', 'dry-run', 'verbose', 'output', 'extra-vars', 'ansible-args']:\n            control_args[arg_name] = arg_value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by Ansible playbooks and stored in the spec yml\n            nested_args[arg_name] = arg_value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process arguments to separate control, nested, and custom\n    for arg_name, arg_value in args.items():\n        # Check if argument is a control argument (common IR behavior controls)\n        if arg_name in ['debug', 'dry-run', 'output', 'verbose', 'generate-answers-file', 'no-log-commands']:\n            control_args[arg_name] = arg_value\n        # Check if argument is a nested argument (used by Ansible playbooks)\n        elif arg_name.startswith('ansible-'):\n            # Handle ansible-args specifically\n            if arg_name == 'ansible-args':\n                # Parse the additional args string\n                if arg_value:\n                    additional_args = arg_value.split(';')\n                    for additional_arg in additional_args:\n                        if '=' in additional_arg:\n                            key, value = additional_arg.split('=', 1)\n                            nested_args[key] = value\n                        else:\n                            nested_args[additional_arg] = None\n            else:\n                # Handle other ansible arguments\n                nested_args[arg_name] = arg_value\n        else:\n            # Default to nested arguments\n            nested_args[arg_name] = arg_value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process all arguments to separate control and nested\n    for arg_key, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These typically include: debug, dry-run, output, verbose, etc.\n        if arg_key in ['debug', 'dry-run', 'output', 'verbose', 'extra-vars', 'ansible-args']:\n            control_args[arg_key] = arg_value\n        else:\n            # All other arguments are considered nested arguments\n            # These will be used by Ansible playbooks and stored in the spec\n            nested_args[arg_key] = arg_value\n    \n    return control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n    \"\"\"\n    Split input arguments to control nested and custom.\n\n    Controls arguments: control the IR behavior. These arguments\n        will not be put into the spec yml file\n    Nested arguments: are used by the Ansible playbooks and will be put\n        into the spec yml file.\n    Custom arguments: Custom ansible variables to be used instead of the\n        normal nested usage.\n\n    :param args: the collected list of args.\n    :return: (dict, dict): flat dicts (control_args, nested_args)\n    \"\"\"\n    control_args = {}\n    nested_args = {}\n    \n    # Process all arguments to separate control and nested\n    for arg_key, arg_value in args.items():\n        # Check if the argument is a control argument (common IR behavior)\n        # These typically start with 'debug', 'verbose', 'dry-run', 'output', etc.\n        if arg_key in ['debug', 'verbose', 'dry-run', 'output', 'generate-answers-file']:\n            control_args[arg_key] = arg_value\n        else:\n            # All other arguments are considered nested or custom\n            # For nested arguments, they are passed directly to the playbook\n            # For custom arguments, they are treated as custom ansible variables\n            # We'll assume that any argument not in the control list is nested\n            # and will be passed to the playbook\n            nested_args[arg_key] = arg_value\n    \n    return control_args, nested_args"]}
{"_id": "6306092e73426c38ae68ad11", "name": "merge_extra_vars", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 94443, "num_retrieved": 285, "inference_time": 13.900726526975632, "generate_results": ["@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            # If extra_var is a dictionary, merge it directly\n            vars_dict.update(extra_var)\n        else:\n            # If extra_var is a string, parse it as key=value\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If split fails, skip invalid format\n                pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            # If extra_var is a dictionary, merge it directly\n            vars_dict.update(extra_var)\n        else:\n            # If extra_var is a string, parse it as key=value\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If split fails, skip invalid format\n                pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            # If extra_var is a dictionary, merge it directly\n            vars_dict.update(extra_var)\n        else:\n            # If extra_var is a string, parse it as key=value\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If split fails, skip invalid format\n                pass\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if ':' in extra_var:\n            key, value = extra_var.split(':', 1)\n            vars_dict[key] = value\n        else:\n            # Handle case where extra_var is just a key without value\n            # This could be a placeholder for a default value or a missing value\n            # In the absence of a value, we'll just add the key to the dict\n            vars_dict[extra_var] = None\n    \n    return vars_dict", "@staticmethod\ndef merge_extra_vars(vars_dict, extra_vars=None):\n    \"\"\"\n    Extend ``vars_dict`` with ``extra-vars``\n\n    :param vars_dict: Dictionary to merge extra-vars into\n    :param extra_vars: List of extra-vars\n    \"\"\"\n    if extra_vars is None:\n        return vars_dict\n    \n    for extra_var in extra_vars:\n        if isinstance(extra_var, dict):\n            # If extra_var is a dictionary, merge it directly\n            vars_dict.update(extra_var)\n        else:\n            # If extra_var is a string, parse it as key=value\n            try:\n                key, value = extra_var.split('=', 1)\n                vars_dict[key] = value\n            except ValueError:\n                # If format is invalid, skip it\n                pass\n    \n    return vars_dict"]}
{"_id": "6306092f73426c38ae68ad13", "name": "ansible_playbook", "project": "redhat-openstack/infrared", "level": "file_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 104356, "num_retrieved": 360, "inference_time": 13.900726526975632, "generate_results": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible command line arguments\n    ansible_args_list = []\n    \n    # Add verbose argument if provided\n    if verbose is not None:\n        ansible_args_list.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        extra_vars_list = []\n        for key, value in extra_vars.items():\n            extra_vars_list.append(f\"--extra-vars='{key}={value}'\")\n        ansible_args_list.extend(extra_vars_list)\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg_name, arg_value in ansible_args.items():\n            if isinstance(arg_value, list):\n                for value in arg_value:\n                    ansible_args_list.append(f\"--{arg_name}={value}\")\n            else:\n                ansible_args_list.append(f\"--{arg_name}={arg_value}\")\n    \n    # Construct the ansible-playbook command\n    ansible_command = [\n        'ansible-playbook',\n        playbook_path\n    ]\n    \n    # Add all the additional arguments\n    ansible_command.extend(ansible_args_list)\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_command,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        \n        # Return the exit code\n        return result.returncode\n        \n    except subprocess.CalledProcessError as e:\n        # Return the exit code from the failed command\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible command line arguments\n    ansible_args_list = []\n    \n    # Add verbose argument if provided\n    if verbose is not None:\n        ansible_args_list.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        extra_vars_list = []\n        for key, value in extra_vars.items():\n            if isinstance(value, dict):\n                # Handle nested dictionaries\n                extra_vars_list.extend(['--extra-vars', f'{key}={value}'])\n            else:\n                extra_vars_list.extend(['--extra-vars', f'{key}={value}'])\n        ansible_args_list.extend(extra_vars_list)\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg_name, arg_value in ansible_args.items():\n            if isinstance(arg_value, list):\n                ansible_args_list.extend([f\"--{arg_name}\", *arg_value])\n            else:\n                ansible_args_list.extend([f\"--{arg_name}\", str(arg_value)])\n    \n    # Construct the ansible-playbook command\n    ansible_command = ['ansible-playbook', playbook_path]\n    ansible_command.extend(ansible_args_list)\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_command,\n            cwd=ir_workspace.path,\n            env=dict(os.environ, ANSIBLE_ROLES_PATH=ir_plugin.roles_path),\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    ansible_cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbose argument if provided\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', ','.join([f'{k}={v}' for k, v in extra_vars.items()])])\n    \n    # Add additional arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            ansible_cmd.extend(['--' + arg, value])\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Prepare the ansible command line arguments\n    ansible_args_list = []\n    \n    # Add verbose flag if specified\n    if verbose is not None:\n        ansible_args_list.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        extra_vars_list = []\n        for key, value in extra_vars.items():\n            extra_vars_list.append(f\"--extra-vars='{key}={value}'\")\n        ansible_args_list.extend(extra_vars_list)\n    \n    # Add custom ansible arguments if provided\n    if ansible_args:\n        for arg_name, arg_value in ansible_args.items():\n            if isinstance(arg_value, list):\n                ansible_args_list.extend([f\"--{arg_name}={val}\" for val in arg_value])\n            else:\n                ansible_args_list.append(f\"--{arg_name}={arg_value}\")\n    \n    # Construct the ansible-playbook command\n    ansible_command = [\n        'ansible-playbook',\n        playbook_path\n    ]\n    \n    # Add all the additional arguments\n    ansible_command.extend(ansible_args_list)\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_command,\n            cwd=ir_workspace.path,\n            capture_output=True,\n            text=True,\n            check=False\n        )\n        \n        # Return the exit code\n        return result.returncode\n        \n    except subprocess.CalledProcessError as e:\n        # Return the exit code from the failed command\n        return e.returncode", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"\n    Wraps the 'ansible-playbook' CLI.\n\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :param playbook_path: the playbook to invoke\n    :param verbose: Ansible verbosity level\n    :param extra_vars: dict. Passed to Ansible as extra-vars\n    :param ansible_args: dict of ansible-playbook arguments to plumb down\n        directly to Ansible.\n    \"\"\"\n    # Construct the ansible-playbook command\n    ansible_cmd = ['ansible-playbook', playbook_path]\n    \n    # Add verbose flag if specified\n    if verbose is not None:\n        ansible_cmd.extend(['-v'] * verbose)\n    \n    # Add extra-vars if provided\n    if extra_vars:\n        ansible_cmd.extend(['-e', ','.join([f'{k}={v}' for k, v in extra_vars.items()])])\n    \n    # Add additional arguments if provided\n    if ansible_args:\n        for arg, value in ansible_args.items():\n            ansible_cmd.extend(['--' + arg, str(value)])\n    \n    # Execute the ansible-playbook command\n    try:\n        result = subprocess.run(\n            ansible_cmd,\n            cwd=ir_workspace.path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            check=False\n        )\n        return result.returncode\n    except subprocess.CalledProcessError as e:\n        return e.returncode"]}
{"_id": "6306093273426c38ae68ad15", "name": "_run_playbook", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 105546, "num_retrieved": 371, "inference_time": 13.98416292667389, "generate_results": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Prepare the playbook path and inventory\n    playbook_path = ir_plugin.playbook\n    inventory_path = ir_workspace.inventory\n    \n    # Prepare the ansible arguments from cli_args\n    ansible_args = []\n    if cli_args:\n        # Convert cli_args to a list of arguments that ansible can understand\n        # This is a simplified version - in practice, you would parse the arguments\n        # and convert them to the appropriate format\n        ansible_args = [\"--extra-vars\", yaml.safe_dump(vars_dict)]\n        \n        # Add any additional arguments from cli_args\n        for arg in cli_args:\n            if arg.startswith(\"--\"):\n                ansible_args.append(arg)\n    \n    # Execute the ansible playbook\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract playbook path from plugin\n    playbook_path = ir_plugin.playbook\n    \n    # Prepare ansible arguments from cli_args\n    ansible_args = []\n    if cli_args:\n        # Convert cli_args to a list of arguments\n        ansible_args = [\"--additional-args\", \" \".join(cli_args)]\n    \n    # Execute ansible playbook with the provided variables and arguments\n    return execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=None,\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n    workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract playbook path from the plugin\n    playbook_path = ir_plugin.playbook\n    \n    # Prepare the ansible arguments from cli_args\n    ansible_args = []\n    if cli_args:\n        # Convert cli_args to a list of arguments that ansible can understand\n        # This is a simplified version - in practice, this would parse the arguments\n        # and convert them to the format ansible expects\n        ansible_args = ['--extra-vars', yaml.safe_dump(vars_dict)]\n        \n        # Add any additional arguments from cli_args\n        # This would typically involve parsing the arguments and converting them\n        # to the appropriate format for ansible\n        for arg in cli_args:\n            if arg.startswith('--'):\n                ansible_args.append(arg)\n    \n    # Execute the ansible playbook with the provided variables and arguments\n    # This is a simplified version - in practice, this would use the execute module\n    # to run the ansible playbook with the specified parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract playbook path from plugin\n    playbook_path = ir_plugin.playbook\n    \n    # Prepare extra variables for Ansible\n    extra_vars = vars_dict\n    \n    # Prepare ansible arguments from cli_args\n    ansible_args = []\n    for arg in cli_args:\n        if arg.startswith(\"--\"):\n            ansible_args.append(arg)\n    \n    # Execute Ansible playbook with the provided variables and arguments\n    return execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,\n        extra_vars=extra_vars,\n        ansible_args=ansible_args\n    )", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"\n    Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n                         workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n    # Extract playbook path from plugin\n    playbook_path = ir_plugin.playbook\n    \n    # Prepare ansible arguments from cli_args\n    ansible_args = []\n    if cli_args:\n        # Convert cli_args to a list of arguments that ansible can understand\n        # This is a simplified version - in practice, this would parse the arguments\n        # and convert them to the appropriate format for ansible\n        ansible_args = [\"--extra-vars\", yaml.safe_dump(vars_dict)]\n        \n        # Add any additional arguments from cli_args\n        # This would typically involve parsing the arguments and converting them\n        # to the appropriate format for ansible\n        for arg in cli_args:\n            if arg.startswith(\"--\"):\n                ansible_args.append(arg)\n    \n    # Execute the ansible playbook\n    # This would typically involve calling the execute.ansible_playbook function\n    # with the appropriate parameters\n    result = execute.ansible_playbook(\n        ir_workspace=ir_workspace,\n        ir_plugin=ir_plugin,\n        playbook_path=playbook_path,\n        verbose=False,\n        extra_vars=vars_dict,\n        ansible_args=ansible_args\n    )\n    \n    return result"]}
{"_id": "63060ada73426c38ae68ad31", "name": "_convert_non_cli_args", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 92205, "num_retrieved": 311, "inference_time": 13.98416292667389, "generate_results": ["def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # No implementation provided in the context\n    # This function is likely to be implemented based on specific type\n    # conversion rules defined in the plugin specifications\n    # For now, return the original values_dict as no conversion rules are defined\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the plugin specification\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the plugin specification\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # Default implementation - no type conversion for now\n    # In a real implementation, this would parse the spec and convert values\n    # based on the expected types defined in the plugin specification\n    return values_dict", "def _convert_non_cli_args(self, parser_name, values_dict):\n    \"\"\"\n    Casts arguments to correct types by modifying values_dict param.\n\n    By default all the values are strings.\n\n    :param parser_name: The command name, e.g. main, virsh, ospd, etc\n    :param values_dict: The dict of with arguments\n    \"\"\"\n    # No implementation provided in the context\n    # This function is likely to be implemented based on specific type\n    # conversion rules defined in the plugin specifications or configuration\n    # For now, return the original values_dict as no conversion rules are defined\n    return values_dict"]}
{"_id": "63060b1a73426c38ae68ad3e", "name": "get_plugin_spec_flatten_dict", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 106267, "num_retrieved": 372, "inference_time": 13.98416292667389, "generate_results": ["def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin = InfraredPlugin(plugin_dir)\n    spec_file_path = os.path.join(plugin_dir, InfraredPlugin.PLUGIN_SPEC_FILE)\n    \n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    # Flatten the spec dictionary\n    flattened_dict = {}\n    \n    # Handle config section if present\n    if 'config' in spec_content:\n        config = spec_content['config']\n        for key, value in config.items():\n            flattened_dict[f'config.{key}'] = value\n    \n    # Handle subparsers section if present\n    if 'subparsers' in spec_content:\n        subparsers = spec_content['subparsers']\n        for key, value in subparsers.items():\n            flattened_dict[f'subparsers.{key}'] = value\n    \n    # Handle description if present\n    if 'description' in spec_content:\n        flattened_dict['description'] = spec_content['description']\n    \n    # Handle plugin_type if present\n    if 'plugin_type' in spec_content:\n        flattened_dict['plugin_type'] = spec_content['plugin_type']\n    \n    # Handle entry_point if present\n    if 'entry_point' in spec_content:\n        flattened_dict['entry_point'] = spec_content['entry_point']\n    \n    # Handle roles_path if present\n    if 'roles_path' in spec_content:\n        flattened_dict['roles_path'] = spec_content['roles_path']\n    \n    return flattened_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin = InfraredPlugin(plugin_dir)\n    spec_path = os.path.join(plugin_dir, InfraredPlugin.PLUGIN_SPEC_FILE)\n    \n    if not os.path.exists(spec_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {spec_path}\")\n    \n    with open(spec_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    if not isinstance(spec_content, dict):\n        raise ValueError(f\"Invalid plugin spec file content: {spec_content}\")\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    \n    # Handle config section\n    if 'config' in spec_content:\n        config = spec_content['config']\n        for key, value in config.items():\n            flat_dict[f\"config.{key}\"] = value\n    \n    # Handle subparsers section\n    if 'subparsers' in spec_content:\n        subparsers = spec_content['subparsers']\n        for key, value in subparsers.items():\n            flat_dict[f\"subparsers.{key}\"] = value\n    \n    # Handle description\n    if 'description' in spec_content:\n        flat_dict['description'] = spec_content['description']\n    \n    # Handle plugin_type\n    plugin_type = spec_content.get('plugin_type', spec_content.get('config', {}).get('plugin_type'))\n    if plugin_type:\n        flat_dict['plugin_type'] = plugin_type\n    \n    # Handle entry_point\n    entry_point = spec_content.get('entry_point', spec_content.get('config', {}).get('entry_point'))\n    if entry_point:\n        flat_dict['entry_point'] = entry_point\n    \n    # Handle roles_path\n    roles_path = spec_content.get('roles_path', spec_content.get('config', {}).get('roles_path'))\n    if roles_path:\n        flat_dict['roles_path'] = roles_path\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin = InfraredPlugin(plugin_dir)\n    spec_path = os.path.join(plugin_dir, InfraredPlugin.PLUGIN_SPEC_FILE)\n    \n    if not os.path.exists(spec_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {spec_path}\")\n    \n    with open(spec_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    if not isinstance(spec_content, dict):\n        raise ValueError(f\"Invalid plugin spec file format: {spec_content}\")\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    \n    # Handle config section\n    if 'config' in spec_content:\n        config = spec_content['config']\n        for key, value in config.items():\n            flat_dict[f\"config.{key}\"] = value\n    \n    # Handle subparsers section\n    if 'subparsers' in spec_content:\n        subparsers = spec_content['subparsers']\n        for key, value in subparsers.items():\n            flat_dict[f\"subparsers.{key}\"] = value\n    \n    # Handle description\n    if 'description' in spec_content:\n        flat_dict['description'] = spec_content['description']\n    \n    # Handle plugin_type\n    if 'plugin_type' in spec_content:\n        flat_dict['plugin_type'] = spec_content['plugin_type']\n    \n    # Handle entry_point\n    if 'entry_point' in spec_content:\n        flat_dict['entry_point'] = spec_content['entry_point']\n    \n    # Handle roles_path\n    if 'roles_path' in spec_content:\n        flat_dict['roles_path'] = spec_content['roles_path']\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin = InfraredPlugin(plugin_dir)\n    spec_file_path = os.path.join(plugin_dir, InfraredPlugin.PLUGIN_SPEC_FILE)\n    \n    if not os.path.exists(spec_file_path):\n        raise FileNotFoundError(f\"Plugin spec file not found at {spec_file_path}\")\n    \n    with open(spec_file_path, 'r') as spec_file:\n        spec_content = yaml.safe_load(spec_file)\n    \n    if not spec_content:\n        return {}\n    \n    # Flatten the spec dictionary\n    flat_dict = {}\n    \n    # Handle config section\n    if 'config' in spec_content:\n        config = spec_content['config']\n        for key, value in config.items():\n            flat_dict[f\"config.{key}\"] = value\n    \n    # Handle subparsers section\n    if 'subparsers' in spec_content:\n        subparsers = spec_content['subparsers']\n        for key, value in subparsers.items():\n            flat_dict[f\"subparsers.{key}\"] = value\n    \n    # Handle description\n    if 'description' in spec_content:\n        flat_dict['description'] = spec_content['description']\n    \n    # Handle plugin_type\n    if 'plugin_type' in spec_content:\n        flat_dict['plugin_type'] = spec_content['plugin_type']\n    \n    # Handle entry_point\n    if 'entry_point' in spec_content:\n        flat_dict['entry_point'] = spec_content['entry_point']\n    \n    # Handle roles_path\n    if 'roles_path' in spec_content:\n        flat_dict['roles_path'] = spec_content['roles_path']\n    \n    return flat_dict", "def get_plugin_spec_flatten_dict(plugin_dir):\n    \"\"\"\n    Creates a flat dict from the plugin spec\n\n    :param plugin_dir: A path to the plugin's dir\n    :return: A flatten dictionary contains the plugin's properties\n    \"\"\"\n    plugin_path = os.path.abspath(os.path.expanduser(plugin_dir))\n    if not os.path.isdir(plugin_path):\n        raise IRFailedToAddPlugin(\"Path to plugin dir '{}' doesn't exist\".format(plugin_dir))\n    \n    spec_path = os.path.join(plugin_path, 'plugin.spec')\n    if not os.path.isfile(spec_path):\n        raise IRFailedToAddPlugin(\"Plugin spec file not found at {}\".format(spec_path))\n    \n    with open(spec_path, 'r') as spec_file:\n        spec_content = spec_file.read()\n    \n    spec_dict = yaml.safe_load(spec_content)\n    \n    if not isinstance(spec_dict, dict):\n        raise IRFailedToAddPlugin(\"Spec file is empty or corrupted: {}\".format(spec_content))\n    \n    # Flatten the dictionary by removing nested structures\n    flat_dict = {}\n    \n    # Handle config section if present\n    if 'config' in spec_dict:\n        config = spec_dict['config']\n        for key, value in config.items():\n            flat_dict['config.{}'.format(key)] = value\n    \n    # Handle subparsers section if present\n    if 'subparsers' in spec_dict:\n        subparsers = spec_dict['subparsers']\n        for key, value in subparsers.items():\n            flat_dict['subparsers.{}'.format(key)] = value\n    \n    # Handle description if present\n    if 'description' in spec_dict:\n        flat_dict['description'] = spec_dict['description']\n    \n    # Handle plugin_type if present\n    if 'plugin_type' in spec_dict:\n        flat_dict['plugin_type'] = spec_dict['plugin_type']\n    \n    # Handle entry_point if present\n    if 'entry_point' in spec_dict:\n        flat_dict['entry_point'] = spec_dict['entry_point']\n    \n    # Handle roles_path if present\n    if 'roles_path' in spec_dict:\n        flat_dict['roles_path'] = spec_dict['roles_path']\n    \n    return flat_dict"]}
{"_id": "63060b1b73426c38ae68ad42", "name": "inject_config", "project": "redhat-openstack/infrared", "level": "class_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 103033, "num_retrieved": 340, "inference_time": 13.98416292667389, "generate_results": ["def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = os.path.join(self.infrared_home, 'ansible.cfg')", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = os.path.join(self.infrared_home, 'ansible.cfg')", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = os.path.join(self.infrared_home, 'ansible.cfg')", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = os.path.join(self.infrared_home, 'ansible.cfg')", "def inject_config(self):\n    \"\"\"\n    Set the environment variable for config path, if it is undefined.\n    \"\"\"\n    if not os.environ.get('ANSIBLE_CONFIG'):\n        os.environ['ANSIBLE_CONFIG'] = os.path.join(self.infrared_home, 'ansible.cfg')"]}
{"_id": "63060b1b73426c38ae68ad43", "name": "extend_cli", "project": "redhat-openstack/infrared", "level": "project_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 86437, "num_retrieved": 272, "inference_time": 13.98416292667389, "generate_results": ["def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would involve parsing the spec and adding\n    # the appropriate CLI options to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would involve parsing the spec dictionary\n    # and adding the appropriate CLI options to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would involve parsing the spec and adding\n    # appropriate CLI arguments to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would involve parsing the spec and adding\n    # the appropriate CLI options to the subparser\n    pass", "def extend_cli(self, root_subparsers):\n    \"\"\"Adds the spec cli options to to the main entry point.\n\n    :param subparser: the subparser object to extend.\n    \"\"\"\n    # This is a placeholder implementation\n    # The actual implementation would involve parsing the spec dictionary\n    # and adding the appropriate CLI options to the subparser\n    pass"]}
{"_id": "62ece4982e6aefcf4aabbd64", "name": "strip_root", "project": "zimeon/ocfl-py", "level": "slib_runnable", "retrieval_method": "bm25", "context_length": 196608, "actual_tokens": 49368, "num_retrieved": 127, "inference_time": 13.98416292667389, "generate_results": ["def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")", "def strip_root(path, root):\n    \"\"\"\n    Remove root from path, throw exception on failure.\n    \"\"\"\n    if path.startswith(root):\n        return path[len(root):]\n    else:\n        raise ValueError(f\"Path {path} does not start with root {root}\")"]}
